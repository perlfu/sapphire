This section provides some information on various
implementation details for the Jikes\trademark RVM optimizing compiler.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Method Compilation}
\label{sec:optdriver}
\index{compilation}
\index{optimizations}
\index{IR}
\index{HIR}
\index{LIR}
\index{MIR}
The fundamental unit for optimization in Jikes RVM is a single method. 
The optimization of a method consists of a series of 
compiler phases performed on the method. These 
phases transform the  
IR (intermediate representation) from bytecodes through 
HIR (high-level intermediate representation), 
LIR (low-level intermediate representation), and 
MIR (machine intermediate representation) and finally into machine code. 
Various optimizing transformations are performed at each level of IR.

\index{OPT\_CompilationPlan class}
\index{VM\_Method class}
\index{OPT\_OptimizationPlanElement class}
An object of the class 
\xlink{{\tt OPT\_CompilationPlan}}{\OPTCompilationPlanURL} 
contains all the  
information necessary to generate machine code for a method. 
An instance of this class includes, among other fields, 
the 
\xlink{{\tt VM\_Method}}{\VMMethodURL} 
to be compiled and the array of 
\xlink{{\tt OPT\_OptimizationPlanElements}}{\OPTOptimizationPlanElementURL} 
which define the compilation steps.
The {\tt execute} method of an
\xlink{{\tt OPT\_CompilationPlan}}{\OPTCompilationPlanURL} 
invokes the optimizing compiler to generate machine code for the method,
executing the compiler phases as listed in the plan's
{\tt OPT\_OptimizationPlanElement}s.

\index{OPT\_OptimizationPlanner class}
The 
\xlink{{\tt OPT\_OptimizationPlanner}}{\OPTOptimizationPlannerURL} 
class defines the standard phases used in a compilation.
This class
contains a static field, called {\tt masterPlan}, which contains all
possible {\tt OPT\_OptimizationPlanElement}s.
The structure of the master plan is 
a tree. Any element may either be an atomic element (a leaf of the 
tree), or an aggregate element (an internal node of the tree).
The master plan has the following general structure:

\begin{itemize}
\item elements which convert bytecodes to HIR
\item elements which perform optimization transformations on the HIR
   \begin{itemize}
   \item elements which perform optimization transformations using SSA form
   \end{itemize}
\item elements which convert HIR to LIR
\item elements which perform optimization transformations on the LIR
   \begin{itemize}
   \item elements which perform optimization transformations using SSA form
   \end{itemize}
\item elements which convert LIR to MIR
\item elements which perform optimization transformations on MIR 
\item elements which convert MIR to machine code
\end{itemize}


\index{optimization plan}
A client (compiler driver) constructs a specific optimization plan by including all the 
{\tt OPT\_OptimizationPlanElement}s contained in the master plan which are 
appropriate for this compilation instance. 
Whether or not an element should be part of a compilation plan is determined 
by its {\tt shouldPerform} method. For each atomic element, the values in the
{\tt OPT\_Options} object are generally used to determine whether the element
should be included in the compilation plan. Each aggregate element must be 
included when any of its component elements must be included. 

Each element must have a {\tt perform} method defined which takes the IR as
a parameter. It is expected, but not required, that the {\tt perform}
method will modify the IR. 
The perform method of an aggregate element will invoke the 
perform methods of its elements.

\index{OPT\_CompilerPhase class}
Each atomic element is an object of the final class 
{\tt OPT\_OptimizationPlanAtomicElement}. The main work of this class
is performed by its {\em phase}, an object of type 
\xlink{{\tt OPT\_CompilerPhase}}{\OPTCompilerPhaseURL}. The
{\tt OPT\_CompilerPhase} class is not final; each phase overrides this class,
in particular it overrides the {\tt perform} method, which is invoked by its 
enclosing element's {\tt perform} method. All the state associated with 
the element
is contained in the {\tt OPT\_CompilerPhase}; no
state is in the element.

Every optimization plan consists of a selection of elements from the master 
plan;
thus two optimization plans associated with different methods 
will share the same component element objects. 
Clearly, it is undesirable to share state 
associated with a particular compilation phase between two
different method compilations. In order to prevent this, the {\tt perform}
method of an atomic element creates a new instance of its phase immediately 
before calling the phase's {\tt perform} method. 
In the case where the phase
contains no state the {\tt newExecution} method of 
{\tt OPT\_CompilerPhase} can be overridden to return the phase itself rather 
than a clone of the phase~\footnote{Since this coding pattern may result
in unintended memory leaks, overriding {\tt newExecution} may be
disallowed in future releases.}.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{IR Operators}
\index{IR}
\index{instructions}
\index{operators}

The optimizing compiler intermediate representation (IR) includes a list
of instructions.  Each instruction includes an operator and zero or
more operands.

\index{OPT\_Operators class}
\index{OperatorList.dat}
The IR operators are defined by the class {\tt OPT\_Operators}, which in
turn is automatically generated from a template by a driver.  The input to the
driver are two files, both called {\tt OperatorList.dat}.  One input
file resides in {\tt \$RVM\_ROOT/rvm/src/vm/compilers/optimizing/ir/instruction} and defines machine-independent
operators.  The other resides in {\tt \$RVM\_ROOT/rvm/src/vm/arch/\{arch\}/compilers/optimizing/ir/instruction}
and defines machine-dependent operators, where \{arch\} is the
specific architecture of interest, such as PowerPC\PowerPCTMFootnote.

Each operator in {\tt OperatorList.dat} is defined by a five-line record,
consisting of:
\begin{itemize}
\item {\tt SYMBOL}: a static symbol to identify the operator
\item {\tt INSTRUCTION\_FORMAT}: the instruction format class that accepts this operator.  See Section~\ref{iformats} for more information.
\item {\tt TRAITS}: a set of characteristics of the operator, composed with a bit-wise or ($|$) operator.  See {\tt OPT\_Operator.java} for a list of valid traits.
\item {\tt IMPLDEFS}: set of registers implicitly defined by this operator; usually applies only to machine-dependent operators
\item {\tt IMPLUSES}: set of registers implicitly used by this operator; usually applies only to machine-dependent operators
\end{itemize}

For example, the entry in {\tt OperatorList.dat} that defines the integer
addition operator is
\begin{verbatim}
INT_ADD
Binary
none
<blank line>
<blank line>
\end{verbatim}

The operator for a conditional branch based on values of two references is
defined by
\begin{verbatim}
REF_IFCOMP
IntIfCmp
branch | conditional
<blank line>
<blank line>
\end{verbatim}

Additionally,  the machine-specific {\tt OperatorList.dat} file contains 
another line of information for use by the assembler.  See the file
for details. 

\PowerPCTMFooter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Instruction Formats}\label{iformats}
\index{instructions}
\index{instructionFormats.java}

Every IR instruction fits one of the pre-defined {\em Instruction Formats}.
The Java package {\tt instructionFormats} defines roughly 75 architecture-independent
instruction formats.  For each instruction format, the package includes a class
that defines a set of static methods by which optimizing compiler
code can access an instruction of that format.

For example, {\tt INT\_MOVE} instructions conform to the {\tt Move}
instruction format.  The following code fragment shows code that uses the
{\tt OPT\_Operators} interface and the {\tt Move} instruction format:
\begin{verbatim}
import instructionFormats.*;
class X {
  void foo(OPT_Instruction s) {
    if (Move.conforms(s)) {     // if this instruction fits the Move format
      OPT_RegisterOperand r1 = Move.getResult(s);
      OPT_Operand r2 = Move.getVal(s);
      System.out.println("Found a move instruction: " + r1 + " := " + r2);
    } else {
      System.out.println(s + " is not a MOVE");
    }
  }
}
\end{verbatim}

This example shows just a subset of the access functions defined for the
Move format.  Other static access functions can set each operand 
(in this case, {\tt Result} and {\tt Val}), query each operand for
nullness, clear operands, create Move instructions, mutate other
instructions into Move instructions, and check the index of a particular
operand field in the instruction.  See the Javadoc reference for a complete
description of the API.

\index{InstructionFormatList.dat}
Each fixed-length instruction format is defined in the text file 
{\tt \$RVM\_ROOT/rvm/src/vm/compilers/optimizing/ir/instruction/InstructionFormatList.dat}.
Each record in this file has four lines:
\begin{itemize}
\item {\tt NAME}: the name of the instruction format
\item {\tt SIZES}: the number of operands defined, defined and used, and used 
\item {\tt SIG}: a description of each operand, each description given
by
\begin{itemize}
\item {\tt D/DU/U}: Is this operand a def, use, or both?
\item {\tt NAME}: the unique name to identify the operand
\item {\tt TYPE}: the type of the operand (a subclass of {\tt OPT\_Operand}
\item {\tt [opt]}: is this operand optional?
\end{itemize}
\item {\tt VARSIG}: a description of repeating operands, used for
variable-length instructions.
\end{itemize}

So for example, the record that defines the {\tt Move} instruction format
is
\begin{verbatim}
Move
1 0 1
"D Result OPT_RegisterOperand" "U Val OPT_Operand"
<blank line>
\end{verbatim}

This specifies that the {\tt Move} format has two operands, one def and one
use.  The def is called {\tt Result} and must be of
type {\tt OPT\_RegisterOperand}.
The use is called {\tt Val} and must be of type {\tt OPT\_Operand}.

A few instruction formats have variable number of operands.  The
format for these records is given at the top of {\tt InstructionFormatList.dat}.
For example, the record for the variable-length {\tt Call} instruction
format is: 
\begin{verbatim}
Call
1 0 3 1 U 4
"D Result OPT_RegisterOperand" \
"U Address OPT_Operand" "U Method OPT_MethodOperand" "U Guard OPT_Operand opt"
"Param OPT_Operand"
\end{verbatim}
This record defines the {\tt Call} instruction format.  The second line
indicates that this format always has at least 4 operands (1 def and 3 uses),
plus a variable number of uses of one other type.  The trailing
4 on line 2 tells the template generator to generate special constructors
for cases of having 1, 2, 3, or 4 of the extra operands.
Finally, the record names the {\tt Call} instruction operands and
constrains the types.  The final line specifies the name and
types of the variable-numbered operands.  In this case, a {\tt Call}
instruction has a variable number of (use) operands called {\tt Param}.
Client code can access the {\tt i}th parameter operand of a {\tt Call}
instruction {\tt s} by calling {\tt Call.getParam(s,i)}.

A number of instruction formats share operands of 
the same semantic meaning and name.  For convenience in accessing
like instruction formats, the template generator supports four
common operand access types:
\begin{itemize}
\item {\tt ResultCarrier}: provides access to an operand of type {\tt OPT\_RegisterOperand} named {\tt Result}.
\item {\tt GuardResultCarrier}: provides access to an operand of type {\tt OPT\_RegisterOperand} named {\tt GuardResult}.
\item {\tt LocationCarrier}: provides access to an operand of type {\tt OPT\_LocationOperand} named {\tt Location}.
\item {\tt GuardCarrier}: provides access to an operand of type {\tt OPT\_Operand} named {\tt Guard}.
\end{itemize}

For example, for any instruction {\tt s} that carries a {\tt Result} operand
(eg. {\tt Move}, {\tt Binary}, and {\tt Unary} formats), client code can call
{\tt ResultCarrier.conforms(s)} and {\tt ResultCarrier.getResult(s)} to access
the {\tt Result} operand.

Finally, a note on rationale.  Religious object-oriented philosophers
will cringe at the InstructionFormats.  Instead, all this
functionality could be implemented more cleanly with a hierarchy of
instruction types exploiting (multiple) inheritance.  We rejected the
class hierarchy approach due to efficiency concerns of frequent
virtual/interface method dispatch and type checks.  Recent
improvements in our interface invocation sequence and dynamic type
checking algorithms may alleviate some of this concern.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BURS Rules}\label{burs}
\index{BURS}
\index{instruction selection}

The optimizing compiler uses the Bottom-Up Rewrite System (BURS) for
instruction selection.  BURS is essentially a tree pattern matching
system derived from Iburg by David R.\ Hanson.   (See ``Engineering a
Simple, Efficient Code-Generator Generator'' by Fraser, Hanson, and
Proebsting, LOPLAS 1(3), Sept.\ 1992.)
The instruction selection rules for each architecture are specified in an
architecture-specific file called {\tt LIR2MIR.rules}, which resides in
{\tt \$RVM\_ROOT/rvm/src/vm/arch/\{arch\}/compilers/optimizing/ir/conversions/lir2mir}, where \{arch\} is the
specific architecture of interest, such as PowerPC\PowerPCTMFootnote.
The rules are 
used in generating a parser, which transforms the IR.

Each rule in {\tt LIR2MIR.rules} is defined by a four-line record,
consisting of:
\begin{itemize}
\item {\tt PRODUCTION}: the tree pattern to be matched.  The format of each
pattern is explained below.
\item {\tt COST}: the cost of matching the pattern as opposed to skipping
it.  It is a Java\trademark expression that evaluates to an integer.
\item {\tt FLAGS}: specifies whether the rule actually represents a sequence
of instructions (EMIT\_INSTRUCTION) or a transformation of operands
(NOFLAGS). Other flags can be used to control the order of code
generation of a node's children.
\item {\tt TEMPLATE}: Java code to emit
\end{itemize}

Each production has a {\em non-terminal}, which denotes a value, followed
by a colon (``:''), followed by a dependence tree that produces that value.
For example, the rule resulting in memory add on the INTEL architecture is
expressed in the following way:
\begin{verbatim}
stm:    INT_STORE(INT_ADD_ACC(INT_LOAD(r,riv),riv),OTHER_OPERAND(r, riv))
ADDRESS_EQUAL(P(p), PLL(p), 17)
EMIT_INSTRUCTION
EMIT(MIR_BinaryAcc.mutate(P(p), IA32_ADD, MO_S(P(p), DW), \
                          BinaryAcc.getValue(PL(p))));
\end{verbatim}
The production in this rule represents the following tree:
\begin{verbatim}
         r     riv
          \    /
         INT_LOAD  riv
             \     /
           INT_ADD_ACC  r  riv
                    \   |  /
                   INT_STORE
\end{verbatim}
where {\tt r} is a non-terminal that represents a register or a tree
producing a register, {\tt riv} is a non-terminal that represents a register
(or a tree producing one) or an immediate value, and {\tt INT\_LOAD},
{\tt INT\_ADD\_ACC} and {\tt INT\_STORE} are operators ({\em terminals}).
{\tt OTHER\_OPERAND} is just an abstraction to make the tree binary.

There are multiple helper functions that can be used in Java code (both cost
expressions and generation templates).  In all code sequences the name
{\tt p} is reserved for the current tree node.  Some of the helper methods
are shortcuts for accessing properties of tree nodes:
\begin{itemize}
\item {\tt P(p)} is used to access the instruction associated with the
current (root) node,
\item {\tt PL(p)} is used to access the instruction associated with the left
child of the current (root) node (provided it exists),
\item {\tt PR(p)} is used to access the instruction associated with the
right child of the current (root) node (provided it exists),
\item similarly, {\tt PLL(p)}, {\tt PLR(p)}, {\tt PRL(p)} and {\tt PRR(p)}
are used to access the instruction associated with the
left child of the left child, right child of the left child, left child of
the right child and right child of the right child, respectively, of the
current (root) node (provided they exist).
\end{itemize}

What the above rule basically reads is the following:\\
If a tree shown above is seen, evaluate the cost expression (which, in this
case, calls a helper function to test whether the addresses in the
{\tt STORE} ({\tt P(p)}) and the {\tt LOAD} ({\tt PLL(p)}) instructions are
equal.  The function returns 17 if they are, and a special value
{\tt INFINITE} if not), and if the cost is acceptable, emit the {\tt STORE}
instruction ({\tt P(p)}) mutated in place into a machine-dependent
add-accumulate instruction ({\tt IA32\_ADD}) that adds a given value to the
contents of a given memory location.

The rules file is used to generate a file called {\tt ir.brg}, which, in
turn, is used to produce a file called {\tt OPT\_BURS\_STATE.java}.

For more information on helper functions look at
{\tt \$RVM\_ROOT/rvm/src/vm/arch/\{arch\}/compilers/optimizing/ir/conversions/lir2mir/OPT\_BURS\_Helpers.java}.
For more information on the BURS algorithm see
{\tt \$RVM\_ROOT/rvm/src/vm/compilers/optimizing/ir/conversions/lir2mir/OPT\_BURS.java}.

\JavaTMFooter

\PowerPCTMFooter


\subsection{OptTestHarness}\label{opttestharness}

For optimizing compiler development, it is sometimes useful to exercise
careful control over which classes are compiled, and with which
optimization level.  In many cases, a {\tt BaseOptSemiSpace} image will
suit this process.  This configuration invokes the optimizing compiler on
each method run.  Since the optimizing compiler is not in the boot image, 
you can modify its classes without re-linking the boot image.  Instead,
{\tt jbuild -nolink} will recompile the source files you edit, without
invoking the time-consuming boot image writing step.

The {\tt OptTestHarness} program provides even more control over the
optimizing compiler.  This driver program allows you to invoke the
optimizing compiler as an ``application'' running on top of the VM.
The most useful configuration for this is probably {\tt
BaseBaseOTHSemiSpace}; this image provides a BaseBase configuration with just
enough support to invoke the optimizing compiler through {\tt
OptTestHarness}.  Like the {\tt BaseOpt} images, you can use {\tt jbuild
-nolink} to skip a time-consuming boot image writing during development.

To use the {\tt OptTestHarness} program:
\begin{verbatim}
% rvm OptTestHarness -class Foo
\end{verbatim}
will invoke the optimizing compiler on all methods of class {\tt Foo.}

\begin{verbatim}
% rvm OptTestHarness -method Foo bar - 
\end{verbatim}
will invoke the optimizing compiler on the first method {\tt bar} of class
{\tt Foo} it loads.

\begin{verbatim}
% rvm OptTestHarness -method Foo bar (I)V; 
\end{verbatim} 
will invoke the optimizing compiler on method {\tt Foo.bar(I)V;}.

You can specify any number of {\tt -method} and {\tt -class} options on
the command line.  Any arguments passed to {\tt OptTestHarness} via {\tt
-oc} will be passed on directly to the optimizing compiler.  So:

\begin{verbatim}
% rvm OptTestHarness -oc:O1 -oc:print_final_hir=true -method Foo bar -
\end{verbatim} 
will compile {\tt Foo.bar} at optimization level {\tt O1} and print
the final HIR.

One other useful option to {\tt OptTestHarness} is {\tt -longcommandline
<filename>}. With this option, {\tt OptTestHarness} reads the command line
from a file.

The source to the {\tt OptTestHarness} resides in 
{\tt \$RVM\_ROOT/rvm/src/tools/optTestHarness}.
