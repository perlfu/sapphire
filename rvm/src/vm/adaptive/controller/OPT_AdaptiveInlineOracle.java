/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
  * Extend the generic OPT_ProfileDirectedInlineOracle
  * with a few minor hooks back into the adaptive system.
  *
  * @author Stephen Fink
  * @author Dave Grove
  */
final class OPT_AdaptiveInlineOracle extends OPT_ProfileDirectedInlineOracle {

  OPT_AdaptiveInlineOracle(OPT_InlinePlan plan) {
    super(plan);
  }

  /*
   * In an adaptive system, we need to record that the opt compiler
   * didn't want to inline a hot edge to avoid triggering a 
   * recompilation for the sole purpose of attempting to inline said edge.
   */
  protected void recordRefusalToInlineHotEdge(VM_CompiledMethod cm, VM_Method caller, int bcX, VM_Method callee) {
    VM_AdaptiveInlining.recordRefusalToInlineHotEdge(cm.getId(), caller, bcX, callee);
  }
  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Event used by the Adaptive Inlining Organizer
 * to notify the controller that a call arc 
 * originating in a hot method has become hot
 * and therefore recompilation of the method should
 * be considered to enable additional profile-directed inlining.
 *
 * @author Dave Grove 
 * @author Matthew Arnold
 */
public final class VM_AINewHotEdgeEvent extends VM_HotMethodEvent 
  implements VM_ControllerInputEvent {

  /**
   * Estimate of the expected benefit if the method is 
   * recompiled AT THE SAME OPT LEVEL with the newly
   * enabled profile-directed inlining.
   * <p>
   * TODO: Think about reasonable ways to encode the expected 
   * boost factor for recompiling at higher opt levels.
   * In the short run, this is academic, since we only plan to
   * create an instance of this event for methods already compiled
   * at max opt level, but it may be required later.
   * <p>
   * NB: Boost factor is a value >= 1.0!
   * (1.0 means no boost, 1.1 means a 10% improvement, etc).
   */
  private double boostFactor;
  public final double getBoostFactor() { return boostFactor; }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   * @param _boostFactor improvement expected by applying FDO
   */
  VM_AINewHotEdgeEvent(VM_CompiledMethod _cm, double _numSamples, double _boostFactor) {
    super(_cm, _numSamples);
    if (VM.VerifyAssertions) VM.assert(_boostFactor >= 1.0);
    boostFactor = _boostFactor;
  }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   * @param _boostFactor improvement expected by applying FDO
   */
  VM_AINewHotEdgeEvent(VM_CompiledMethod _cm, int _numSamples, double _boostFactor) {
    this(_cm, (double)_numSamples, _boostFactor);
  }


  public final String toString() {
    return "NewHotEdgeEvent: "+super.toString()+
      ", boost factor = "+getBoostFactor();
  }


  /**
   * Called when the controller is ready to process this event.
   * Simply passes itself to the recompilation strategy.
   */
  public void process() {
    VM_CompiledMethod cmpMethod = getCompiledMethod();
    VM_Controller.recompilationStrategy.considerHotCallEdge(cmpMethod,this);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;
import java.io.*;

/**
 *  VM_AdaptiveInlining.java
 *
 *  Collection of static methods to assist with adaptive inlining.
 *
 *  @author Stephen Fink
 *  @modified Michael Hind
 *  @modified Peter F. Sweeney
 *  @modified Matthew Arnold
 *  @modified Dave Grove
 */
class VM_AdaptiveInlining {
  final private static boolean DEBUG = false;

  /** Interface */

  /**
   * Set parameters.
   * Must be called after parsing command-line.
   */
  static void boot(VM_AOSOptions options) {
    setHotEdgeThreshold(options.INITIAL_AI_THRESHOLD);

    // create and register the dcg as a decayable object
    dcg = new VM_PartialCallGraph(); 
    VM_RuntimeMeasurements.registerDecayableObject(dcg);

    plan = new OPT_ContextFreeInlinePlan();
    if (options.USE_OFFLINE_INLINE_PLAN) {
      try {
	//  Read the plan from disk
	String fn = options.OFFLINE_INLINE_PLAN_NAME;
	LineNumberReader in = new LineNumberReader(new FileReader(fn));
	VM_AdaptiveInlining.plan.readObject(in, VM_SystemClassLoader.getVMClassLoader());
      } catch (Exception e) {
	e.printStackTrace();
	throw new OPT_OptimizingCompilerException("Inline",
					        "Error creating offline plan");
      }
      // now make an inlining oracle
      VM_RuntimeCompiler.offlineInlineOracle = 
	new OPT_AdaptiveInlineOracle(plan);
    }
  }

  /** 
   * Get the inlining oracle for a method
   */
  static OPT_AdaptiveInlineOracle getInlineOracle(VM_Method m) {
    OPT_AdaptiveInlineOracle oracle = null;
    synchronized(plan) {
      if (VM_Controller.options.ADAPTIVE_INLINING) {
	oracle = new OPT_AdaptiveInlineOracle(plan);
      }
    }
    return oracle;
  }

  /**
   * Return a pointer to the dynamic call graph
   */
  static VM_PartialCallGraph getPartialCallGraph() { return dcg; }

  /**
   * Recompute the set of "hot" edges used for adaptive inlining.
   */
  private static int recomputeHotEdgesTrips = 0;
  public static Vector recomputeHotEdges() {
    if(DEBUG) {
      VM.sysWrite(" VM_AdaptiveInlining.recomputeHotEdges() hotEdgeThreshold: "
		  + hotEdgeThreshold +" "+(++recomputeHotEdgesTrips)+"\n");
    }
    Vector vectorOfTriples = new Vector();
      
    /** Compute the set of hot edges */
    for (java.util.Iterator i = dcg.getEdges(); i.hasNext(); ) {
      VM_CallSiteTriple triple = (VM_CallSiteTriple)i.next();
      if(DEBUG)VM.sysWrite(" :"+ triple +"\n");
	 
      double weight = triple.getWeight()/nYieldPointsTaken;
      if (weight >= hotEdgeThreshold) { 
	VM_Method caller = triple.getCaller();
	VM_Method callee = triple.getCallee();
	int bcIndex = triple.getBytecodeIndex();
	plan.addRule(caller,bcIndex,callee);
	vectorOfTriples.addElement(triple);
      }
    }

    if(DEBUG){ VM.sysWrite("\nEdges found:\n");
    for (int i=0; i<vectorOfTriples.size(); i++) {
      VM_CallSiteTriple triple = 
	(VM_CallSiteTriple)vectorOfTriples.elementAt(i);
      VM.sysWrite((i+1)+": "+triple.toString()+"\n");
    }  }

    if(DEBUG)VM.sysWrite(" VM_AdaptiveInlining.recomputeHotEdges() exit\n");

    // now adjust the AI Threshold
    double newThreshold = Math.max(hotEdgeThreshold/2.0,
				   VM_Controller.options.FINAL_AI_THRESHOLD);
    setHotEdgeThreshold(newThreshold);

    return vectorOfTriples;
  }


  /**
   * Hook to allow the AdaptiveInliningOracle to record that the opt compiler
   * was aware that a call edge was hot, but still refused to inline it.
   */
  static void recordRefusalToInlineHotEdge(int cmid, 
					   VM_Method caller, 
					   int bcX, 
					   VM_Method callee) {
    VM_CallSiteTriple edge = new VM_CallSiteTriple(caller, bcX, callee);
    Integer key = new Integer(cmid);
    NonInlinedElement oldEdges = (NonInlinedElement)nonInlinedEdges.get(key);
    NonInlinedElement p = oldEdges;
    while (p != null) {
      if (p.cmid == cmid && 
	  p.edge.getCaller() == edge.getCaller() &&
	  p.edge.getCallee() == edge.getCallee() &&
	  p.edge.getBytecodeIndex() == edge.getBytecodeIndex()) {
	return;
      }
      p = p.next;
    }
    if (DEBUG) 
      VM.sysWrite("Recording that "+edge+" was not inlined into "+cmid+"\n");
    nonInlinedEdges.put(key, new NonInlinedElement(cmid, edge, oldEdges));
  }


  /**
   * Allow AI Organizer to check to see if the compiler previously refused to inline a hot edge.
   */
  static boolean knownNonInlinedEdge(int cmid, VM_CallSiteTriple edge) {
    Integer key = new Integer(cmid);
    NonInlinedElement oldEdges = (NonInlinedElement)nonInlinedEdges.get(key);
    NonInlinedElement p = oldEdges;
    while (p != null) {
      if (p.cmid == cmid && 
	  p.edge.getCaller() == edge.getCaller() &&
	  p.edge.getCallee() == edge.getCallee() &&
	  p.edge.getBytecodeIndex() == edge.getBytecodeIndex()) {
	if (DEBUG) VM.sysWrite ("Squashing "+edge+" into "+cmid);
	return true;
      }
      p = p.next;
    }
    return false;
  }

  /**
   * Called when a compiled version becomes obsolete 
   * (to clear the now irrelevant data)
   */
  static void clearNonInlinedEdges(int cmid) {
    Integer key = new Integer(cmid);
    nonInlinedEdges.remove(key);
  }
    
  // Mapping from Integer(CMID) to nonInlinedElement
  private static java.util.HashMap nonInlinedEdges = new java.util.HashMap();
  static class NonInlinedElement {
    int cmid;
    VM_CallSiteTriple edge;
    NonInlinedElement next;
    NonInlinedElement(int c, VM_CallSiteTriple e, NonInlinedElement n) {
      cmid = c;
      edge = e;
      next = n;
    }
  }


  /**
   * Set the threshold for hot edges.
   * If threshold is x, then an edge must accout for at least x% of
   * the total number of dynamic calls to be "hot"
   */
  static void setHotEdgeThreshold(double x) {
    hotEdgeThreshold = x;
  }

  /**
   */
  static void report() {
    VM.sysWrite("Adaptive inlining context-free plan:\n");
    synchronized(plan) {
      VM.sysWrite(plan.toString() + "\n");
    }
  }

  /**
   * record that more yield points have been taken
   * @param n
   */
  static void incrementNumYieldPoints(double n) {
    synchronized(plan) {
      nYieldPointsTaken += n; 
    }
  }
  /**
   * decay the number of yield points taken
   */
  static void decay() {
    synchronized(plan) {
      nYieldPointsTaken /= VM_Controller.options.DECAY_RATE; 
    }
  }
  /**
   */
  static double getNumYieldPoints() { return nYieldPointsTaken; }

  /** Implementation */
  static VM_PartialCallGraph dcg;
  static OPT_ContextFreeInlinePlan plan;
  static double hotEdgeThreshold = 1.0;
  // (decayed) number of yield points taken: maintained by AI organizer
  static double nYieldPointsTaken = 0.0; 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class encapsulates the analytic model used by the controller
 * to guide multi-level recompilation decisions.  An early version of
 * this model is described in the OOPSLA'2000 paper, but we've made
 * some improvements since then...
 *
 * @see VM_MultiLevelAdaptiveModel
 *
 * @author Mike Hind
 * @author Dave Grove
 * @author Peter Sweeney
 * @author Stephen Fink
 * @author Matthew Arnold 
 */
abstract class VM_AnalyticModel extends VM_RecompilationStrategy {

  //---- Interface ------
  // Code that inherits from VM_AnalyticModel must define the
  // following behavior

  /**
   * Initialize the set of "optimization choices" that the
   * cost-benefit model will consider when using will consider when
   * using adaptive compilation.
   */
  abstract void populateRecompilationChoices(); 

  /**
   * Compute the set of optimization choices that should be
   * considered by the cost-benefit model, given the previous compiler.  
   *
   * @param prevCompiler The compiler compiler that was used to 
   *                     comile cmpMethod
   * @param cmpMethod The compiled method being considered
   */
  abstract VM_RecompilationChoice[] 
    getViableRecompilationChoices(int prevCompiler, 
				  VM_CompiledMethod cmpMethod);


  // ----------------------------------------------------- 
  // Below code that is (currently) common to all recompilation
  // strategies that use the analytic model.

  /**
   * Initialize the analytic model:
   *
   *  NOTE: The call to super.init() uses the command line options to
   *  set up the optimization plans, so this must be run after the
   *  command line options are available.  
   */
  void init() {

    // Do the common initialization first
    super.init();

    // setup the recompilation choices that are available to the
    // analytic model
    populateRecompilationChoices();
  }

    
  /**
   * This method is the main decision making loop for all
   * recompilation strategies that use the analytic model.  
   * <p>
   * Given a HotMethodRecompilationEvent, this code will determine 
   * IF the method should be recompiled, and if so, HOW to perform 
   * the recompilation, i.e., what compilation plan should be used.
   * The method returns a controller plan, which contains the compilation
   * plan and other goodies.
   *
   * @param cmpMethod the compiled method of interest
   * @param hme       the VM_HotMethodRecompilationEvent
   * @return the controller plan to be used or NULL, if no 
   *                   compilation is to be performed.  */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {
    // Compiler used for the previous compilation
    int prevCompiler = getPreviousCompiler(cmpMethod); 
    if (prevCompiler == -1) {
      return null; // Not a method that we can recompile (trap, JNI).
    }
    
    VM_ControllerPlan plan = VM_ControllerMemory.findMatchingPlan(cmpMethod);
    double prevCompileTime = 
      getPreviousCompilationTime(hme, plan, prevCompiler);
    if (prevCompileTime < 0.0) {
      // For one of a number of reasons, we've decided that this method is
      // not a candidate for recompilation at this time.
      return null;
    }
    
    // Now we know the compiler that generated the method (prevCompiler).
    // the compile time it took to generate it (prevCompileTime), and that 
    // the method is a potential candidate for additional recompilation. 
    // So, next decide what, if anything, should be done now.  
    // We consider doing nothing (ie leaving the method at the current 
    // opt level, which incurs no  compilation cost), and recompiling the 
    // method at each greater compilation level.
    
    double futureTimeForMethod = futureTimeForMethod(hme);
    
    // initialize bestAction as doing nothing, which means we'll 
    // spend just as much time in the method in the future as we have so far.
    VM_RecompilationChoice bestActionChoice = null;
    double bestActionTime = futureTimeForMethod;
    
    if (VM.LogAOSEvents) { 
      VM_AOSLogging.recordControllerEstimateCostDoNothing
	(cmpMethod.getMethod(),
	 VM_CompilerDNA.getOptLevel(prevCompiler),
	 bestActionTime);
    }
    
    // Get a vector of optimization choices to consider
    VM_RecompilationChoice[] recompilationChoices =
      getViableRecompilationChoices(prevCompiler,cmpMethod);
    
    // Consider all choices in the vector of possibilities
    for (int i=0; i< recompilationChoices.length; i++) {
      VM_RecompilationChoice choice = recompilationChoices[i];

      // Get the cost and benefit of this choice
      double cost = choice.getCost(prevCompiler, prevCompileTime);
      double futureExecutionTime = 
	choice.getFutureExecutionTime(prevCompiler,futureTimeForMethod);
      
      double curActionTime = cost + futureExecutionTime;
      
      if (VM.LogAOSEvents) { 
	VM_AOSLogging.recordControllerEstimateCostOpt
	  (cmpMethod.getMethod(),
	   choice.toString(),
	   curActionTime);
      }
      
      if (curActionTime < bestActionTime) {
	bestActionTime = curActionTime;
	bestActionChoice = choice;
      }
    }
    
    // if the best action is the previous than we don't need to recompile
    if (bestActionChoice == null) {	
      plan = null;
    } else {
      plan = bestActionChoice.makeControllerPlan(cmpMethod, prevCompiler,
						 futureTimeForMethod,
						 bestActionTime);
    }
    return plan;
  }



  /**
   * This function defines how the analytic model handles a
   * VM_AINewHotEdgeEvent.  The basic idea is to use the model to
   * evaluate whether it would be better to do nothing or to recompile
   * at the same opt level, assuming there would be some "boost" after
   * performing inlining.  
   */
  void considerHotCallEdge(VM_CompiledMethod cmpMethod, 
			   VM_AINewHotEdgeEvent event) {

    // Compiler used for the previous compilation
    int prevCompiler = getPreviousCompiler(cmpMethod); 
    if (prevCompiler == -1) {
      return; // Not a method we can recompile (trap, JNI).
    }

    VM_ControllerPlan plan = VM_ControllerMemory.findMatchingPlan(cmpMethod);
    double prevCompileTime = 
      getPreviousCompilationTime(event, plan, prevCompiler);
    if (prevCompileTime < 0.0) {
      // For one of a number of reasons, we've decided that this method is
      // not a candidate for recompilation at this time.
      return;
    }

    // Use the model to caclulate expected cost of (1) doing nothing
    // and (2) recompiling at the same opt level with the FDO boost
    double futureTimeForMethod = futureTimeForMethod(event);
    double futureTimeForFDOMethod = 
      prevCompileTime + (futureTimeForMethod/event.getBoostFactor());
    
    if (VM.LogAOSEvents) { 
      int prevOptLevel = VM_CompilerDNA.getOptLevel(prevCompiler);
      VM_AOSLogging.recordControllerEstimateCostDoNothing(cmpMethod.getMethod(),
							  prevOptLevel,
							  futureTimeForMethod);
      VM_AOSLogging.recordControllerEstimateCostOpt(cmpMethod.getMethod(),
						    "O"+prevOptLevel+"AI",
						    futureTimeForFDOMethod);
    }

    if (futureTimeForFDOMethod < futureTimeForMethod) {
      // Profitable to recompile with FDO, so do it.
      int optLevel = VM_CompilerDNA.getOptLevel(prevCompiler);
      double priority = futureTimeForMethod - futureTimeForFDOMethod;
      plan = createControllerPlan(cmpMethod.getMethod(), 
				  optLevel, null, 
				  cmpMethod.getId(), 
				  event.getBoostFactor(),
				  priority);
      
      plan.execute();
    }
  }


  /**
   * How much time do we expect to spend in the method in the future if
   * we take no recompilation action?
   * The key assumption is that we'll spend just as much time 
   * executing in the the method in the future as we have done so far
   * in the past.
   * 
   * @param hme The VM_HotMethodEvent in question
   * @return estimate of future execution time to be spent in this method
   */
   double futureTimeForMethod(VM_HotMethodEvent hme) {
    VM_AOSOptions opts = VM_Controller.options;
    double numSamples = hme.getNumSamples();
    double timePerSample;
    if (!VM.UseEpilogueYieldPoints) { 
      // NOTE: we take two samples per timer interrupt, so we have to
      // adjust here (otherwise we'd give the method twice as much time
      // as it actually deserves).
      timePerSample = (opts.SAMPLE_FREQ_MILLIS / 2.0) * (1.0 - opts.DARK_MATTER);
    } else {
      // If we use epilogue yield points, we only have 1 sample per interrupt
      //  prologue => calling method
      //  backedge/epilogue => current method
      timePerSample = opts.SAMPLE_FREQ_MILLIS * (1.0 - opts.DARK_MATTER);
    }

    double timeInMethodSoFar = numSamples * timePerSample;
    return timeInMethodSoFar;
  }


}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * This class contains top level adaptive compilation subsystem functions.
 *
 * @author Michael Hind
 * @author Dave Grove
 * @author Stephen Fink
 */
class VM_Controller implements VM_Callbacks.ExitMonitor,
			       VM_Callbacks.AppRunCompleteMonitor {

  /**
   * Signals when the options and (optional) logging mechanism are enabled
   */
  public static boolean enabled = false;

  /**
   * Controller subsystem control options
   */
  public static VM_AOSOptions options = null;
  
  /**
   * Deferred command line arguments for the opt compiler
   */
  private static String[] optCompilerOptions = new String[0];
  /**
   * Add a deferred command line argument
   */
  public static void addOptCompilerOption(String arg) {
    String[] tmp = new String[optCompilerOptions.length+1];
    for (int i=0; i< optCompilerOptions.length; i++) {
      tmp[i] = optCompilerOptions[i];
    }
    tmp[optCompilerOptions.length] = arg;
    optCompilerOptions = tmp;
  }
  /**
   * Get the deferred command line arguments
   */
  public static String[] getOptCompilerOptions() {return optCompilerOptions;}

  /**
   * The controller thread, it makes all the decisions
   * (the thread sets this field when it is created.)
   */
  public static VM_ControllerThread controllerThread = null;

  /**
   * Thread that will perform opt-compilations as directed by the controller
   * (the thread sets this field when it is created.)
   */
  public static VM_CompilationThread compilationThread = null;

  /**
   * Threads that will organize profile data as directed by the controller
   */
  public static Vector organizers = new Vector();


  /**
   * A blocking priority queue where organizers place events to 
   * be processed by the controller
   * (an input to the controller thread)
   */
  public static VM_BlockingPriorityQueue controllerInputQueue;

  /**
   * A blocking priority queue where the controller will place methods 
   * to be opt compiled
   * (an output of the controller thread)
   */
  public static VM_BlockingPriorityQueue compilationQueue;

  /**
   * The strategy used to make recompilation decisions
   */
  public static VM_RecompilationStrategy recompilationStrategy;

  /**
   *  a controller virtual clock, ticked every thread switch.
   */
  public static int controllerClock = 0;

  /**
   * The main hot method raw data object.
   */
  public static VM_MethodCountData methodSamples;

  /**
   * Initialize the controller subsystem (called from VM.boot)
   * This method is called AFTER the command line options are processed.
   */
  private static boolean booted = false;
  public static void boot() {
    // check to see if the AOS options were created during the process
    // of check command line args, if not we'll create a default one
    if (options == null) {
      options = new VM_AOSOptions();
    }
    
    // Signal that the options and (optional) logging mechanism are set
    // VM_RuntimeCompiler checks this flag
    enabled = true;

    // Initialize the controller input queue
    controllerInputQueue = 
      new VM_BlockingPriorityQueue(options.CONTROLLER_INPUT_QUEUE_SIZE,
				   new VM_BlockingPriorityQueue.CallBack() {
				       void aboutToWait() { controllerThread.aboutToWait(); }
				       void doneWaiting() { controllerThread.doneWaiting(); }
				     });

    compilationQueue = 
      new VM_BlockingPriorityQueue(options.COMPILATION_QUEUE_SIZE);

    // Create the analytic model used to make cost/benefit decisions.
    if (options.ADAPTIVE_RECOMPILATION) {
      // Multi-level adaptive, ala OOPSLA 2000
      recompilationStrategy = new VM_MultiLevelAdaptiveModel();
    } 
    else {
      // SLA, ala OOPSLA 2000
      recompilationStrategy = new VM_SingleLevelAdaptive();
    }

    // boot the runtime measurement systems
    VM_RuntimeMeasurements.boot();

    // Initialize subsystems, if being used
    VM_AdaptiveInlining.boot(options);
    
    // boot any instrumentation options
    VM_Instrumentation.boot(options);

    // boot the aos database
    VM_AOSDatabase.boot(options);

    createControllerThread();

    VM_Callbacks.addExitMonitor(new VM_Controller());
    VM_Callbacks.addAppRunCompleteMonitor(new VM_Controller());

    booted=true;
  }

  /**
   * To be called when the VM is about to exit.
   * @param value the exit value
   */
  public void notifyExit(int value) {
    report();
  }

  /**
   * To be called when the application completes one of its run
   */
  public void notifyAppRunComplete(int i) {
    if (VM.LogAOSEvents) VM_AOSLogging.appRunComplete();
  }

  // Create the ControllerThread
  static void createControllerThread() {
    Object sentinel = new Object();
    VM_ControllerThread tt = new VM_ControllerThread(sentinel);
    tt.makeDaemon(true);
    tt.start();
    // wait until controller threads are up and running.
    try {
      synchronized(sentinel) {
	sentinel.wait();
      }
    } catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to start up controller subsystem");
    }
  }


  /**
   * Process any command line arguments passed to the controller subsystem.
   * <p>
   * This method has the responsibility of creating the options object
   * if it does not already exist
   * <p>
   * NOTE: All command line argument processing should be handled via
   * the automatically generated code in VM_AOSOptions.java.  
   * Don't even think of adding handwritten stuff here! --dave
   *
   * @param arg the command line argument to be processed
   */
  public static void processCommandLineArg(String arg) {
    if (options == null) {
      options = new VM_AOSOptions();
    }
    
    if (!options.processAsOption("-X:aos", arg)) {
      VM.sysWrite("vm: illegal adaptive configuration directive \""+arg+"\" specified as -X:aos:"+arg+"\n");
      VM.sysExit(-1);
    }
  }

  /**
   * This method is called when the VM is exiting to provide a hook to allow
   * the adpative optimization subsystem to generate a summary report.
   * It can also be called directly from driver programs to allow
   * reporting on a single run of a benchmark that the driver program
   * is executing in a loop (in which case the adaptive system isn't actually
   * exiting.....so some of the log messages may get a little wierd).
   */
  public static void report() {
    if (!booted) return;
    VM_ControllerThread.report();
    VM_RuntimeMeasurements.report();

    for (Enumeration e = organizers.elements(); e.hasMoreElements(); ) {
      VM_Organizer organizer = (VM_Organizer)e.nextElement();
      organizer.report();
    }
    if (options.DUMP_AI_DECISIONS) {
      VM_AdaptiveInlining.report();
    }

    if (options.REPORT_STATIC_PROGRAM_STATS) {
      VM_OptStaticProgramStats.report();
    }

    if (options.FINAL_REPORT_LEVEL >= 2) {
      VM_EdgeCounts.dumpCounts();
    }

    if (VM.LogAOSEvents) VM_AOSLogging.systemExiting();
  }


  /**
   * Stop all AOS threads and exit the adaptive system.
   * Can be used to assess code quality in a steady state by
   * allowing the adaptive system to run "for a while" and then
   * stoppping it
   */
  public static void stop() {
    if (!booted) return;
    VM.sysWrite("\nAOS: Killing all adaptive system threads\n");
    for (Enumeration e = organizers.elements(); e.hasMoreElements(); ) {
      VM_Organizer organizer = (VM_Organizer)e.nextElement();
      organizer.kill(new ThreadDeath());
    }
    compilationThread.kill(new ThreadDeath());
    controllerThread.kill(new ThreadDeath());
    VM_RuntimeMeasurements.stop();
    report();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Abstract parent class for events from organizers to the controller. 
 *
 * @author Stephen Fink 
 */
interface VM_ControllerInputEvent {

   /** 
    * This method is called by the controller upon dequeuing this
    * event from the controller input queue
    */
   void process();
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Hashtable;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Enumeration;
import java.util.LinkedList;
import java.io.PrintStream;

/**
 *  This class records decisions taken by the controller.  It will remember
 *  controller plans, which contain compilation plans and other goodies,
 *  and allows searching for previous decisions
 *
 *  @author Michael Hind
 *  @author Stephen Fink
 */
final class VM_ControllerMemory implements VM_Constants {

  /**
   *  This is a hashtable of controller plans indexed on the method ID.  
   *  Each method ID can have a list of such plans associated with.
   */
  private static Hashtable table;

  /**
   * Number of times controller is awoken and did nothing.
   */
  private static int didNothing = 0;

  /**
   * Number of times controller is awoken
   */
  private static int awoken     = 0;

  // counters for chosen opt levels
  private static int numMethodsConsidered               = 0;
  private static int numMethodsScheduledForRecomp       = 0;
  private static int numBase                            = 0;
  private static int numOpt0                            = 0;
  private static int numOpt1                            = 0;
  private static int numOpt2                            = 0;
  private static int numOpt3                            = 0;
  private static int numOpt4                            = 0;

  static int getNumAwoken()                    { return awoken; }
  static int getNumDidNothing()                { return didNothing; }
  static int getNumMethodsConsidered()         { return numMethodsConsidered; }
  static int getNumMethodsScheduledForRecomp() 
    { return numMethodsScheduledForRecomp; }
  static int getNumBase()                       { return numBase; }
  static int getNumOpt0()                       { return numOpt0; }
  static int getNumOpt1()                       { return numOpt1; }
  static int getNumOpt2()                       { return numOpt2; }
  static int getNumOpt3()                       { return numOpt3; }
  static int getNumOpt4()                       { return numOpt4; }

  static void incrementNumAwoken()              { awoken++; }
  static void incrementNumDidNothing()          { didNothing++; }
  static void incrementNumMethodsConsidered()   { numMethodsConsidered++; }
  static void incrementNumMethodsScheduledForRecomp()  
    { numMethodsScheduledForRecomp++; }
  static void incrementNumBase()                { numBase++; }
  static void incrementNumOpt0()                { numOpt0++; }
  static void incrementNumOpt1()                { numOpt1++; }
  static void incrementNumOpt2()                { numOpt2++; }
  static void incrementNumOpt3()                { numOpt3++; }
  static void incrementNumOpt4()                { numOpt4++; }

  static void init() {
    table = new Hashtable();
  }
 
  /**
   *  Inserts a controller plan keyed on the underlying method
   *
   *  @param plan the controller plan to insert
   */
  static void insert(VM_ControllerPlan plan) {

    if (VM.LogAOSEvents) {
      VM_Method method = plan.getCompPlan().getMethod();
      numMethodsScheduledForRecomp++;
      int optLevel = plan.getCompPlan().options.getOptLevel();
      switch (optLevel) {
      case 0:  numOpt0++; break;
      case 1:  numOpt1++; break;
      case 2:  numOpt2++; break;
      case 3:  numOpt3++; break;
      case 4:  numOpt4++; break; 
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown Opt Level");
      }
    }

    // first check to see if there is a plan list for this method
    LinkedList planList = findPlan(plan.getCompPlan().method);

    if (planList == null) {
      // create a plan list, with the single element being this plan
      planList = new LinkedList();

      // no synch needed here because the planList is not in the table yet
      planList.addLast(plan);

      // insert in the hashtable using the method ID as the hash value
      table.put(new Integer(plan.getCompPlan().method.getDictionaryId()), 
		planList);
    } else {
      // add the current plan to the end of the list
      synchronized(planList) {
	planList.addLast(plan);
      }
    }

    // tell the plan what list it is on
    plan.setPlanList(planList);

  }

  /**
   * Looks for a controller plan for the passed method
   *
   * @param VM_Method the method to look for
   * @return the list of controller plans for this method if one exists, 
   *         otherwise, null
   */
  static LinkedList findPlan(VM_Method method) {
    return (LinkedList) table.get(new Integer(method.getDictionaryId()));
  }

  /**
   *  Find the plan for the compiled method that is passed
   *  @param cmpMethod the compiled method of interest
   *  @return the matching plan or null if none exists.
   */
  static VM_ControllerPlan findMatchingPlan(VM_CompiledMethod cmpMethod) {
    VM_Method method = cmpMethod.getMethod();

    LinkedList planList = findPlan(method);
    if (planList == null) {
      return null;
    } else{
      // iterate over the planList until we get to this item
      boolean found = false;
      VM_ControllerPlan curPlan = null;
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  curPlan = (VM_ControllerPlan) iter.next();

	  // exit when we find ourselves
	  if (curPlan.getCMID() == cmpMethod.getId()) {
	    found = true;
	    break;
	  }
	} // more to process
      }

      if (!found) {
	// there was a plan for this method, but not for this compiled method
	curPlan = null;
      }

      return curPlan;
    }
  }

  /**
   *  Determine if the passed method should be considered as a candidate
   *  for _initial_ AOS recompilation. 
   *  A method should not be reconsider for initial AOS recompilation if 
   *  a plan already exists for the method whose status is IN_PROGRESS, 
   *  COMPLETED, OUTDATED, or ABORTED because of compilation error.
   * 
   *  @param method the method of interest
   *  @return whether the method should be considered or not
   */
  static boolean shouldConsiderForInitialRecompilation(VM_Method method) {
    LinkedList planList = findPlan(method);
    if (planList == null) {
      return true;
    } else {
      // iterate over the planList until we find a plan whose status is
      // inprogress, completed, 
      synchronized(planList) {
	boolean found = false;
	VM_ControllerPlan curPlan = null;
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  curPlan = (VM_ControllerPlan) iter.next();

	  // exit when we find ourselves
	  byte status = curPlan.getStatus();
	  if (status == VM_ControllerPlan.COMPLETED || 
	      status == VM_ControllerPlan.IN_PROGRESS || 
	      status == VM_ControllerPlan.ABORTED_COMPILATION_ERROR || 
	      status == VM_ControllerPlan.OUTDATED) {
	    return false;
	  }
	}
      }
      return true;  // we didn't find any, so return true
    }
  }


  /**
   * Return true if there is a plan with the given status for the given method
   *
   * @param method the method of interest
   * @param status the status of interest
   * @return whether or not there is plan with that status for the method
   */
  static boolean planWithStatus(VM_Method method, byte status) {
    LinkedList planList = findPlan(method);
    if (planList != null) {
      // iterate over the planList until we find a plan with status 'status'
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();
	  if (curPlan.getStatus() == status) {
	    return true;
	  }
	}
      }
    }
    return false;
  }

  /**
   * Return true if there is a completed plan with the given opt level for 
   * the give method
   *
   * @param method the method of interest
   * @param optLevel the opt level of interest
   * @return whether or not there is completed plan with that level 
   *             for the method
   */
  static boolean completedPlanWithOptLevel(VM_Method method, int optLevel) {
    LinkedList planList = findPlan(method);
    if (planList != null) {
      // iterate over the planList until we find a completed plan with the
      // opt level passed
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();
	  if (curPlan.getStatus() == VM_ControllerPlan.COMPLETED &&
	      curPlan.getCompPlan().options.getOptLevel() == optLevel) {
	    return true;
	  }
	}
      }
    }
    return false;
  }


  /**
   * Looks for the last controller plan for the passed method
   *
   * @param VM_Method the method to look for
   * @return the last controller plan for this method if it exists, 
   *         otherwise, null
   */
  static VM_ControllerPlan findLatestPlan(VM_Method method) {
    LinkedList planList = findPlan(method);
    if (planList == null) {
      return null;
    } else {
      return (VM_ControllerPlan) planList.getLast();
    }
  }

  /**
   * This method summarizes the recompilation actions taken for all methods
   * in this object and produces a report to the passed PrintStream.
   * @param log the stream to print to
   */
  static void printFinalMethodStats(PrintStream log) {
    // We will traverse the hash table and for each method record its status as
    // one of the following
    //    B -> 0 -> 1 -> 2
    //    B -> 0 -> 1 
    //    B -> 0 
    //    B      -> 1 -> 2
    //    B -> 0      -> 2
    //    B           -> 2
    //    B      -> 1
    //
    //  We encode these possibilities by turning on 1 of three bits for 0, 1, 2
    //  Also, for all methods that eventually get to level 2, they can be 
    //  recompiled an arbitrary amount of times.  We record this in in a counter.

    final int MAX_BIT_PATTERN = 7;
    int summaryArray[] = new int[MAX_BIT_PATTERN+1];
    int totalRecompsAtLevel2 = 0;

    // traverse table and give a summary of all actions that have occurred
    if (table != null) {
      for (Enumeration enum = table.keys(); enum.hasMoreElements() ;) {
        Integer intObject = (Integer) enum.nextElement();
	LinkedList planList = (LinkedList) table.get(intObject);

	int bitPattern = 0;
	int recompsAtLevel2 = 0;
	VM_ControllerPlan plan = null;
	
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	plan = (VM_ControllerPlan) iter.next();

	// only process plans that were completed or completed and outdated 
	// by subsequent plans for this method
	byte status = plan.getStatus();
	if (status == VM_ControllerPlan.COMPLETED || 
	    status == VM_ControllerPlan.OUTDATED) {
	    int optLevel = plan.getCompPlan().options.getOptLevel();

	    // check for recomps at level 2
	    if (optLevel == 2 && bitIsSet(bitPattern, 2)) {
	      recompsAtLevel2++;
	    }

	    bitPattern = setBitPattern(bitPattern, optLevel);
	  } // if
	} // while
      
	if (VM_Controller.options.LOGGING_LEVEL >= 2) {
	    log.println("Method: "+ plan.getCompPlan().getMethod() 
			+", bitPattern: "+ bitPattern
			+", recompsAtLevel2: "+ recompsAtLevel2);
	}

	summaryArray[bitPattern]++;
	totalRecompsAtLevel2 = totalRecompsAtLevel2 + recompsAtLevel2;
      }
    }

    // Print the summary
    int totalUniqueMethods = 0;
    for (int i=1; i<=MAX_BIT_PATTERN; i++) {
      log.print("    Base");
      for (int optLevel=0;  optLevel <=2; optLevel++) {
	if (bitIsSet(i, optLevel)) {
	  log.print(" -> "+ optLevel);
	}
      }
      log.println(": "+ summaryArray[i]);
      totalUniqueMethods = totalUniqueMethods + summaryArray[i];
    }
    log.println("  Num recompilations At level 2: "+ totalRecompsAtLevel2);
    log.println("  Num unique methods recompiled: "+ totalUniqueMethods +"\n");
  }

  /**
   *  set the optLevel bit in the passed bitPattern and return the result
   *  @param bitPattern
   *  @param optLevel
   */
  static int setBitPattern(int bitPattern, int optLevel) {
    int newPattern = 1;
    newPattern = newPattern << optLevel;
    return newPattern | bitPattern;
  }

  /**
   * check if the bit position defined by the 2nd parm is set in the first parm
   * @param bitPattern
   * @param optLevel
   * @return whether the passed bit is set
   */
  static boolean bitIsSet(int bitPattern, int optLevel) {
    int newPattern = 1;
    newPattern = newPattern << optLevel;
    return (newPattern & bitPattern) > 0;
  }

  static String asString() {
    return table.toString();
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.LinkedList;
import java.util.ListIterator;

/**
 * An instance of this class describes a compilation decision made by
 * the controller
 *
 * Constraints:
 *   Given the plan list of a method:
 * 	Only one plan will have status COMPLETED
 * 	Multiple plans may have status OUTDATED
 *	Only one plan will have status IN_PROGRESS
 *
 * status states:
 * UNINITIALIZED -> IN_PROGRESS -> COMPLETED -> OUTDATED
 *             \              \--> ABORTED_COMPILATION_ERROR (never recompile method)
 *             \--> ABORTED_QUEUE_FULL
 *
 * @author Michael Hind
 */
final class VM_ControllerPlan {

  // The plan was created, but the setStatus method was never called
  static final byte UNINITIALIZED = 0;

  // The plan was successfully completed, i.e., the method was recompiled
  static final byte COMPLETED = 1;

  // Compilation began the method, but failed in an error
  static final byte ABORTED_COMPILATION_ERROR = 2;

  // The plan was aborted because the compilation queue was full
  static final byte ABORTED_QUEUE_FULL = 3;

  // The compilation is still in progress
  static final byte IN_PROGRESS = 4;

  // The compilation completed, but a new plan for the same method also 
  // completed, so this is not the most recent completed plan
  static final byte OUTDATED = 5;

  // This is used by clients to initialize local variables for Java semantics
  static final byte UNKNOWN = 99;   
  
  /**
   *  The associate compilation plan 
   */
  private OPT_CompilationPlan compPlan; 

  /**
   *  The time we created this plan
   */
  private int timeCreated;

  /**
   *  The time compilation began
   */
  private int timeInitiated = -1;

  /**
   *  The time compilation end
   */
  private int timeCompleted = -1;

  /**
   *  The CPU time it took for the compilation
   */
  private double compilationCPUTime;

  /**
   *  The speedup we were expecting
   */
  private double expectedSpeedup;

  /**
   *  The priority associated with this plan
   */
  private double priority;

  /**
   *  The compiled method ID for this plan
   */
  private int CMID;

  /**
   *  The compiled method ID for the previous plan for this method
   */
  private int prevCMID;

  /**
   *  The status of this plan
   */
  private byte status;

  /**
   *  The list that we are onstatus of this plan
   */
  private LinkedList planList; 


  /**
   * Construct a controller plan
   *
   * @param compPlan     The compilation plan
   * @param timeCreated  The "time" this plan was created
   * @param prevCMID     The previous compiled method ID
   * @param expectedSpeedup	Expected recompilation benefit
   * @param priority     How important is executing this plan?
   */
  public VM_ControllerPlan(OPT_CompilationPlan compPlan, 
			   int timeCreated, 
			   int prevCMID, 
			   double expectedSpeedup,
			   double priority) {
    this.compPlan = compPlan;
    this.timeCreated = timeCreated;
    this.prevCMID = prevCMID;
    this.status = VM_ControllerPlan.UNINITIALIZED;
    this.expectedSpeedup = expectedSpeedup;
    this.priority = priority;
  }
  

  /**
   * Execute the plan.
   * 
   * @return true on success, false on failure
   */
  boolean execute() {
    // mark plan as in progress and insert it into controller memory
    setStatus(VM_ControllerPlan.IN_PROGRESS);
    VM_ControllerMemory.insert(this);
      
    // Attempt to add plan to compilation queue.
    boolean succeeded = 
      VM_Controller.compilationQueue.insert(getPriority(), this);

    // Logging, and record failure in plan if necessary.
    if (succeeded) {
      if (VM.LogAOSEvents) 
	VM_AOSLogging.recompilationScheduled(getCompPlan(), getPriority()); 
    } else {
      if (VM.LogAOSEvents) 
	VM_AOSLogging.recompilationQueueFull(getCompPlan()); 
      setStatus(VM_ControllerPlan.ABORTED_QUEUE_FULL); 
    }

    return succeeded;
  }



  /**
   * The compilation plan
   */
  public OPT_CompilationPlan getCompPlan() { return compPlan; }

  /**
   * The expected speedup <em>for this method </em> due to this recompilation
   */
  public double getExpectedSpeedup() { return expectedSpeedup; }

  /**
   * The priority (how important is it that this plan be executed)
   */
  public double getPriority() { return priority; }

  /**
   * The time this plan was created
   */
  public int getTimeCreated() { return timeCreated;  }
   
  /**
   * The time (according to the controller clock) compilation of this plan 
   * began.
   */
  public int getTimeInitiated() { return timeInitiated; }
  public void setTimeInitiated(int t) { timeInitiated = t; }
  

  /**
   * The time (according to the controller clock) compilation of this plan 
   * completed.
   */
  public int getTimeCompleted() { return timeCompleted; }
  public void setTimeCompleted(int t) { timeCompleted = t; }


  /**
   * The CPU time in milliseconds actually consumed by the compilation
   * thread to execute this plan. 
   */
  public double getCompilationCPUTime() { return compilationCPUTime; }
  public void setCompilationCPUTime(double t) { compilationCPUTime = t; }


  /**
   * CMID (compiled method id) associated with the code produced 
   * by executing this plan
   */
  public int getCMID() { return CMID; }
  public void setCMID(int x) { CMID = x; }


  /**
   * CMID (compiled method id) associated with the *PREVIOUS* compiled 
   * version of this method
   */
  public int getPrevCMID() { return prevCMID; }


  /**
   * Status of this compilation plan, choose from the values above
   */
  public byte getStatus() { return status; }

  public void setStatus(byte newStatus) { 
    status = newStatus; 

    // if we are marking this plan as completed, all previous completed plans
    // for this method should be marked as OUTDATED
    if (newStatus == COMPLETED) {
      // iterate over the planList until we get to this item
      ListIterator iter = planList.listIterator();
      while (iter.hasNext()) {
	VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();

	// exit when we find ourselves
	if (curPlan == this) break;

	if (curPlan.getStatus() == COMPLETED) {
	  curPlan.status = OUTDATED;
	}
      } // more to process
    }
  }

  /**
   * List of plans for a source method
   */
  public LinkedList getPlanList() { return planList; }
  public void setPlanList(LinkedList list) { planList = list; }

  public String getStatusString() {
    switch (status) {
    case UNINITIALIZED:             return "UNINITIALIZED";
    case COMPLETED:                 return "COMPLETED";
    case ABORTED_COMPILATION_ERROR: return "ABORTED_COMPILATION_ERROR";
    case ABORTED_QUEUE_FULL:        return "ABORTED_QUEUE_FULL";
    case IN_PROGRESS:               return "IN_PROGRESS";
    case OUTDATED:                  return "OUTDATED";
    case UNKNOWN:                   return "UNKNOWN (not error)";
    default:                        return "**** ERROR, UNKNOWN STATUS ****";
    }
  }

  public String toString() {
    StringBuffer buf = new StringBuffer();

    buf.append("Method: "+ getCompPlan().method
	       +"\n\tCompiled Method ID: " + CMID
	       +"\n\tPrevious Compiled Method ID: " + prevCMID
	       +"\n\tCreated at "+ timeCreated
	       +"\n\tInitiated at "+ timeInitiated
	       +"\n\tCompleted at "+ timeCompleted
	       +"\n\tCPU Time Consumed: "+ compilationCPUTime
	       +"\n\tExpected Speedup: "+ expectedSpeedup
	       +"\n\tPriority: "+ priority
	       +"\n\tStatus: "+ getStatusString()
	       +"\n\tComp. Plan Level: "+compPlan.options.getOptLevel() +"\n");
    return buf.toString();
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * This class implements the controller thread.  This entity is the brains of 
 * the adaptive optimization system.  It communicates with the runtime 
 * measurements subsystem to instruct and gather profiling information.  
 * It also talks to the compilation threads to generate 
 *     a) instrumented executables;
 *     b) optimized executables; 
 *     c) static information about a method; or 
 *     d) all of the above.
 *
 *  @author Michael Hind
 *  @author David Grove
 *  @author Stephen Fink
 *  @author Peter Sweeney
 */
class VM_ControllerThread extends VM_Thread {

  /**
   * constructor
   * @param sentinel: an object to signal when up and running
   */
  VM_ControllerThread(Object sentinel)  { this.sentinel = sentinel; }
  private Object sentinel;


  /**
   * This method is the entry point to the controller, it is called when
   * the controllerThread is created.
   */
  public void run() {
    // save this object so others can access it, if needed
    VM_Controller.controllerThread = this;

    // Bring up the logging system
    if (VM.LogAOSEvents) VM_AOSLogging.boot();
    if (VM.LogAOSEvents) VM_AOSLogging.controllerStarted();

    // Create measurement entities that are NOT related to 
    // adaptive recompilation
    createProfilers();

    if (!VM_Controller.options.adaptive()) {
      // We're running an AOS bootimage with a non-adaptive primary strategy. 
      // We already set up any requested profiling infrastructure, so nothing
      // left to do but exit.
      controllerInitDone();
      VM.sysWrite("\nAOS: In non-adaptive mode; controller thread exiting.\n");
      return; // controller thread exits.
    }

    // Create the organizerThreads and schedule them
    createOrganizerThreads();

    // Create the compilationThread and schedule it
    createCompilationThread();

    // Initialize the controller "memory"
    VM_ControllerMemory.init();

    // Initialize the CompilerDNA class
    VM_CompilerDNA.init();

    // Create our set of standard optimization plans.
    VM_Controller.recompilationStrategy.init();

    controllerInitDone();

    // Enter main controller loop.
    // Pull an event to process off of 
    // VM_Controller.controllerInputQueue and handle it.  
    // If no events are on the queue, then the deleteMin call will 
    // block until an event is available.
    // Repeat forever.
    while (true) {
      if (VM_Controller.options.EARLY_EXIT &&
	  VM_Controller.options.EARLY_EXIT_TIME < VM_Controller.controllerClock) {
	VM_Controller.stop();
      }
      Object event = VM_Controller.controllerInputQueue.deleteMin();
      ((VM_ControllerInputEvent)event).process();
    }
  }

  // Now that we're done initializing, signal the sentinel object.
  private void controllerInitDone() {
    try {
      synchronized(sentinel) {
	sentinel.notify();
      }
    } catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to start up controller subsystem");
    }
  }

  
  /**
   * Called when the controller thread is about to wait on 
   * VM_Controller.controllerInputQueue
   */
  public void aboutToWait() {
  }


  /**
   * Called when the controller thread is woken after waiting on 
   * VM_Controller.controllerInputQueue
   */
  public void doneWaiting() {
    if (VM.LogAOSEvents) VM_ControllerMemory.incrementNumAwoken();
  }


  ///////////////////////
  // Initialization.
  //  Create AOS threads.
  //  Initialize AOS data structures that depend on command line arguments.
  ///////////////////////

  /**
   *  Create the compilationThread and schedule it
   */
  private void createCompilationThread() {
    VM_CompilationThread ct = new VM_CompilationThread();
    VM_Controller.compilationThread = ct;
    ct.makeDaemon(true);
    ct.start();
  }

  /**
   * Create profiling entities that are NOT used for adaptive recompilation
   */
  private void createProfilers() {
    VM_AOSOptions opts = VM_Controller.options;

    if (opts.GATHER_PROFILE_DATA) {
      VM_MethodCountData tmp = new VM_MethodCountData();
      VM_RuntimeMeasurements.registerReportableObject(tmp);
      VM_MethodListener methodListener = 
        new VM_AccumulatingMethodListener(opts.INITIAL_SAMPLE_SIZE, 
					  false, tmp);
      VM_RuntimeMeasurements.installMethodListener(methodListener);
      methodListener.activate();
    }
  }


  /**
   *  Create the organizerThreads and schedule them
   */
  private void createOrganizerThreads() {
    VM_AOSOptions opts = VM_Controller.options;

    // Primary backing store for method sample data
    VM_Controller.methodSamples = new VM_MethodCountData();

    // Select organizers to drive method recompilation 
    int filterOptLevel = opts.ADAPTIVE_RECOMPILATION ? 
      opts.FILTER_OPT_LEVEL : opts.DEFAULT_OPT_LEVEL;
    VM_Organizer methodOrganizer = null;
    if (opts.windowing()) {
      VM_BasicMethodListener methodListener = 
	new VM_BasicMethodListener(opts.INITIAL_SAMPLE_SIZE);
      methodOrganizer = 
	new VM_MethodSampleOrganizer(methodListener, filterOptLevel);
    } else if (opts.windowingWithHistory()) {
      VM_BasicMethodListener methodListener = 
	new VM_BasicMethodListener(opts.INITIAL_SAMPLE_SIZE);
      methodOrganizer = 
	new VM_SlopeDetectingMethodSampleOrganizer(methodListener,
						   filterOptLevel,
						   opts.MSO_ADJUST_BOUNDS,
						   opts.MSO_NUM_EPOCHS);
    } else {
      VM.sysFail("Unimplemented selection of method organizer");
    }
    VM_Controller.organizers.addElement(methodOrganizer);


    // Decay runtime measurement data 
    if (opts.ADAPTIVE_INLINING) {
      VM_Organizer decayOrganizer = 
	new VM_DecayOrganizer(new VM_YieldCounterListener(opts.DECAY_FREQUENCY));
      VM_Controller.organizers.addElement(decayOrganizer);
    }
    
    if (opts.ADAPTIVE_INLINING) {
      VM_Organizer AIOrganizer = 
	new VM_AIByEdgeOrganizer(new VM_EdgeListener());
      VM_Controller.organizers.addElement(AIOrganizer);
    }

    for (Enumeration e = VM_Controller.organizers.elements(); 
	 e.hasMoreElements(); ) {
      VM_Organizer o = (VM_Organizer)e.nextElement();
      o.makeDaemon(true);
      o.start();
    }
  }


  /**
   * Final report
   */
  public static void report() {
    if (VM.LogAOSEvents) VM_AOSLogging.controllerCompleted();
  }

} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Abstract parent class for events from organizers to the controller 
 * used to communicate that a method should be considered as a candidate
 * for recompilation.
 *
 * @author Dave Grove 
 */
abstract class VM_HotMethodEvent {

  /**
   * The compiled method associated querries.
   */
  private VM_CompiledMethod cm;
  public final int getCMID() { return cm.getId(); }
  public final VM_CompiledMethod getCompiledMethod() { return cm; }
  public final VM_Method getMethod() { return cm.getMethod();  }
  public final boolean isOptCompiled() {
    return cm.getCompilerType() == VM_CompiledMethod.OPT;
  }
  public final int getOptCompiledLevel() {
    if (!isOptCompiled()) return -1;
    return ((VM_OptCompiledMethod)cm).getOptLevel();
  }


  /**
   * Number of samples attributed to this method.
   */
  private double numSamples;
  public final double getNumSamples() { return numSamples; }

  /**
   * @param _cm the compiled method 
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodEvent(VM_CompiledMethod _cm, double _numSamples) {
    if (VM.VerifyAssertions) {
      VM.assert(_cm != null, "Don't create me for null compiled method!");
      VM.assert(_numSamples >= 0.0, "Invalid numSamples value");
    }
    cm = _cm;
    numSamples = _numSamples;
  }

  /**
   * @param _cm the compiled method 
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodEvent(VM_CompiledMethod _cm, int _numSamples) {
    this(_cm, (double)_numSamples);
  }

  public String toString() {
    return getMethod()+ " = " +getNumSamples();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Event used by the basic recompilation organizer
 * to notify the controller that a method is hot.
 *
 * @author Dave Grove 
 * @author Stephen Fink
 */
public final class VM_HotMethodRecompilationEvent extends VM_HotMethodEvent 
  implements VM_ControllerInputEvent {

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodRecompilationEvent(VM_CompiledMethod _cm, double _numSamples) {
    super(_cm, _numSamples);
  }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodRecompilationEvent(VM_CompiledMethod _cm, int _numSamples) {
    this(_cm, (double)_numSamples);
  }

  public String toString() {
    return "HotMethodRecompilationEvent: "+super.toString();
  }


  /**
   * This function defines how the controller handles a
   * VM_HotMethodRecompilationEvent.  Simply passes the event to the
   * recompilation strategy.
   */
  public void process() {
    VM_ControllerPlan plan =
      VM_Controller.recompilationStrategy.considerHotMethod(getCompiledMethod(), this);

    if (VM.LogAOSEvents) VM_ControllerMemory.incrementNumMethodsConsidered();

    // If plan is still null we decided not to recompile.
    if (plan != null) {
      plan.execute();
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Implements the multi-level adaptive strategy using an analytic
 * model, as described in the OOPSLA 2000 paper.  Most behavior
 * inherited from VM_AnalyticModel.  This class defines the the specific 
 * recompilation choices that should be considered by the analytic model.
 *
 * @author Matthew Arnold 
 */
class VM_MultiLevelAdaptiveModel extends VM_AnalyticModel {


  /**
   * Initialize the set of "optimization choices" that the
   * cost-benefit model will consider.
   *
   * This method is conceptually simply, but becomes more complex
   * because sets of choices are precomputed and stored in a table so
   * they do not need to be recomputed to answer queries.
   * */

  void populateRecompilationChoices() {

    int maxOptLevel =  VM_Controller.options.MAX_OPT_LEVEL;
    int maxCompiler =  VM_CompilerDNA.getCompilerConstant(maxOptLevel);
    allOptLevelChoices = new VM_RecompileOptChoice[maxOptLevel+1];

    // Create one main list of all possible recompilation choices that
    // will be considered.  For each opt-level, create a recompilation
    // choice for that opt-level and record it indexed by opt-level
    for (int optLevel=0; optLevel <= maxOptLevel; optLevel++) {
      allOptLevelChoices[optLevel] = 
	  new VM_RecompileOptChoice(optLevel);
    }

    // Given the above choices, create lookup table so that the
    // controller's calls to
    // getViableRecompilationChoices(prevCompiler) are answered as
    // efficiently as possible.
    createViableOptionLookupTable(maxCompiler);

  }


  /**
   * Compute the set of optimization choices that should be
   * considered by the cost-benefit model.  
   *
   * @param prevCompiler The compiler compiler that was used to 
   *                     comile cmpMethod
   * @param cmpMethod The compiled method being considered
   */
  VM_RecompilationChoice[] getViableRecompilationChoices(int prevCompiler,
							  VM_CompiledMethod cmpMethod) {

     // Return the precomputed set of choices given the previous compiler
     return viableChoices[prevCompiler];
  }


  //----- Implementaiton ------

  
  /**
   * Setup a lookup table that maps a "previous compiler" to a set
   * of viable recompilation choices.  In this case, a viable choice
   * is any compiler > prevCompiler. 
   */
  protected void createViableOptionLookupTable(int maxCompiler) {
    
    viableChoices = new 
      VM_RecompilationChoice[maxCompiler][];

    // A temp place to store the list of viable choices
    VM_RecompilationChoice[] temp = new VM_RecompilationChoice[maxCompiler];

    // For each potential value of the previous compiler
    for (int prevCompiler=VM_CompilerDNA.BASELINE;
	 prevCompiler < maxCompiler;
	 prevCompiler++) {
      
      // Consider each choice in the list of all choices.
      // If it is greater than cur compiler, add it.
      int curSlot=0;
      for (int i=0; i<allOptLevelChoices.length; i++) {
	if (allOptLevelChoices[i].getCompiler() > prevCompiler) {
	  // Add the current opt-level as a choice to consider when
	  // the previous compiler is prevCompiler
	  temp[curSlot++] = allOptLevelChoices[i];
	}
      }
      
      // Now that you know how many choices there are, create an array
      // of them and copy the choices in.
      viableChoices[prevCompiler] = new VM_RecompilationChoice[curSlot];
      for (int i=0; i<curSlot; i++) {
	viableChoices[prevCompiler][i] = temp[i];
	temp[i]=null;
      }
    }
  }


  /** 
   * List of all opt-level choices that can be considered by the
   * cost-benefit model 
   */
  protected  VM_RecompileOptChoice[] allOptLevelChoices;

  /**
   * Keep a map from previous compiler to a set of recompilation
   * choices.  After initialization, viableChoices[x][y] means that if
   * x is the previous compiler, y makes sense as a possible
   * recompilation choice.
   */
  protected  VM_RecompilationChoice[][] viableChoices;



}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A recompilation choice represents an action (or a set of actions)
 * that can be considered by the controller's analytic model.
 *
 * @author Matthew Arnold
 */

abstract class VM_RecompilationChoice {

  //--- Interface ---

  /**
   * What is the cost of selecting this recompilation choice
   *
   * @param prevCompiler The previous compiler 
   * @param prevCompileTime The compile time when compiled with the
   *        previous compiler
   * @return The expected cost of exeuting this recompilation choice
   */
  abstract double getCost(int prevCompiler, double prevCompileTime);

  /**
   * What is the benefit of executing this recompilation choice, given
   * the estimated future time for the method if nothing changes?  
   *
   * @param prevCompiler The previous compiler 
   * @param futureExecutionTime The expected future execution time of
   *        the method if left running with the previous compiler.
   * @return The expected future execution time if this choice were selected 
   */
  abstract double getFutureExecutionTime(int prevCompiler, 
					 double futureExecutionTime);

  /**
   * Return a controller plan that will start this recompilation choice 
   * in action
   *
   * @param cmpMethod The method in question
   * @param prevCompiler The previous compiler
   * @param prevTimeFormethod The estimated future time had nothing been done
   * @param bestActionTime The estimated total time implementing this choice
   * @return The controller plan implementing this recompilation choice
   */
  abstract VM_ControllerPlan makeControllerPlan(VM_CompiledMethod cmpMethod,
						int prevCompiler,
						double prevTimeFormethod,
						double bestActionTime);

}







/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_Recompilation Strategy
 *
 * An abstract class providing the interface to the decision making
 * component of the controller.  There are currently two types
 * recompilation strategies implemented for the Jikes RVM, 
 * (both discussed in OOPSLA 2000 paper)
 *
 *  1) Multi-level adaptive strategy using an analytic model (see
 *     VM_AnalyticModel.java and VM_MultiLevelAdaptiveModel.java) 
 *
 *  2) Single level strategy not using a model 
 *     (See VM_SingleLevelAdaptive.java)
 *
 * @author Matthew Arnold
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

abstract class VM_RecompilationStrategy {

  //------  Interface -------

  /**
   * A hot method has been passed to the controller by an organizer
   */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {
    // Default behavior, do nothing.
    return null;
  }


  /**
   * A hot call edge has been passed to the controller by an organizer
   */
  void considerHotCallEdge(VM_CompiledMethod cmpMethod, 
			   VM_AINewHotEdgeEvent event) {
    // Default behavior, do nothing.
  }


  // -- Functionality common to all recompilation strategies (at least
  //    for now) --


  /**
   *  Initialize the recompilation strategy.
   *
   *  Note: This uses the command line options to set up the
   *  optimization plans, so this must be run after the command line
   *  options are available.  
   */
  void init() {
    createOptimizationPlans();
  }


  /**
   * This helper method creates a ControllerPlan, which contains a 
   * CompilationPlan, for the passed method using the passed optimization 
   * level and instrumentation plan.
   * 
   * @param method the VM_Method for the plan
   * @param optLevel the optimization level to use in the plan
   * @param instPlan the instrumentation plan to use
   * @param prevCMID the previous compiled method ID
   * @param expectedSpeedup  expected speedup from this recompilation
   * @param priority a measure of the oveall benefit we expect to see
   *                 by executing this plan.
   * @return the compilation plan to be used 
   */

   VM_ControllerPlan createControllerPlan(VM_Method method, 
					  int optLevel,
					  OPT_InstrumentationPlan instPlan,
					  int prevCMID,
					  double expectedSpeedup,
					  double priority) {

     // Construct the compilation plan (varies depending on strategy)
     OPT_CompilationPlan compPlan = createCompilationPlan(method,
							optLevel,
							instPlan);

    if (VM_Controller.options.ADAPTIVE_INLINING) {
      OPT_InlineOracle inlineOracle = 
	VM_AdaptiveInlining.getInlineOracle(method);
      compPlan.setInlineOracle(inlineOracle);
    }

    // Create the controller plan
    return new VM_ControllerPlan(compPlan, VM_Controller.controllerClock, 
				 prevCMID, expectedSpeedup, priority);
  }


  /**
   * Construct a compilation plan that will compile the given method
   * with instrumentation.  
   *
   * @param method The method to be compiled with instrumentation
   * @param optLevel The opt-level to recompile at 
   * @param instPlan The instrumentation plan
   */
  OPT_CompilationPlan createCompilationPlan(VM_Method method, 
					    int optLevel,
					    OPT_InstrumentationPlan instPlan) {

     // Construct a plan from the basic pre-computed opt-levels
     return new OPT_CompilationPlan(method, _optPlans[optLevel],
				    null, _options[optLevel]);
   }

  /**
   * Compute the previous compile time for the compiled code
   * associated with the argument VM_HotMethodEvent.
   *
   * @param hme the VM_HotMethodEvent
   * @param plan the VM_ControllerPlan for the compiled method (may be null)
   * @param prevCompiler the previous compiler 
   * 
   * @return previous compile time.  A value < 0.0 encodes that the 
   *         method should not be considered for recompilation.
   */
   double getPreviousCompilationTime(VM_HotMethodEvent hme, 
				     VM_ControllerPlan plan, 
				     int prevCompiler) {
    VM_Method method = hme.getMethod();
    if (plan == null) {
      // Our caller did not find a matching plan for this compiled method.
      // Therefore the code was not generated by 
      // the AOS recompilation subsystem. 
      if (VM_ControllerMemory.shouldConsiderForInitialRecompilation(method)) {
	// AOS has not already taken action to address the situation 
	// (or it attempted to take action, and the attempt failed in a way
	//  that doesn't preclude trying again,
	//  for example the compilation queue could have been full).  
	// But, since the compiled method wasn't generated by AOS, we don't 
	// have a measured compilation time, so we'll have to  
	// approximate prevCompileTime based on prevCompiler and the size of
	// the method's bytecodes in order to let the model figure out what
	// recompilation action should be taken.
	double baselineCompilationTime = 
	  ((double)method.getBytecodes().length) / 
	  VM_CompilerDNA.getBaselineCompilationRate();

	return baselineCompilationTime * 
	  VM_CompilerDNA.getCompileTimeRatio(VM_CompilerDNA.BASELINE, 
					     prevCompiler);
      } else {
	// AOS has already taken action to address the situation, and thus
	// we need to handle this as an old compiled version of a 
        // method still being live on some thread's stack.
	if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme); 
	VM_Controller.methodSamples.reset(hme.getCMID());
	return -1.0;
      }
    } else {
      // A matching plan was found.
      if (plan.getStatus() == VM_ControllerPlan.OUTDATED || 
	  VM_ControllerMemory.planWithStatus(method, 
					     VM_ControllerPlan.IN_PROGRESS)) {
	// (a) The HotMethodEvent actually corresponds to an 
        // old compiled version of the method
	// that is still live on some thread's stack or 
        // (b) AOS has already initiated a plan that hasn't
	// completed yet to address the situation. 
        // Therefore don't initiate a new recompilation action.
	if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme); 
	VM_Controller.methodSamples.reset(hme.getCMID());
	return -1.0;
      }
      if (VM_ControllerMemory.planWithStatus(method, 
			     VM_ControllerPlan.ABORTED_COMPILATION_ERROR)) {
	// AOS failed to successfully recompile this method before.  
        // Don't try it again.
	return -1.0;
      }
      // use the measured compilation time of the previous AOS 
      // recompilation of method.
      return plan.getCompilationCPUTime();
    }
  }


  /**
   *  This method returns true if we've already tried to recompile the
   *  passed method.  It does not guarantee that the compilation was
   *  successful.
   * 
   *  @param method the method of interest
   *  @return whether we've tried to recompile this method
   */
   boolean previousRecompilationAttempted(VM_Method method) {
    return  VM_ControllerMemory.findLatestPlan(method) != null;
  }

  /**
   *  This method retrieves the previous compiler constant.
   */
   int getPreviousCompiler(VM_CompiledMethod cmpMethod) {
    switch(cmpMethod.getCompilerType()) {
    case VM_CompiledMethod.TRAP: 
    case VM_CompiledMethod.JNI:
      return -1; // don't try to optimize these guys!
    case VM_CompiledMethod.BASELINE:
      { 
	// Prevent the adaptive system from recompiling certain classes
	// of baseline compiled methods.
	if (cmpMethod.getMethod().getDeclaringClass().isDynamicBridge()) {
	  // The opt compiler does not implement this calling convention.
	  return -1;
	} 
	if (cmpMethod.getMethod().getDeclaringClass().isBridgeFromNative()) {
	  // The opt compiler does not implement this calling convention.
	  return -1;
	}
	if (VM_Collector.MOVES_OBJECTS && !cmpMethod.getMethod().isInterruptible()) {
	  // A crude filter to identify the subset of core VM methods that 
	  // can't be recompiled because we require their code to be non-moving.
	  // We really need to do a better job of this to avoid missing too many opportunities.
	  return -1;
	}
	return 0;
      }
    case VM_CompiledMethod.OPT:
      VM_OptCompiledMethod optMeth = (VM_OptCompiledMethod)cmpMethod;
      return VM_CompilerDNA.getCompilerConstant(optMeth.getOptLevel());
    default:
      if (VM.VerifyAssertions) VM.assert(false, "Unknown Compiler");
      return -1;
    }
  }

  /**
   * What is the maximum opt level that is vallid according to this strategy?
   */
  int getMaxOptLevel() {
    return Math.max(VM_Controller.options.DEFAULT_OPT_LEVEL, 
		    VM_Controller.options.MAX_OPT_LEVEL);
  }


  /**
   * Create the default set of <optimization plan, options> pairs
   * Process optimizing compiler command line options.
   * <p>
   * If VM_Controller.options.ADAPTIVE_RECOMPILATION is False, 
   * then don't use cost benefit model, but recompile all methods 
   * at VM_Contoller.options.DEFAULT_OPT_LEVEL (single 
   * level adaptive).  If this is the case, then generate warning if  
   * optimizing compiler command line options for optimization levels other  
   * than VM_Controller.options.DEFAULT_OPT_LEVEL.
   */
  private  OPT_OptimizationPlanElement[][] _optPlans;
  private  OPT_Options[] _options;

   void createOptimizationPlans() {
    OPT_Options options = new OPT_Options();

    int maxOptLevel = getMaxOptLevel();
    _options = new OPT_Options[maxOptLevel+1];
    _optPlans = new OPT_OptimizationPlanElement[maxOptLevel+1][];
    String[] optCompilerOptions = VM_Controller.getOptCompilerOptions();
    for (int i=0; i<= maxOptLevel; i++) {
      _options[i] = (OPT_Options)options.clone();
      _options[i].setOptLevel(i);		// set optimization level specific optimiations
      processCommandLineOptions(_options[i],i,maxOptLevel,optCompilerOptions);
      _optPlans[i]=OPT_OptimizationPlanner.createOptimizationPlan(_options[i]);
      if (_options[i].PRELOAD_CLASS != null) {
	VM.sysWrite("In an adaptive system, PRELOAD_CLASS should be specified with -X:aos:irc not -X:aos:opt\n");
	VM.sysExit(1);
      }
    }
  }

  /**
   * Process the command line arguments and pass the appropriate ones to the 
   * OPT_Options
   * 
   * @param options The options being constructed
   * @param optLevel The level of the options being constructed
   * @param maxOptLevel The maximum valid opt level
   * @param optCompilerOptions The list of command line options
   */
  void processCommandLineOptions(OPT_Options options, int optLevel, int maxOptLevel,
			 String optCompilerOptions[]) {

    String prefix = "opt"+optLevel+":";
    for (int j=0; j<optCompilerOptions.length; j++) {
      if (optCompilerOptions[j].startsWith("opt:")) {
	String option = optCompilerOptions[j].substring(4);
	if (!options.processAsOption("-X:aos:opt:", option)) {
	  VM.sysWrite("vm: Unrecognized optimizing compiler command line argument: \""
		      +option+"\" passed in as "
		      +optCompilerOptions[j]+"\n");
	}
      } else if (optCompilerOptions[j].startsWith(prefix)) {
	if (!VM_Controller.options.ADAPTIVE_RECOMPILATION &&
	    VM_Controller.options.DEFAULT_OPT_LEVEL != optLevel) {
	  VM.sysWrite("***WARNING: a command line option for optimization "+
		      "level "+optLevel+
		      " is not allowed when single level adaptivity is "+
		      VM_Controller.options.DEFAULT_OPT_LEVEL+"\n");
	  continue;
	}
	String option = optCompilerOptions[j].substring(5);
	if (!options.processAsOption("-X:aos:"+prefix, option)) {
	  VM.sysWrite("vm: Unrecognized optimizing compiler command line argument: \""
		      +option+"\" passed in as "
		      +optCompilerOptions[j]+"\n");
	}
      }
    }
    // TODO: check for optimization levels that are invalid; that is, 
    // greater than optLevelMax.
    //
    for (int j=0; j<optCompilerOptions.length; j++) {
      if (!optCompilerOptions[j].startsWith("opt")) {
	// This should never be the case!
	continue;
      }
      if (! optCompilerOptions[j].startsWith("opt:")) {
	// must specify optimization level!
	int endPoint = optCompilerOptions[j].indexOf(":");
	if (endPoint == -1) {
	  VM.sysWrite("vm: Unrecognized optimization level in optimizing compiler command line argument: \""
		      +optCompilerOptions[j]+"\"\n");
	}
	String optLevelS;
	try {
	  optLevelS = optCompilerOptions[j].substring(3,endPoint);
	} catch (IndexOutOfBoundsException e) {
	  VM.sysWrite("vm internal error: trying to find opt level has thrown indexOutOfBoundsException\n");
	  e.printStackTrace();
	  continue;
	}
	try {
	  Integer optLevelI = new Integer(optLevelS);
	  int cmdOptLevel = optLevelI.intValue();
	  if (cmdOptLevel > maxOptLevel) {
	    VM.sysWrite("vm: Invalid optimization level in optimizing compiler command line argument: \""
			+optCompilerOptions[j]+"\"\n"+
			"  Specified optimization level "+cmdOptLevel+
			" must be less than "+maxOptLevel+"\n");
	  }
	} catch (NumberFormatException e) {
	  VM.sysWrite("vm: Unrecognized optimization level in optimizing compiler command line argument: \""
		      +optCompilerOptions[j]+"\"\n");
	}
      }
    }
    VM_RuntimeOptCompilerInfrastructure.setNoCacheFlush(options);
  }
}




/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_RecompileOptChoice 
 *
 * Represents the recompilation choice of simply recompiling the
 * method in question at a particular opt-level.  The cost is the
 * expected compilation time at that level, and the benefit is the
 * execution improvement of executing at that level.
 *
 *
 * @author Matthew Arnold */

class VM_RecompileOptChoice extends VM_RecompilationChoice {

  /**
   * Constructor
   */
  VM_RecompileOptChoice(int level) {
    this.thisChoiceOptLevel = level;
    this.thisChoiceCompiler = 
      VM_CompilerDNA.getCompilerConstant(level); 
  }

  /**
   * What is the cost of executing this plan?
   *
   * @param prevCompiler The previous compiler 
   * @param prevCompileTime The compile time when compiled with the
   *        previous compiler
   * @return The expected cost of exeuting this recompilation choice
   */
  double getCost(int prevCompiler, double prevCompileTime) {

    double compileTimeFactor = 
      VM_CompilerDNA.getCompileTimeRatio(prevCompiler, getCompiler());

    return prevCompileTime * compileTimeFactor   // compile time
      + VM_Controller.options.FIXED_RECOMPILATION_OVERHEAD;   // fixed cost "brake" 
  }


  /**
   * What is the benefit of executing this plan, given the estimated
   * future time for the method if nothing changes?
   *
   * @param prevCompiler The previous compiler 
   * @param futureExecutionTime The expected future execution time of
   *        the method if left running with the previous compiler.
   * @return The expected future execution time if this choice were selected 
   */
  double getFutureExecutionTime(int prevCompiler, 
				double futureTimeForMethod) {

    double rtFactor = 
      VM_CompilerDNA.getBenefitRatio(prevCompiler, 
				     getCompiler());
    
    return futureTimeForMethod / rtFactor; // future execution time

  }

  /**
   * Return a controller plan that will start this recompilation
   * choice in action.  In this case, simply create a plan to
   * recompile at level "optLevel"
   *
   * @param cmpMethod The method in question
   * @param prevCompiler The previous compiler
   * @param prevTimeFormethod The estimated future time had nothing been done
   * @param bestActionTime The estimated total time implementing this choice
   * @return The controller plan implementing this recompilation choice
   */
  VM_ControllerPlan makeControllerPlan(VM_CompiledMethod cmpMethod,
				       int prevCompiler, 
				       double prevTimeForMethod,
				       double bestActionTime) {
    double speedup = 
      VM_CompilerDNA.getBenefitRatio(prevCompiler, getCompiler());
    double priority = prevTimeForMethod - bestActionTime;
    return VM_Controller.recompilationStrategy.
      createControllerPlan(cmpMethod.getMethod(), thisChoiceOptLevel, 
			   null, cmpMethod.getId(), speedup, priority);

    
  }

  /**
   * How should this choice be displayed?
   */
  public String toString() {
    return "O" + getOptLevel();
  }

  /**
   * Which opt-level is associated with this choice?
   */
  int getOptLevel() {
    return thisChoiceOptLevel;
  }

  /**
   * Which "compiler" (@see VM_CompilerDNA) is associated with this choice?
   */
  int getCompiler() {
    return thisChoiceCompiler;
  }

  //----  Implementation -----

  /** The opt level associated with this recompilation choice */ 
  private int thisChoiceOptLevel;

  /** The "compiler" (see VM_CompilerDNA) that is associated with this choice */
  private int thisChoiceCompiler;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A single-level adaptive model that behaves like the "SLA"
 * strategies in the OOPSLA 2000 paper.  If a method is hot enough to
 * make it to the controller (ie, get past the organizer), it is
 * recompiled at the fixed level.
 *
 * NOTE: This strategy does NOT use a cost-benefit model.
 *
 * @author Matthew arnold 
 * @author Dave Grove 
 * @author Stephen Fink
 */

class VM_SingleLevelAdaptive extends VM_RecompilationStrategy {


  /**
   * A hot method has been passed to the controller by an organizer.
   * Optimize it!  
   */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {

    VM_Method method = cmpMethod.getMethod();

    VM_ControllerPlan plan = null;

    // Don't try to recompile if we know it won't work
    if (VM_ControllerMemory.shouldConsiderForInitialRecompilation(method)) {
      int optLevel = VM_Controller.options.DEFAULT_OPT_LEVEL;
      int prevCompiler = VM_CompilerDNA.BASELINE;
      int newCompiler = VM_CompilerDNA.getCompilerConstant(optLevel);
      double speedup = 
	VM_CompilerDNA.getBenefitRatio(prevCompiler, newCompiler);
      plan = createControllerPlan(method, optLevel,
				  null, cmpMethod.getId(),
				  speedup, 1.0);
    } else {
      if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme);
    }
    return plan;
  } 


  /**
   * What is the maximum opt level that is vallid according to this strategy?
   */
  int getMaxOptLevel() {
    return VM_Controller.options.DEFAULT_OPT_LEVEL;
  }



}


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * The representation of a call site as a triple: a caller, 
 * call site (as bytecode offset), and callee.
 *
 * SJF: I've modified this class to also hold an integer edge weight, rather
 * than keeping a lookaside hash table.  This may compromise OO design
 * a little, but should be more efficient for current clients.
 *
 * @author Peter F. Sweeney
 * @author Stephen Fink
 * @date   23 May 2000
 */

public final class VM_CallSiteTriple {

  public static final boolean DEBUG = false;
  
  /**
   * Current method.
   */
  VM_Method caller;
  /**
   * Get caller
   * @return call site's caller
   */
  public VM_Method getCaller() { return caller; }
  /**
   * Who is called.  null if not known.
   */
  VM_Method callee;
  /**
   * Get callee
   * @return call site's callee
   */
  public VM_Method getCallee() { return callee; }
  /**
   * Bytecode index of callsite in caller.
   * @return call site's bytecode index in caller
   */
  int bcIndex;		// bytecode index (in caller) of call site
  /**
   * Get call site's bytecode index
   * @ return call site's bytecode index
   */
  public int getBytecodeIndex() {return bcIndex;}
  
  /**
   * Edge weight
   */
  double weight;
  public double getWeight() { return weight; }
  public void setWeight(double w) { weight = w; }
  public void incrementWeight() { weight+=1.0; }
  
  /**
   * Decay the weight
   * @param rate the value to decay by
   */
  public void decayWeight(double rate) { 
    weight /= rate; 
  }
  
  /**
   * Constructor
   * @param caller call site caller
   * @param bcIndex bytecode index of call site
   * @param callee call site callee
   */
  VM_CallSiteTriple(VM_Method caller, int bcIndex, VM_Method callee) 
  {
    this.caller  = caller;
    this.bcIndex = bcIndex;
    this.callee  = callee;
  }
   
  /**
   * Generate string representation of a call site
   * @return string representation of call site
   */
  public String toString() {
    return " <"+caller+", "+bcIndex+", "+callee+">"+"(wt:"+weight+")";
  }
  /**
   * Determine if two call sites are the same.  Exact match: no wild cards.
   *
   * @param obj call site to compare to
   * @return true if call sites are the same; otherwise, return false
   */
  public boolean equals(Object obj) {
    if (!(obj instanceof VM_CallSiteTriple)) return false;
    if (obj == null) return false;
    VM_CallSiteTriple triple = (VM_CallSiteTriple)obj;
    boolean returnValue = false;
    try {
      returnValue = (caller.toString().compareTo(triple.caller.toString())==0);
      if (returnValue == true) {
	returnValue = (callee.toString().compareTo(triple.callee.toString())==0);
	if (returnValue == true) {
	  if (bcIndex != triple.bcIndex && bcIndex != -1 && triple.bcIndex != -1) {
	    returnValue = false;
	  }
	}
      }
    } catch (NullPointerException e) {
      VM.sysWrite("***VM_CallSiteTriple.equals("+obj+
		  ") compareTo of names failed!\n");
      returnValue = false;
    }
    return returnValue;
  }
   
  /**
   * Compute a call site's hash code
   *
   * @return hash code
   */
  public int hashCode() {
    int result = 7;
    if (caller != null) result += caller.hashCode();
    if (callee != null) result += callee.hashCode();
    result += bcIndex;
    return result;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
  * This class implements the Comparator comparing two 
  * VM_CallSiteTriple objects.
  *
  * We can either choose to implement a partial order based on
  * the String representation, or by weight.
  *
  * @author Peter Sweeney
  * @modified Stephen Fink
  * @modified Michael Hind
  * @date   25 May 2000
  */

import java.util.*;

class VM_CallSiteTripleComparator implements java.util.Comparator {

  public static boolean debug = false;
  
  /**
   * Boolean flag that determines if the comparison of two call sites
   * takes into account their weights.
   */
  private boolean byWeight = false;

  /** Interface */
    
  /**
   * Constructor
   */
  VM_CallSiteTripleComparator(boolean byWeight) {
    this.byWeight = byWeight;
  }
  /**
   * Constructor
   */
  VM_CallSiteTripleComparator() {
    this.byWeight = false;
  }

  /** 
   * Compare the string representation of two triples
   *
   * @returns -1 iff o1 < o2
   * @returns +1 iff o1 > o2
   * @returns 0 if o1 and o2 are the same callsites or 
   *		if they only differ by their bytecode indices and 
   *		one index is -1.
   */
  private int compareByName(VM_CallSiteTriple t1, VM_CallSiteTriple t2) 
  {
    String s1 = t1.toString();
    String s2 = t2.toString();

    return s1.compareTo(s2);
  }
  /** 
   * Compare the weight of two triples
   *
   * @returns -1 iff w(t1) < w(t2)
   * @returns +1 iff w(t1) > w(t2)
   * @returns compareByName(t1,t2) otherwise
   */
  private int compareByWeight(VM_CallSiteTriple t1, VM_CallSiteTriple t2) 
  {
    double w1 = t1.getWeight();
    double w2 = t2.getWeight();

    if (w1 < w2) return -1;
    if (w1 > w2) return 1;
    return compareByName(t1,t2);
  }
  /** 
   * @param o1, o2 two VM_CallSiteTriple to be compared.
   * @returns -1 iff o1 < o2
   * @returns +1 iff o1 > o2
   * @returns 0 if o1 and o2 are the same callsites or 
   *		if they only differ by their bytecode indices and 
   *		one index is -1.
   */
  public int compare(Object o1, Object o2) 
  {
    // This should never happen!
    if (!(o1 instanceof VM_CallSiteTriple) || 
	!(o2 instanceof VM_CallSiteTriple)) {
      VM.sysWrite("***VM_CallSiteTripleComparator.compare():"+
		  " one object is not of type VM_CallSiteTriple\n");
      VM.sysExit(-1);
    }

    if (o1.equals(o2)) return 0;

    VM_CallSiteTriple t1 = (VM_CallSiteTriple)o1;
    VM_CallSiteTriple t2 = (VM_CallSiteTriple)o2;
    if (byWeight) return compareByWeight(t1,t2);
    else return compareByName(t1,t2);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A partial call graph (PCG) is implemented as a set of edges and each
 * edge is represented as a VM_CallSiteTriple object.
 * A PCG's internal representation is a HashMap of edges:
 *	hashCodeOf(E) -> E where 
 *             E is VM_CallSiteTriple<caller,bytecodeOffset,callee>
 *
 * @author Peter F. Sweeney
 * @modified Michael Hind
 * @date   24 May 2000
 */

public final class VM_PartialCallGraph implements VM_Decayable {

  public static final boolean DEBUG = false;
  
  /**
   * Constructor
   */
  VM_PartialCallGraph() {
    findTriples = new java.util.HashMap();
    totalEdgeWeights = 0.0;
  }
  
  /**
   * Return an iterator over the edges in the PCG
   *
   * @return iterator over edges 
   */
  java.util.Iterator getEdges() {
    return findTriples.values().iterator();
  }
  
  /**
   *  Visit each edge and decay its weight. 
   */
  public void decay() { 
    if (DEBUG) {VM.sysWrite(" Before decay\n");  dump(); }
    
    double rate = VM_Controller.options.AI_DECAY_RATE;
    
    synchronized(findTriples) {
      for (java.util.Iterator iterator = getEdges(); iterator.hasNext();) {
	VM_CallSiteTriple triple = (VM_CallSiteTriple)iterator.next();
	triple.decayWeight(rate);
      }
      totalEdgeWeights /= rate;
    }
    
    if (DEBUG) {VM.sysWrite(" After decay\n");  dump(); }
  }
  
  /**
   * Increment the edge represented by the input parameters, 
   * creating it if needed.
   *
   * @param caller   method making the call
   * @param bytecode call site, if -1 then no call site is specified.
   * @param callee   method called
   */
  public void incrementEdge(VM_Method caller, int bcIndex, VM_Method callee) {
    
    VM_CallSiteTriple triple = findOrCreateEdge(caller, bcIndex, callee);
    
    triple.incrementWeight();
    totalEdgeWeights += 1.0;
  }
  
  /**
   * Find the edge in the partial call graph, if not found add it.
   *
   * @param caller   method making the call
   * @param bytecode call site, if -1 then no call site is specified.
   * @param callee   method called
   * @return         edge
   */
  public VM_CallSiteTriple findOrCreateEdge(VM_Method caller, 
					    int bcIndex, 
					    VM_Method callee) 
  {
    if (findTriples == null) {
      VM.sysWrite("FIND TRIPLES NULL");
    }
    if(DEBUG)VM.sysWrite(" VM_PartialCallGraph.findEdge("+caller+", "+
			 callee+", "+bcIndex+") entered\n");
    if (caller == null) {
      VM.sysWrite("***Error: VM_PartialCallGraph.findEdge("+caller+", "+
		  callee+") has null caller!\n");
      new Exception().printStackTrace();
    }
    if (callee == null) {
      VM.sysWrite("***Error: VM_PartialCallGraph.findEdge("+caller+", "+
		  callee+") has null callee!\n");
      new Exception().printStackTrace();
    }
    VM_CallSiteTriple triple = new VM_CallSiteTriple(caller, bcIndex, callee);
    
    synchronized(findTriples) {
      if (findTriples.containsKey(triple)) {
	if(DEBUG) VM.sysWrite
		    ("  VM_PartialCallGraph.findEdge() edge already called!\n");
	triple = (VM_CallSiteTriple)findTriples.get(triple);
      } else {
	if(DEBUG) VM.sysWrite
		    ("  VM_PartialCallGraph.findEdge() FIRST time edge called!\n");
	findTriples.put(triple,triple);
      }
    }
    
    if(DEBUG)VM.sysWrite(" VM_PartialCallGraph.increment() exit\n");
    return triple;
  }
  
  /**
   * Dump out set of edges in sorted order.
   */
  public void dump() {
    VM.sysWrite("VM_PartialCallGraph.dump()\n");
    VM.sysWrite("  Number of edges "+findTriples.size()+", total weight: "+totalEdgeWeights+"\n");
    java.util.TreeSet treeSet = new java.util.TreeSet(new VM_CallSiteTripleComparator(true));
    try {
      synchronized(findTriples) {
	treeSet.addAll(findTriples.values());
      }
    } catch (ClassCastException e) {
      VM.sysWrite("***VM_PartialCallGraph.dump(): addAll threw CallCastException!\n");
      VM.sysExit(-1);
    }
    int i=0;
    for (java.util.Iterator iterator = treeSet.iterator(); iterator.hasNext();) {
      VM_CallSiteTriple triple = null;
      try {
	triple = (VM_CallSiteTriple)iterator.next();
      } catch (java.util.NoSuchElementException e) {
	VM.sysWrite("***OPT_PCG.dump(): iterator.next() returns NoSuchElementException!\n");
	VM.sysExit(-1);
      }
      i++;
      VM.sysWrite(i+": "+triple.toString()+"\n");
    }
  }
  
  /**
   * Get sum of all edge weights in the partial call graph
   * @return edge weight sum
   */
  double getTotalEdgeWeights() { return totalEdgeWeights; }
  /*
   * hashcodeOf(callers,call site,callees) -> triple
   */
  private java.util.HashMap findTriples;
  /*
   * sum of all edge weights in the graph
   */
  private double totalEdgeWeights;
  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_AOSDatabase.java
 * 
 * Used to keep track of the various data structures that make up the
 * AOS database.  
 *
 * @author Matthew Arnold 
 */
public final class VM_AOSDatabase 
{
  /** 
   * Static links to data objects that are "whole-program" (as opposed
   * to per-method)
    */
  static VM_MethodInvocationCounterData methodInvocationCounterData;
  static VM_YieldpointCounterData yieldpointCounterData;
  static VM_StringEventCounterData instructionCounterData;
  static VM_StringEventCounterData debuggingCounterData;
 
  /**
   * Called at startup
   **/
  static void boot(VM_AOSOptions options)
  {
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;

/**
 * A container for recording how often a method is executed.
 *
 * @author Dave Grove
 * @author Michael Hind
 * @modified Peter Sweeney
 */
public final class VM_MethodCountData 
  implements VM_Decayable, VM_Reportable {

  private static final boolean DEBUG = false;
  
  /**
   * Sum of values in count array that are decayed over time.
   */
  private double totalCountsTaken;

  /**
   * How many counts have really been taken (ignoring decay).
   */
  private double undecayedTotalCountsTaken;

  /**
   * Count array: counts how many times a method is executed.
   * Constraint: counts[0] is not used.
   */
  private double[] counts;
  /**
   * Maps count array index to compiled method id.
   * Constraint: cmids[0] is not used.
   */
  private int[] cmids;
  /**
   * Maps compiled method id to count array index.
   * '0' implies that there is no entry in the count array for this cmid
   */
  private int[] map;
  /**
   * Next available count array entry.
   */
  private int nextIndex;
  
  /**
   * Constructor
   */
  VM_MethodCountData() {
    initialize();
  }

  /**
   * Reset fields.
   */
  private void initialize() {
    int numCompiledMethods = VM_CompiledMethods.numCompiledMethods();
    map = new int[ numCompiledMethods + (numCompiledMethods >>> 2) ];
    counts = new double[256];
    cmids = new int[256];
    nextIndex = 1;
    totalCountsTaken = 0;
    undecayedTotalCountsTaken = 0;
  }

  /** 
   * Drain a buffer of compiled method id's and update the count array.
   *
   * @param countBuffer a buffer of compiled method id's
   * @param numCounts the number of valid entries in the buffer
   */
  public final synchronized void update(int[] countBuffer, int numCounts) {
    for (int i=0; i<numCounts; i++) {
      int cmid = countBuffer[i];
      int index = findOrCreateHeapIdx(cmid);
      counts[index]++;      // Record count
      heapifyUp(index);     // Fix up the heap
    }
    totalCountsTaken += numCounts;
    undecayedTotalCountsTaken += numCounts;
    if (DEBUG) validityCheck();
  }

  /**
   * Increment the count for a compiled method id.
   *
   * @param cmid compiled method id
   * @param numCounts number of counts
   */
  public final synchronized void update(int cmid, double numCounts) {
    int index = findOrCreateHeapIdx(cmid);
    counts[index] += numCounts;       // Record counts
    heapifyUp(index);                 // Fix up the heap
    totalCountsTaken += numCounts;
    undecayedTotalCountsTaken += numCounts;
    if (DEBUG) validityCheck();
  }

  /**
   * Decay the method counts.
   */
  public final synchronized void decay() {
    double rate = VM_Controller.options.DECAY_RATE;
    for (int i=1; i<nextIndex; i++) {
      counts[i] /= rate;
    }
    totalCountsTaken /= rate;
  }


  /** 
   *  Print the counted (nonzero) methods.
   *  To get a sorted list, pipe the output through sort -n -r.
   */
  public final synchronized void report() {
    VM.sysWrite("Method counts: A total of "+totalCountsTaken+
		" times counted (undecayed  "+undecayedTotalCountsTaken+")\n");
    for (int i=1; i<nextIndex; i++) {
      double percent = 100 * countsToHotness(counts[i]);
      VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmids[i]);
      VM.sysWrite(counts[i] + " ("+percent+"%) ");
      if ( cm == null ) {
        VM.sysWrite("OBSOLETE");		// Compiled Method Obsolete
      } else {
        VM_Method m = cm.getMethod();
        VM.sysWrite(m);
        if (m.getDeclaringClass().isInBootImage()) {
	  VM.sysWrite("\n\tBOOT");
        }
      }
      VM.sysWrite("\n");
    }    
  }

  /**
   * Reset (clear) the method counts
   */
  public final synchronized void reset() {
    initialize();
  }

  /**
   * Get the current count for a given compiled method id.
   *
   * @param cmid compiled method id
   */
  public final synchronized double getData(int cmid) {
    int index = findHeapIdx(cmid);
    if (index > 0) {
      return counts[index];
    } else {
      return 0.0;
    }
  }

  /**
   * Reset (set to 0.0) the count for a given compiled method id.
   *
   * @param cmid compiled method id
   */
  public final synchronized void reset(int cmid) {
    int index = findHeapIdx(cmid);
    if (index > 0) {
      // Cmid does have a value in the heap. 
      // (1) clear map[cmid].
      // (2) shrink the heap by one slot.  
      //     (a) If index is the last element in the heap we have nothing 
      //         to do after we decrement nextIndex.
      //     (b) If index is not the last element in the heap, then move the
      //         last heap element to index and heapify.
      map[cmid] = 0;
      nextIndex--;
      if (index < nextIndex) {
	double oldValue = counts[index];
	counts[index] = counts[nextIndex];
	cmids[index] = cmids[nextIndex];
	map[cmids[index]] = index;
	if (counts[index] > oldValue) {
	  heapifyUp(index);
	} else {
	  heapifyDown(index);
	}
      } 
    }
    if (DEBUG) validityCheck();
  }

  /**
   * Set the raw data for a given cmid to the specified value
   *
   * @param cmid compiled method id
   * @param newVal new value for compiled method id count
   */
  public final synchronized void setData(int cmid, double newVal) {
    if (newVal == 0.0) {
      reset(cmid); // Prefer reset, since it frees up a slot in the heap.
    } else {
      int index = findOrCreateHeapIdx(cmid);
      double oldVal = counts[index];
      counts[index] = newVal;
      if (newVal > oldVal) {
	heapifyUp(index);
      } else {
	heapifyDown(index);
      }
    }
    if (DEBUG) validityCheck();
  }

  /**
   * Enqueue events describing the "hot" methods on the organizer's event queue.
   *
   * @param filterOptLevel filter out all methods already compiled at 
   *                       this opt level (or higher)
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   */
  public final synchronized void insertHotMethods(int filterOptLevel, 
						  double threshold) {
    if (DEBUG) validityCheck();
    insertHotMethodsInternal(1, filterOptLevel, hotnessToCounts(threshold));
  }


  /**
   * Collect the hot methods that have been compiled at the given opt level.
   *
   * @param optLevel  target opt level
   * @param threshold hotness value above which the method is considered to
   *                  be hot. (0.0 to 1.0)
   * @return a VM_MethodCountSet containing an
   * 		array of compiled methods and an array of their counts.
   * 
   */
  public final synchronized VM_MethodCountSet collectHotMethods(int optLevel, 
								double threshold) {
    if (DEBUG) validityCheck();
    Vector collect = new Vector();
    collectHotOptMethodsInternal(1, collect, hotnessToCounts(threshold), optLevel);

    // now package the data into the form the caller expects.
    int numHotMethods = collect.size();
    double[] numCounts = new double[numHotMethods];
    VM_CompiledMethod[] hotMethods = new VM_CompiledMethod[numHotMethods];
    for (int i=0; i<numHotMethods; i++) {
      VM_HotMethodEvent event = (VM_HotMethodEvent)collect.elementAt(i);
      hotMethods[i] = event.getCompiledMethod();
      numCounts[i] = event.getNumSamples();
    }
    return new VM_MethodCountSet(hotMethods, numCounts);
  }

  /** 
   * Convert from a [0.0...1.0] hotness value to the number of counts
   * that represents that fraction of hotness
   *
   * @param hotness a value [0.0...1.0]
   * @return a number of counts
   */
  private double hotnessToCounts(double hotness) {
    return totalCountsTaken * hotness;
  }

  /**
   * Convert a value to a [0.0...1.0] fractional hotness value
   *
   * @param numCounts number of counts
   * @return a value [0.0...1.0]
   */
  private double countsToHotness(double numCounts) {
    if (VM.VerifyAssertions) VM.assert(numCounts <= totalCountsTaken);
    return numCounts / totalCountsTaken;
  }

  /**
   * Convert a possibly decayed numCounts into an
   * "undecayed" count value for use in model calculations of the 
   * time spent executing in this method so far.
   *
   * @param numCounts number of decayed counts
   * @return number of undecayed counts
   */
  private double numCountsForModel(double numCounts) {
    return undecayedTotalCountsTaken * countsToHotness(numCounts);
  }

  /**
   * Recursive implementation of insertHotMethods. Exploit heap property.
   * Note threshold has been converted into a count value by my caller!
   *
   * @param index count array index
   * @param filterOptLevel filter out all methods already compiled at 
   *                       this opt level (or higher)
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   */
  private void insertHotMethodsInternal(int index, 
					int filterOptLevel, 
					double threshold) {
    if (index < nextIndex) {
      if (counts[index] > threshold) {
	int cmid = cmids[index];
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm == null) {			// obsolete and deleted
	  reset(cmid);				// free up this slot
	  // Visit new one in the slot
	  insertHotMethodsInternal(index, filterOptLevel, threshold);
	} else {
	  int compilerType = cm.getCompilerType();
	  // Enqueue it unless it's either a trap method or already 
	  // opt compiled at filterOptLevel or higher.
	  if (!(compilerType == VM_CompiledMethod.TRAP ||
		(compilerType == VM_CompiledMethod.OPT && 
		 (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {
	    double ns = numCountsForModel(counts[index]);
	    VM_HotMethodRecompilationEvent event = 
	      new VM_HotMethodRecompilationEvent(cm, ns);
	    if (VM_Controller.controllerInputQueue.prioritizedInsert(ns, event)){
	      if (VM.LogAOSEvents) {
		VM_AOSLogging.controllerNotifiedForHotness(cm, ns);
	      }
	    } else {
	      if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	    }
	  }
	
	  // Since I was hot enough, also consider my children.
	  insertHotMethodsInternal(index * 2, filterOptLevel, threshold);
	  insertHotMethodsInternal(index * 2 + 1, filterOptLevel, threshold);
	}
      }
    }
  }

  /**
   * Recursive implementation of collectHotOptNMethods. 
   * Exploit heap property. 
   * Constraint: threshold has been converted into a count value by my caller!
   *
   * @param index count array index
   * @param collect vector used to collect output.
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   * @param optLevel target opt level to look for.
   */
  private void collectHotOptMethodsInternal(int index, 
					    Vector collect, 
					    double threshold, 
					    int optLevel) {
    if (index < nextIndex) {
      if (counts[index] > threshold) {
	int cmid = cmids[index];
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm == null) {			// obsolete and deleted
	  reset(cmid);				// free up this slot
	  // Visit new one in the slot
	  collectHotOptMethodsInternal(index, collect, threshold, optLevel);
	} else {
	  int compilerType = cm.getCompilerType();
	  if (compilerType == VM_CompiledMethod.OPT && 
	      ((VM_OptCompiledMethod)cm).getOptLevel() == optLevel) {
	    double ns = numCountsForModel(counts[index]);
	    collect.add(new VM_HotMethodRecompilationEvent(cm, ns));
	  }
	
	  // Since I was hot enough, also consider my children.
	  collectHotOptMethodsInternal(index * 2, collect, threshold, optLevel);
	  collectHotOptMethodsInternal(index * 2 + 1, collect, threshold, optLevel);
	}
      }
    }
  }

  /**
   * Either find the index that is already being used to hold the counts
   * for cmid or allocate a new entry in the heap for cmid.
   *
   * @param cmid compiled method id
   * @return count array index
   */
  private int findOrCreateHeapIdx(int cmid) {
    if (cmid >= map.length) {
      growHeapMap(cmid);
    }
    int index = map[cmid];
    if (index == 0) {
      // A new cmid. Allocate a heap entry for it.
      index = nextIndex++;
      if (index >= counts.length) {
	growHeap();
      }
      counts[index] = 0.0;
      cmids[index] = cmid;
      map[cmid] = index;
    }
    return index;
  }
    
    
    /**
   * Find the index that is already being used to hold the counts for cmid.
   * If no such index exists, return 0.
   *
   * @param cmid compiled method id
   */
  private int findHeapIdx(int cmid) {
    if (cmid < map.length) {
      int index = map[cmid];
      return index;
    } else {
      return 0;
    }
  }


  /**
   * Grow the map to be at least as large as would be required to map cmid
   *
   * @param cmid compiled method id
   */
  private void growHeapMap(int cmid) {
    int[] newMap = new int[Math.max((int)(map.length * 1.25), cmid+1)];
    for (int j=0; j<map.length; j++) {
      newMap[j] = map[j];
    }
    map = newMap;
  }

  /**
   * Increase the size of the count's backing arrays
   */
  private void growHeap() {
    double[] tmp1 = new double[counts.length * 2];
    for (int i=1; i< counts.length; i++) {
      tmp1[i] = counts[i];
    }
    counts = tmp1;
    int[] tmp2 = new int[cmids.length * 2];
    for (int i=1; i< cmids.length; i++) {
      tmp2[i] = cmids[i];
    }
    cmids = tmp2;
  }

  /**
   * Restore the heap property after increasing a count array entry's value
   *
   * @param index of count array entry
   */
  private void heapifyUp(int index) {
    int current = index;
    int parent = index / 2;
    while (parent > 0 && counts[parent] < counts[current]) {
      swap(parent, current);
      current = parent;
      parent = parent / 2;
    }
  }

  /**
   * Restore the heap property after decreasing a count array entry's value
   *
   * @param index of count array entry
   */
  private void heapifyDown(int index) {
    int current = index;
    int child1 = current * 2;
    while (child1<nextIndex) {
      int child2 = current * 2 + 1;
      int larger = 
	(child2<nextIndex && counts[child2]>counts[child1]) ? child2 : child1;
      if (counts[current] >= counts[larger]) break; // done
      swap(current, larger);
      current = larger;
      child1 = current * 2;
    }
  }

  /**
   * Swap the heap entries at i and j.
   *
   * @param i count array index
   * @param j count array index
   */
  private void swap(int i, int j) {
    double tmpS = counts[i];
    counts[i] = counts[j];
    counts[j] = tmpS;
    int tmpC = cmids[i];
    cmids[i] = cmids[j];
    cmids[j] = tmpC;
    map[cmids[i]] = i;
    map[cmids[j]] = j;
  }

  /**
   * Validate that internal fields are consistent.
   * This is very expensive.  Only use for debugging purposes.
   */
  private void validityCheck() {
    if (DEBUG && VM.VerifyAssertions) {
      // (1) Verify map and cmids are in synch
      for (int i=0; i<map.length; i++) {
	VM.assert(map[i] == 0 || cmids[map[i]] == i);
      }
      for (int i=1; i<nextIndex; i++) {
	VM.assert(map[cmids[i]] == i);
      }

      // Verify that heap property holds on data.
      for (int i=2; i<nextIndex; i++) {
	VM.assert(counts[i] <= counts[i/2]);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$
/**
 * Wrapper around a pair of parallel arrays:
 *  (1) an array of compiled method id's
 *  (2) an array of counts: how many times each compiled method id is counted
 *
 * @author Dave Grove 
 * @modified Peter Sweeney
 */
public final class VM_MethodCountSet {
  /**
   * array of compiled methods
   */
  VM_CompiledMethod[] cms;
  /**
   * array of counts
   */
  double[] counters;

  /**
   * Constructor
   *
   * @param _cms array of compiled method ids
   * @param _counters array of counters
   */
  VM_MethodCountSet(VM_CompiledMethod[] _cms, double[] _counters) {
    if (VM.VerifyAssertions) VM.assert(_cms.length == _counters.length);
    cms = _cms;
    counters= _counters;
  }

  /**
   * String representation of fields
   * 
   * @return string representation of compiled method id's and thier counts
   */
  public String toString() {
    String ans = "";
    for (int i=0; i<cms.length; i++) {
      ans += cms[i] + " = " + counters[i] + "\n";
    }
    return ans;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Vector;
import java.util.Enumeration;


/** 
 *
 * OPT_InsertInstructionCounters.java
 *
 * The following OPT phase inserts counters on all instructions in the
 * IR.  It maintians one counter for each operand type, so it output
 * how many loads were executed, how many int_add's etc.  This is
 * useful for debugging and assessing the accuracy of optimizations.
 *
 * Note: The counters are added at the end of HIR, so the counts will
 * NOT reflect any changes to the code that occur after HIR.
 * 
 * @author Matthew Arnold 
 *
 **/

class OPT_InsertInstructionCounters  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_INSTRUCTION_COUNTERS;
   }

   final String getName() { return "InsertInstructionCounters"; }

   /**
    * Insert a counter on every instruction, and group counts by
    * opcode type.  
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // the boot image, or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 ir.method.getDeclaringClass().isInBootImage() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     // Get the data object that handles the counters
     VM_StringEventCounterData data = 
       VM_AOSDatabase.instructionCounterData;

     // Create a vector of basic blocks up front because the blocks
     // are modified as we iterate below.
     Vector bbList = new Vector();
     for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	  bbe.hasMoreElements(); ) {
       OPT_BasicBlock bb = bbe.next();
       bbList.add(bb);
     }
     
     // Iterate through the basic blocks
     for (Enumeration e = bbList.elements();
	  e.hasMoreElements(); ) {
       OPT_BasicBlock bb = (OPT_BasicBlock) e.nextElement();
       
       // Add instructions to vector so enumeration doesn't mess
       // things up.  There is probably a better way to do this, but
       // it doesn't matter because this is a debugging phase.
       Vector iList = new Vector();
       OPT_Instruction i = bb.firstInstruction();
       while (i!=null && i!=bb.lastInstruction()) {
	 iList.add(i);
	 i = i.nextInstructionInCodeOrder();
       }
       
       // Iterate through all the instructions in this block.
       for (Enumeration instructions = iList.elements();
	    instructions.hasMoreElements();) {
	 i = (OPT_Instruction) instructions.nextElement();

	 // Skip dangerous instructions
	 if (i.operator() == LABEL ||
	     Prologue.conforms(i))
	   continue;
	 
	 if (i.isBranch() || 
	     i.operator() == RETURN) {
	   
	   // It's a branch, so you need to be careful how you insert the 
	   // counter.
	   OPT_Instruction prev = i.prevInstructionInCodeOrder();
	   
	   // If the instruction above this branch is also a branch,
	   // then we can't instruction as-is because a basic block
	   // must end with branches only.  Solve by splitting block.
	   if (prev.isBranch()) {
	     OPT_BasicBlock newBlock = bb.splitNodeWithLinksAt(prev,ir);
	     bb.recomputeNormalOut(ir);
	   }
	   
	   // Use the name of the operator as the name of the event
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(i.operator().toString());
	   
	   // Insert the new instruction into the code order
	   i.insertBefore(counterInst);      
	 }
	 else {
	   // It's a non-branching instruction.  Insert counter after
	   // the instruction.
	   
	   // Use the name of the operator as the name of the event
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(i.operator().toString());

	     i.insertBefore(counterInst);
	 }
       }
     }
   }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 *
 * OPT_InsertMethodInvocationCounter.java
 *
 * An OPT_Phase that inserts a method invocation counter on the first
 * basic block of the method.  It uses a
 * VM_InstrumentedEventCounterManager to obtain the space to put the
 * counters.
 *
 * Note: one counter data, (VM_MethodInvocationCounterData) is shared
 * across all methods, and is initialized at boot time.  This is
 * unlike other kinds of instrumentation (such as basic block
 * counters) where a separate data object is maintained for each
 * method.
 *
 * @author Matthew Arnold 
 *
 **/

class OPT_InsertMethodInvocationCounter  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {
   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_METHOD_COUNTERS_OPT;
   }

   final String getName() { return "InsertMethodInvocationCounters"; }

   /**
    * Insert basic block counters
    * 
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     OPT_BasicBlock firstBB = ir.cfg.entry();

     VM_MethodInvocationCounterData data = 
       VM_AOSDatabase.methodInvocationCounterData;

     int cmid = ir.compiledMethod.getId();

     // Create a dummy instruction that is later converted into an
     // increment of the appropriate VM_CounterArray element.
     OPT_Instruction c = data.createEventCounterInstruction(cmid);

     // Insert it at the beginnging of the basic block
     firstBB.prependInstructionRespectingPrologue(c);
   }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/** 
 *
 * OPT_InsertYieldPointCounters.java
 *
 *
 * An opt compiler phase that inserts yieldpoint counters.  Searches
 * for all yieldpoint instructions and inserts an increment after
 * them, using the VM_CounterArrayManager counter manager to implement
 * the counters.
 *
 * @author Matthew Arnold 
 */

class OPT_InsertYieldpointCounters  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_YIELDPOINT_COUNTERS;
   }

   final String getName() { return "InsertYieldpointCounters"; }

   /**
    * counters after all yieldpoint instructions
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // the boot image, or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 ir.method.getDeclaringClass().isInBootImage() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     VM_YieldpointCounterData data = 
       VM_AOSDatabase.yieldpointCounterData;

     if (OPT_InsertYieldpointCounters.DEBUG) {
       VM.sysWrite("OPT_InsertYieldpointCounters.perform() " + 
		   ir.method + "\n");
     }
     // For each yieldpoint, insert a counter.
     for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	  bbe.hasMoreElements(); ) {
       OPT_BasicBlock bb = bbe.next();

       if (OPT_InsertYieldpointCounters.DEBUG) {
	 VM.sysWrite("Considering basic block " + bb.toString() + "\n");
       	  bb.printExtended();
       }
       
       OPT_Instruction i =  bb.firstInstruction();
       while (i!=null && i!=bb.lastInstruction()) {

	 if (i.operator() == YIELDPOINT_PROLOGUE ||
             i.operator() == YIELDPOINT_EPILOGUE ||
             i.operator() == YIELDPOINT_BACKEDGE) {
 	   String prefix = yieldpointPrefix(i.operator());
	   double incrementValue = 1.0;
 	   if (bb == ir.cfg.entry()) {
 	     prefix = "METHOD ENTRY ";
	   }
	   else {
	     prefix = "BACKEDGE ";
	     incrementValue=1.0;  
	   }

	   // Create an instruction to increment the counter for this
	   // method.  By appending the prefix and method name, it
	   // maintains a separate counter for each method, and
	   // separates between method entry and backedges.
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(prefix+ir.method.toString(),
					   incrementValue);

	   // Insert the new instruction into the code order
	   i.insertAfter(counterInst);      
	 }

	 i = i.nextInstructionInCodeOrder();
       }
     }
   }

  /**
   * Return a string based version of the passed yieldpoint operator
   * @param op the yieldpoint operator
   * @return a string based on the type of yieldpoint operator
   */
  private static String yieldpointPrefix(OPT_Operator op) {
    if (op == YIELDPOINT_PROLOGUE) return "Prologue";
    if (op == YIELDPOINT_EPILOGUE) return "Epilogue";
    if (op == YIELDPOINT_BACKEDGE) return "Backedge";
    return "ERROR";
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.util.Enumeration;
import java.util.Vector;
import instructionFormats.*;

/** 
 *  OPT_LowerInstrumentation
 *
 *  This phase takes converts "instrumentation instructions" that were
 *  inserted by previous instrumentation phases and "lowers" them,
 *  converting them to the actual instructions that perform the
 *  instrumentation.
 *
 *  @author Matthew Arnold
 *
 **/

class OPT_LowerInstrumentation  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     if (options.INSERT_INSTRUCTION_COUNTERS ||
	 options.INSERT_METHOD_COUNTERS_OPT ||
	 options.INSERT_DEBUGGING_COUNTERS ||
	 options.INSERT_YIELDPOINT_COUNTERS)
     return true;
    return false;
   }

   final String getName() { return "LowerInstrumentation"; }

   /**
    * Finds all instrumented instructions and calls the appropriate code to 
    * convert it into the real sequence of instrumentation instructions.
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {
     // Convert all instrumentation instructions into actual counter code
     lowerInstrumentation(ir);

     // TODO: For efficiency, should proably call OPT_Simple, or
     // branch optimizations or something.
   }
  
  /**
   * Actually perform the lowering
   *
    * @param ir the governing IR
   */ 
  static final void lowerInstrumentation(OPT_IR ir) {
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      //bb.printExtended();
    }
    
    Vector vector = new Vector();
    
    // Go through all instructions and find the instrumented ones.
    // We put them in a vector and expand them later because if we
    // expanded them on the fly we mess up the enumeration.
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      
      OPT_Instruction i = bb.firstInstruction();
      while (i!=null && i!=bb.lastInstruction()) {
	
	if (i.operator() == INSTRUMENTED_EVENT_COUNTER) {
	  vector.add(i);
	}
	i = i.nextInstructionInCodeOrder();
      }
    }
    
    // Now go through the instructions and "lower" them by calling
    // the counter manager to convert them into real instructions
    boolean didSomething = false;
    Enumeration e = vector.elements();
    while (e.hasMoreElements()) {
      OPT_Instruction i = (OPT_Instruction) e.nextElement();
      
      // Have the counter manager for this data convert this into the
      // actual counting code.  For now, we'll hard code the counter
      // manager.  Ideally it should be stored in the instruction,
      // (to allow multipe counter managers.  It would also make this
      // code independant of the adaptive system..)
      OPT_InstrumentedEventCounterManager counterManager = 
	VM_Instrumentation.eventCounterManager;
      
      counterManager.mutateOptEventCounterInstruction(i,ir);
      didSomething=true;
    }
    
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      //       bb.printExtended();
    }
  } // end of perform
  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * VM_AOSInstrumentationPlan.java
 *
 * Defines: 
 * class VM_AOSInstrumentationPlan
 *
 * An instance of this class is created for each method that is
 * instrumented by the adaptive system.  It serves as a place to put
 * information that is needed by the instrumentation phases.  Is is
 * different from an OPT_InstrumentationPlan because it contains
 * information that the non-adaptive opt-compiler can't see.
 *
 *
 * @author Matthew Arnold
 *
 **/

class VM_AOSInstrumentationPlan extends OPT_InstrumentationPlan
{

  
  /**
   * Construct empty plan, must setup manually
   **/ 
  VM_AOSInstrumentationPlan(VM_Method method) {
    this.method = method;
  }

  /**
   * Construct based on options
   **/ 
  VM_AOSInstrumentationPlan(VM_AOSOptions options, VM_Method method)
  {
    // If we want to collect method invocation counts.
    if (options.INSERT_METHOD_COUNTERS_OPT) {
    }
  }

  /** 
   * Initialize instrumentation by the opt compiler immediately before
   * compilation begins.
   **/
  void initInstrumentation(VM_Method method)
  {
  }

  /** 
   * Called after compilation is complete.  If instrumentation has
   * occured, perform some cleanup/finalization
   **/

  void finalizeInstrumentation(VM_Method method)
  {

  }

  /** The method that this plan is for */
  private VM_Method method;
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  This class is a separate thread whose job is to monitor a (priority)
 *  queue of compilation plans.  Whenever the queue is nonempty, this
 *  thread will pick the highest priority compilation plan from the queue
 *  and invoke the OPT compiler to perform the plan.
 *
 *  No intelligence is contained in this class.  All policy decisions are
 *  made by the controllerThread.
 *
 *  @author Michael Hind
 *  @author David Grove
 */
class VM_CompilationThread extends VM_Thread {

  /**
   * This is the main loop of the compilation thread. It's job is to 
   * remove controller plans from the compilation queue and perform
   * them.
   */
  public void run() {
    if (VM.LogAOSEvents) VM_AOSLogging.compilationThreadStarted();

    // Make a blocking call to deleteMin to get a plan and then execute it. 
    // Repeat...
    while (true) {
      VM_ControllerPlan plan = 
	(VM_ControllerPlan)VM_Controller.compilationQueue.deleteMin();
      recompile(plan);
    }
  }

  /**
   * This method will recompile the method designated by the passed 
   * controller plan.  It also 
   *  1) credits the samples associated with the old compiled method
   *     ID to the new method ID and clears the old value.
   *  2) clears inlining information
   *  3) updates the status of the controller plan
   * @param plan the controller plan to use for the recompilation
   */
  private void recompile(VM_ControllerPlan plan) {
    OPT_CompilationPlan cp = plan.getCompPlan();

    plan.setTimeInitiated(VM_Controller.controllerClock);
    if (VM.LogAOSEvents) VM_AOSLogging.recompilationStarted(cp); 

    if (cp.options.PRINT_METHOD) {
      VM.sysWrite("-oc:O"+cp.options.getOptLevel()+" \n");
    }
    
    // must hold classloader lock while compiling.
    // Update compilation thread timing information to prepare for new run.
    double now = VM_Time.now();
    cpuTotalTime += (now - cpuStartTime);
    cpuStartTime = now;
    double start = cpuTotalTime;

    // Compile the method.
    int newCMID = VM_RuntimeOptCompilerInfrastructure.recompileWithOpt(cp);

    // Update compilation thread timing information and compute time 
    // taken during this compilation.
    now = VM_Time.now();
    cpuTotalTime += (now - cpuStartTime);
    cpuStartTime = now;
    double end = cpuTotalTime;
    double compileTime = (end - start) * 1000.0; // Convert seconds to milliseconds.
      
    // transfer the samples from the old CMID to the new CMID.
    // scale the number of samples down by the expected speedup 
    // in the newly compiled method.
    int prevCMID = plan.getPrevCMID();
    double expectedSpeedup = plan.getExpectedSpeedup();
    double oldNumSamples = VM_Controller.methodSamples.getData(prevCMID);
    double newNumSamples = oldNumSamples / expectedSpeedup;
    VM_Controller.methodSamples.reset(prevCMID);
    if (newCMID > -1) {
      VM_Controller.methodSamples.setData(newCMID, newNumSamples);
    }

    // set the status of the plan accordingly
    if (newCMID != -1) {
      plan.setStatus(VM_ControllerPlan.COMPLETED);
      VM_AdaptiveInlining.clearNonInlinedEdges(prevCMID);
    } else {
      plan.setStatus(VM_ControllerPlan.ABORTED_COMPILATION_ERROR);
    }

    plan.setCMID(newCMID);
    plan.setCompilationCPUTime(compileTime);
    plan.setTimeCompleted(VM_Controller.controllerClock);
    if (VM.LogAOSEvents) {
      if (newCMID == -1) {
	VM_AOSLogging.recompilationAborted(cp);
      } else {
	VM_AOSLogging.recompilationCompleted(cp);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.*;
import java.util.*;

/**
 * This class codifies the cost/benefit properties of the various compilers
 * used in the adaptive optimization system.
 *
 * @author: Michael Hind
 */
class VM_CompilerDNA implements VM_Constants {

  private static final String[] compilerNames = {"Baseline", "Opt0", "Opt1", "Opt2"};
  final static int BASELINE = 0;
  final static int OPT0 = 1;
  final static int OPT1 = 2;
  final static int OPT2 = 3;

  /**
   *  The number of compilers available
   */
  private static int numCompilers;

  /**
   *  Average bytecodes compiled per millisec
   */
  //-#if RVM_FOR_AIX
  /*
   *  These numbers were from a shadow on June 28, 2002 on AIX/PPC (munchkin)
   */
  private static final double[] compilationRates = {431.78, 9.72, 4.04, 1.44};
  //-#else
  /*
   *  These numbers were from a shadow on July 7, 2002 on Linux/IA32 (turangalila)
   */
  private static final double[] compilationRates = {742.78, 15.68, 6.25, 2.52};
  //-#endif

  /**
   * What is the execution rate of each compiler normalized to the 1st compiler
   */
  //-#if RVM_FOR_AIX
  /*
   *  These numbers were from a shadow on June 28, 2002 on AIX/PPC (munchkin)
   */
  private static final double[] speedupRates = {1.00, 4.09, 5.27, 5.66};
  //-#else
  /*
   *  These numbers were from a shadow on July 7, 2002 on Linux/IA32 (turangalila)
   */
  private static final double[] speedupRates = {1.00, 3.55, 4.68, 4.75};
  //-#endif

  /**
   * Benefits of moving from one compilation level to another
   * USAGE NOTE: The data is layed out in a upper triangular matrix
   */
  private static double[][] benefitRatio;

  /**
   * Compile time ratio of one compilation level to another
   * For example, if compiler1 (say OPT1) compiles at 50 bc/msec
   * and compiler2 (say OPT2) compiles at 100 bc/msec, 
   *    compileTimeRatio[OPT1][OPT2] = 2
   * USAGE NOTE: The data is layed out in a upper triangular matrix 
   */
  private static double[][] compileTimeRatio;

  /**
   * This method returns the expected speedup from going from compiler1 to compiler2
   * @param compiler1
   * @param compiler2
   * @return the benefit ratio (speedup) of moving from compiler1 to compiler2
   */
  static public double getBenefitRatio(int compiler1, int compiler2) {
    return benefitRatio[compiler1][compiler2];
  }

  /**
   * What is the additional overhead (relative to compiler1 compile time)
   * of compile2 compile time.  For example, if compiler1 compiles at
   * 50 bc/msec and compiler2 compiles at 100 bc/msec, this method returns 2
   * @param compiler1 the compiler whose compile time we compare to
   * @param compiler2 the compiler's compile time we care about 
   * @return the additional overhead (relative to compiler1 compile time)
   * of compile2 compile time
   */
  static public double getCompileTimeRatio(int compiler1, int compiler2) {
    return compileTimeRatio[compiler1][compiler2];
  }

  /**
   * Returns the compilation rates of the baseline compiler in 
   *  bytecodes/millisecond
   * @return the compilation rates of the baseline compiler in 
   *   bytecodes/millisecond
   */
  static public double getBaselineCompilationRate() {
    return compilationRates[BASELINE];
  }

  /**
   * initialize static fields
   */
  static void init()  { 
    // check to see if the raw rates are specified during boot time
    if (VM_Controller.options.USE_COMPILER_DNA_FILE) {
      //  Read the DNA values from disk
      readDNA();
    }

    numCompilers = compilerNames.length;

    benefitRatio = new double[numCompilers][numCompilers];
    compileTimeRatio = new double[numCompilers][numCompilers];

    if (VM.LogAOSEvents) {
      for (int i=0; i < compilationRates.length; i++) {
	VM_AOSLogging.reportCompilationRate(i, compilationRates[i]);
      }
      for (int i=0; i < speedupRates.length; i++) {
	VM_AOSLogging.reportSpeedupRate(i, speedupRates[i]);
      }
    }

    // fill in the upper triangular matrices
    for (int prevCompiler = 0; 
	 prevCompiler < numCompilers; 
	 prevCompiler++) {

      benefitRatio[prevCompiler][prevCompiler] = 1.0;
      compileTimeRatio[prevCompiler][prevCompiler] = 1.0;

      for (int nextCompiler = prevCompiler+1; 
	   nextCompiler < numCompilers; 
	   nextCompiler++) {

	benefitRatio[prevCompiler][nextCompiler] = 
	  speedupRates[nextCompiler] / speedupRates[prevCompiler];

	// Since compilation rates are not relative to the 1st compiler
	//  we invert the division.
	compileTimeRatio[prevCompiler][nextCompiler] = 
	  compilationRates[prevCompiler] / compilationRates[nextCompiler];  

	if (VM.LogAOSEvents) {
	  VM_AOSLogging.reportBenefitRatio(
			   prevCompiler, nextCompiler,
			   benefitRatio[prevCompiler][nextCompiler]);

	  VM_AOSLogging.reportCompileTimeRatio(
			   prevCompiler, nextCompiler,
			   compileTimeRatio[prevCompiler][nextCompiler]);
	}
	
      }
    }
  }


  /** 
   * Read a serialized representation of the DNA info
   */
  static private void readDNA() {
    try {

      LineNumberReader in =
	new LineNumberReader(new FileReader(VM_Controller.options.COMPILER_DNA_FILE_NAME));

      // Expected Format
      //   CompilationRates  aaa.a  bbbb.b cccc.c dddd.d ....
      //   SpeedupRates      aaa.a  bbbb.b cccc.c dddd.d ....
      processOneLine(in, "CompilationRates", compilationRates);
      processOneLine(in, "SpeedupRates", speedupRates);
    }
    catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to open controller DNA file");
    }
  }

  /**
   *  Helper method to read one line of the DNA file
   *  @param in the LineNumberReader object
   *  @param title the title string to look for
   *  @param valueHolder the array to hold the read values
   */
  static private void processOneLine(LineNumberReader in, String title,
				     double[] valueHolder) throws IOException {

    String s = in.readLine();
    if (VM.VerifyAssertions) VM.assert(s != null);
    
    // parse the string
    StringTokenizer parser = new StringTokenizer(s);
    
    // make sure the title matches
    String token = parser.nextToken();
    if (VM.VerifyAssertions) VM.assert(token.equals(title));
    
    // walk through the array, making sure we still have tokens
    for (int i=0;
	 parser.hasMoreTokens() && i < valueHolder.length;
	 i++) {

      // get the available token
      token = parser.nextToken();
      
      // convert token to a double
      valueHolder[i] = Double.valueOf(token).doubleValue();
    }
  }

  /**
   * returns the number of compilers 
   * @return the number of compilers 
   */
  static public int getNumberOfCompilers() {
    return numCompilers;
  }


  /**
   * A mapping from an Opt compiler number to the corresponding Opt level
   * @param compiler the compiler constant of interest
   * @return the Opt level that corresponds to the Opt compiler constant passed
   */
  public static int getOptLevel(int compiler) {
    switch (compiler) {
      case BASELINE: return -1;
      case OPT0: return 0;
      case OPT1: return 1;
      case OPT2: return 2;
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown compiler constant\n");
	return -99;
    }
  }

  /**
   * maps a compiler constant to a string
   * @param compiler
   * @return the string that represents the passed compiler constant
   */
  public static String getCompilerString(int compiler) {
    return compilerNames[compiler];
  }

  /**
   * maps opt levels to the compiler
   * @param optLevel opt level
   * @return the opt level that corresponds to the passed compiler constant
   */
  public static int getCompilerConstant(int optLevel) {
    switch (optLevel) {
      case 0: return OPT0;
      case 1: return OPT1;
      case 2: return OPT2;
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown Opt Level\n");
	return -99;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import instructionFormats.*;

/**
 * VM_CounterArrayManager.java
 *
 * An implementation of a OPT_InstrumentedEventCounterManager .  It
 * uses an unsynchronized two dimensional array of doubles to allocate
 * it's counters. (see OPT_InstrumentedEventCounterManager.java for a
 * description of a counter manager)
 * 
 * NOTE: Much of this class was stolen from VM_CounterArray.java, which
 * is now gone.
 *
 * @author Matthew Arnold
 *
 **/


final class VM_CounterArrayManager extends OPT_InstrumentedEventCounterManager
  implements OPT_Operators, OPT_Constants {

  static final boolean DEBUG=false;

  /**
   *  This method is called my a VM_ManagedData object to obtain space
   *  in the counter manager.  A handle or "ID" is returned for the
   *  data to identify it's counter space.
   *
   * @param countersNeeded The number of counters being requested 
   * @return The handle for this data's counter space.
   **/
  synchronized public int registerCounterSpace(int countersNeeded) {
    if (counterArrays.length == numCounterArrays) {
      expandCounterArrays();
    }

    // return the handle of the next available counter array
    int handle = numCounterArrays;

    // resize the appropriate counter array
    resizeCounterSpace(handle,countersNeeded);

    numCounterArrays++;

    return handle;
  }

  /**
   *  This method is called to change the number of counters needed by
   *  a particular data.
   *
   * @param handle  The handle describing which the data to be resized
   * @param countersNeeded The number of counters being requested 
   **/
  synchronized public void resizeCounterSpace(int handle, int countersNeeded) {
    // allocate the new array
    double[] temp = new double[countersNeeded];
    
    // transfer the old data to the new array
    if (counterArrays[handle] != null) {
      for (int i=0; i<counterArrays[handle].length; i++) {
	temp[i] = counterArrays[handle][i];
      }
    }
    
    // switch to the new counter array
    counterArrays[handle] = temp;
  }


  /**
   * Return the value of a particular counter
   *
   * @param handle The handle describing which the data to look in
   * @param index The relative index number of the counter
   * @return The value of the counter
   */
  public double getCounter(int handle, int index) {
    return counterArrays[handle][index];
  }

  /**
   * Set the value of a particular counter
   *
   * @param handle The handle describing which the data to look in
   * @param index The relative index number of the counter
   * @param value The new value of the counter
   */
  public void setCounter(int handle, int index, double value) {
    counterArrays[handle][index] = value;
  }


  /**
   * Create a place holder instruction to represent the counted event.
   *
   * @param counterHandle  The handle of the array for the method
   * @param index  Index within that array
   * @param incrementValue The value to add to the counter
   * @return The counter instruction
   **/
  public OPT_Instruction createEventCounterInstruction(int handle, int index,
						       double incrementValue) {

    // Doubles are annoying. They are too big to fit into the
    // instruction, so they must be loaded from the JTOC.  That means
    // we need to make sure the increment value is actually in the
    // JTOC.

    long l = Double.doubleToLongBits(incrementValue);
    int offset = VM_Statics.findOrCreateDoubleLiteral(l);

    // Now create the instruction to be returned.
    OPT_Instruction c = 
      InstrumentedCounter.create(INSTRUMENTED_EVENT_COUNTER, 
				 new OPT_IntConstantOperand(handle),
				 new OPT_IntConstantOperand(index),
				 new OPT_DoubleConstantOperand(incrementValue,
							       offset));
    c.bcIndex = INSTRUMENTATION_BCI;

    return c;
  }


  /**
   *  Take an event counter instruction and mutate it into IR
   *  instructions that will do the actual counting.
   *
   *  Precondition: IR is in LIR
   *
   * @param s The counter instruction to mutate
   * @param ir The governing IR
   **/
  public void mutateOptEventCounterInstruction(OPT_Instruction counterInst, 
					       OPT_IR ir) {
    if (VM.VerifyAssertions)
      VM.assert(InstrumentedCounter.conforms(counterInst));

    OPT_IntConstantOperand intOp =
      InstrumentedCounter.getData(counterInst);
    int handle = intOp.value;
    intOp = InstrumentedCounter.getIndex(counterInst);
    int index = intOp.value;

    // Get the base of array
    OPT_RegisterOperand counterArray =  OPT_ConvertToLowLevelIR.
      getStatic(counterInst, ir, VM_Entrypoints.counterArrayManagerCounterArraysField);

    // load counterArrays[handle]
    OPT_RegisterOperand array2 =
      InsertALoadOffset(counterInst,
                        ir, REF_ALOAD,
                        VM_Type.JavaLangObjectType,
                        counterArray, handle);
    OPT_ConvertToLowLevelIR.
      doArrayLoad(counterInst.prevInstructionInCodeOrder(), ir, INT_LOAD, 2);
                                                                               
    // load counterArrays[handle][index]
    OPT_RegisterOperand origVal =
      InsertALoadOffset(counterInst,
                        ir, DOUBLE_ALOAD,
                        VM_Type.DoubleType,
                        array2, index);
    OPT_ConvertToLowLevelIR.
      doArrayLoad(counterInst.prevInstructionInCodeOrder(),ir, DOUBLE_LOAD, 3);

    
    OPT_Operand incOperand = InstrumentedCounter.getIncrement(counterInst);
    // Insert increment instruction
    OPT_RegisterOperand newValue =
      OPT_ConvertToLowLevelIR.InsertBinary(counterInst, ir, DOUBLE_ADD,
                                           VM_Type.DoubleType, origVal,
                                           incOperand.copy());

    // Store it
    OPT_Instruction store = AStore.mutate(counterInst,DOUBLE_ASTORE,
                                         newValue, array2.copyU2D(),
                                         OPT_IRTools.I(index),null,null);
    OPT_ConvertToLowLevelIR.doArrayStore(store, ir, DOUBLE_STORE, 3);
                                       
  }

  /**
   * Insert array load off before s in the instruction stream.
   * @param s the instruction to insert before
   * @param ir the containing IR
   * @param operator the operator to insert
   * @param type the type of the result
   * @param reg2 the base to load from
   * @param offset the offset to load at
   * @return the result operand of the inserted instruction
   */
  static OPT_RegisterOperand InsertALoadOffset (OPT_Instruction s, OPT_IR ir,
                                                OPT_Operator operator,
                                                VM_Type type,
                                                OPT_Operand reg2,
                                                int offset){
    OPT_RegisterOperand regTarget = ir.regpool.makeTemp(type);
    OPT_Instruction s2 = ALoad.create(operator, regTarget, reg2,
                                      OPT_IRTools.I(offset),
                                      null,null);
    s.insertBack(s2);
    return  regTarget.copyD2U();
  }    


  /**
   * Still  under construction.
   */
  public void insertBaselineCounter()
  {
  }

  /**
   * decay counters
   *
   * @param handle, the identifier of the counter array to decay
   * @param rate, the rate at which to decay, i.e. a value of 2 will divide
   *                all values in half
   */
  static void decay(int handle, double rate) {
    int len = counterArrays[handle].length;
    for (int i=0; i<len; i++) {
      counterArrays[handle][i] /= rate;
    }
  }
 
  /** Implementation */
  static final int INITIAL_COUNT = 10;
  static final int INCREMENT = 10;
  static int numCounterArrays = 0;
  static double[][] counterArrays = new double[INITIAL_COUNT][];

  /**
   * increment the number of counter arrays
   */
  private static void expandCounterArrays() {
    // expand the number of counter arrays
    double[][] temp = new double[counterArrays.length*2][];

    // transfer the old counter arrays to the new storage
    for (int i=0; i<counterArrays.length; i++) {
      temp[i] = counterArrays[i];
    }
    counterArrays = temp;
  }

} // end of class



/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_CounterNameFunction.java
 *
 * @author Stephen Fink
 *
 * This interface defines a function that takes an integer and
 * returns a string corresponding to that integer.
 *
 **/

interface VM_CounterNameFunction {

   String getName(int key);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_Instrumentation.java
 *
 * This class is used to provide general functionality useful to
 * instrumenting methods.
 *
 * @author Matthew Arnold
 *
*/

final class VM_Instrumentation
{

  /**
   * A pointer to a OPT_InstrumentedEventCounterManager, (See
   * VM_InstrumentedEventCounterManager.java for the idea behind a
   * counter manager) There can be multiple managers in use at the
   * same time (for example, one per method)., but for now we just use
   * one for everything.
   **/
  public static OPT_InstrumentedEventCounterManager eventCounterManager;


  /**
   * Called at boot time
   **/
  static void boot(VM_AOSOptions options) 
  {
    
    // If the system may perform any instrumentation that uses managed
    // event counters, initialize a counter manager here.  
    if (options.INSERT_INSTRUCTION_COUNTERS ||
	options.INSERT_METHOD_COUNTERS_OPT ||
	options.INSERT_YIELDPOINT_COUNTERS ||
	options.INSERT_DEBUGGING_COUNTERS) {
      eventCounterManager = new VM_CounterArrayManager();
    }

    // If inserting method counters, initialize the counter space for
    // the invocation counters, using the eventCounterManager from above.
    if (options.INSERT_METHOD_COUNTERS_OPT) {
      VM_AOSDatabase.methodInvocationCounterData = 
	new VM_MethodInvocationCounterData(eventCounterManager);

      // Method Counters have only one array of counters for the whole
      // program, so initialize it here. Make it automitacally double
      // in size when needed.
      VM_AOSDatabase.methodInvocationCounterData.
	automaticallyGrowCounters(true);

      // Report at end
      VM_RuntimeMeasurements.
	registerReportableObject(VM_AOSDatabase.methodInvocationCounterData);
    }

    /**
     * If collecting yieldpoint counts, initialize the 
     * data here.
     **/
    if (options.INSERT_YIELDPOINT_COUNTERS) {
      // Create it here, because we need only one array of numbers,
      // not one per method.
      VM_AOSDatabase.yieldpointCounterData = 
	new VM_YieldpointCounterData(eventCounterManager);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.yieldpointCounterData);

    }

    /**
     * If collecting instruction counts, initialize the 
     * data here.
     **/
    if (options.INSERT_INSTRUCTION_COUNTERS) {
      VM_AOSDatabase.instructionCounterData = 
	new VM_StringEventCounterData(eventCounterManager,
				      "Instruction Counter");
      VM_AOSDatabase.instructionCounterData.automaticallyGrowCounters(true);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.instructionCounterData);
    }

    /**
     * If collecting instruction counts, initialize the 
     * data here.
     **/
    if (options.INSERT_DEBUGGING_COUNTERS) {
      VM_AOSDatabase.debuggingCounterData = 
	new VM_StringEventCounterData(eventCounterManager,
				      "Debugging Counters");
      VM_AOSDatabase.debuggingCounterData.automaticallyGrowCounters(true);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.debuggingCounterData);
    }

  }

  /**
   * Calling this routine causes all future compilations not to insert
   * instrumentation, regardless of what the options say.  Used during
   * system shutdown.  Note, this method will not stop instrumentation
   * in currently compiled methods from executing.
   * 
   */
  static void disableInstrumentation() {
    instrumentationEnabled=false;
  }

  /**
   * Enable instrumentations, so that future compilations will not
   * perform any instrumentation.
   * 
   */
  static void enableInstrumentation() {
    instrumentationEnabled=true;
  }

  /**
   * Is it currently O.K. to compile a method and insert instrumentation?
   */
  static boolean instrumentationEnabled() {
    return instrumentationEnabled;
  }
  static private boolean instrumentationEnabled=true;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import java.util.Vector;

/**
 * VM_ManagedCounterData
 *
 * This class provides the basic functionality for instrumented data
 * that use counters allocated from a VM_InstrumentedEventCounterManager.  
 * It provides the basic interface to access counters,  forwarding
 * those requests to the counter manager.
 *
 * @author Matthew Arnold
 *
**/

class VM_ManagedCounterData
{

  static final boolean DEBUG=false;

  /**
   *  Constructor
   *
   * @param counterManager The counterManager that will provide the counter space
   */
  VM_ManagedCounterData(OPT_InstrumentedEventCounterManager counterManager) 
  {
    // Basic block instrumentation is performed using a common counter
    // allocation for the whole method.  It requests that space here.
    this.counterManager = counterManager;
  }


  /**
   * This method must be called before creating any counters for this
   * data.  It registers this data with the counter manager and gets a
   * "handle" that is coded into the counter instruction.  If you need
   * to change the number of counters in this data AFTER you have
   * created counters, use void
   * VM_ManagerdCounterData.resizeCounters(int) instead.
   *
   * @param countersNeeded How many counters are needed by this data
   */
  public void initializeCounters(int countersNeeded)
  {
    // Confirm that this method is called only once.  Once a handle is
    // assigned, it should not be changed.  Use resizeCounters(int) to
    // change the size of the data.
    if (VM.VerifyAssertions)
      VM.assert(handle == -1);

    this.numCounters = countersNeeded;
    // Register  this many counters with the counter manager
    this.handle = counterManager.registerCounterSpace(countersNeeded);
  }

  /** 
   * Tell the data to automatically expand the counters if there is a
   * request to count an event that is greater than the current size.
   *
   * @param autoGrow Whether the counters should grow automatically. 
   */
  public void automaticallyGrowCounters(boolean autoGrow){

    final int INITIAL_COUNTER_SIZE = 20;

    automaticallyGrowCounters = autoGrow;
    if (automaticallyGrowCounters)
      initializeCounters(INITIAL_COUNTER_SIZE);
  }

  /**
   * Used to reset the number of counters for this data
   * 
   * @param countersNeeded The number of counters needed
   */
  public void resizeCounters(int countersNeeded)
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    
    counterManager.resizeCounterSpace(this.getHandle(),countersNeeded);
    numCounters = countersNeeded;
  }

  /**
   * Return the count for the given (relative) index
   * 
   * @param counterNumber The event number within the data
   * @return The count associated with this counter
   */
  public double getCounter(int counterNumber)
  {
    // Confirm that counters have been initialized 
    //  (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    return counterManager.getCounter(this.getHandle(), counterNumber);
  }

  /**
   * Set the count for the given index
   * 
   * @param counterNumber The event number within the data
   * @param value The new value of the counter
   */
  public void setCounter(int counterNumber, double value)
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions) {
      VM.assert(handle != -1);
    }
    if (counterNumber >= getNumCounters()) {
      if (automaticallyGrowCounters) {
	while (counterNumber >= getNumCounters()) 
	  resizeCounters(getNumCounters()*2);
      }
      else {
	VM.assert(false);
      }
    }

    counterManager.setCounter(this.getHandle(), counterNumber, value);
  }


  /**
   * Return the number of counters currently allocated for this data
   *
   *  @return the number of counters 
   */
  public int getNumCounters()
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    return numCounters;
  }

  /**
   * Counter Managers give id's that identify the counter space they
   * have given to each data. This method returns that ID. 
   *
   * @return The handle given to this data object by the counter manager.
   **/
  public int getHandle()
  {
    return handle;
  }
  
  /**
   * Return the counter manager for this data.
   * 
   * @return the counter manager object
   */
  public OPT_InstrumentedEventCounterManager getCounterManager()
  {
    return counterManager;
  }

  /**
   * Create a place holder instruction to represent an increment of a
   * particular counted event.  Simply forwards the request to the
   * counter manager.
   *
   * @param counterNumber The number of the counter to increment 
   * @return The instruction that will update the given counter
   */
  OPT_Instruction createEventCounterInstruction(int counterNumber) {
    return createEventCounterInstruction(counterNumber,1.0);
  }

  /**
   * Create a place holder instruction to represent the counted event.
   * Simply forwards the request to the counter manager.
   *
   * @param counterNumber The number of the counter to increment 
   * @param incrementValue The value to add to the given counter
   * @return The instruction that will update the given counter
   */
  OPT_Instruction createEventCounterInstruction(int counterNumber,
						double incrementValue)
  {
    // Confirm that counters have been initialized 
    if (VM.VerifyAssertions) {
      VM.assert(handle != -1);
    }

    // If we automatically growing counters, see if we need to.
    if (counterNumber >= numCounters) {
      if (automaticallyGrowCounters) {
	while (counterNumber >= numCounters)
	  resizeCounters(getNumCounters()*2);
      }
      else{
	// Should we put a warning here?? Not sure.  
      }
    }
    return getCounterManager().createEventCounterInstruction(getHandle(),
							     counterNumber,
							     incrementValue);
  }

   /**
    *  This method prints the (sorted) nonzero elements a counter
    *  array.
    *
    * @param handle identifier of the counter array to print
    * @param f a function that gets the "name" for each counter
    */
   final void report(VM_CounterNameFunction f) {
    double sum = 0;
    Vector vec = new Vector();

    // set up a vector of non-zero counts
    for (int i=0; i < getNumCounters(); i++) {
      double count = getCounter(i);
      if (count > 0.0) {
	sum += count;
	String s = f.getName(i);
	vec.addElement(new Counter(s,count));
      }
    }

    // sort the vector in decreasing order
    sort(vec);

    // print
    for (Enumeration e = vec.elements(); e.hasMoreElements(); ) {
       Counter c = (Counter)e.nextElement();
       String s = c.name;
       double count = c.count;
       double percent = (100 * count) / sum;
       VM.sysWrite(count + "/" + sum + " = " + percent + "% " + s + "\n");
    }
  }

  /**
   * Sort a Vector<Counter> by decreasing count.
   * (code borrowed from InstructionSampler.java)
   * Shell sort
   * Reference: "The C Programming Language", Kernighan & Ritchie, p. 116
   */
  private void sort(Vector v) {
     int n = v.size();
     for (int gap = n/2; gap > 0; gap /= 2) {
       for (int i = gap; i<n; ++i) {
          for (int j = i-gap; j >=0; j-=gap) {
	     double a = ((Counter)v.elementAt(j)).count;
	     double b = ((Counter)v.elementAt(j+gap)).count;
	     if (a>=b) break;
	     swap(v,j,j+gap);
	  }
       }
     }
  }
  
  // Interchange vec[i] with vec[j]
  private void swap (Vector vec, int i, int j) {
     Object t = vec.elementAt(i);
     vec.setElementAt(vec.elementAt(j),i);
     vec.setElementAt(t,j);
  }

 

  /* -----   Implementation   ---- */

  /**
   * How many counters are needed by this data?
   **/
  protected int numCounters=0;

  /**
   *  When a data object is registered with a counter manager, it is
   *  given an id, which is stored here.
   **/
   
   protected int handle=-1;

  /**
   * Basic block instrumentation stores its counters using an
   * abstracted counter allocation technique (a counterManager)
   **/
  protected OPT_InstrumentedEventCounterManager counterManager=null;
  
  protected boolean automaticallyGrowCounters = false;

 /**
  * Auxiliary class
  */
 class Counter {
     String name;
     double count;
     Counter(String s, double c) { name=s; count = c; }
 }

}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_MethodInvocationCounterData.java
 *
 * An instance of this class is used to store method counters.  It is
 * initialized at startup, and instrumentation phase
 * OPT_InsertMethodInvocationCounter.java inserts instrumentation that
 * writes into this data.
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;

final class VM_MethodInvocationCounterData extends VM_ManagedCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_MethodInvocationCounterData(OPT_InstrumentedEventCounterManager manager)
  {
    // Call superclass constructor
    super(manager);
  }

  /**
   *  Part of VM_Reportable interface.  Called on system exit
   **/
  public void report()
  {
    super.report(new VM_MethodNameFunction());
  }

  /**
   *  Part of VM_Reportable interface
   **/
  public void reset()  
  { 
    VM.assert(false, "TODO: implement reset for VM_BasicBlockCounterDatabase"); 
  }

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_MethodNameFunction.java
 *
 * @author Stephen Fink
 *
 * This class takes a compiled method id and returns a string
 * representation of the method name.
 *
 **/

class VM_MethodNameFunction implements VM_CounterNameFunction {

   /**
    * @param key the compiled method id of a method
    */
   public String getName(int key) {
     VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(key);
     if (cm == null) {
       return "OBSOLETE";
     } else {
       return cm.getMethod().toString();
     }
   }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_StringEventCounterData.java
 * 
 * A generic data object that maps strings to counters.  The key
 * method is "OPT_Instruction getCounterInstructionForEvent(String)"
 * which, given a string, returns a counter instruction that
 * increments the corresponding counter for that string.
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;
import java.util.Enumeration;

class VM_StringEventCounterData extends VM_ManagedCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_StringEventCounterData(OPT_InstrumentedEventCounterManager manager,
			    String name)
  {
    // Call superclass constructor
    super(manager);
    
    dataName = name;
  }

  /**
   * Given a string, find or create the counter associated and return
   * and instruction to increment that counter.  
   *
   * @param event The name of the event
   * @return An instruction to increment the count associated with the event.
   */
  OPT_Instruction getCounterInstructionForEvent(String event) {
    return getCounterInstructionForEvent(event,1.0);
  }

  /**
   * Given a string, find or create the counter associated and return
   * and instruction to increment that counter.  
   *
   * @param event The name of the event
   * @param incrementValue The value to add to counter
   * @return An instruction that will update the count associated with the event.
   *
   */
  OPT_Instruction getCounterInstructionForEvent(String event, 
						double incrementValue) {

    // Get (or create) the counter for this string and return it.
    int counterIdx = getOrCreateCounterIndexForString(event);

    return createEventCounterInstruction(counterIdx,incrementValue);
  }

  /**
   * Convert a double to string with maximum precision.
   * @param num double to convert
   */
  protected static String doubleToString(double num) {
    long whole = (long)num;
    if (whole == Long.MAX_VALUE || whole == Long.MIN_VALUE)
      return Double.toString(whole);
    double fract = Math.abs(num - (double)whole);
    String res = Long.toString(whole);
    if (fract != 0.0) {
      String f2s = Double.toString(fract + 1.0);
      res += f2s.substring(1);
    }
    return res;
  }

  /**
   * Part of VM_Reportable interface
   * Print a report at the end of execution
   */
  public void report()
  {
    // Turn off future instrumentation to avoid hanging during 
    // iteration
    VM_Instrumentation.disableInstrumentation();

    VM.sysWrite("Printing " + dataName + ":\n");
    VM.sysWrite("--------------------------------------------------\n");
    double total=0;
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();

      int counterIdx = getCounterIndexForString(stringName);
      double counterVal = getCounter(counterIdx);
      VM.sysWrite(doubleToString(counterVal) + " " + stringName + "\n");
      total += counterVal;
    }
    VM.sysWrite("Total: " + doubleToString(total) + "\n");
  }

  /**
   * For a given string, return the number of the counter associated
   * with this string.  If this string doesn't already have a counter, 
   * reserve one. 
   *
   * @param str The string for which you want the counter number
   * @return The counter number for this string

   */
  public int getOrCreateCounterIndexForString(String str) {

    int counterIdx = getCounterIndexForString(str);
    if (counterIdx == -1) {
      // Use new counter
      counterIdx = ++ eventNumber;
      // remember it, and return it
      stringToCounterMap.put(str,new Integer(eventNumber));
    }

    return counterIdx;
  }


  /**
   * For a given string, return the number of the counter associated
   * with this string.  Ideally this number would be completely hidden
   * from the outside world, but for efficiency it is made public.
   *
   * @param str The string for which you want the counter number
   * @return The counter number for this string, or -1 if the string has no 
             counter associated with it. 
   */
  public int getCounterIndexForString(String str) {

    int counter = -1;
    Integer counterNum = (Integer) stringToCounterMap.get(str);
    if (counterNum != null) 
      counter = counterNum.intValue();

    return counter;
  }

  /**
   *  Part of VM_Reportable interface
   **/
  public void reset() { 
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();
      int counterIdx = getCounterIndexForString(stringName);
      setCounter(counterIdx, 0.0);
    }
  }

 /** 
  *  Map strings to a counter location
  */
  protected  Hashtable stringToCounterMap = new Hashtable();

  /**
   * A string description of this data;
   */
  String dataName= "";

  /** 
   * Used to keep track of how many counters have been used so far.
   */ 
  int eventNumber=-1;

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_YieldpointCounterData.java
 *
 * An extension of VM_StringEventCounterData so that the printing can
 * be specialized for yieldpoints.  Otherwise, the functionality is
 * identical.
 *
 * @see VM_StringEventCounterData.java
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;
import java.util.Enumeration;

final class VM_YieldpointCounterData extends VM_StringEventCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_YieldpointCounterData(OPT_InstrumentedEventCounterManager manager)
  {
    // Call superclass constructor
    super(manager,"Yieldpoint Counter");

    automaticallyGrowCounters(true);
  }

  /**
   *  Called at end when data should dump it's contents.
   */
  public void report()
  {
    // Turn off future instrumentation so that the data structures do
    // not change while we are iterating over them
    VM_Instrumentation.disableInstrumentation();

    VM.sysWrite("Printing " + dataName + ":\n");
    VM.sysWrite("--------------------------------------------------\n");
    double total=0;
    double methodEntryTotal=0;
    double backedgeTotal=0;
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();

      Integer counterNum = (Integer) stringToCounterMap.get(stringName);
      double count = getCounter(counterNum.intValue());

      VM.sysWrite(count + " " + stringName + "\n");
      total += count;
      
      // If it's a method entry event
      if (stringName.indexOf("METHOD ENTRY") != -1)
	methodEntryTotal += count;
      
      if (stringName.indexOf("BACKEDGE") != -1)
	backedgeTotal += count;

    }
    VM.sysWrite("Total backedges: " + backedgeTotal + "\n");
    VM.sysWrite("Method Entry Total: " + methodEntryTotal + "\n");
    VM.sysWrite("Total Yieldpoints: " + total + "\n");
  }

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  This interface defines the decay method.  Implementors are 
 *  eligible for decay if they register with the 
 *  VM_RuntimeMeasurements class.
 *
 *  @author Michael Hind
 */

interface VM_Decayable {

  /**
   *  Called periodically when it is time to decay runtime mesaurment data
   */
  public void decay();

}





/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Interface for all reportable objects that are managed by the runtime
 * measurements.
 *
 * @author Peter Sweeney
 */

interface VM_Reportable { 
  /**
   * generate a report
   */
  void report(); 
  /**
   * reset (clear) data set being gathered
   */
  void reset();  
}







/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * RuntimeMeasurements manages listeners, decayable objects, and 
 * reportable objects.
 *
 * A listener is installed by an organizer, and activated at thread
 * switch time by VM_Thread.  Depending on the update method that the
 * listener supports, it can be either a method, context, or a null 
 * listener.  Currently we have different registries for different 
 * listeners.  An alternative design is to have one register with where 
 * entries are tagged.
 *
 * A decayable object implements the VM_Decayable interface.
 * Anyone can register a decayable object,
 * The VM_DecayOrganizer periodically decays all objects that have 
 * been registers.
 *
 * A reportable object implements the Reportable interface, and 
 * is typically registered and used by the instrumentation subsystem. 
 * A reportReporableObject can be reset, and reported.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 * @modified Peter Sweeney
 */
abstract class VM_RuntimeMeasurements {

  /**
   * listeners for methods
   */
  static VM_MethodListener[] methodListeners = new VM_MethodListener[0];
  /**
   * listeners for contexts
   */
  static VM_ContextListener[] contextListeners = new VM_ContextListener[0];
  /**
   * listeners for nulls
   */
  static VM_NullListener[] nullListeners = new VM_NullListener[0];

  private static int activateMethodListeners_count = 0;
  private static int activateContextListeners_count = 0;
  private static int activateNullListeners_count = 0;
  /**
   * Install a method listener
   * @param s method listener to be installed
   */
  static synchronized void installMethodListener(VM_MethodListener s) { 
    int numListeners = methodListeners.length;
    VM_MethodListener[] tmp = new VM_MethodListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = methodListeners[i];
    }
    tmp[numListeners] = s;
    methodListeners = tmp;
  }

  /**
   * Install a context listener
   * @param s context listener to be installed
   */
  static synchronized void installContextListener(VM_ContextListener s) { 
    int numListeners = contextListeners.length;
    VM_ContextListener[] tmp = new VM_ContextListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = contextListeners[i];
    }
    tmp[numListeners] = s;
    contextListeners = tmp;
  }

  /**
   * Install a null listener
   * @param s null listener to be installed
   */
  static synchronized void installNullListener(VM_NullListener s) { 
    int numListeners = nullListeners.length;
    VM_NullListener[] tmp = new VM_NullListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = nullListeners[i];
    }
    tmp[numListeners] = s;
    nullListeners = tmp;
  }

  /**
   * Determine if at least one active method listener exists
   * @return true if at least one active method listener
   */
  static boolean hasMethodListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = methodListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }
  /**
   * Determine if at least one active context listener exists
   * @return true if at least one active context listener
   */
  static boolean hasContextListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = contextListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }
  /**
   * Determine if at least one active null listener exists
   * @return true if at least one active null listener
   */
  static boolean hasNullListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = nullListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }

  /**
   * Notify RuntimeMeasurements that method listeners should be activated
   *
   * @param cmid a compiled method id
   * @param callerCmid a compiled method id for the caller, -1 if none
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *           EPILOGUE?
   */
  static void activateMethodListeners(int cmid, int callerCmid, int whereFrom) throws VM_PragmaUninterruptible {
    activateMethodListeners_count++;     
    VM_MethodListener[] tmp = methodListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
 	tmp[i].update(cmid, callerCmid, whereFrom);
      }
    }
  }

  /**
   * Notify RuntimeMeasurements that context listeners should be activated.
   *
   * @param sfp		a pointer to a stack frame
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  static void activateContextListeners(VM_Address sfp, int whereFrom) throws VM_PragmaUninterruptible {
    activateContextListeners_count++;     
    VM_ContextListener[] tmp = contextListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
 	tmp[i].update(sfp, whereFrom);
      }
    }
  }

  /**
   * Notify RuntimeMeasurements that null listeners should be activated.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  static void activateNullListeners(int whereFrom) throws VM_PragmaUninterruptible {
    activateNullListeners_count++;     
    VM_NullListener[] tmp = nullListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
	tmp[i].update(whereFrom);
      }
    }
  }

  /**
   * The currently registered decayable objects
   */
  static Vector decayObjects = new Vector();

  /**
   * Counts the number of decay events
   */
  static int decayEventCounter = 0;

  /**
   *  Register an object that should be decayed.
   *  The passed object will have its decay method called when the
   *  decaying thread decides it is time for the system to decay.
   */
  static void registerDecayableObject(VM_Decayable obj) {
    decayObjects.add(obj);
  }

  /**
   * Decay all registered decayable objects.
   */
  static void decayDecayableObjects() {
    decayEventCounter++;
    if (VM.LogAOSEvents) VM_AOSLogging.decayingCounters();
    
    for (Enumeration e=decayObjects.elements(); e.hasMoreElements();) {
      VM_Decayable obj = (VM_Decayable) e.nextElement();
      obj.decay();
    }
  }

  /**
   * The currently registered reportable objects
   */
  static Vector reportObjects = new Vector();

  /** 
   * Register an object that wants to have its report method called
   * whenever VM_RuntimeMeasurements.report is called
   */
  static void registerReportableObject(VM_Reportable obj) {
    reportObjects.add(obj);
  }

  /**
   * Reset to all registered reportable objects
   */
  public static void resetReportableObjects() {
    for (Enumeration e=reportObjects.elements(); e.hasMoreElements();) {
      VM_Reportable obj = (VM_Reportable)e.nextElement();
      obj.reset();
    }
  }    
  /**
   * Report to all registered reportable objects
   */
  private static void reportReportableObjects() {
    for (Enumeration e=reportObjects.elements(); e.hasMoreElements();) {
      VM_Reportable obj = (VM_Reportable)e.nextElement();
      obj.report();
    }
  }    
  
  /**
   * Report the current state of runtime measurements
   */
  static void report() {
    reportReportableObjects();
    
    if (VM.LogAOSEvents) {
      VM_AOSLogging.listenerStatistics(activateMethodListeners_count,
				       activateContextListeners_count,
				       activateNullListeners_count);
      VM_AOSLogging.decayStatistics(decayEventCounter);

      for (int i = 0, n = VM_Scheduler.threads.length; i < n; i++) {
	VM_Thread t = VM_Scheduler.threads[i];
	if (t != null) {
	  VM_AOSLogging.threadExiting(t);
	}
      }
    }
  }

  /**
   * Stop the runtime measurement subsystem
   */
  static synchronized void stop() {
    methodListeners = new VM_MethodListener[0];
    contextListeners = new VM_ContextListener[0];
    nullListeners = new VM_NullListener[0];
  }
    
  /**
   * Called from VM_Thread.terminate.
   */
  public static void monitorThreadExit() {
    if (VM.LogAOSEvents) VM_AOSLogging.threadExiting(VM_Thread.getCurrentThread());
  }
  
  /**
   * Called when the VM is booting
   */
  static void boot() { }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener that accumulates samples into a VM_MethodCountData.
 *
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
final class VM_AccumulatingMethodListener extends VM_MethodListener 
  implements VM_Uninterruptible {

  /**
   * @param sampleSize the initial sampleSize for the listener
   * @param notifyOrganizer should the listener notify an organizer
   *                    when its threshold is reached?
   * @param data the VM_MethodCountData object to use to accumulate the samples
   */
  public VM_AccumulatingMethodListener(int sampleSize, 
				       boolean notifyOrganizer,
				       VM_MethodCountData data) {
    super(sampleSize, notifyOrganizer);
    this.data = data;
  }

  /** 
   * Update data with the current samples and generate a cumulative report.
   * Reset ourselves, since the sample buffer has been drained into data.
   */
  public void report() throws VM_PragmaInterruptible {
    processSamples();
    reset();
    VM.sysWrite("\nMethod sampler report");
    data.report();
  }

  /**
   * Process the samples.
   */
  public void processSamples() {
    data.update(samples, getNumSamples());
  }

  /**
   * @return the method data being updated by the listener.
   */
  public final VM_MethodCountData getData() { return data; }

  // The cummulative method sample data associated with this listener
  protected VM_MethodCountData data;
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener that relies on its organizer alone to process
 * the sample data.
 *
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
final class VM_BasicMethodListener extends VM_MethodListener 
  implements VM_Uninterruptible {

  /**
   * @param sampleSize the initial sampleSize for the listener
   */
  public VM_BasicMethodListener(int sampleSize) {
    super(sampleSize, true);
  }

  /** 
   * Nothing to report.
   */
  public void report() {
    VM.sysWrite("BasicMethodListener has nothing to report!");
  }


  /**
   * We rely on the organizer to process the samples, therefore
   * nothing to do.
   */
  public void processSamples() { }

} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This object that is invoked when online measurement information must 
 * be collected.
 *
 * @author Peter Sweeney
 * @date   2 June 2000
 */
abstract class VM_ContextListener extends VM_Listener implements VM_Uninterruptible {

  /**
   * Entry point when listener is awoken.
   *
   * @param sfp  pointer to stack frame where call stack should start 
   *             to be examined.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *            EPILOGUE?
   */
  abstract public void update(VM_Address sfp, int whereFrom);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * A VM_EdgeListener defines a listener 
 * that computes a call graph edge from the call stack.
 * After a parameterized number of edges are collected, 
 * it notifies its organizer that the threshold is reached.
 *
 * Defines update's interface.
 *
 * VM_EdgeListener communicates with an organizer through a 
 * integer array, buffer.  Each time this listener is called, 
 * it places a triple of integers in buffer that correspond to
 * the callee, caller, and machine code offset of the call site
 *
 * @author Peter Sweeney
 * @author Michael Hind
 * @date   May 18, 2000
 *
 */

class VM_EdgeListener extends VM_ContextListener 
  implements VM_Uninterruptible, VM_StackframeLayoutConstants {

  protected static final boolean DEBUG = false;

  /**
   * buffer provides the communication channel between the listener and the
   * organizer.
   * The buffer contains an array of triples <callee, caller, address> where
   * the caller and callee are VM_CompiledMethodID's.
   * Initially, buffer contains zeros.  The listener adds triples.
   * When the listener hits the end of the buffer, notify the organizer.
   * (Alternatively, could make the buffer circular.)
   */
  private int[] buffer;

  /**
   * the index in the buffer of the next free triple
   */
  private int nextIndex;

  /**
   * Number of samples to be taken before issuing callback to controller 
   */
  private int desiredSamples;

  /**
   *  Number of samples taken so far
   */
  protected int samplesTaken = 0;

  /**
   * Number of times update is called
   */
  protected int calledUpdate = 0;

  /**
   * Constructor
   */
   public VM_EdgeListener() {
      buffer         = null;
      desiredSamples = 0;
  }

  /**
   * returns the number of times that update is called
   * @returns the number of times that update is called
   */
  int getTimesUpdateCalled() { 
    return calledUpdate; 
  }

  /**
   * Setup buffer and buffer size.  
   * This method must be called before any data can be written to
   * the buffer.
   *
   * @param buffer the allocated buffer to contain the samples, size should
   *      be a muliple of 3
   */
  public void setBuffer(int[] buffer) {
    // ensure buffer is proper length
    if (VM.VerifyAssertions) {
      VM.assert(buffer.length%3 == 0);
    }

    if (DEBUG) {
      VM.sysWrite("VM_EdgeListener.setBuffer("+buffer.length+"): enter\n");     
    }

    this.buffer    = buffer;
    desiredSamples = buffer.length / 3;
    resetBuffer();
  }

  /**
   * This method is called when a call stack edge needs to be 
   * sampled.  Expect the sfp argument to point to the stack frame that
   * contains the target of the edge to be sampled.
   *
   * RESTRICTION: the execution time of this method is time critical 
   * (we don't want another thread switch to occur inside of it).  
   * Therefore, this method simple stuffs integers into buffer.
   *
   * RESTRICTION: while GC is disabled, do not preform any operation that can
   * allocate space!
   *
   * @param sfp  a pointer to the stack frame that corresponds to the callee of
   *             the call graph edge that is to be sampled.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  public final void update(VM_Address sfp, int whereFrom) {
    if (DEBUG) {
      VM.sysWrite("VM_EdgeListener.update("+sfp.toInt()+","+whereFrom+
		  "): enter "+samplesTaken+"\n");     
    }

    calledUpdate++;

    // don't take a sample for back edge yield points
    if (whereFrom == VM_Thread.BACKEDGE) return; 

    if (buffer == null) {
      VM.sysWrite("***Error: VM_EdgeListener.update() called "+
		  "before setBuffer() is called!\n");
      VM.sysExit(-1);
    }

    int calleeCMID    = 0;
    int callerCMID    = 0;
    VM_Address returnAddress = VM_Address.zero();

    // While GC is disabled, don't do string concatenation!
    VM_Processor.getCurrentProcessor().disableThreadSwitching();
     
    if (VM_Magic.getMemoryWord(sfp) == STACKFRAME_SENTINAL_FP) {
      if (DEBUG) VM.sysWrite(" Walking off end of stack!\n");	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    calleeCMID = VM_Magic.getCompiledMethodID(sfp);
    if (calleeCMID == INVISIBLE_METHOD_ID) {
      if (DEBUG){
	VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	VM.sysWrite(calleeCMID); VM.sysWrite("\n");       
      } 
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    if (VM_CompiledMethods.getCompiledMethod(calleeCMID) == null) {
      VM.sysWrite("VM_EdgeListener:update: Found a callee cmid (");
      VM.sysWrite(calleeCMID, false);
      VM.sysWrite(") with a null compiled method. ");
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      VM.sysFail("Exiting VM");
    }

    returnAddress = VM_Magic.getReturnAddress(sfp); // return address in caller
    sfp = VM_Magic.getCallerFramePointer(sfp);      // caller's frame pointer
    if(VM_Magic.getMemoryWord(sfp) == STACKFRAME_SENTINAL_FP) {
      if (DEBUG) VM.sysWrite(" Walking off end of stack\n");	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }
    callerCMID = VM_Magic.getCompiledMethodID(sfp);
    if (callerCMID == INVISIBLE_METHOD_ID) {
      if (DEBUG) { 
	VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	VM.sysWrite(callerCMID); VM.sysWrite("\n"); 
      }	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    if (VM_CompiledMethods.getCompiledMethod(callerCMID) == null) {
      VM.sysWrite("VM_EdgeListener:update: Found a caller cmid (");
      VM.sysWrite(calleeCMID, false);
      VM.sysWrite(") with a null compiled method, exiting");
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      VM.sysFail("Exiting VM");
    }

    // store the offset of the return address from the beginning of the 
    // instruction
    VM_CompiledMethod callerCM = VM_CompiledMethods.getCompiledMethod(callerCMID);
    VM_Address beginningOfMachineCode = VM_Magic.objectAsAddress(callerCM.getInstructions());
    int callSite = returnAddress.diff(beginningOfMachineCode);

    if (DEBUG){ 
      VM.sysWrite("  <");VM.sysWrite(calleeCMID);VM.sysWrite(",");
      VM.sysWrite(callerCMID);VM.sysWrite(",");VM.sysWrite(returnAddress);
      VM.sysWrite(">\n");
    }

    if (DEBUG) { 
      VM_Address fp = VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());
      int compiledMethodID = 0;
      for(int i=1; i<6; i++) {
	if (VM_Magic.getMemoryWord(fp) == STACKFRAME_SENTINAL_FP) {
	  VM.sysWrite(" Walking off end of stack\n"); break;
	}
	compiledMethodID = VM_Magic.getCompiledMethodID(fp);
	if (compiledMethodID == INVISIBLE_METHOD_ID) {
	  VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	  VM.sysWrite(compiledMethodID); VM.sysWrite("\n");     continue;
	}
	VM.sysWrite("   Stack frame ");VM.sysWrite(i);VM.sysWrite(": ");
	VM.sysWrite(compiledMethodID);
	if (true) {
	  VM_CompiledMethod compiledMethod = null;
	  compiledMethod = VM_CompiledMethods.getCompiledMethod(compiledMethodID);
	  VM_Method  method = compiledMethod.getMethod();
	  VM.sysWrite(method);
	}
	VM.sysWrite("\n");
	fp = VM_Magic.getCallerFramePointer(fp);
      }
    }

    // done with stack inspection, re-enable GC
    VM_Processor.getCurrentProcessor().enableThreadSwitching();

    // Try to get 3 buffer slots and update nextIndex appropriately
    int idx = VM_Synchronization.fetchAndAdd(this, 
					     VM_Entrypoints.edgeListenerNextIndexField.getOffset(),
					     3);

    // Ensure that we got slots for our sample, if we don't (because another
    // thread is racing with us) we'll just ignore this sample
    if (idx < buffer.length) {
      buffer[idx+0] = calleeCMID;
      buffer[idx+1] = callerCMID;
      buffer[idx+2] = callSite;

      // Determine which sample we just completed.
      // fetchAndAdd returns the value before the increment, add one to
      // determine which sample we were
      int sampleNumber = 
	VM_Synchronization.fetchAndAdd(this, 
				       VM_Entrypoints.edgeListenerSamplesTakenField.getOffset(),
				       1) + 1;

      // If we are the last sample, we need to take action
      if (sampleNumber == desiredSamples) {
	thresholdReached();
      } 
    } 
  }

  /** 
   *  report() noop
   */
  public final void report() {}

  /**
   * Called when threshold is reached.
   */
  public void thresholdReached() {
    if (DEBUG) VM.sysWrite("VM_EdgeListener.thresholdReached(): enter\n");

    passivate();
    
    // Notify the organizer thread
    notifyOrganizer();
    if (DEBUG) VM.sysWrite("VM_EdgeListener.thresholdReached(): exit\n");
  }

  /**
   * Reset (in preparation of starting a new sampling window)
   */
  public void reset() {
     if (DEBUG) VM.sysWrite("VM_EdgeListener.reset(): enter\n");     
     samplesTaken = 0;
     calledUpdate = 0;
     resetBuffer();
  }

  /**
   *  Resets the buffer
   */
  private void resetBuffer() {
    for (int i=0; i<buffer.length; i++) {
      buffer[i] = 0;
    }
    nextIndex = 0;
  }
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_Listener object is invoked when online measurement information 
 * needs to be collected.
 *
 * This class does not define the update() method, the call back method from
 * the runtime when a sample should be taken.
 * The expectation is that immediately derived classes define an interface to
 * the update() method from which classes may be further derived.
 *
 * CONSTRAINTS:
 * Classes that are derived from VM_Listener 
 * must inherit directly from VM_Uninterruptible to ensure that they
 * are not interrupted by a thread switch.  
 * Since thread switching is disabled, listeners are 
 * expected to complete execution quickly, and therefore, 
 * must do a minimal amount of work.
 *
 * @author Peter Sweeney
 * @modified Dave Grove
 */
abstract class VM_Listener implements VM_Uninterruptible {

  /**
   * Entry point to dump what has been collected.
   */
  abstract public void report() throws VM_PragmaInterruptible;

  /**
   * Is the listener currently active (interested in getting "update" calls)
   */
  public final boolean isActive() { return active; }

  /**
   * Transition listener to active state
   */
  public final void activate() { active = true; }
  
  /**
   * Transition listener to passive state 
   */
  public final void passivate() { active = false; }

  /**
   * Organizer associated with this listener.
   */
  public final void setOrganizer(VM_Organizer organizer) {
    this.organizer = organizer;
  }

  /**
   * Wake up the organizer thread (if any) associated with the listener
   */
  public final void notifyOrganizer() {
    if (organizer != null) {
      synchronized(organizer) {
	try {
	  organizer.notify();
	} catch (Exception e) {
	  e.printStackTrace();
	}
      }
    }
  }

  // Is the listener active or passive?
  private boolean active = false;
  // My organizer.
  private VM_Organizer organizer;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener defines a listener to collect method invocation samples.
 *
 * Samples are collected in a buffer.  
 * When sampleSize samples have been collected, thresholdReached is called.
 *  
 * Defines update's interface to be a compiled method identifier, CMID.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
abstract class VM_MethodListener extends VM_Listener 
  implements VM_Uninterruptible {

  /**
   * Number of samples to be processed before calling thresholdReached
   */
  protected int sampleSize;  
  
  /**
   * Next available index in the sample array
   */
  protected int nextIndex;
  
  /**
   * Number of samples taken so far
   */
  protected int numSamples;
  
  /**
   * The sample buffer
   * Key Invariant: samples.length >= sampleSize
   */
  protected int[] samples;
  
  /**
   * Is this listener supposed to notify its organizer when thresholdReached?
   */
  protected boolean notifyOrganizer;
  

  /**
   * @param sampleSize the initial sampleSize for the listener
   * @param notifyOrganizer should the listener notify an organizer
   *                    when its threshold is reached?
   */
  public VM_MethodListener(int sampleSize, boolean notifyOrganizer) {
    this.sampleSize = sampleSize;
    this.notifyOrganizer = notifyOrganizer;
    samples = new int[sampleSize];
  }


  /** 
   * This method is called when a time based sample occurs.
   * It parameter "cmid" represents the compiled method ID of the method
   * which was executing at the time of the sample.  This method
   * bumps the counter and checks whether a threshold is reached.
   * <p>
   * NOTE: There can be multiple threads executing this method at the 
   *       same time. We attempt to ensure that the resulting race conditions
   *       are safely handled, but make no guarentee that every sample is
   *       actually recorded. We do try to make it somewhat likely that 
   *       thresholdReached is called exactly once when numSamples reaches 
   *       sampleSize, but there are still no guarentees.
   *
   * @param cmid the compiled method ID to update
   * @param callerCmid a compiled method id for the caller, -1 if none
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  public final void update(int cmid, int callerCmid, int whereFrom) {
    if (VM.UseEpilogueYieldPoints) {
      // Use epilogue yieldpoints.  We increment one sample
      // for every yieldpoint.  On a prologue, we count the caller.
      // On backedges and epilogues, we count the current method.
      if (whereFrom == VM_Thread.PROLOGUE) {
	// Before getting a sample index, make sure we have something to insert
	if (callerCmid != -1) {
	  int sampleNumber = recordSample(callerCmid);
	  checkSampleSize(sampleNumber);
        } // nothing to insert
      } else { 
        // loop backedge or epilogue.  
	int sampleNumber = recordSample(cmid);
	checkSampleSize(sampleNumber);
      }
    } else {
      // Original scheme: No epilogue yieldpoints.  We increment two samples
      // for every yieldpoint.  On a prologue, we count both the caller
      // and callee.  On backedges, we count the current method twice.
      if (whereFrom == VM_Thread.PROLOGUE) {
        // Increment both for this method and the caller
	int sampleNumber = recordSample(cmid);
	if (callerCmid != -1) {
	  sampleNumber = recordSample(callerCmid);
	}
	checkSampleSize(sampleNumber);
      } else { 
        // loop backedge.  We're only called once, so need to take
        // two samples to avoid penalizing methods with loops.
	int sampleNumber = recordSample(cmid);
	sampleNumber = recordSample(cmid);
	checkSampleSize(sampleNumber);
      }
    }
  }

  /**
   * This method records a sample containing the CMID (compiled method ID)
   * passed.  Since multiple threads may be taking samples concurrently,
   * we use fetchAndAdd to distribute indices into the buffer AND to record
   * when a sample is taken.  (Thread 1 may get an earlier index, but complete
   * the insertion after Thread 2.)
   *
   * @param CMID compiled method ID to record
   * @return the sample number that was inserted or -1, if we were unlucky
   *
   * NOTE: if we are unlucky that we didn't get to insert the sample (because
   *  there was a slot when we started, but another thread stole it from us)
   *  we simply don't insert the sample.
   */
  private int recordSample(int CMID) {  
    // reserved the next available slot
    int idx = VM_Synchronization.fetchAndAdd(this, VM_Entrypoints.methodListenerNextIndexField.getOffset(), 1);
    // make sure it is valid
    if (idx < sampleSize) {
      samples[idx] = CMID;

      // sampleNumber has the value before we incremented, add one to 
      // determine which sample we were
      int sampleNumber =  VM_Synchronization.fetchAndAdd(this, VM_Entrypoints.methodListenerNumSamplesField.getOffset(), 1) + 1;
      return sampleNumber;
    } else {
      return -1;
    }
  }

  /**
   * This method checks to see if the parameter passed was the last sample
   * If so, it passivates this listener and reports that the threshold has
   * been reached.
   *
   * @param sampleNumber the sample number was just taken 
   *      valid samples will be in [1..sampleSize]
   *      invalid sample will be -1 
   */
  private void checkSampleSize(int sampleNumber) {
    if (sampleNumber == sampleSize) { 
      passivate();
      thresholdReached();
    }
  }

  /**
   * When the threshold is reached either notify our organizer or
   * handle it ourselves by processing the samples, resetting, and 
   * activating ourselves again.
   */
  public void thresholdReached() {
    int numSamples = getNumSamples();
    for (int i=0; i<numSamples; i++) {
      int id = samples[i];
    }

    if (notifyOrganizer) {
      notifyOrganizer();
    } else {
      processSamples();
      reset();
      activate();
    }
  }


  /**
   * process the buffer of samples
   */
  public abstract void processSamples(); 


  /**
   * Reset the buffer to prepare to take more samples.
   */
  public void reset() {
    nextIndex = 0;
    numSamples = 0;
  }


  /**
   * updates the sample size for this listener
   * doesn't worry about any samples in the current buffer
   * @param newSampleSize the new sample size value to use, 
   */
  public final void setSampleSize(int newSampleSize) throws VM_PragmaInterruptible {
    sampleSize = newSampleSize; 
    if (sampleSize > samples.length) {
      samples = new int[newSampleSize];
      nextIndex = 0;
      numSamples = 0;
    }
  }

  /**
   * @return the current sample size/threshold value
   */
  public final int getSampleSize() { return numSamples; }

  /**
   * @return the buffer of samples
   */
  public final int[] getSamples() { return samples; }

  /**
   * @return how many samples have been taken
   */
  public final int getSamplesTaken() { return numSamples; }

  /**
   * @return how many samples in the array returned by getSamples
   *         are valid (min(getSamplesTaken(), getSampleSize())).
   */
  public final int getNumSamples() {
    int x = getSamplesTaken();
    int y = getSampleSize();
    return (x < y) ? x : y;
  }
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_NullListener is an object that is invoked when
 * online measurement information must be collected.
 *
 * Defines update's interface.
 *
 * @author Peter Sweeney
 * @date   2 June 2000
 */

abstract class VM_NullListener extends VM_Listener implements VM_Uninterruptible {
  /**
   * Entry point when listener is awoken.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *             EPILOGUE?
   */
  abstract public void update(int whereFrom);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * A VM_YieldCounterListener samples yield points, and
 * notifies an Organizer when a threshold is reached.
 *
 * In effect, this class provides a way to "wake up" an infrequent
 * service periodically.
 *
 * @author Stephen Fink
 * @modified Peter Sweeney
 */
class VM_YieldCounterListener extends VM_NullListener implements VM_Uninterruptible {

  /**
   * Constructor
   *
   * @param yieldThreshold  the threshold of when to call organizer
   */
  public VM_YieldCounterListener(int yieldThreshold) {
    this.yieldThreshold = yieldThreshold;
  }

  /** 
   * This method is called when its time to record that a 
   * yield point has occurred.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *             EPILOGUE?
   */
  public void update(int whereFrom) {
     nYields++;
     if (nYields >= yieldThreshold) {
	synchronized(this) {
	  // now that we're in a critical section, double-check that
	  // another thread has not yet processed this threshold reached event.
     	  if (nYields >= yieldThreshold) {
	    passivate();
            notifyOrganizer();
            totalYields += nYields;
            nYields = 0;
	  }
        }
     }
  }

  public void report() {
     VM.sysWriteln("Yield points counted: ", totalYields);
  }

  private int yieldThreshold;
  private int nYields = 0;
  private int totalYields = 0;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * An organizer of call graph edge information that is used for 
 * adaptive inlining.
 *
 * VM_AIByEdgeOrganizer communicates with an edge listener through a 
 * integer array, denoted buffer.  When this organizer is woken up
 * via threshold reached, it processes the sequence of triples 
 * that are contained in buffer.
 * After the buffer is processed, the organizer checks to see
 * if any methods compiled at VM_Controller.options.MAX_OPT_LEVEL
 * should be recompiled due to inlining opportunities.
 * 
 * EXPECTATION: buffer is filled all the way up with triples.
 * 
 * Potential problems: The system does not retain new edges between
 * organizer runs.  There could be a race condition where a
 * ??? where what?? --dave
 * 
 * @author Peter Sweeney 
 * @author Dave Grove
 * @modified Stephen Fink
 * @modified Michael Hind
 * @modified Matthew Arnold
 */
class VM_AIByEdgeOrganizer extends VM_Organizer implements VM_Decayable {

  private final static boolean DEBUG = false;

  /*
   * buffer provides the communication channel between the edge listener
   * and the organizer.
   * The buffer contains an array of triples <callee, caller, address> where
   * the caller and callee are VM_CompiledMethodID's, and address identifies
   * the call site.
   * bufferSize is the number of triples contained in buffer.
   * The edge listener adds triples.  
   * At some point the listener deregisters itself and notifies the organizer 
   * by calling thresholdReached().
   */
  private int[] buffer;
  private int   bufferSize;
  private int   numberOfBufferTriples;

  /**
   *	Representation of call graph.
   */
  private VM_PartialCallGraph callGraph;
  /*
   *  The edge listener
   */
  VM_EdgeListener edgeListener;

  /**
   * Constructor
   */
  VM_AIByEdgeOrganizer(VM_EdgeListener edgeListener) {
     if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.<init>(): enter\n");     
     this.edgeListener = edgeListener;
     edgeListener.setOrganizer(this);
  }

  /**
   */
  public void decay() {
     VM_AdaptiveInlining.decay();
  }

  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.initialize(): enter\n");

    if (VM.LogAOSEvents) VM_AOSLogging.AIByEdgeOrganizerThreadStarted();

    numberOfBufferTriples = VM_Controller.options.AI_SAMPLE_SIZE;

    bufferSize = numberOfBufferTriples * 3;
    buffer     = new int[bufferSize];

    edgeListener.setBuffer(buffer); 

    // allocate internal data structures.
    callGraph   = VM_AdaptiveInlining.getPartialCallGraph();

    // Install and activate the edge listener
    VM_RuntimeMeasurements.installContextListener(edgeListener);
    edgeListener.activate();

    // register as decayable
    VM_RuntimeMeasurements.registerDecayableObject(this);

    if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.initialize(): exit\n");
  }

  /**
   * Method that is called when the sampling threshold is reached.
   * Process contents of buffer: 
   *    add call graph edges and increment their weights.
   */
  void thresholdReached() {
    if(DEBUG)
      VM.sysWrite("VM_AIByEdgeOrganizer.thresholdReached(): enter and reregister.\n");

    VM_AdaptiveInlining.incrementNumYieldPoints(edgeListener.
                getTimesUpdateCalled());
    for (int i=0; i<bufferSize; i=i+3) {
      int calleeCMID = buffer[i+0];
      VM_CompiledMethod compiledMethod   = VM_CompiledMethods.getCompiledMethod(calleeCMID);
      if (compiledMethod == null) continue;
      VM_Method callee = compiledMethod.getMethod();
      int callerCMID = buffer[i+1];
      compiledMethod   = VM_CompiledMethods.getCompiledMethod(callerCMID);
      if (compiledMethod == null) continue;
      VM_Method stackFrameCaller = compiledMethod.getMethod();
       
      int MCOffset = buffer[i+2];
      int bytecodeIndex = -1;
      VM_Method caller = null;

      switch (compiledMethod.getCompilerType()) {
      case VM_CompiledMethod.TRAP:
      case VM_CompiledMethod.JNI:
	if (DEBUG) VM.sysWrite("Skipping sample with TRAP/JNI caller");
	continue;
      case VM_CompiledMethod.BASELINE:
	{
	  VM_BaselineCompiledMethod baseCompiledMethod = 
	    (VM_BaselineCompiledMethod)compiledMethod;
	  // note: the following call expects the offset in INSTRUCTIONS!
	  bytecodeIndex = baseCompiledMethod.findBytecodeIndexForInstruction
	    (MCOffset>>VM.LG_INSTRUCTION_WIDTH);
	  caller = stackFrameCaller;
	}
	break;
      case VM_CompiledMethod.OPT:
	{
	  VM_OptCompiledMethod optCompiledMethod = (VM_OptCompiledMethod)compiledMethod;
	  VM_OptMachineCodeMap mc_map = optCompiledMethod.getMCMap();
	  try {
	    bytecodeIndex = mc_map.getBytecodeIndexForMCOffset(MCOffset);
	    if (bytecodeIndex == -1) {
	      // this can happen we we sample a call 
	      // to a runtimeSerivce routine. 
	      // We aren't setup to inline such methods anyways, 
	      // so skip the sample.
	      if (DEBUG) VM.sysWrite("  *** SKIP SAMPLE "+
				     stackFrameCaller+"@"+compiledMethod+
				     " at MC offset "+MCOffset+
				     " calling "+callee+
				     " due to invalid bytecodeIndex\n");
	      continue; // skip sample.
	    }
	  } catch (java.lang.ArrayIndexOutOfBoundsException e) {
	    VM.sysWrite("  ***ERROR: getBytecodeIndexForMCOffset("+MCOffset
			+") ArrayIndexOutOfBounds!\n");
	    e.printStackTrace(); caller = stackFrameCaller;
	    continue;  // skip sample
	  } catch (OPT_OptimizingCompilerException e) {
	    VM.sysWrite("***Error: SKIP SAMPLE: can't find bytecode index in OPT compiled "+
			stackFrameCaller+"@"+compiledMethod+" at MC offset "+MCOffset+"!\n");
	    continue;  // skip sample
	  }
	  
	  try {
	    caller = mc_map.getMethodForMCOffset(MCOffset);
	  } catch (java.lang.ArrayIndexOutOfBoundsException e) {
	    VM.sysWrite("  ***ERROR: getMethodForMCOffset("
			+MCOffset+") ArrayIndexOutOfBounds!\n");
	    e.printStackTrace(); caller = stackFrameCaller;
	    continue;
	  } catch (OPT_OptimizingCompilerException e) {
	    VM.sysWrite("***Error: SKIP SAMPLE: can't find caller in OPT compiled "+
			stackFrameCaller+"@"+compiledMethod+" at MC offset "
			+MCOffset+"!\n");
	    continue;  // skip sample
	  }

	  if (caller == null) {
	    VM.sysWrite("  ***ERROR: getMethodForMCOffset("+
			MCOffset+") returned null!\n");
	    caller = stackFrameCaller;
	    continue;  // skip sample
	  }
	}
	break;
      }

      // increment the call graph edge, adding it if needed
      callGraph.incrementEdge(caller, bytecodeIndex, callee);
    }

    // If using an offline inline plan, don't recompute anything, and don't 
    // notify the controller.
    if (!VM_Controller.options.USE_OFFLINE_INLINE_PLAN) {
      // force a recomputation of the current state of hot edges
      Vector vectorOfTriples = VM_AdaptiveInlining.recomputeHotEdges();
      
      if(DEBUG) {
	VM.sysWrite("\nNew edges found:\n");
	for (int i=0; i<vectorOfTriples.size(); i++) {
	  VM_CallSiteTriple triple = (VM_CallSiteTriple)
                vectorOfTriples.elementAt(i);
	  VM.sysWrite((i+1)+": "+triple.toString()+"\n");
	}
      }
      
      VM_MethodCountSet HM_data = 
	VM_Controller.methodSamples.collectHotMethods(VM_Controller.options.MAX_OPT_LEVEL,
						      VM_Controller.options.AI_METHOD_HOTNESS_THRESHOLD);
      if (VM.LogAOSEvents) 
	VM_AOSLogging.AIorganizerFoundHotMethods(HM_data.cms.length);
      
      findMethodsToRecompile(vectorOfTriples, HM_data);
    }
    if(DEBUG) callGraph.dump();

    // Clear listener and activate it again.
    edgeListener.reset();
    edgeListener.activate();

    if(DEBUG)VM.sysWrite("VM_AIByEdgeOrganizer.thresholdReached(): exit\n");
  }  

   /*
    * Given a vector of new edges and a set of methods, determine if 
    * a method in the set contains a call site corresponding to an edge 
    * in the vector and that call site has not been inlined.
    *
    * @param vectorOfTriples	new edges that are hot in call graph
    * @param hotMethodSet	methods that are hot and compiled at max opt level.
    *
    */
   private void findMethodsToRecompile(Vector vectorOfTriples,
				       VM_MethodCountSet hotMethodSet) {
     if (DEBUG) VM.sysWrite("\nVM_AIByEdgeOrganizer.findMethodsToRecompile() "
			    + hotMethodSet.cms.length+"\n");

     if (vectorOfTriples.isEmpty() || hotMethodSet.cms.length == 0) {
       if (DEBUG) VM.sysWrite("  return early\n");
       return;
     }

     // Consider each hot max opt level method
     for (int i=0; i<hotMethodSet.cms.length; i++) {
       VM_CompiledMethod hotMethod = hotMethodSet.cms[i];
       int cmid                    = hotMethod.getId();
       double numSamples           = hotMethodSet.counters[i];
       VM_OptMachineCodeMap mcMap  = ((VM_OptCompiledMethod)hotMethod).getMCMap();
       double edgeHotness          = 0.0;

       if (DEBUG) VM.sysWrite(" Process hot method: "+
			      hotMethod.getMethod()+" with "+numSamples+"\n");
       
       if (!hotMethod.getMethod().isInterruptible()) {
	 // This is required because a very small subset of uninterruptible methods
	 // need to have their code in a non-moving heap. 
	 // For now, we use this simple, but conservative test to avoid trouble.
	 if (DEBUG) VM.sysWrite("Not selecting uninterruptible method for recompilation "+hotMethod.getMethod());
	 continue;
       }

       // For each edge, see if the callsite is present, 
       // but the callee is absent in hotMethod.
       for (Enumeration triples = vectorOfTriples.elements(); 
                triples.hasMoreElements(); ) {
	 VM_CallSiteTriple triple = (VM_CallSiteTriple)triples.nextElement();
	 if (!VM_AdaptiveInlining.knownNonInlinedEdge(cmid, triple)) {
	   VM_Method caller = triple.getCaller();
	   int bytecodeIndex= triple.getBytecodeIndex();
	   VM_Method callee = triple.getCallee();
	   if (DEBUG) VM.sysWrite("   Edge candidate "+triple+"\n");

	   try {
	     if (mcMap.callsitePresent(caller, bytecodeIndex)) {
	       if (DEBUG) VM.sysWrite(" FOUND EDGE: "+triple+
				      " that can be inlined into "
				      +hotMethod.getMethod()+"\n");
	       edgeHotness += triple.getWeight();
	       if (VM.LogAOSEvents) 
		 VM_AOSLogging.inliningOpportunityDetected(hotMethod, 
							   numSamples, 
							   triple);
	     }
	   }
	   catch (Throwable e){
	     VM.sysWrite("ERROR in adaptive system! Exception caught!\n");
	     VM.sysWrite(" AI Organizer considering edge "+triple+
			 " to be inlined into "
			   +hotMethod.getMethod()+"\n");
	     e.printStackTrace();
	   }
	 }
       }

       // Notify the controller if we found a candidate edge in hotMethod
       if (edgeHotness > 0.0001) {
	 edgeHotness   /= VM_AdaptiveInlining.getNumYieldPoints();
	 double boost   = 
	   1.0 + (VM_Controller.options.MAX_EXPECTED_AI_BOOST * edgeHotness);
	 VM_AINewHotEdgeEvent event = 
	   new VM_AINewHotEdgeEvent(hotMethod, numSamples, boost);
	 if (!VM_Controller.controllerInputQueue.prioritizedInsert(numSamples, 
								   event)) {
	   if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	 } else {
	   if (VM.LogAOSEvents) {
	     VM_AOSLogging.controllerNotifiedForInlining(hotMethod, 
							 numSamples, 
							 boost);
	   }
	 }
       }
     }
     
     if (DEBUG) 
       VM.sysWrite("\nVM_AIByEdgeOrganizer.findMethodsToRecompile() exit\n\n");
   }

  /**
   * Last opportunity to say something.
   * Dump call graph and edge weights.
   */
  public void report() {
     if (VM_Controller.options.FINAL_REPORT_LEVEL >= 2) {
       VM.sysWrite("\n\nVM_AIByEdgeOrganizer.report()\n");
       VM.sysWrite(" callGraph dump\n");
       callGraph.dump();
     }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer that periodically decays runtime counters
 *
 * @author Michael Hind
 **/
final class VM_DecayOrganizer extends VM_Organizer {

  // Yield point listener: will wake up this organizer periodically
  private VM_YieldCounterListener listener;

  /**
   * @param listener the associated listener
   */
  VM_DecayOrganizer(VM_YieldCounterListener listener) {
    this.listener   = listener;
    listener.setOrganizer(this);
  }

  /**
   * Initialization: install and activate our listener.
   */
  public void initialize() {
    VM_RuntimeMeasurements.installNullListener(listener);
    listener.activate();
  }

  /**
   * Method that is called when the sampling threshold is reached
   * We decay the decayable objects and activate the listener again.
   */
  void thresholdReached() {
    VM_RuntimeMeasurements.decayDecayableObjects();
    listener.activate();
  }  
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer for method listener information. 
 * <p>
 * This organizer is designed to work well with non-decayed 
 * cumulative method samples.  The basic idea is that each time 
 * the sampling threshold is reached we update the accumulated method 
 * sample data with the new data and then notify the controller of all 
 * methods that were sampled in the current window.
 * 
 * @author Dave Grove
 */
final class VM_MethodSampleOrganizer extends VM_Organizer {

  /**
   *  Filter out all opt-compiled methods that were compiled 
   * at this level or higher.
   */
  private int filterOptLevel;

  /**
   *  The listener
   */
  private VM_BasicMethodListener listener;

  /**
   * @param listener         the associated listener
   * @param filterOptLevel   filter out all opt-compiled methods that 
   *                         were compiled at this level or higher
   */
  VM_MethodSampleOrganizer(VM_BasicMethodListener listener, 
			   int filterOptLevel) {
    this.listener         = listener;
    this.filterOptLevel   = filterOptLevel;
    listener.setOrganizer(this);
  }

  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (VM.LogAOSEvents) 
      VM_AOSLogging.methodSampleOrganizerThreadStarted(filterOptLevel);

    // Install and activate my listener
    VM_RuntimeMeasurements.installMethodListener(listener);
    listener.activate();
  }

  /**
   * Method that is called when the sampling threshold is reached
   */
  void thresholdReached() {
    if (VM.LogAOSEvents) VM_AOSLogging.organizerThresholdReached();

    int numSamples = listener.getNumSamples();
    int[] samples = listener.getSamples();

    // (1) Update the global (cumulative) sample data
    VM_Controller.methodSamples.update(samples, numSamples);
    
    // (2) Remove duplicates from samples buffer.
    //     NOTE: This is a dirty trick and may be ill-advised.
    //     Rather than copying the unique samples into a different buffer
    //     we treat samples as if it was a scratch buffer.
    //     NOTE: This is worse case O(numSamples^2) but we expect a 
    //     significant number of duplicates, so it's probably better than
    //     the other obvious alternative (sorting samples).
    int uniqueIdx = 1;
  outer:
    for (int i=1; i<numSamples; i++) {
      int cur = samples[i];
      for (int j=0; j<uniqueIdx; j++) {
	if (cur == samples[j]) continue outer;
      }
      samples[uniqueIdx++] = cur;
    }

    // (3) For all samples in 0...uniqueIdx, if the method represented by
    //     the sample is compiled at an opt level below filterOptLevel
    //     and the total (cumulative) number of samples attributed to the
    //     method is above our absolute minimum, then report it to the
    //     controller. We have an absolute minimum value to avoid
    //     considering methods that haven't been sampled enough times to
    //     give us at least some reason to think that the fact that they
    //     were sampled wasn't just random bad luck.
    //     NOTE: this minimum is since the beginning of time, not
    //           just the current window.
    for (int i=0; i<uniqueIdx; i++) {
      int cmid = samples[i];
      double ns = VM_Controller.methodSamples.getData(cmid);
      if (ns >= VM_Controller.options.MIN_SAMPLES) {
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm != null) {		// not already obsoleted
	  int compilerType = cm.getCompilerType();

	  // Enqueue it unless it's either a trap method or already opt
	  // compiled at filterOptLevel or higher.
	  if (!(compilerType == VM_CompiledMethod.TRAP ||
	        (compilerType == VM_CompiledMethod.OPT && 
	         (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {
	    VM_HotMethodRecompilationEvent event = 
	      new VM_HotMethodRecompilationEvent(cm, ns);
	    if (VM_Controller.controllerInputQueue.prioritizedInsert(ns, event)){
	      if (VM.LogAOSEvents) {
	        VM_AOSLogging.controllerNotifiedForHotness(cm, ns);
	      }
	    } else {
	      if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	    }
	  }
	}
      }
    }
    
    // (4) Get the listener ready to go and activate it for the next 
    //     sampling window.
    listener.reset();
    listener.activate();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An VM_Organizer acts an an intermediary between the low level 
 * online measurements and the controller.  An organizer may perform
 * simple or complex tasks, but it is always simply following the 
 * instructions given by the controller.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 */
abstract class VM_Organizer extends VM_Thread {

  /**
   * Called when thread is scheduled.
   */
  public void run() {

    initialize();

    while (true) {
      // sleep until awoken
      synchronized(this) {
        try {
	  wait();
        }
        catch (InterruptedException e) {
	  e.printStackTrace();
        }
      }
      // we've been awoken, process the information
      try {
	thresholdReached();
      }
      catch (Exception e) {
	VM.sysWrite("AOS: WARNING: exception in organizer "+this+"\n");
	e.printStackTrace();

	// Is there a more elegant way to make this exception fatal to
	// the application?
	System.exit(-1);
      }
    } 
  }

  /**
   * Last opportunity to say something.
   */
  public void report() {}

  /**
   * Method that is called when the sampling threshold is reached
   */
  abstract void thresholdReached();

  /**
   * Organizer specific setup.  
   * A good place to install and activate any listeners.
   */
  abstract protected void initialize();

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer for method listener information. 
 * <p>
 * This organizer is designed to work well with non-decayed 
 * cumulative method samples.  The basic idea is that each time 
 * the sampling threshold is reached we update the accumulated method 
 * sample data with the new data and then notify the controller of all 
 * methods that were sampled in the current window.
 * <p>
 * It augments this basic mechanism by tracking estimates of how 
 * many samples are expected for the method in the next sampling window.
 * It then uses the error between the estimated and actual samples 
 * to detect methods that appear to be "ramping up" or "ramping down."  
 * When such methods are detected, the organizer adjusts the number of 
 * samples it reports to the  controller for that method, within bounds 
 * specified by the controller.
 * <p>
 * TODO: Rather than adjusting the num samples reported (ie lying) to
 * the controller, we should add a ramp up/down factor to
 * the hot method event and explictly include it in the model calculation.
 * I'm kludging it for now because it isn't clear if (1) 
 * we can get estimators that will detect the ramp up/down effects accurately
 * enough to be useful and (2) even if we can detect ramp up/down 
 * that adjusting the number of samples will make enough of a difference to
 * overcome the overhead of keeping the history.
 * 
 * @author Dave Grove
 */
final class VM_SlopeDetectingMethodSampleOrganizer extends VM_Organizer {

  private static final boolean DEBUG = false;

  /**
   * Filter out all opt-compiled methods that were compiled 
   * at this level or higher.
   */
  private int filterOptLevel;

  /**
   *  The listener
   */
  private VM_BasicMethodListener listener;

  /**
   * Bounds on adjustment to num samples made by delta from recent history.
   */
  private double adjustmentBounds;
  
  /**
   * mapping from cmid to history array
   */
  private int[] historyMap;

  /**
   * The history arrays.
   * We do not use history[0].
   * For each entry = history[i]:
   *    entry[CMID_IDX]    is the cmid.
   *    entry[EPOCH_IDX]   is the last epoch in which this cmid was sampled.
   *    entry[ENTRY_IDX+k] is the number of samples taken of cmid
   *                       in epoch (k mod numEpochs)
   */
  private int[][] history;
  private static final int FIRST_ENTRY = 1;
  private static final int NOT_MAPPED = 0;
  private static final int CMID_IDX  = 0;
  private static final int EPOCH_IDX = 1;
  private static final int ENTRY_IDX = 2;

  /** idx of next available history entry */
  private int nextEntry = FIRST_ENTRY;

  /**
   * The current epoch number.
   */
  private int curEpoch;

  /**
   * How many epochs should we maintain?
   */
  private int numEpochs;

  /**
   * @param listener         the associated listener
   * @param filterOptLevel   filter out all opt-compiled methods that 
   *                         were compiled at this level or higher
   * @param adjustmentBounds the organizer is allowed to adjust sample
   *                         data by a factor of anywhere from 
   *                         (1-adjustmentBounds) to (1+adjustmentBounds)
   * @param numEpochs        history window size
   */
  VM_SlopeDetectingMethodSampleOrganizer(VM_BasicMethodListener listener, 
					 int filterOptLevel,
					     double adjustmentBounds,
					     int numEpochs) {
    this.listener         = listener;
    this.filterOptLevel   = filterOptLevel;
    this.adjustmentBounds = adjustmentBounds;
    this.numEpochs        = numEpochs;
    this.historyMap       = new int[(int)(VM_CompiledMethods.numCompiledMethods() * 1.25)];
    if (VM.VerifyAssertions) VM.assert(NOT_MAPPED == 0);
    if (VM.VerifyAssertions) VM.assert(NOT_MAPPED != FIRST_ENTRY);
    this.history          = new int[128][];
    listener.setOrganizer(this);
  }


  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (VM.LogAOSEvents) 
      VM_AOSLogging.methodSampleOrganizerThreadStarted(filterOptLevel);

    // Install and activate my listener
    VM_RuntimeMeasurements.installMethodListener(listener);
    listener.activate();
  }


  /**
   * Method that is called when the sampling threshold is reached
   */
  void thresholdReached() {
    if (VM.LogAOSEvents) VM_AOSLogging.organizerThresholdReached();
    prepareForNewEpoch();
    processRawData();
    listener.reset();
    listener.activate();
  }


  // Prepare history data structures to process a new epoch of samples
  private void prepareForNewEpoch() {
    // advance epoch
    if (++curEpoch == numEpochs) {
      curEpoch = 0;
    }
    
    // remove completely defunct entries or clear the data in
    // entry[ENTRY_IDX+curEpoch] to prepare to put new data there.
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      if (entry[EPOCH_IDX] == curEpoch) {
	clearHistory(entry[CMID_IDX]);
      } else {
	entry[ENTRY_IDX+curEpoch] = 0;
      }
    }
  }
  
  
  // Process the raw data from the listener by accumulating samples into
  // the entries for curEpoch, updating the global method sample data,
  // and notifying the controller of interesting methods (reporting their
  // adjusted number of samples -- see TODO in class header comment).
  private void processRawData() {
    int numSamples = listener.getNumSamples();
    int[] samples = listener.getSamples();
    
    for (int i=1; i<numSamples; i++) {
      int cmid = samples[i];
      int[] entry = getEntry(cmid);
      entry[ENTRY_IDX+curEpoch]++;
      entry[EPOCH_IDX] = curEpoch;
    }

    if (DEBUG) dumpHistory();

    for (int i= FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      if (entry[EPOCH_IDX] == curEpoch) {
	int cmid = entry[CMID_IDX];
	int samplesThisTime = entry[ENTRY_IDX+curEpoch];
	double samplesThisTimeDouble = (double)samplesThisTime;

	// (1) update global sampling data
	VM_Controller.methodSamples.update(cmid, samplesThisTimeDouble);
	double totalSamples = VM_Controller.methodSamples.getData(cmid);

	// (2) See if the controller cares about this method. 
	//     If it does, then make a prediction and notify the controller.
	//     The Controller cares unless it's either a trap method or 
	//     already opt compiled at filterOptLevel or higher.
	//     But, we require that a method be sampled at least 3.0 times
	//     before we report it to the controller to avoid reporting truly
	//     cold but randomly sampled once or twice methods.
	if (totalSamples > 3.0) {
	  VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	  if (cm != null) {
	    int compilerType = cm.getCompilerType();
	    if (!(compilerType == VM_CompiledMethod.TRAP ||
		  (compilerType == VM_CompiledMethod.OPT && 
		   (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {

	      // (2a) compute prediction
	      int histSamples = -samplesThisTime;
	      for (int j=0; j<numEpochs; j++) {
		histSamples += entry[ENTRY_IDX+j];
	      }
	      double prediction = 
		((double)histSamples) / ((double)(numEpochs-1));
	      double adjFactor;
	      if (prediction > 0.00001) {
		double slope = samplesThisTimeDouble/prediction;
		if (DEBUG) {
		  VM_Method meth = cm.getMethod();
		  VM.sysWrite("For method "+meth+"("+cmid+") with thisTime="+samplesThisTime+
			      " and total="+totalSamples+ " and prediction ="+prediction+
			      " and history ");
		  for (int k=0; k<numEpochs; k++) {
		    VM.sysWrite(entry[ENTRY_IDX+k],false);
		    VM.sysWrite(" ");
		  }
		  VM.sysWrite("\n\tWe compute a slope of "+slope);
		}

		// Bound adjustment factor as previously instructed by controller
		adjFactor = slope;
		if (adjFactor < 1.0) {
		  adjFactor = 1.0 - (0.5 * (1.0 - slope)); // dampen adjustment
		  if (adjFactor < adjustmentBounds) {
		    adjFactor = adjustmentBounds;
		  } 
		} else {
		  adjFactor = 1.0 + (0.5 * (slope - 1.0 )); // dampen adjustment
		  if (adjFactor > adjustmentBounds + 1.0) {
		    adjFactor = 1.0 + adjustmentBounds;
		  }
		}
	      } else {
		adjFactor = 1.0 + (adjustmentBounds / 2.0);
		if (DEBUG) {
		  VM_Method meth = cm.getMethod();
		  VM.sysWrite("No sample history for method "+meth+"("+cmid+")");
		}
	      }

	      // (2b) adjust totalSamples based on how different it is from the prediction
	      totalSamples = adjFactor * totalSamples;
	      if (DEBUG) VM.sysWrite(" leading to an adjustment factor of "+adjFactor+
				     " and adjusted totalSamples="+
				     totalSamples+"\n");

	      // (2c) tell the controller about this method
	      VM_HotMethodRecompilationEvent event = 
		new VM_HotMethodRecompilationEvent(cm, totalSamples);
	      if (VM_Controller.controllerInputQueue.prioritizedInsert(totalSamples, event)){
		if (VM.LogAOSEvents) {
		  VM_AOSLogging.controllerNotifiedForHotness(cm, totalSamples);
		}
	      } else {
		if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	      }
	    }
	  }
	}
      }
    }
  }

  private void dumpHistory() {
    VM.sysWrite("\n######\ncurEpoch = "+curEpoch+
		", numEpochs = "+numEpochs+"\n");
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      VM.sysWrite("cmid = "+entry[CMID_IDX]+", lastEpoch ="+entry[EPOCH_IDX]+
		  "\thistory = ");
      for (int j=0; j<numEpochs; j++) {
	VM.sysWrite(entry[ENTRY_IDX+j],false);
	VM.sysWrite(" ");
      }
      VM.sysWrite("\n");
    }

    VM.sysWrite("Checking history map...");
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      if (historyMap[history[i][CMID_IDX]] != i) {
	VM.sysWrite("Mismatch between historyMap and entry (1) "+i);
      }
    }
    for (int i=0; i<historyMap.length; i++) {
      if (historyMap[i] != 0) {
	if (history[historyMap[i]][CMID_IDX] != i) {
	  VM.sysWrite("Mismatch between historyMap and entry (2) "+i);
	}
      }
    }
    VM.sysWrite("...done\n");
  }

  // get the entry[] for a cmid, allocating new entries and
  // growing the backing store as needed.
  private int[] getEntry(int cmid) {
    if (cmid >= historyMap.length) {       // grow historyMap
      int[] tmp = new int[(int)(VM_CompiledMethods.numCompiledMethods() * 1.25)];
      for (int i=0; i< historyMap.length; i++) {
	tmp[i] = historyMap[i];
      }
      historyMap = tmp;
    }
    if (historyMap[cmid] == NOT_MAPPED) {
      int idx = nextEntry++;
      if (idx >= history.length) {         // grow history
	int[][] tmp = new int[history.length*2][];
	for (int i=0; i<history.length; i++) {
	  tmp[i] = history[i];
	}
	history = tmp;
      }
      if (VM.VerifyAssertions) VM.assert(history[idx] == null);
      history[idx] = new int[ENTRY_IDX+numEpochs];
      history[idx][CMID_IDX] = cmid;
      historyMap[cmid] = idx;
      return history[idx];
    } else {
      return history[historyMap[cmid]];
    }
  }

  // remove an entry
  private void clearHistory(int cmid) {
    if (DEBUG) VM.sysWrite("Clearing history for "+cmid+"\n");
    int entryIdx = historyMap[cmid];
    if (entryIdx != NOT_MAPPED) {
      historyMap[cmid] = NOT_MAPPED;
      nextEntry--;
      if (entryIdx<nextEntry) {
	history[entryIdx] = history[nextEntry];
	historyMap[history[entryIdx][CMID_IDX]] = entryIdx;
      }
      history[nextEntry] = null;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.io.*;

/**
 * This class provides logging functionality for the Adaptive Optimization System
 *
 * Right now this is fairly primitive, an evolving number of events are
 * defined and log entries are quite unsophisticated.
 * Some obvious TODO items:
 *  -- compact encoding of log entries
 *  -- some notion of log format versions
 *  -- ...
 *
 * NOTE: All code that writes to the log is synchronized on the PrintStream
 *      object to avoid interspersed messages, which can happen when the
 *      compilation thread and the controller thread try to log a message
 *      "at the same time".
 * 
 * ***When is the log file flushed and closed?
 * ***Do we want to put report() information in the log?
 *
 * The current logging levels are:
 *   0  Do no logging
 *   1  Do minimal logging at startup and VM exit.  
 *      If at all possible, do not log anything during program execution.
 *      This logging level is supposed to produce minimal performance pertubation.
 *   2  Log interesting AOS events and controller actions
 *   3  Exhaustively log pretty much everything that is going on
 *
 * @author Dave Grove
 * @author Michael Hind
 * @modified Peter Sweeney
 */
class VM_AOSLogging {

  /*
   * The output file stream, where all log messages will go
   */
  private static PrintStream log;
  
   /*
    * Record that the AOS logging has been booted.
    * Needed to allow fast exit from reporting to ensure
    * that when no class is specified to be run but "-help" is specified, 
    * don't want null pointer exception to occur!
    */
   private static boolean booted = false;

   /**
    * Return whether AOS logging has booted.
    * @return whether AOS logging has booted
    */
   public static boolean booted() {
     return booted;
   }

  /**
   * Called from VM_ControllerThread.run to initialize the logging subsystem
   */
  public static void boot() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      try {
	log = new PrintStream(new FileOutputStream(VM_Controller.options.LOGFILE_NAME));

	// This statement will force the compilation of println, so it
	// is needed regardless of the particular content of the message!
	synchronized (log) {
	  log.println(VM_Controller.controllerClock +" Logging enabled\n");
	  log.println(VM_Controller.options);
	}
      }
      catch (IOException e) {
	VM.sysWrite("IOException caught in VM_AOSLogging.java while trying to create and start log file.\n");
	VM.sysWrite("Please check for file permission problems\n");
      }
    }
    booted = true;
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 1
  ////////////////////////////////////////////////////////////////

  /**
   * Called from VM_Controller.report to allow a last message to the logging
   *  system
   */
  public static void systemExiting() {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +" System Exiting\n");
      }
    }
  }

  /**
   * Called from VM_RuntimeMeasurements when the argument thread is terminating
   * to allow us to record the time spent in the thread.
   * @param t the thread of interest
   */
  public static void threadExiting(VM_Thread t) {
    if (!booted) return; // fast exit
    try {
      if (VM_Controller.options.LOGGING_LEVEL >= 1) {
	synchronized (log) {
	  log.println(VM_Controller.controllerClock +
		      " ThreadIndex: " + t.getIndex() + " "
		      + t.getClass().getName() + " "
		      + " Time: " 
		      + t.cpuTotalTime
		      + " status("
		      + (  t.isIdleThread ?     "i"         // idle daemon
			   : t.isGCThread   ?     "g"       // gc daemon
			   : t.isDaemon     ?     "d"       // user daemon
			   :                      "" )
		      + (!t.isAlive     ?     "!" : "")     // dead/alive
		      + (t.cpuStartTime > 0 ? "+" : "-")    // running/stopped
		      + ")"
		      );
	}
      }
    } catch (NullPointerException e) {
      // ignore.  A thread exited before the AOS Logging system was
      // initialized.  It can't be interesting.
    }
  }

  /**
   * Call this method when the controller thread initially begins executing
   */
  public static void controllerStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Controller thread started");
      }
    }
  }

  /**
   * Call this method to dump statistics on how often listeners are invoked.
   * @param method method listener info
   * @param context context listener info
   * @param nll null listener info
   */
  public static void listenerStatistics(int method, int context, int nll) {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Listeners called:"+
		    "\n\t method "+method+"\n\t context "+context
		    +"\n\t null "+nll);
      }
    }
  }

  /**
   * Call this method to dump statistics related to decaying
   * @param decayCount the number of decay events
   */
  public static void decayStatistics(int decayCount) {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >=  1) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		  +" Decay Organizer Statistics: \n\t"+
		  " Num of Decay events: "+
		  decayCount+"\n");
      }
    }
  }

  /**
   * Call this method when one run of the application is completed
   */
  public static void appRunComplete() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Application completed a run");
      }
    }
  }

  /**
   * Call this method when the controller thread is exiting
   */
  public static void controllerCompleted() {
    if (!booted) return; // fast exit
  
    int awoken = VM_ControllerMemory.getNumAwoken(); 
    int didNothing = VM_ControllerMemory.getNumDidNothing();
    int numMethodsConsidered = VM_ControllerMemory.getNumMethodsConsidered();
    int numMethodsScheduledForRecomp = 
                    VM_ControllerMemory.getNumMethodsScheduledForRecomp(); 
    int numOpt0 = VM_ControllerMemory.getNumOpt0();
    int numOpt1 = VM_ControllerMemory.getNumOpt1();
    int numOpt2 = VM_ControllerMemory.getNumOpt2();
    int numOpt3 = VM_ControllerMemory.getNumOpt3();

    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		  +" Controller thread exiting ... "
		  +"\n  Num times Controller thread is awoken: "+
		  awoken
		  +"\n  Num times did nothing: "+ didNothing +" ("+
		  ((int)((float)didNothing/(float)awoken * 100))
		  +"%)\n  Num methods baseline compiled: "+
		  VM_ControllerMemory.getNumBase()
		  +"\n  Num methods considered for recompilation: "+
		  numMethodsConsidered
		  +"\n  Num methods chosen to recompile: "+ 
		  numMethodsScheduledForRecomp +" ("+
		  ((int) ((float) numMethodsScheduledForRecomp
		                         /  numMethodsConsidered * 100))
		  +"%)\n  Opt Levels Chosen: "
		  +"\n\t Opt Level 0: "+ numOpt0 +" ("+
		  ((int) ((float) numOpt0/numMethodsScheduledForRecomp * 100))

		  +"%)\n\t Opt Level 1: "+ numOpt1 +" ("+
		  ((int) ((float) numOpt1/numMethodsScheduledForRecomp * 100))
		  +"%)\n"

		  +"\t Opt Level 2: "+ numOpt2 +" ("+
		  ((int) ((float) numOpt2/numMethodsScheduledForRecomp * 100))
		  +"%)\n"

		  +"\t Opt Level 3: "+ numOpt3 +" ("+
		  ((int) ((float) numOpt3/numMethodsScheduledForRecomp * 100))
		  +"%)\n\n");

	// Let the controller memory summarize itself to the log file
	VM_ControllerMemory.printFinalMethodStats(log);
      }
    }
  }


  /**
   * Call this method when the compilation thread initially begins executing
   */
  public static void compilationThreadStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation thread started");
      }
    }
  }

  /**
   * Call this method when the compilation thread is exiting
   */
  public static void compilationThreadCompleted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation thread exiting");
      }
    }
  }

  /**
   * Call this method when the organizer thread initially begins executing
   * @param filterOptLevel the opt level that we are filtering
   */
  public static void methodSampleOrganizerThreadStarted(int filterOptLevel) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Method Sample Organizer thread started");
	log.println("  filterOptLevel: "+ filterOptLevel);
      }
    }
  }

  /**
   * Call this method when the organizer thread initially begins executing
   */
  public static void AIByEdgeOrganizerThreadStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Adaptive Inlining (AI) by Edge Organizer thread started");
      }
    }
  }

  /**
   * This method reports the basic speedup rate for a compiler
   * @param compiler the compiler you are reporting about
   * @param rate the speedup rate
   */
  public static void reportSpeedupRate(int compiler, double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" SpeedupRate for "+ 
		    VM_CompilerDNA.getCompilerString(compiler)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   * This method reports the basic compilation rate for a compiler
   * @param compiler the compiler you are reporting about
   * @param rate the compilation rate (bytecodes per millisecond)
   */
  public static void reportCompilationRate(int compiler, double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation Rate (bytecode/msec) for "+ 
		    VM_CompilerDNA.getCompilerString(compiler)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   *  This method reports the benefit ratio from one compiler to the other
   *  @param compiler1 the first compiler
   *  @param compiler2 the second compiler
   *  @param rate the improvement from going from a compiler1-compiled method
   *                   to a compiler2-compiled method
   */
  public static void reportBenefitRatio(int compiler1, 
					int compiler2, 
					double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Benefit Ratio from "+
		    VM_CompilerDNA.getCompilerString(compiler1)
		    +" compiler to "+
		    VM_CompilerDNA.getCompilerString(compiler2)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   *  This method reports the compile time ratio from one compiler to
   *  the other
   *  @param compiler1 the first compiler
   *  @param compiler2 the second compiler
   *  @param rate the ratio of compiler1 compilation rate to 
   *                compiler2 compilation rate
   */
  public static void reportCompileTimeRatio(int compiler1, 
					    int compiler2, 
					    double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compile Time Ratio of "+
		    VM_CompilerDNA.getCompilerString(compiler1)
		    +" compiler to "+
		    VM_CompilerDNA.getCompilerString(compiler2)
		    +" compiler: "+ rate);
      }
    }
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 2
  ////////////////////////////////////////////////////////////////

  /**
   * This method logs the scheduling of a recompilation,
   * i.e., it being inserted in the compilation queue.
   * @param plan the OPT_Compilation plan being executed.
   * @param priority a number from 0.0 to 1.0 encoding the plan's priority.
   */
  public static void recompilationScheduled(OPT_CompilationPlan plan, 
					    double priority) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Scheduling level "+
		    plan.options.getOptLevel() +" recompilation of "+
		    plan.method+" (plan has priority "+priority+")");
      }
    }
  }

  /**
   * This method logs when a recompilation could not occur
   * because the queue is full
   * @param plan the OPT_Compilation plan that would have been executed
   */
  public static void recompilationQueueFull(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Level "+ plan.options.getOptLevel()
		    +" recompilation postponed of "+plan.method);
      }
    }
  }

  /**
   * This method logs when the controller could not be notified 
   * of an event because the controllerInputQueue was full.
   * @param event the event the controller would have been told about
   */
  public static void controllerInputQueueFull(Object event) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock + 
		    " Unable to inform controller of "+event+ " due to full input queue");
      }
    }
  }

  /**
   * This method logs the beginning of an adaptively selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationStarted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +" Recompiling (at level "+
		    plan.options.getOptLevel() +") "+ plan.method);
      }
    }
  }

  /**
   * This method logs the successful completion of an adaptively 
   * selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationCompleted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +"  Recompiled (at level "+
		    plan.options.getOptLevel() +") " +plan.method);
      }
    }
  }

  /**
   * This method logs the abortion of an adaptively selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationAborted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Failed recompiling (at level "+ 
		    plan.options.getOptLevel()+" "+ plan.method);
      }
    }
  }

  /**
   * this method logs the event when the controller discovers a method that has 
   * been recompiled and the previous version is still regarded as hot, 
   * i.e., still on the stack and signficant.
   */
  public static void oldVersionStillHot(VM_HotMethodEvent hme) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Found a method with an old version still hot "+ hme);
      }
    }
  }

  /**
   * This method logs when the decay organizer runs.
   */
  public static void decayingCounters() {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Decaying clock and decayable objects");
      }
    }
  }

  /**
   * This Method logs when the organizer thread has reached its
   * sampling threshold
   */
  public static void organizerThresholdReached() {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" OrganizerThread reached sample size threshold\n");
      }
    }
  }

  /**
   * This method reports a bulk detection of hot max-opt-level methods to 
   * have their call edges inspected.
   *
   * @param numMethods the total number of max opt level methods found to be hot
   *                     and will have their call edges inspected
   */
  public static void AIorganizerFoundHotMethods(int numMethods) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer found "+numMethods
		    +" hot max-opt-level methods, will inspect their call edges.");
      }
    }
  }

  /**
   * This method logs that the a hot call edge from an max-opt-level
   * method has been identified.
   * 
   * @param hotMethod	method to be recompiled,
   * @param numSamples  number of samples attributed to the method 
   * @param boost 	expected boost factor
   */
  public static void inliningOpportunityDetected(VM_CompiledMethod hotMethod,
						 double numSamples, 
						 VM_CallSiteTriple triple) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer found method "+hotMethod.getMethod()+
		    " with "+numSamples+" samples that has an edge "+
		    triple+" that can be inlined");
      }
    }
  }

  /**
   * This method logs that the controller is notified of a 
   * candidate to be recompiled due to inlining opportunities;
   * i.e., the method has been inserted in the controller queue.
   * @param hotMethod	method to be recompiled,
   * @param numSamples	number of samples attributed to the method
   * @param triple	edge that should be inlined.
   */
  public static void controllerNotifiedForInlining(VM_CompiledMethod hotMethod,
						   double numSamples, 
						   double boost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer notified controller that method "+
		    hotMethod.getMethod()+" with "+numSamples+
		    " samples could be recompiled with a boost of "+boost);
      }
    }
  }

  /**
   * This method logs that the controller is notified of a 
   * candidate to be recompiled due to hotness;
   * i.e., the method has been inserted in the controller queue.
   * @param hotMethod	method to be recompiled, and
   * @param numSamples	number of samples attributed to the method
   */
  public static void 
    controllerNotifiedForHotness(VM_CompiledMethod hotMethod,
				 double numSamples) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Controller notified that method "+hotMethod.getMethod()+
		    " has "+numSamples+" samples");
      }
    }
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 3
  ////////////////////////////////////////////////////////////////

  /**
   * This method logs a controller cost estimate for doing nothing
   * @param method the method of interest
   * @param optLevel the opt level being estimated, -1 = baseline
   * @param cost  the computed cost for this method and level
   */
  public static void 
    recordControllerEstimateCostDoNothing(VM_Method method, 
					  int optLevel,
					  double cost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 3) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		    +"  Estimated cost of doing nothing (leaving at ");
	if (optLevel == -1) {
	  log.print("baseline");
	}
	else {
	  log.print("O"+ optLevel);
	}
	log.println(") to "+ method +" is "+ cost);
      }
    }
  }

  /**
   * This method logs a controller cost estimate
   * @param method the method of interest
   * @param choiceDesc
   @ @param cost  the computed cost for this method and level
   */
  public static void 
    recordControllerEstimateCostOpt(VM_Method method, 
				    String choiceDesc,
				    double cost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 3) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +"  Estimated cost of OPT compiling "+
		    method + " at " + choiceDesc +
		    " is "+ cost);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class extends VM_PriorityQueue to safely
 * support multiple producers/consumers where 
 * the consumers are blocked if no objects are available
 * to consume.
 *
 * @author Dave Grove
 * @author Michael Hind
 */
class VM_BlockingPriorityQueue extends VM_PriorityQueue {

  /**
   * Used to notify consumers when about to wait and when notified
   * Default implementation does nothing, but can be overriden as needed by client.
   */
  public static class CallBack {
    void aboutToWait() {}
    void doneWaiting() {}
  }
  CallBack callback;

  /**
   * @param initialSize the initial number of elements
   * @param _cb the callback object
   */
  VM_BlockingPriorityQueue(int initialSize, CallBack _cb) {
    super(initialSize);
    callback = _cb;
  }

  /**
   * @param initialSize the initial number of elements
   */
  VM_BlockingPriorityQueue(int initialSize) {
    this(initialSize, new CallBack());
  }

  /**
   * Insert the object passed with the priority value passed
   * 
   * Notify any sleeping consumer threads that an object
   * is available for consumption.
   *
   * @param _priority  the priority to 
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized final public boolean insert(double _priority, Object _data) { 
    boolean success = super.insert(_priority, _data);
    if (success) {
      try {
	notifyAll();
      } catch (Exception e) {
	// TODO: should we exit or something more dramatic?
	VM.sysWrite("Exception occurred while notifying that element was inserted!\n");
      }
    }
    return success;
  }

  /**
   * Remove and return the front (minimum) object.  If the queue is currently
   * empty, then block until an object is available to be dequeued.
   * @param callback a VM_BlockingPriorityQueueCallback object. 
   * @return the front (minimum) object.
   */
  synchronized final public Object deleteMin() {
    // While the queue is empty, sleep until notified that an object has been enqueued.
    while (isEmpty()) {
      try {
	callback.aboutToWait();
	wait();
	callback.doneWaiting();
      } catch (InterruptedException e) {
	// TODO: should we exit or something more dramatic?
	VM.sysWrite("Interrupted Exception occurred!\n");
      }
    }

    // When we get to here, we know the queue is non-empty, so dequeue an object and return it.
    return super.deleteMin();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class implements a priority queue using the standard
 * (balanced partially-ordered tree, i.e., "heap") algorithm.  
 * Smaller priority objects are in the front of the queue.
 *
 * @author Michael Hind
 */
class VM_PriorityQueue {

  private static final boolean DEBUG = false;

  /**
   * the queue, we use elements 1..size
   */
  private VM_PriorityQueueNode[] queue; 

  /**
   * the index of the last valid entry
   *  size = queue.length - 1
   */
  private int size;     

  /**
   * the number of elements actually in the queue
   */
  private int numElements; 
  
  /**
   * Constructor
   * @param initialSize the initial number of elements
   */
  VM_PriorityQueue(int initialSize) {
    // We don't use element #0
    int allocSize = initialSize+1;
    queue = new VM_PriorityQueueNode[allocSize];
    
    for (int i=0; i<allocSize; i++) {
      queue[i] = new VM_PriorityQueueNode();
    }

    // We use elements 1..size
    size = initialSize;
    numElements = 0;
  }

  /**
   * Determines number of elements in the queue
   * @return number of elements in the queue
   */
  synchronized final public int numElements() {
    return numElements;
  }

  /**
   * Checks if the queue is empty
   * @return is the queue empty?
   */
  synchronized final boolean isEmpty() {
    return numElements == 0;
  }

  /**
   * Checks if the queue is full
   * @return is the queue full?
   */
  synchronized final boolean isFull() {
    return numElements == size;
  }

  /**
   * Starting at the position passed, swap with parent until heap condition
   * is satisfied, i.e., bubble up
   * @param startingElement the position to start at 
   */
  private void reheapify(int startingElement) {
    int current = startingElement;
    int parent = numElements / 2;
    // keep checking parents that violate the magic condition
    while (parent > 0 && queue[parent].priority < queue[current].priority) {
      //	System.out.println("Parent: "+ parent +", Current: "+ current);
      //	System.out.println("Contents before: "+ this);
      // exchange parrent and current values
      VM_PriorityQueueNode tmp = queue[parent];
      queue[parent] = queue[current];
      queue[current] = tmp;
      
      //	System.out.println("Contents after: "+ this);
      // go up 1 level
      current = parent;
      parent = parent / 2;
    }
  }

  /**
   * Insert the object passed with the priority value passed
   * @param _priority  the priority of the inserted object
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized public boolean insert(double _priority, Object _data) {
    if (isFull()) {
      return false;
    }

    numElements++;
    queue[numElements].data = _data;
    queue[numElements].priority = _priority;
    
    // re-heapify
    reheapify(numElements);
    return true;
  }

  /**
   * Insert the object passed with the priority value passed, but if the queue
   *   is full, a lower priority object is removed to make room for this object.
   *   If no lower priority object is found, we don't insert.
   * @param _priority  the priority to 
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized public boolean prioritizedInsert(double _priority, Object _data) {
    if (DEBUG) VM.sysWrite("prioInsert: prio: "+_priority+",size: "+
			   size +", numElements: "+ numElements +")\n");

    // the queue isn't full just use the regular insert
    if (!isFull()) {
      return insert(_priority, _data);
    }

    // search the leaves of the tree and find the lowest priority element
    //  "numElements" is the last element.  The first leaf is the next
    //  node after its parent, i.e,  numElements/2  + 1
    //  We'll go from this value up to numElements.
    int firstChild = numElements/2 + 1;
    int evictee = -1;
    double evicteePriority = _priority;  // start of with the priority of the insertor
    for (int i=firstChild; i<=numElements; i++) {
      if (queue[i].priority < evicteePriority) {
	if (DEBUG) VM.sysWrite("  candidate at entry "+ i 
			       +", prio: "+queue[i].priority +")\n");
	evictee = i;
	evicteePriority = queue[i].priority;
      }
    }

    // Did we find an evictee?
    if (evictee != -1) {
      queue[evictee].data = _data;
      queue[evictee].priority = _priority;
    
      if (DEBUG) VM.sysWrite("  evicting entry "+ evictee +")\n");

      // re-heapify
      reheapify(evictee);
      return true;
    }
    else {
      // didn't find a spot :-(
      return false;
    }
  }

  /**
   * Remove and return the front (minimum) object
   * @return the front (minimum) object or null if the queue is empty.
   */
  synchronized public Object deleteMin() {
    if (isEmpty()) return null;

    Object returnValue = queue[1].data;
    // move the "last" element to the root and reheapify by pushing it down
    queue[1].priority = queue[numElements].priority;
    queue[1].data = queue[numElements].data;
    numElements--;
    
    // reheapify!!!
    int current = 1;
    
    // The children live at 2*current and  2*current+1 
    int child1 = 2 * current;
    while (child1 <= numElements) {
      int child2 = 2 * current + 1;
      
      // find the smaller of the two children
      int smaller;
      if (child2 <= numElements && queue[child2].priority > queue[child1].priority) {
	smaller = child2;
      } else {
	smaller = child1;
      }
      
      if (queue[smaller].priority <= queue[current].priority) {
	break;
      }
      else {
	// exchange parrent and current values
	VM_PriorityQueueNode tmp = queue[smaller];
	queue[smaller] = queue[current];
	queue[current] = tmp;
	
	// go down 1 level
	current = smaller;
	child1 = 2 * current;
      }
    }
    return returnValue;
  }

  /**
   *  Return the priority of front object without removing it
   *  @return the priority of the front object
   */
  synchronized final public double rootValue() {
    if (VM.VerifyAssertions) VM.assert(!isEmpty());

    return queue[1].priority;
  }

  /**
   *  Prints the contents of the queue
   *  @return the queue contents
   */
  synchronized public String toString() {
    StringBuffer sb = new StringBuffer(" --> ");
    sb.append("Dumping Queue with "+ numElements +" elements:\n");
    if (numElements >= 1) {
      sb.append("\t");
    }

    for (int i=1; i<=numElements; i++) {
      sb.append(queue[i].toString());
      if (i<numElements)
	sb.append("\n\t");
    }
    return sb.toString();
  }
}

/**
 * A local class that holds the nodes of the priority tree
 */
class VM_PriorityQueueNode {

  /**
   * the value to compare on, larger is better
   */
  public double priority; 

  /**
   * the associated data 
   */
  public Object data;     

  public String toString() {
    return (new StringBuffer(data +" ... ["+ priority +"]")).toString();
  }

}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Dummy class containing enough references to force java compiler
 * to find every class comprising the vm, so everything gets recompiled
 * by just compiling "Dummy.java".
 *
 * The minimal set has to be discovered by trial and error. Sorry.
 *
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_WriteBarrier;
class Dummy {
  static VM                         a;
  static VM_TableBasedDynamicLinker b;
  static VM_DynamicLinker           c;
  static VM_Runtime                 d;
  static VM_Reflection              e;
  static java.lang.Void             g;
  static java.io.InputStreamReader  h;
  static java.io.StreamTokenizer    i;
  static java.net.PlainSocketImpl   j;
  static java.net.ServerSocket      k;
  static VM_WriteBarrier            l;
  static VM_JNIFunctions            m;
  static VM_JNIStartUp              n;
  static VM_RecompilationManager    o;
  //-#if RVM_WITH_CONCURRENT_GC
  static VM_RCBuffers               p; // not used by opt yet, but referenced in VM_Entrypoints
  static VM_OptRCWriteBarrier       q; // not used by opt yet, but referenced in VM_Entrypoints
  //-#endif
  static VM_MultianewarrayHelper    r;
  static VM_Address                 s;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A virtual machine.
 * Implements VM_Uninterruptible to suppress thread switching in boot() and
 * sysCall() prologues.
 *
 * @author Derek Lieber (project start).
 * @date 21 Nov 1997 
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

public class VM extends VM_Properties implements VM_Constants, 
VM_Uninterruptible { 
  //----------------------------------------------------------------------//
  //                          Initialization.                             //
  //----------------------------------------------------------------------//

  /** 
   * Prepare vm classes for use by boot image writer.
   * @param classPath class path to be used by VM_ClassLoader
   * @param bootCompilerArgs command line arguments for the bootimage compiler
   */ 
  static void initForBootImageWriter(String classPath, 
                                     String[] bootCompilerArgs) 
    throws VM_ResolutionException, VM_PragmaInterruptible {
    writingBootImage = true;
    init(classPath, bootCompilerArgs);
  }

  /**
   * Prepare vm classes for use by tools.
   * @exception VM_ResolutionException
   */
  static void initForTool() throws VM_ResolutionException, VM_PragmaInterruptible  {
    runningTool = true;
    LoadLocalVariableTables = true;  // make sure to load the local table
    init(System.getProperty("java.class.path"), null);
  }

  /**
   * Prepare vm classes for use by tools.
   * @param classpath class path to be used by VM_ClassLoader
   * @exception VM_ResolutionException
   */
  static void initForTool(String classpath) throws VM_ResolutionException, VM_PragmaInterruptible {
    runningTool = true;
    LoadLocalVariableTables = true;  // make sure to load the local table
    init(classpath, null);
  }

  static int verbose = 0;  // Show progress of boot 

  /**
   * Begin vm execution.
   * The following machine registers are set by "C" bootstrap program 
   * before calling this method:
   *    JTOC_POINTER        - required for accessing globals
   *    FRAME_POINTER       - required for accessing locals
   *    THREAD_ID_REGISTER  - required for method prolog (stack overflow check)
   * @exception Exception
   */
  public static void boot() throws Exception, VM_PragmaLogicallyUninterruptible {
    VM.writingBootImage = false;
    VM.runningVM        = true;
    VM.runningAsSubsystem = false;

    if (verbose >= 1) VM.sysWriteln("Booting");

    // Set up the current VM_Processor object.  The bootstrap program
    // has placed a pointer to the current VM_Processor in a special
    // register.
    if (verbose >= 1) VM.sysWriteln("Setting up current VM_Processor");
    VM_ProcessorLocalState.boot();


    // Finish thread initialization that couldn't be done in boot image.
    // The "stackLimit" must be set before any method calls, 
    // because it's accessed by compiler-generated stack overflow checks.
    //
    if (verbose >= 1) VM.sysWriteln("Doing thread initialization");
    VM_Thread currentThread  = VM_Scheduler.threads[VM_Magic.getThreadId() >>> VM_ThinLockConstants.TL_THREAD_ID_SHIFT];
    currentThread.stackLimit = VM_Magic.objectAsAddress(currentThread.stack).add(STACK_SIZE_GUARD);

    VM_Processor.getCurrentProcessor().activeThreadStackLimit = currentThread.stackLimit;

    // get pthread_id from OS and store into vm_processor field
    // 
    if (!BuildForSingleVirtualProcessor)
      VM_Processor.getCurrentProcessor().pthread_id = 
        VM.sysCall0(VM_BootRecord.the_boot_record.sysPthreadSelfIP);

    VM.TraceClassLoading = (VM_BootRecord.the_boot_record.traceClassLoading == 1);   

    // Initialize memory manager's write barrier.
    // This must happen before any putfield or arraystore of object refs
    // because the buffer is accessed by compiler-generated write barrier code.
    //
    if (verbose >= 1) VM.sysWriteln("Setting up write barrier");
    if (VM_Collector.NEEDS_WRITE_BARRIER) {
      VM_Collector.setupProcessor( VM_Processor.getCurrentProcessor() );
    }

    // Initialize memory manager.
    //    This must happen before any uses of "new".
    //
    if (verbose >= 1) VM.sysWriteln("Setting up memory manager");
    VM_Collector.boot(VM_BootRecord.the_boot_record);

    // Reset the options for the baseline compiler to avoid carrying them over from
    // bootimage writing time.
    // 
    VM_BaselineCompiler.initOptions();

    // Create class objects for static synchronized methods in the bootimage.
    // This must happen before any bootimage static synchronized methods 
    // can be invoked.
    if (verbose >= 1) VM.sysWriteln("Creating class objects for static synchronized methods");
    createClassObjects();

    // Fetch arguments from program command line.
    //
    if (verbose >= 1) VM.sysWriteln("Fetching command-line arguments");
    VM_CommandLineArgs.fetchCommandLineArguments();

    // Initialize class loader.
    //
    if (verbose >= 1) VM.sysWriteln("Initializing class loader");
    String vmClasses = VM_CommandLineArgs.getVMClasses();
    VM_ClassLoader.boot(vmClasses);

    //
    // At this point the virtual machine is running as a single thread 
    // that can perform dynamic compilation and linking (by compiler/linker 
    // that's part of boot image).  All that remains is to initialize the 
    // java class libraries, start up the thread subsystem, and launch
    // the user level "main" thread.
    //

    // Initialize statics that couldn't be placed in bootimage, either 
    // because they refer to external state (open files), or because they 
    // appear in fields that are unique to RVM implementation of 
    // standard class library (not part of standard jdk).
    // We discover the latter by observing "host has no field" and 
    // "object not part of bootimage" messages printed out by bootimage 
    // writer.
    //

    if (verbose >= 1) VM.sysWriteln("Running various class initializers");
    //-#if RVM_WITH_GNU_CLASSPATH
    java.lang.ref.Reference.lock = new Object();
    //-#else
    runClassInitializer("java.io.FileDescriptor");
    //-#endif

    runClassInitializer("java.lang.Runtime");
    runClassInitializer("java.lang.System");
    System.boot();
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.io.FileDescriptor");
    //-#endif
    runClassInitializer("java.io.File");
    runClassInitializer("java.lang.Boolean");
    runClassInitializer("java.lang.Byte");
    runClassInitializer("java.lang.Short");
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.lang.Number");
    //-#endif
    runClassInitializer("java.lang.Integer");
    runClassInitializer("java.lang.Long");
    runClassInitializer("java.lang.Float");
    runClassInitializer("java.lang.Double");
    runClassInitializer("java.lang.Character");
    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    runClassInitializer("com.ibm.oti.io.CharacterConverter");
    //-#endif
    runClassInitializer("java.util.Hashtable");
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.lang.Class");
    runClassInitializer("gnu.java.io.EncodingManager");
    runClassInitializer("java.lang.Thread");
    runClassInitializer("java.lang.ThreadGroup");
    runClassInitializer("java.io.PrintWriter");
    System.makeStandardStreams();
    runClassInitializer("gnu.java.lang.SystemClassLoader");
    if( gnu.java.lang.SystemClassLoader.NO_SUCH_ARCHIVE == null)
      gnu.java.lang.SystemClassLoader.NO_SUCH_ARCHIVE = new Object();
    //-#endif
    runClassInitializer("java.lang.String");
    runClassInitializer("java.lang.ClassLoader");
    runClassInitializer("com.ibm.JikesRVM.librarySupport.ReflectionSupport");
    runClassInitializer("java.lang.Math");
    runClassInitializer("java.lang.RuntimePermission");
    runClassInitializer("java.util.TimeZone");
    runClassInitializer("java.util.Locale");
    runClassInitializer("java.util.Calendar");
    runClassInitializer("java.util.GregorianCalendar");
    runClassInitializer("java.util.ResourceBundle");
    runClassInitializer("java.util.zip.ZipEntry");
    runClassInitializer("java.util.zip.Inflater");
    runClassInitializer("java.util.zip.DeflaterHuffman");
    runClassInitializer("java.util.zip.InflaterDynHeader");
    runClassInitializer("java.util.zip.InflaterHuffmanTree");
    runClassInitializer("java.util.jar.Attributes$Name");

    // Initialize compiler that compiles dynamically loaded classes.
    //
    if (verbose >= 1) VM.sysWriteln("Initializing runtime compiler");
    VM_RuntimeCompiler.boot();


    // Process virtual machine directives.
    //
    if (verbose >= 1) VM.sysWriteln("Processing VM directives");
    String[] applicationArguments = VM_CommandLineArgs.processCommandLineArguments();
    if (applicationArguments.length == 0) {  
      VM.sysWrite("vm: please specify a class to execute\n");
      VM.sysExit(1);
    }

    // Allow Baseline compiler to respond to command line arguments
    // The baseline compiler ignores command line arguments until all are processed
    // otherwise printing may occur because of compilations ahead of processing the
    // method_to_print restriction
    //
    if (verbose >= 1) VM.sysWriteln("Compiler processing rest of boot options");
    VM_BaselineCompiler.postBootOptions();


    // Allow Collector to respond to command line arguments
    //
    if (verbose >= 1) VM.sysWriteln("Collector processing rest of boot options");
    VM_Collector.postBoot();

    //-#if RVM_WITH_GNU_CLASSPATH
    VM_SystemClassLoader.getVMClassLoader().jarCache = null;
    //-#endif

    // Work around class incompatibilities in boot image writer
    // (JDK's java.lang.Thread does not extend VM_Thread) [--IP].
    if (verbose >= 1) VM.sysWriteln("Constructing mainThread");
    Thread      xx         = new MainThread(applicationArguments);
    VM_Address  yy         = VM_Magic.objectAsAddress(xx);
    VM_Thread   mainThread = (VM_Thread)VM_Magic.addressAsObject(yy);

    // record the main thread and the name of the main application class.
    _mainApplicationClassName = applicationArguments[0];
    _mainThread = mainThread;

    VM_Lock.boot();

    // Begin multiprocessing.
    //

    VM_Scheduler.boot(mainThread);
    if (VM.VerifyAssertions) 
      VM.assert(VM.NOT_REACHED);
  }

  private static VM_Class[] classObjects = new VM_Class[0];
  /**
   * Called by the compilers when compiling a static synchronized method
   * during bootimage writing.
   */
  static void deferClassObjectCreation(VM_Class c) throws VM_PragmaInterruptible {
    for (int i=0; i<classObjects.length; i++) {
      if (classObjects[i] == c) return; // already recorded
    }
    VM_Class[] tmp = new VM_Class[classObjects.length+1];
    System.arraycopy(classObjects, 0, tmp, 0, classObjects.length);
    tmp[classObjects.length] = c;
    classObjects = tmp;
  }
  /**
   * Create the java.lang.Class objects needed for 
   * static synchronized methods in the bootimage.
   */
  private static void createClassObjects() throws VM_PragmaInterruptible {
    for (int i=0; i<classObjects.length; i++) {
      classObjects[i].getClassForType();
    }
  }

  /**
   * Run <clinit> method of specified class, if that class appears 
   * in bootimage.
   * @param className
   */
  private static void runClassInitializer(String className) throws VM_PragmaInterruptible {
    VM_Atom  classDescriptor = 
      VM_Atom.findOrCreateAsciiAtom(className.replace('.','/')).descriptorFromClassName();
    VM_Class cls = VM_ClassLoader.findOrCreateType(classDescriptor, VM_SystemClassLoader.getVMClassLoader()).asClass();
    if (cls.isInBootImage()) {
      VM_Method clinit = cls.getClassInitializerMethod();
      clinit.compile();
      VM_Magic.invokeClassInitializer(clinit.getCurrentInstructions());
      cls.setAllFinalStaticJTOCEntries();
    }

  }

  //----------------------------------------------------------------------//
  //                         Execution environment.                       //
  //----------------------------------------------------------------------//

  /**
   * Verify a runtime assertion (die w/traceback if assertion fails).
   * Note: code your assertion checks as 
   * "if (VM.VerifyAssertions) VM.assert(xxx);"
   * @param b the assertion to verify
   */
  public static void assert(boolean b) {
    assert(b, null);
  }

  /**
   * Verify a runtime assertion (die w/message and traceback if 
   * assertion fails).   Note: code your assertion checks as 
   * "if (VM.VerifyAssertions) VM.assert(xxx,yyy);"
   * @param b the assertion to verify
   * @param message the message to print if the assertion is false
   */
  static void assert(boolean b, String message) {
    if (!VM.VerifyAssertions) {
      // somebody forgot to conditionalize their call to assert with
      // "if (VM.VerifyAssertions)"
      _assertionFailure("vm internal error: assert called when !VM.VerifyAssertions");
    }

    if (!b) _assertionFailure(message);
  }

  private static void _assertionFailure(String message) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline {
    if (message == null) message = "vm internal error at:";
    if (VM.runningVM) {
      sysFail(message);
    }
    throw new RuntimeException(message);
  }


  /**
   * Format a 32 bit number as "0x" followed by 8 hex digits.
   * Do this without referencing Integer or Character classes, 
   * in order to avoid dynamic linking.
   * TODO: move this method to VM_Services.
   * @param number
   * @return a String with the hex representation of the integer
   */
  static String intAsHexString(int number) throws VM_PragmaInterruptible {
    char[] buf   = new char[10];
    int    index = 10;
    while (--index > 1) {
      int digit = number & 0x0000000f;
      buf[index] = digit <= 9 ? (char)('0' + digit) : (char)('a' + digit - 10);
      number >>= 4;
    }
    buf[index--] = 'x';
    buf[index]   = '0';
    return new String(buf);
  }

  /**
   * Low level print to console.
   * @param value  what is printed
   */
  public static void sysWrite(VM_Atom value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    value.sysWrite();
  }

  /**
   * Low level print to console.
   * @param value  what is printed
   */
  public static void sysWrite(VM_Member value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    VM.sysWrite(value.getDeclaringClass().getDescriptor());
    VM.sysWrite(".");
    VM.sysWrite(value.getName());
    VM.sysWrite(" ");
    VM.sysWrite(value.getDescriptor());
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   */
  public static void sysWrite(String value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      VM_Processor.getCurrentProcessor().disableThreadSwitching();
      for (int i = 0, n = value.length(); i < n; ++i) {
        sysWrite(value.charAt(i));
      }
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
    } else {
      System.err.print(value);
    }
  }

  /**
    * Low level print to console.
   * @param value	what is printed
   */
  public static void sysWrite(char value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall1(VM_BootRecord.the_boot_record.sysWriteCharIP, value);
    else
      System.err.print(value);
  }


  /**
   * Low level print to console.  Can't pass doubles yet so just print to 2 decimal places.
   * @param value   double to be printed
   *
   */
  public static void sysWrite(double value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int ones = (int) value;
      int hundredths = (int) (100.0 * (value - ones));
      sysWrite(ones, false); 
      sysWrite(".");
      if (hundredths < 10)
        sysWrite("0");
      sysWrite(hundredths, false);
    }
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value	what is printed
   */
  public static void sysWrite(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int mode = (value < -(1<<20) || value > (1<<20)) ? 2 : 0; // hex only or decimal only
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, mode);
    } else {
      System.err.print(value);
    }
  }


  public static void sysWriteField(int fieldWidth, String s) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    sysWrite(s);
    int len = s.length();
    while (fieldWidth > len++) sysWrite(" ");
  }

  /**
   * Low level print to console.
   * @param value	print value and left-fill with enough spaces to print at least fieldWidth characters
   */
  public static void sysWriteField(int fieldWidth, int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    int len = 1, temp = value;
    if (temp < 0) { len++; temp = -temp; }
    while (temp >= 10) { len++; temp /= 10; }
    while (fieldWidth > len++) sysWrite(" ");
    if (runningVM) 
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, 0);
    else 
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value	print value and left-fill with enough spaces to print at least fieldWidth characters
   */
  public static void sysWriteField(int fieldWidth, VM_Atom s) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    int len = s.length();
    while (fieldWidth > len++) sysWrite(" ");
    sysWrite(s);
  }

  /**
   * Low level print to console.
   * @param value	what is printed, as hex only
   */
  public static void sysWriteHex(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, 2 /*just hex*/);
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   * @param hexToo  how to print: true  - print as decimal followed by hex
   *                              false - print as decimal only
   */
  public static void sysWrite(int value, boolean hexToo) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, hexToo?1:0);
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   */
  public static void sysWrite(long value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    sysWrite(value, true);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   * @param hexToo  how to print: true  - print as decimal followed by hex
   *                              false - print as decimal only
   */
  public static void sysWrite(long value, boolean hexToo) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int val1, val2;
      val1 = (int)(value>>32);
      val2 = (int)(value & 0xFFFFFFFF);
      sysCall3(VM_BootRecord.the_boot_record.sysWriteLongIP, val1, val2, hexToo?1:0);
    } else
      System.err.print(value);
  }

  /**
   * A group of multi-argument sysWrites with optional newline.
   */
  public static void sysWriteln ()                     throws VM_PragmaNoInline { sysWrite("\n"); }
  public static void sysWrite   (VM_Address addr)      throws VM_PragmaNoInline { sysWriteHex(addr.toInt()); }
  public static void sysWriteln (int i)                throws VM_PragmaNoInline { sysWrite(i);   sysWriteln(); }
  public static void sysWriteln (double d)             throws VM_PragmaNoInline { sysWrite(d);   sysWriteln(); }
  public static void sysWriteln (long l)               throws VM_PragmaNoInline { sysWrite(l);   sysWriteln(); }
  public static void sysWriteln (String s)             throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(); }
  public static void sysWrite   (String s, int i)           throws VM_PragmaNoInline { sysWrite(s);   sysWrite(i); }
  public static void sysWriteln (String s, int i)           throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(i); }
  public static void sysWrite   (String s, double d)        throws VM_PragmaNoInline { sysWrite(s);   sysWrite(d); }
  public static void sysWriteln (String s, double d)        throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(d); }
  public static void sysWrite   (String s, long i)           throws VM_PragmaNoInline { sysWrite(s);   sysWrite(i); }
  public static void sysWriteln (String s, long i)           throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(i); }
  public static void sysWrite   (int i, String s)           throws VM_PragmaNoInline { sysWrite(i);   sysWrite(s); }
  public static void sysWriteln (int i, String s)           throws VM_PragmaNoInline { sysWrite(i);   sysWriteln(s); }
  public static void sysWrite   (String s1, String s2)      throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); }
  public static void sysWriteln (String s1, String s2)      throws VM_PragmaNoInline { sysWrite(s1);  sysWriteln(s2); }
  public static void sysWrite   (String s, VM_Address addr) throws VM_PragmaNoInline { sysWrite(s);   sysWriteHex(addr.toInt()); }
  public static void sysWriteln (String s, VM_Address addr) throws VM_PragmaNoInline { sysWrite(s);   sysWriteHex(addr.toInt()); sysWriteln(); }
  public static void sysWrite   (String s1, String s2, int i)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWrite(i); }
  public static void sysWriteln (String s1, String s2, int i)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWriteln(i); }
  public static void sysWrite   (String s1, int i, String s2)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i);  sysWrite(s2); }
  public static void sysWriteln (String s1, int i, String s2)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i);  sysWriteln(s2); }
  public static void sysWrite   (String s1, String s2, String s3)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWrite(s3); }
  public static void sysWriteln (String s1, String s2, String s3)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWriteln(s3); }
  public static void sysWrite   (int i1, String s, int i2)     throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s);  sysWrite(i2); }
  public static void sysWriteln (int i1, String s, int i2)     throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s);  sysWriteln(i2); }
  public static void sysWrite   (int i1, String s1, String s2) throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s1); sysWrite(s2); }
  public static void sysWriteln (int i1, String s1, String s2) throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s1); sysWriteln(s2); }
  public static void sysWrite   (String s1, int i1, String s2, int i2) throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i1); sysWrite(s2); sysWrite(i2); }
  public static void sysWriteln (String s1, int i1, String s2, int i2) throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i1); sysWrite(s2); sysWriteln(i2); }

  /**
   * Exit virtual machine due to internal failure of some sort.
   * @param message  error message describing the problem
   */
  public static void sysFail(String message) throws VM_PragmaNoInline {
    // print a traceback and die
    VM_Scheduler.traceback(message);
    VM.shutdown(1);
  }

  /**
   * Exit virtual machine.
   * @param value  value to pass to host o/s
   */
  public static void sysExit(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline {
    // SJF: I don't want this method inlined, since I use it as a
    // breakpoint for the jdp regression test.
    if (runningVM) {
      System.out.flush();
      System.err.flush();
      VM_Callbacks.notifyExit(value);
      VM.shutdown(value);
    } else {
      System.exit(value);
    }
  }

  /**
   * Shut down the virtual machine.
   * Should only be called if the VM is running.
   * @param value  exit value
   */
  public static void shutdown(int value) {
    if (VM.VerifyAssertions) VM.assert(VM.runningVM);
    if (VM.runningAsSubsystem) {
      // Terminate only the system threads that belong to the VM
      VM_Scheduler.processorExit(value);
    } else {
      sysCall1(VM_BootRecord.the_boot_record.sysExitIP, value);
    }
  }

  /**
   * Create a virtual processor (aka "unix kernel thread", "pthread").
   * @param jtoc  register values to use for thread startup
   * @param pr
   * @param ti
   * @param fp
   * @return virtual processor's o/s handle
   */
  static int sysVirtualProcessorCreate(VM_Address jtoc, VM_Address pr, 
                                       int ti, VM_Address fp) {
    return sysCall4(VM_BootRecord.the_boot_record.sysVirtualProcessorCreateIP,
                    jtoc.toInt(), pr.toInt(), ti, fp.toInt());
  }

  /**
   * Bind execution of current virtual processor to specified physical cpu.
   * @param cpuId  physical cpu id (0, 1, 2, ...)
   */
  static void sysVirtualProcessorBind(int cpuId) {
    sysCall1(VM_BootRecord.the_boot_record.sysVirtualProcessorBindIP, cpuId);
  }

  /**
   * Yield execution of current virtual processor back to o/s.
   */
  public static void sysVirtualProcessorYield() {
    //-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
    return;
    //-#else
    sysCall0(VM_BootRecord.the_boot_record.sysVirtualProcessorYieldIP);
    //-#endif
  }

  /**
   * Start interrupt generator for thread timeslicing.
   * The interrupt will be delivered to whatever virtual processor happens 
   * to be running when the timer expires.
   */
  static void sysVirtualProcessorEnableTimeSlicing() {
    sysCall0(VM_BootRecord.the_boot_record.sysVirtualProcessorEnableTimeSlicingIP);
  }

  //-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
  //-#else
  static void sysWaitForVirtualProcessorInitialization() {
    sysCall0(VM_BootRecord.the_boot_record.sysWaitForVirtualProcessorInitializationIP);
  }

  static void sysWaitForMultithreadingStart() {
    sysCall0(VM_BootRecord.the_boot_record.sysWaitForMultithreadingStartIP);
  }

  static void sysInitializeStartupLocks(int howMany) {
    sysCall1(VM_BootRecord.the_boot_record.sysInitializeStartupLocksIP, howMany);
  }
  //-#endif

  //-#if RVM_FOR_POWERPC
  /**
   * Make calls to host operating system services.
   * @param ip address of a function in sys.C 
   * @return integer value returned by function in sys.C
   */
  public static int sysCall0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall0(ip, VM_BootRecord.the_boot_record.sysTOC);
  }

  /**
   * sysCall1
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return integer value returned by function in sys.C
   */
  public static int sysCall1(int ip, int p1) throws VM_PragmaInline {
    return VM_Magic.sysCall1(ip, VM_BootRecord.the_boot_record.sysTOC, p1);
  }

  /**
   * sysCall2
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall2(int ip, int p1, int p2) throws VM_PragmaInline {
    return  VM_Magic.sysCall2(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2);
  }

  /**
   * sysCall3
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall3(int ip, int p1, int p2, int p3) throws VM_PragmaInline {
    return  VM_Magic.sysCall3(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2, p3);
  }

  /**
   * sysCall4
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @param p4
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall4(int ip, int p1, int p2, int p3, int p4) throws VM_PragmaInline {
    return VM_Magic.sysCall4(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2, p3, p4);
  }

  /**
   * sysCall_L_0
   * @param ip  address of a function in sys.C 
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_0(ip, VM_BootRecord.the_boot_record.sysTOC);
  }

  /**
   * sysCall_L_I
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_I(int ip, int p1) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_I(ip, VM_BootRecord.the_boot_record.sysTOC, p1);
  }

  /**
   * sysCallAD
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCallAD(int ip, int p1, double p2) throws VM_PragmaInline {
    return  VM_Magic.sysCallAD(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2);
  }

  //-#endif
  //-#if RVM_FOR_IA32
  /**
   * sysCall0
   * @param ip  address of a function in sys.C 
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall0(int ip) throws VM_PragmaInline {
    return  VM_Magic.sysCall0(ip);
  }

  /**
   * sysCall1
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall1(int ip, int p1) throws VM_PragmaInline {
    return  VM_Magic.sysCall1(ip, p1);
  }

  /**
   * sysCall2
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall2(int ip, int p1, int p2) throws VM_PragmaInline {
    return  VM_Magic.sysCall2(ip, p1, p2);
  }

  /**
   * sysCall3
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall3(int ip, int p1, int p2, int p3) throws VM_PragmaInline {
    return  VM_Magic.sysCall3(ip, p1, p2, p3);
  }

  /**
   * sysCall4
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @param p4
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall4(int ip, int p1, int p2, int p3, int p4) throws VM_PragmaInline {
    return VM_Magic.sysCall4(ip, p1, p2, p3, p4);
  }

  /**
   * sysCall_L_0
   * @param ip  address of a function in sys.C 
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_0(ip);
  }

  /**
   * sysCall_L_I
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_I(int ip, int p1) throws VM_PragmaInline {
    return  VM_Magic.sysCall_L_I(ip, p1);
  }

  /**
   * sysCallAD
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCallAD(int ip, int p1, double p2) throws VM_PragmaInline {
    return VM_Magic.sysCallAD(ip, p1, p2);
  }

  //-#endif

  //----------------//
  // implementation //
  //----------------//

  /**
   * Create class instances needed for boot image or initialize classes 
   * needed by tools.
   * @param vmClassPath places where vm implemention class reside
   * @param bootCompilerArgs command line arguments to pass along to the 
   *                         boot compiler's init routine.
   */
  private static void init(String vmClassPath, String[] bootCompilerArgs) 
    throws VM_ResolutionException, VM_PragmaInterruptible {
      // create dummy boot record
      //
      VM_BootRecord.the_boot_record = new VM_BootRecord();

      // initialize type subsystem - create type descriptions for java.lang.Object 
      // and the classes whose methods it calls. we do this in an order chosen to 
      // ensure that offset and size information needed by the compiler to 
      // perform "direct" (non-dynamically linked) calls is ready before any 
      // method calls are compiled.
      //
      VM_Statics.init();
      VM_MagicNames.init();
      VM_ClassLoader.init(vmClassPath);
      VM_Class object       = VM_Type.JavaLangObjectType.asClass();
      VM_Class string       = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("Ljava/lang/String;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class stringBuffer = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("Ljava/lang/StringBuffer;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class vm           = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("LVM;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class runtime      = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("LVM_Runtime;"), VM_SystemClassLoader.getVMClassLoader()).asClass();

      // initialize JNI environment
      VM_JNIEnvironment.init();

      // load class descriptions
      //
      object.load();
      string.load();
      stringBuffer.load();
      vm.load();
      runtime.load();

      // generate size and offset information needed for compiling methods of java.lang.Object
      //
      object.resolve();
      string.resolve();
      stringBuffer.resolve();
      vm.resolve();
      runtime.resolve();
      // initialize remaining subsystems needed for compilation
      //
      VM_Entrypoints.init();
      VM_OutOfLineMachineCode.init();
      if (writingBootImage) // initialize compiler that builds boot image
        VM_BootImageCompiler.init(bootCompilerArgs);
      VM_Runtime.init();
      VM_Scheduler.init();
      VM_Collector.init();
    }

  /**
   * The following two methods are for use as guards to protect code that 
   * must deal with raw object addresses in a collection-safe manner 
   * (ie. code that holds raw pointers across "gc-sites").
   *
   * Authors of code running while gc is disabled must be certain not to 
   * allocate objects explicitly via "new", or implicitly via methods that, 
   * in turn, call "new" (such as string concatenation expressions that are 
   * translated by the java compiler into String() and StringBuffer() 
   * operations). Furthermore, to prevent deadlocks, code running with gc 
   * disabled must not lock any objects. This means the code must not execute 
   * any bytecodes that require runtime support (eg. via VM_Runtime) 
   * such as:
   *   - calling methods or accessing fields of classes that haven't yet 
   *     been loaded/resolved/instantiated
   *   - calling synchronized methods
   *   - entering synchronized blocks
   *   - allocating objects with "new"
   *   - throwing exceptions 
   *   - executing trap instructions (including stack-growing traps)
   *   - storing into object arrays, except when runtime types of lhs & rhs 
   *     match exactly
   *   - typecasting objects, except when runtime types of lhs & rhs 
   *     match exactly
   *
   * Recommendation: as a debugging aid, VM_Allocator implementations 
   * should test "VM_Thread.disallowAllocationsByThisThread" to verify that 
   * they are never called while gc is disabled.
   */
  public static void disableGC() throws VM_PragmaInline, VM_PragmaInterruptible  { 
    // current (non-gc) thread is going to be holding raw addresses, therefore we must:
    //
    // 1. make sure we have enough stack space to run until gc is re-enabled
    //    (otherwise we might trigger a stack reallocation)
    //
    // 2. force all other threads that need gc to wait until this thread
    //    is done with the raw addresses
    //
    // 3. ensure that this thread doesn't try to allocate any objects
    //    (because an allocation attempt might trigger a collection that
    //    would invalidate the addresses we're holding)
    //

    VM_Thread myThread = VM_Thread.getCurrentThread();

    // 1.
    //
    if (VM_Magic.getFramePointer().sub(STACK_SIZE_GCDISABLED).LT(myThread.stackLimit))
      VM_Thread.resizeCurrentStack(myThread.stack.length + (STACK_SIZE_GCDISABLED >> 2), null);

    // 2.
    //
    VM_Processor.getCurrentProcessor().disableThreadSwitching();

    // 3.
    //
    if (VM.VerifyAssertions) {
      VM.assert(myThread.disallowAllocationsByThisThread == false); // recursion not allowed
      myThread.disallowAllocationsByThisThread = true;
    }
  }

  /**
   * enable GC
   */
  public static void enableGC() throws VM_PragmaInline { 
    if (VM.VerifyAssertions) {
      VM_Thread myThread = VM_Thread.getCurrentThread();
      // recursion not allowed
      VM.assert(myThread.disallowAllocationsByThisThread == true); 
      myThread.disallowAllocationsByThisThread = false;
    }
    VM_Processor.getCurrentProcessor().enableThreadSwitching();
  }

  private static String _mainApplicationClassName;
  private static VM_Thread _mainThread;

  /**
   * getMainMethod
   * @return the main method of the main thread
   */
  public static VM_Method getMainMethod() throws VM_PragmaInterruptible {
    if(VM.VerifyAssertions) VM.assert(_mainThread != null);
    return ((MainThread)_mainThread).getMainMethod();
  } 

  /**
   * Place to set breakpoints (called by compiled code).
   */
  public static void debugBreakpoint() throws VM_PragmaNoInline {
    // the following forces this method to have a prologue.
    // In general, jdp cannot set breakpoints in opt methods that
    // have no prologues.
    VM_Magic.pragmaNoOptCompile();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Flags that specify the configuration of our virtual machine.
 *
 * Note: Final controls require the whole vm to be recompiled and 
 *       rebuilt after their values are changed.
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public abstract class VM_Configuration {

  public static final boolean BuildForPowerPC =
	//-#if RVM_FOR_POWERPC
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean BuildForIA32 =
	//-#if RVM_FOR_IA32
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean LITTLE_ENDIAN = BuildForIA32;

  public static final boolean BuildForAix =
	//-#if RVM_FOR_AIX
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean BuildForLinux =
	//-#if RVM_FOR_LINUX
	  true;
	//-#else
	  false;
	//-#endif

  // Assertion checking.
  //  false --> no assertion checking at runtime
  //  true  --> execute assertion checks at runtime
  //
  // Note: code your assertion checks as 
  // "if (VM.VerifyAssertions) VM.assert(xxx);"
  //
  public static final boolean VerifyAssertions = 
        //-#if RVM_WITHOUT_ASSERTIONS
          false;
        //-#else
          true;
        //-#endif

  // Verify that Uninterruptible methods actually cannot be interrupted.
  // Disabled for just a little longer since we get too many false positives
  public static final boolean VerifyUnint = false && VerifyAssertions;

  // Ignore supression pragma and print all warning messages.
  public static final boolean ParanoidVerifyUnint = false;

  // Multiprocessor operation.
  //  false --> vm will use multiple processors (requires operatying system that
  //            supports posix pthread, e.g., AIX)
  //  true  --> vm will use just one processor and no
  //            synchronization instructions
  //
  public static final boolean BuildForSingleVirtualProcessor =
	//-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
	  true;
	//-#else
	  false;
	//-#endif

  // Use count of method prologues executed rather than timer interrupts to drive
  // preemptive thread switching.  Non preemptive thread switching is achieved by
  // setting the number of prologues between thread switches to infinity (-1).
  //
  public static final boolean BuildForDeterministicThreadSwitching =
	//-#if RVM_WITH_DETERMINISTIC_THREAD_SWITCHING
	  true;
	//-#else
        //-#if RVM_WITHOUT_PREEMPTIVE_THREAD_SWITCHING 
          true;
        //-#else
          false;
	//-#endif
	//-#endif

  // Normally, a word in memory is used to signal need for a thread switch
  // On PowerPC a control register can be used (and will be set by the interrupt handler).
  // However, this distribution of virtual processors interrupted is very unfair on a multiprocessor.
  // Therefore, the control register is only used for single-virtual-processor builds.
  //
  public static final boolean BuildForThreadSwitchUsingControlRegisterBit = 
	//-#if RVM_FOR_POWERPC
	//-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
          true;
        //-#else
          false;
	//-#endif
	//-#else
          false;
	//-#endif

  // Is this an adaptive build?
  public static final boolean BuildForAdaptiveSystem =
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Dynamic type checking implementations.
  // We have two:
  //  (1) FastDynamicTypeCheck as described in Alpern, Cocchi, & Grove JVM'01
  //  (2) otherwise do an inline type equality check and an inline
  //      cache lookup for the last successful type comparison
  //      falling back to an out of line lookup routine.  This is 
  //      approximately what is done by the IBM product DK (see Ishizaki et al)
  public static final boolean BuildForFastDynamicTypeCheck = true;

  // Interface method invocation.
  // We have five mechanisms:
  //   IMT-based (Alpern, Cocchi, Fink, Grove, and Lieber). 
  //    - embedded directly in the TIB
  //    - indirectly accessed off the TIB
  //    both IMT schemes require BuildForFastDynamicTypeCheck.
  //   ITable-based
  //    - directly indexed (by interface id) iTables. 
  //       requires BuildForFastDynamicTypeCheck.
  //    - searched (at dispatch time); does not require FastDTC
  //   Naive, class object is searched for matching method on every dispatch.
  public static final boolean BuildForIMTInterfaceInvocation = true && 
                                              BuildForFastDynamicTypeCheck;
  public static final boolean BuildForIndirectIMT = true && 
                                              BuildForIMTInterfaceInvocation;
  public static final boolean BuildForEmbeddedIMT = !BuildForIndirectIMT && 
                                              BuildForIMTInterfaceInvocation;
  public static final boolean BuildForITableInterfaceInvocation = true && 
                                              !BuildForIMTInterfaceInvocation;
  public static final boolean DirectlyIndexedITables = false && 
                                              BuildForFastDynamicTypeCheck;

  // Compiler support for garbage collection and stack management.
  //
  public static final boolean BuildForConcurrentGC =
      //-#if RVM_WITH_CONCURRENT_GC
        true;
      //-#else
        false;
      //-#endif

  // Compiler support for real-time garbage collection
  //
  public static final boolean BuildForRealtimeGC =
      //-#if RVM_WITH_REALTIME_GC
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithRedirectSlot =
      //-#if RVM_WITH_REDIRECT_SLOT
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithLazyRedirect =
      //-#if RVM_WITH_LAZY_REDIRECT
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithEagerRedirect =
      //-#if RVM_WITH_EAGER_REDIRECT
        true;
      //-#else
        false;
      //-#endif

  // Epilogue yieldpoints increase sampling accuracy for adaptive recompilation.
  // In particular, they are key for large, leaf, loop-free methods.
  public static final boolean UseEpilogueYieldPoints =
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Operating system resource monitoring.
  //
  // Getting accurate compilation time information is critical for
  // good adaptive system performance.
  public static final boolean BuildForCpuMonitoring = 
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Adaptive compilation.
  //
  public static final boolean LogAOSEvents =
      //-#if RVM_WITHOUT_AOS_LOG 
        false;
      //-#else
        true;
      //-#endif

  // Use synchronization on PowerPC to access one word fields declared volatile and
  // on all platforms to access two word fields declared volatile.
  //
  // If this control is not set, volatile fields will not be kept in registers but stale
  // values may still be visible in processor caches.
  //
  // Note: for best results the following controls should also be set:
  //     BuildForPrematureClassResolution - so that volatile fields can be recognized as such at compilation time
  //     BuildForLazyCompilation - to avoid sucking in the whole world with premature class resolution.
  //
  public static final boolean BuildForStrongVolatileSemantics = 
      //-#if RVM_WITH_STRONG_VOLATILE_SEMANTICS
        true;
      //-#else
        false;
      //-#endif
  
  // Resolve classes when a compiler encounters a reference to a not yet loaded class.
  // 
  // Note: setting BuildForLazyCompilation will prevent a cascade effect from sucking in too much irrelevant stuff.
  //
  public static final boolean BuildForPrematureClassResolution = 
      //-#if RVM_WITH_PREMATURE_CLASS_RESOLUTION
        true;
      //-#else
        false
        || BuildForStrongVolatileSemantics // TEMP!! remove this clause when dynamic linking for strong volatiles is implemented
        ;
      //-#endif

  // Lazy vs. eager method compilation during class loading.
  //
  public static final boolean BuildForLazyCompilation =
      //-#if RVM_WITHOUT_LAZY_COMPILATION
        false;
      //-#else
        true;
      //-#endif

  // Capture threads that have gone Native (JNI) and not come back.  Issolate them
  // in Native.  Create a new (Native) virtual processor for them.  And create (or revive)
  // new pThreads to run the old virtual processors.
  //
  public static final boolean BuildWithNativeDaemonProcessor = 
	//-#if RVM_WITHOUT_NATIVE_DAEMON_PROCESSOR
	  false;
	//-#else
	  !BuildForSingleVirtualProcessor
	    && !BuildForConcurrentGC;
	//-#endif

  // The following configuration objects are final when disabled, but
  // non-final when enabled.
  
  //-#if RVM_FOR_STRESSGC
  public static boolean ParanoidGCCheck       = true;
  public static boolean ForceFrequentGC       = true;
  //-#else
  public final static boolean ParanoidGCCheck  = false;
  public final static boolean ForceFrequentGC  = false;
  //-#endif

  public final static boolean CompileForGCTracing =
      //-#if RVM_WITH_GCTk_GCTRACE
	true;
      //-#else
        false;
      //-#endif

  //-#if RVM_FOR_IA32
  /**
   * Is ESI dedicated to always hold the processor register?
   */
  public final static boolean dedicatedESI = true;
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Constants describing vm object, stack, and register characteristics.
 * Some of these constants are architecture-specific
 * and some are (at the moment) architecture-neutral.
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public interface VM_Constants
extends   VM_ThinLockConstants,         // architecture-neutral
          VM_TIBLayoutConstants,        // architecture-neutral
          VM_StackframeLayoutConstants, // architecture-neutral
          VM_RegisterConstants,         // architecture-specific
          VM_TrapConstants              // architecture-specific
   {
   // Bit pattern used to represent a null object reference.
   // Note: I don't think this is actually used consistently [--DL]
   // 
   static final int VM_NULL = 0;
   
   // For assertion checking things that should never happen.
   //
   static final boolean NOT_REACHED = false;

   // For assertion checking things that aren't ready yet.
   //
   static final boolean NOT_IMPLEMENTED = false;
  
   static final int BYTES_IN_ADDRESS_LOG = 2;
   static final int BYTES_IN_ADDRESS = 1<<BYTES_IN_ADDRESS_LOG;

   // Reflection uses an integer return from a function which logically
   // returns a triple.  The values are packed in the interger return value
   // by the following masks.
   static final int REFLECTION_GPRS_BITS = 5;
   static final int REFLECTION_GPRS_MASK = (1 << REFLECTION_GPRS_BITS) - 1;
   static final int REFLECTION_FPRS_BITS = 5;
   static final int REFLECTION_FPRS_MASK = (1 << REFLECTION_FPRS_BITS) - 1;

   }
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Flags that control the behavior of our virtual machine.
 * 
 * Typically these are properties that can be set from the command line
 * (and thus are NOT final).  All final properties should be 
 * declared in VM_Configuration
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public class VM_Properties extends VM_Configuration {

  // The VM class hierarchy is used in three ways:
  //    - by boot image writer to create an executable vm image
  //    - by tools that wish use VM classes for generic java programming
  //    - by vm image itself, at execution time
  // The following flags specify which behavior is desired.
  //
  
  /**
   * use classes for boot image generation? (see BootImageWriter)
   */
  public static boolean writingBootImage; 
  /**
   * use classes for generic java programming?
   */
  public static boolean runningTool;      
  /**
   * use classes for running actual VM?
   */
  public static boolean runningVM;        
  /**
   * if true, don't exit from the process
   */
  public static boolean runningAsSubsystem;  

  // Use count of method prologues executed rather than timer interrupts to drive
  // preemptive thread switching.  Non preemptive thread switching is achieved by
  // setting the number of prologues between thread switches to infinity (-1).
  //
  public static int deterministicThreadSwitchInterval =
	//-#if RVM_WITHOUT_PREEMPTIVE_THREAD_SWITCHING 
	  -1;
	//-#else
        //-#if RVM_WITH_DETERMINISTIC_THREAD_SWITCHING
           1000;
        //-#else // the normal case (timer-driven preemptive thread switching)
	  0;
	//-#endif
	//-#endif

  // suppress code gen when system is running on remote interpreter portion of jdp.
  public static boolean runningAsJDPRemoteInterpreter;

  /**
   * The following is set on by -verbose:class command line arg.
   * When true, it generates messages to the sysWrite stream summarizing
   * class loading activities
   */
  public static boolean verboseClassLoading = false;

  // Symbolic info to support debugger.
  //
  public static boolean LoadLocalVariableTables = false;

  /**
   * The following is set on by -X:measureCompilation=true command line arg.
   * When true, it times compilations and generates a report at VM exit.
   */
  public static boolean MeasureCompilation      = false;  

  /**
   * The following is set on by -X:verify=true command line arg.
   * When true, it invokes the bytecode verifier
   */
  public static boolean VerifyBytecode = false;  

  // Runtime subsystem tracing.
  //
  public static final boolean TraceDictionaries       = false;
  public static final boolean TraceStatics            = false;
  public static final boolean TraceDynamicLinking     = false;
  public static final boolean TraceFileSystem         = false;
  public static final boolean TraceThreads            = false;
  public static final boolean TraceStackTrace         = false;
  public static boolean TraceClassLoading             = false;

  // Baseline compiler tracing.
  //
  public static final boolean TraceAssembler         = false; // PPC only

  // Baseline compiler reference map tracing.
  //
  public static final boolean TraceStkMaps                  = false;
  public static final boolean ReferenceMapsStatistics       = false;
  public static final boolean ReferenceMapsBitStatistics    = false;

  // Event logging.
  //
  public static final boolean BuildForEventLogging      = false;
  public static       boolean EventLoggingEnabled       = false;  // TODO!! make this final, see profiler/VM_EventLogger.java
  public static final boolean BuildForNetworkMonitoring = false;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/**
 * Constants exported by the assembler
 * @author Julian Dolby
 */
interface VM_AssemblerConstants {
  static final String [] CONDITION = {
   "O", "NO", "LLT", "LGE", "EQ", "NE", "LLE", "LGT", "S", "NS", "PE", "PO", "LT", "GE", "LE", "GT" 
  };

  static final byte   O = 0x0; // (overflow)
  static final byte  NO = 0x1; // (not overflow)
  static final byte LLT = 0x2; // logically less than (below)
  static final byte LGE = 0x3; // logically greater than or equal (not below) 
  static final byte  EQ = 0x4; // equal (zero)
  static final byte  NE = 0x5; // not equal (not zero)
  static final byte LLE = 0x6; // logically less than or equal (not above)
  static final byte LGT = 0x7; // logically greater than (above)
  static final byte   S = 0x8; // (sign) negative??
  static final byte  NS = 0x9; // (not sign) positive or zero??
  static final byte  PE = 0xA; // (even parity)
  static final byte  PO = 0xB; // (odd parity)
  static final byte  U  = 0xA; // (unordered floating point #s)
  static final byte  NU = 0xB; // (ordered floating point #s)
  static final byte  LT = 0xC; // less than
  static final byte  GE = 0xD; // greater than or equal (not less than)
  static final byte  LE = 0xE; // less than or equal (not greater than)
  static final byte  GT = 0xF; // greater than 

  // scale factors for SIB bytes
  static final short BYTE  = 0;
  static final short SHORT = 1;
  static final short WORD  = 2;
  static final short LONG  = 3;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/** 
 * @author Julian Dolby
 */
class VM_Lister implements VM_Constants {

  private static final int PREFIX_AREA_SIZE = 4;
  private static final int OP_AREA_SIZE     = 9;
  private static final int SOURCE_AREA_SIZE = 16;
  private static final int DEST_AREA_SIZE   = 16;

  VM_Assembler asm;
  boolean lockPrefix = false;

  VM_Lister (VM_Assembler asm) {
    this.asm = asm;
  }

  final void lockPrefix() {
    lockPrefix = true;
  }

  final void OP (int i, String op) {
    i = begin(i, op);
    VM.sysWrite(right("", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void I (int i, String op, int n) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(n) + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
    
  final void R (int i, String op, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RD (int i, String op, byte R0, int d) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RI (int i, String op, byte R0, int n) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RDI (int i, String op, byte R0, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RNI (int i, String op, byte R0, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RR (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RDR (int i, String op, byte R0, int d, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RDRI (int i, String op, byte R0, int d, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RRD (int i, String op, byte R0, byte R1, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R1] + "]", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RNR (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RN (int i, String op, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(" ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RRN (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + GPR_NAMES[R1] + "]", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RXD (int i, String op, byte R0, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDI (int i, String op, byte R0, byte X, short s, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFD (int i, String op, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RA (int i, String op, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + hex(d) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDI (int i, String op, byte X, short s, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RAI (int i, String op, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + hex(d) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRR (int i, String op, byte R0, byte R1, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RNRI (int i, String op, byte R0, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "] ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RRI (int i, String op, byte R0, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RRXD (int i, String op, byte R0, byte R1, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R1] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RXDR (int i, String op, byte R0, byte X, short s, int d, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDRI (int i, String op, byte R0, byte X, short s, int d, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDRR (int i, String op, byte R0, byte X, short s, int d, byte R1, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRFD (int i, String op, byte R0, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RFDR (int i, String op, byte X, short s, int d, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDRI (int i, String op, byte X, short s, int d, byte R0, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDRR (int i, String op, byte X, short s, int d, byte R0, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRA (int i, String op, byte R0, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RAR (int i, String op, int d, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RARI (int i, String op, int d, byte R0, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RARR (int i, String op, int d, byte R0, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  private final int begin(int i, String op) {
    if (lockPrefix) i--;
    VM.sysWrite(right(hex(i),6) + "| ");
    if (lockPrefix) {
      VM.sysWrite(right("LOCK", PREFIX_AREA_SIZE) + " ");
    } else {
      VM.sysWrite(right("", PREFIX_AREA_SIZE) + " ");
    }
    VM.sysWrite( left(op, OP_AREA_SIZE));
    return i;
  }

  private final void end(int i) {
    VM.sysWrite(" | ");
    asm.writeLastInstruction(i);
    VM.sysWrite("\n");
    lockPrefix = false;
  }

  private final static boolean isFP(String op) {
    return op.startsWith("F");
  }

  private final static String left (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(0,w);
    for (int i=n; i<w; i++) {
      s = s + " ";
    }
    return s; 
  }

  private final static String left (int i, int w) {
    return left(decimal(i), w); 
  }

  private final static String right (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(n-w);
    for (int i=n; i<w; i++) {
      s = " " + s;
    } 
    return s; 
  }

  private final static String right (int i, int w) {
    return right(decimal(i), w); 
  }

  final static String decimal (int n) {
    if (n==0) return "0";
    String sign = "";
    if (n<0) {
      sign = "-";
      n = -n;
    }
    String result = "";
    while (0<n) {
      int i = n%10;
      n /= 10;
      if (i==0) result = "0" + result;
      else if (i==1) result = "1" + result;
      else if (i==2) result = "2" + result;
      else if (i==3) result = "3" + result;
      else if (i==4) result = "4" + result;
      else if (i==5) result = "5" + result;
      else if (i==6) result = "6" + result;
      else if (i==7) result = "7" + result;
      else if (i==8) result = "8" + result;
      else if (i==9) result = "9" + result;
    }
    return (sign + result);
  }

  private final static String decimal (short s) {
    return decimal((int) s);
  }

  final static String hex (int i) {
    return (hex((short) (i>>16)) + hex((short) i));
  }

  final static String hex (short i) {
    return (hex((byte) (i>>8)) + hex((byte) i));
  }

  final static String hex (byte b) {
    int  i = b & 0xFF;
    byte j = (byte) (i/0x10);
    String s;
         if (j==0x0) s = "0";
    else if (j==0x1) s = "1";
    else if (j==0x2) s = "2";
    else if (j==0x3) s = "3";
    else if (j==0x4) s = "4";
    else if (j==0x5) s = "5";
    else if (j==0x6) s = "6";
    else if (j==0x7) s = "7";
    else if (j==0x8) s = "8";
    else if (j==0x9) s = "9";
    else if (j==0xA) s = "A";
    else if (j==0xB) s = "B";
    else if (j==0xC) s = "C";
    else if (j==0xD) s = "D";
    else if (j==0xE) s = "E";
    else             s = "F";
    j = (byte) (i%0x10);
    String t;
	 if (j==0x0) t = "0";
    else if (j==0x1) t = "1";
    else if (j==0x2) t = "2";
    else if (j==0x3) t = "3";
    else if (j==0x4) t = "4";
    else if (j==0x5) t = "5";
    else if (j==0x6) t = "6";
    else if (j==0x7) t = "7";
    else if (j==0x8) t = "8";
    else if (j==0x9) t = "9";
    else if (j==0xA) t = "A";
    else if (j==0xB) t = "B";
    else if (j==0xC) t = "C";
    else if (j==0xD) t = "D";
    else if (j==0xE) t = "E";
    else             t = "F";
    return s + t;
  }

  final void noteBytecode (int i, String bcode) {
    VM.sysWrite("[" + decimal(i) + "] " + bcode + "\n");
  }

  final void comment (int i, String comment) {
    VM.sysWrite(right(hex(i),6) + "| " + comment + "\n");
  }

  final void comefrom (int i, int j) {
    VM.sysWrite(right(hex(i),6) + "| <<< " + right(hex(j),6) + "\n");
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/** 
 * @author Julian Dolby
 */
class VM_MachineCode {

  /* interface */

  VM_MachineCode (INSTRUCTION[] i, int[] bm) {
    instructions = i;
    bytecodeMap  = bm;
  }

  final INSTRUCTION[] getInstructions () {
    return instructions;
  }

  final int[] getBytecodeMap () {
    return bytecodeMap;
  }

  /* implementation */

  private INSTRUCTION[] instructions;
  private int  [] bytecodeMap;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author Dave Grove
 */
interface VM_RegisterConstants {
  //---------------------------------------------------------------------------------------//
  //               RVM register usage conventions - Intel version.                         //
  //---------------------------------------------------------------------------------------//
    
  static final byte LG_INSTRUCTION_WIDTH = 0;             // log2 of instruction width in bytes
  static final String INSTRUCTION_ARRAY_SIGNATURE = "[B"; // signature for "array of instructions"
    
  // Symbolic values for fixed-point registers.
  // These values are used to assemble instructions and as indices into:
  //    VM_Registers.gprs[]
  //    VM_Registers.fprs[]
  //    VM_GCMapIterator.registerLocations[]
  //    VM_RegisterConstants.GPR_NAMES[]
  //
  static final byte EAX = 0x0;
  static final byte ECX = 0x1;
  static final byte EDX = 0x2;
  static final byte EBX = 0x3;
  static final byte ESP = 0x4;
  static final byte EBP = 0x5;
  static final byte ESI = 0x6;
  static final byte EDI = 0x7;

  // Mnemonics corresponding to the above constants.
  static final String[] GPR_NAMES = {
    "eax", "ecx", "edx", "ebx", "esp", "ebp", "esi", "edi"
  };

  static final byte FP0 = 0x0;
  static final byte FP1 = 0x1;
  static final byte FP2 = 0x2;
  static final byte FP3 = 0x3;
  static final byte FP4 = 0x4;
  static final byte FP5 = 0x5;
  static final byte FP6 = 0x6;
  static final byte FP7 = 0x7;
  
  static final String [] FPR_NAMES = {
    "FP0", "FP1", "FP2", "FP3", "FP4", "FP5", "FP6", "FP7"
  };
    
  // Register sets (``range'' is a misnomer for the alphabet soup of
  // of intel registers)
  //

  // Note: the order here is important.  The opt-compiler allocates
  // the volatile registers in the order they appear here.
  static final byte[]  VOLATILE_GPRS = { EAX, EDX, ECX };
  static final int NUM_VOLATILE_GPRS = VOLATILE_GPRS.length;
    
  // Note: the order here is very important.  The opt-compiler allocates
  // the nonvolatile registers in the reverse of order they appear here.
  // EBX must be last, because it is the only non-volatile that can
  // be used in instructions that are using r8 and we must ensure that
  // opt doesn't skip over another nonvol while looking for an r8 nonvol.
  static final byte[]  NONVOLATILE_GPRS = { EBP, EDI, EBX};
  static final int NUM_NONVOLATILE_GPRS = NONVOLATILE_GPRS.length;
    
  static final byte[]  VOLATILE_FPRS = { FP0, FP1, FP2, FP3, FP4, FP5, FP6, FP7 }; 
  static final int NUM_VOLATILE_FPRS = VOLATILE_FPRS.length;
    
  static final byte[]  NONVOLATILE_FPRS = {  }; 
  static final int NUM_NONVOLATILE_FPRS = NONVOLATILE_FPRS.length;

  /*
   * These constants represent the number of volatile registers used
   * to pass parameters in registers.  They are defined to mean that
   * the first n registers in the corresponding set of volatile
   * registers are used to pass parameters.
   */
  static final int NUM_PARAMETER_GPRS = 2;
  static final int NUM_PARAMETER_FPRS = 4;
  static final int NUM_RETURN_GPRS = 2;
  static final int NUM_RETURN_FPRS = 1;


  // Dedicated registers.
  //
  static final byte STACK_POINTER              = ESP;
  static final byte PROCESSOR_REGISTER         = ESI;
   
  static final byte NUM_GPRS                   =  8;
  static final byte NUM_FPRS                   =  8;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * The machine state comprising a thread's execution context.
 *
 * @author Bowen Alpern
 * @author David Grove
 */
public class VM_Registers implements VM_Constants, VM_Uninterruptible {

  // The following are used both for thread context switching
  // and for software/hardware exception reporting/delivery.
  //
  public int    gprs[]; // general purpose registers
  public double fprs[]; // floating point registers
  public VM_Address ip;     // instruction address register
  public VM_Address fp;     // frame pointer
  
  // set by C hardware exception handler and VM_Runtime.athrow 
  // and reset by each implementation of VM_ExceptionDeliverer.deliverException
  //
  public boolean inuse; // do exception registers currently contain live values?
  
  public VM_Registers() {
    gprs = new int[NUM_GPRS];
    fprs = new double[NUM_FPRS];
  }
  
  /**
   * Return framepointer for the deepest stackframe
   */
  public final VM_Address getInnermostFramePointer() {
    return fp;
  }
  
  /**
   * Return next instruction address for the deepest stackframe
   */
  public final VM_Address getInnermostInstructionAddress() {
    return ip;
  }

  /**
   * update the machine state as if the stackframe were unwound.
   */
  public final void unwindStackFrame() {
    ip = VM_Magic.getReturnAddress(fp);
    fp = VM_Magic.getCallerFramePointer(fp);
  }

  /**
   * set ip & fp. used to control the stack frame at which a scan of
   * the stack during GC will start, for ex., the top java frame for
   * a thread that is blocked in native code during GC.
   */
  public final void setInnermost(VM_Address newip, VM_Address newfp) {
    ip = newip;
    fp = newfp;
  }

  /**
   * set ip and fp values to those of the caller. used just prior to entering
   * sigwait to set fp & ip so that GC will scan the threads stack
   * starting at the frame of the method that called sigwait.
   */
  public final void setInnermost() {
    VM_Address current_fp = VM_Magic.getFramePointer();
    ip = VM_Magic.getReturnAddress(current_fp);
    fp = VM_Magic.getCallerFramePointer(current_fp);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */

//$Id$

/**----------------------------------------------------------------------
 *                   Stackframe layout conventions - Intel version.  
 *-----------------------------------------------------------------------
 *
 * A stack is an array of "slots", declared formally as integers, each slot
 * containing either a primitive (byte, int, float, etc), an object pointer,
 * a machine code pointer (a return address pointer), or a pointer to another
 * slot in the same stack (a frame pointer). The interpretation of a slot's 
 * contents depends on the current value of IP, the machine instruction 
 * address register.
 * Each machine code generator provides maps, for use by the garbage collector,
 * that tell how to interpret the stack slots at "safe points" in the
 * program's execution.
 *
 * Here's a picture of what a stack might look like in memory.
 *
 * Note: this (array) object is drawn upside down compared to other objects
 * because the hardware stack grows from high memory to low memory, but
 * array objects are layed out from low memory to high (header first).
 * <pre>
 *  hi-memory
 *              +---------------+                                            ...
 *              |     IP=0      |                                             .
 *              +---------------+                                             .
 *          +-> |     FP=0      |   <-- "end of vm stack" sentinal            .
 *          |   +---------------+                                             . caller's frame
 *          |   |    cmid=0      |   <-- "invisible method" id                .
 *          |   +---------------+                                          ---.
 *          |   |   parameter0  |  \                                        | .
 *          |   +---------------+   \ parameter area                        | .
 *          |   |   parameter1  |   /  (== caller's operand stack area)     | .
 *   ---    |   +---------------+  /                                        |...
 *    |     |   |   saved IP    |  <-- return address (in caller)           |
 *    |      \  +---------------+                                           |
 *  header FP-> |   saved FP    |  <-- this frame's caller's frame          |
 *    |         +---------------+                                           |
 *    |         |    cmid       |  <-- this frame's compiledmethod id       |
 *    |         +---------------+                                           |
 *    |         |   saved GPRs  |  \                                        |
 *    |         +---------------+   \ nonvolatile register save area        |
 *    |         |   saved FPRS  |   /                                       | frame
 *    |         +---------------+                                           |
 *    |         |   local0      |  \                                        |
 *   body       +---------------+   \_local variables area                  |
 *    |         |   local1      |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |   operand0    |  \                                        |
 *    |         +---------------+   \_operand stack area                    |
 *    |    SP-> |   operand1    |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |     ...       |                                           |
 *   ---        +===============+                                          ---
 *              |     ...       |
 *              +---------------+
 * stackLimit-> |     ...       | \
 *              +---------------+  \_guard region for detecting & processing stack overflow
 *              |     ...       |  /
 *              +---------------+ /
 *              |(object header)|
 *  low-memory  +---------------+
 *
 *
 *
 *  The opt compiler uses a different stackframe layout
 *
 *  hi-memory
 *              +---------------+                                            ...
 *              |     IP=0      |                                             .
 *              +---------------+                                             .
 *          +-> |     FP=0      |   <-- "end of vm stack" sentinal           .
 *          |   +---------------+                                             . caller's frame
 *          |   |    cmid=-1    |   <-- "invisible method" id                .
 *          |   +---------------+                                          ---.
 *          |   |   parameter0  |  \                                        | .
 *          |   +---------------+   \ parameter area                        | .
 *          |   |   parameter1  |   /  (== caller's operand stack area)     | .
 *   ---    |   +---------------+  /                                        |...
 *    |     |   |   saved IP    |  <-- return address (in caller)           |
 *    |      \  +---------------+                                           |
 *  header FP-> |   saved FP    |  <-- this frame's caller's frame          |
 *    |         +---------------+                                           |
 *    |         |    cmid       |  <-- this frame's compiledmethod id       |
 *   ---        +---------------+                                           |
 *    |         |               |                                           |
 *    |         |  Spill Area   |  <-- spills and other method-specific     |
 *    |         |     ...       |      compiler-managed storage             |
 *    |         +---------------+                                           |
 *    |         |   Saved FP    |     only SaveVolatile Frames              |   
 *    |         |    State      |                                           |
 *    |         +---------------+                                           |
 *    |         |  VolGPR[0]    |                                           |
 *    |         |     ...       |     only SaveVolatile Frames              |   
 *    |         |  VolGPR[n]    |                                           |
 *    |         +---------------+                                           | 
 *   body       |  NVolGPR[k]   |  <-- info.getUnsignedNonVolatileOffset()  | frame
 *    |         |     ...       |   k == info.getFirstNonVolatileGPR()      |
 *    |         |  NVolGPR[n]   |                                           |
 *    |         +---------------+                                           |
 *    |         |  NVolFPR[k]   |                                           |
 *    |         |     ...       |   k == info.getFirstNonVolatileFPR()      |
 *    |         |  NVolFPR[n]   |                                           |
 *    |         +---------------+                                           |
 *    |         |   parameter0  |  \                                        |
 *    |         +---------------+   \_parameters to callee frame            |
 *    |    SP-> |   parameter1  |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |     ...       |                                           |
 *   ---        +===============+                                          ---
 *              |     ...       |
 *              +---------------+
 * stackLimit-> |     ...       | \
 *              +---------------+  \_guard region for detecting & processing stack overflow
 *              |     ...       |  /
 *              +---------------+ /
 *              |(object header)|
 *  low-memory  +---------------+
 *
 * </pre>
 *
 * @author David Grove
 * @author Bowen Alpern
 */
interface VM_StackframeLayoutConstants  {

   static final int STACKFRAME_RETURN_ADDRESS_OFFSET   =  4; // offset of caller's return address from FP
   static final int STACKFRAME_FRAME_POINTER_OFFSET    =  0; // base of this frame
   static final int STACKFRAME_METHOD_ID_OFFSET        = -4; // offset of method id from FP
   static final int STACKFRAME_BODY_OFFSET             = -8; // offset of work area from FP
   static final int STACKFRAME_HEADER_SIZE             = 12; // size of frame header, in bytes
   
   // space to save entire FPU state.  The FPU state is saved only for 'bridge' frames
   static final int FPU_STATE_SIZE       	       = 108;

   static final int STACKFRAME_SENTINAL_FP = -2; // fp value indicating end of stack walkback
   static final int INVISIBLE_METHOD_ID    = -1; // marker for "assembler" frames that have no associated VM_Method

   // Stackframe alignment.
   // Align to 8 byte boundary for good floating point save/restore performance (on powerPC, anyway).
   //
   static final int STACKFRAME_ALIGNMENT = 8;
   static final int STACKFRAME_ALIGNMENT_MASK = STACKFRAME_ALIGNMENT - 1; // roundedUpSize = (size + STACKFRAME_ALIGNMENT_MASK) & ~STACKFRAME_ALIGNMENT_MASK
   
   // Sizes for stacks and subregions thereof.
   // Values are in bytes and must be a multiple of 4 (size of a stack slot).
   //
   static final int STACK_SIZE_GROW      = 8*1024; // how much to grow stack when overflow detected
   static final int STACK_SIZE_GUARD     = 8*1024; // max space needed for stack overflow trap processing
   static final int STACK_SIZE_SYSCALL   = 4*1024; // max space needed for any native code called by vm
   static final int STACK_SIZE_DLOPEN    = 30*1024; // max space needed for dlopen sys call
   static final int STACK_SIZE_GCDISABLED= 4*1024; // max space needed while running with gc disabled
   
   // Complications:
   // - STACK_SIZE_GUARD must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that frames allocated by stack growing code will fit within guard region.
   // - STACK_SIZE_GROW must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that, if stack is grown prior to disabling gc or calling native code,
   //   the new stack will accomodate that code without generating a stack overflow trap.
   // - Values chosen for STACK_SIZE_NATIVE and STACK_SIZE_GCDISABLED are pure guesswork
   //   selected by trial and error.
   
   // Stacks for "normal" threads grow as needed by trapping on guard region.
   // Stacks for "boot" and "collector" threads are fixed in size and cannot grow.
   //
   static final int STACK_SIZE_NORMAL    = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +   8*1024; // initial stack space to allocate for normal    thread (includes guard region)

   static final int STACK_SIZE_BOOT      = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for boot      thread (includes guard region)
   static final int STACK_SIZE_COLLECTOR = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for collector thread (includes guard region)
   static final int STACK_SIZE_MAX       = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED + 244*1024; // upper limit on stack size (includes guard region)
   
   static final int STACK_SIZE_JNINATIVE_GROW = 0; // TODO!!;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *
 * @author Bowen Alpern
 * @author David Grove
 */
interface VM_TrapConstants
   {

       /** 
	*  This base is added to the numeric trap codes in VM_Runtime.java
	* to yield the intel trap number that is given to INT instructions
	*/
       public final static byte RVM_TRAP_BASE = (byte) 0x40;

   }
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frame  built by the Baseline compiler
 * An Instance of this class will iterate through a particular 
 * reference map of a method returning the offsets of any refereces
 * that are part of the input parameters, local variables, and 
 * java stack for the stack frame.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_BaselineGCMapIterator extends VM_GCMapIterator 
  implements VM_BaselineConstants,
	     VM_Uninterruptible {
  private static final boolean TRACE_ALL = false;
  private static final boolean TRACE_DL  = false; // dynamic link frames

  //-------------//
  // Constructor //
  //-------------//

  // 
  // Remember the location array for registers. This array needs to be updated
  // with the location of any saved registers.
  // This information is not used by this iterator but must be updated for the
  // other types of iterators (ones for the quick and opt compiler built frames)
  // The locations are kept as addresses within the stack.
  //
  
  public VM_BaselineGCMapIterator(int registerLocations[]) {
    this.registerLocations = registerLocations; // (in superclass)
    dynamicLink  = new VM_DynamicLink();
  }

  //-----------//
  // Interface //
  //-----------//

  //
  // Set the iterator to scan the map at the machine instruction offset provided.
  // The iterator is positioned to the beginning of the map
  //
  //   method - identifies the method and class
  //   instruction offset - identifies the map to be scanned.
  //   fp  - identifies a specific occurrance of this method and
  //         allows for processing instance specific information
  //         i.e JSR return address values
  //
  //  NOTE: An iterator may be reused to scan a different method and map.
  //
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address fp) {
    currentMethod = compiledMethod.getMethod();
      
    // setup superclass
    //
    framePtr = fp;
      
    // setup stackframe mapping
    //
    maps      = ((VM_BaselineCompiledMethod)compiledMethod).referenceMaps;
    mapId     = maps.locateGCPoint(instructionOffset, currentMethod);
    mapOffset = 0;
    if (mapId < 0) {
      // lock the jsr lock to serialize jsr processing
      VM_ReferenceMaps.jsrLock.lock();
      maps.setupJSRSubroutineMap(framePtr, mapId, compiledMethod);
    }
    if (VM.TraceStkMaps || TRACE_ALL ) {
      VM.sysWrite("VM_BaselineGCMapIterator setupIterator mapId = ");
      VM.sysWrite(mapId);
      VM.sysWrite(".\n");
    }
      
    // setup dynamic bridge mapping
    //
    bridgeTarget                   = null;
    bridgeParameterTypes           = null;
    bridgeParameterMappingRequired = false;
    bridgeRegistersLocationUpdated = false;
    bridgeParameterIndex           = 0;
    bridgeRegisterIndex            = 0;
    bridgeRegisterLocation         = VM_Address.zero();
    bridgeSpilledParamLocation     = VM_Address.zero();
    
    if (currentMethod.getDeclaringClass().isDynamicBridge()) {
      VM_Address        ip                       = VM_Magic.getReturnAddress(fp);
                        fp                       = VM_Magic.getCallerFramePointer(fp);
      int               callingCompiledMethodId  = VM_Magic.getCompiledMethodID(fp);
      VM_CompiledMethod callingCompiledMethod    = VM_CompiledMethods.getCompiledMethod(callingCompiledMethodId);
      int               callingInstructionOffset = ip.diff(VM_Magic.objectAsAddress(callingCompiledMethod.getInstructions()));

      callingCompiledMethod.getDynamicLink(dynamicLink, callingInstructionOffset);
      bridgeTarget                    = dynamicLink.methodRef();
      bridgeParameterTypes            = bridgeTarget.getParameterTypes();
      if (dynamicLink.isInvokedWithImplicitThisParameter()) {
	bridgeParameterInitialIndex     = -1;
	bridgeSpilledParamInitialOffset =  8; // this + return addr
      } else {	
	bridgeParameterInitialIndex     =  0;
	bridgeSpilledParamInitialOffset =  4; // return addr
      }
      bridgeSpilledParamInitialOffset  += (4 * bridgeTarget.getParameterWords());
      if (callingCompiledMethod.getCompilerType() == VM_CompiledMethod.BASELINE) {
	bridgeSpilledParameterMappingRequired = false;
      } else {
	bridgeSpilledParameterMappingRequired = true;
      }
    }
        
    reset();
  }
  
  // Reset iteration to initial state.
  // This allows a map to be scanned multiple times
  //
  public void reset() {
    mapOffset = 0;

    if (bridgeTarget != null) {
      bridgeParameterMappingRequired = true;
      bridgeParameterIndex           = bridgeParameterInitialIndex;
      bridgeRegisterIndex            = 0;
      bridgeRegisterLocation         = framePtr.add(STACKFRAME_FIRST_PARAMETER_OFFSET); // top of frame
      bridgeSpilledParamLocation     = framePtr.add(bridgeSpilledParamInitialOffset);
    }
  }

  // Get location of next reference.
  // A zero return indicates that no more references exist.
  //
  public VM_Address getNextReferenceAddress() {
    if (mapId < 0) {
      mapOffset = maps.getNextJSRRef(mapOffset);
    } else {
      mapOffset = maps.getNextRef(mapOffset, mapId);
    }
    if (VM.TraceStkMaps || TRACE_ALL) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = ");
      VM.sysWriteHex(mapOffset);
      VM.sysWrite(".\n");
      VM.sysWrite("Reference is ");
      VM.sysWriteHex ( VM_Magic.getMemoryWord ( framePtr.add(mapOffset) ) );
      VM.sysWrite(".\n");
      if (mapId < 0) 
	VM.sysWrite("Offset is a JSR return address ie internal pointer.\n");
    }

    if (mapOffset != 0) {
      if (bridgeParameterMappingRequired)
	// TODO  clean this
	return (framePtr.add(mapOffset - BRIDGE_FRAME_EXTRA_SIZE ));
      else
	return (framePtr.add(mapOffset) );
    } else if (bridgeParameterMappingRequired) {
      if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	VM.sysWrite("getNextReferenceAddress: bridgeTarget="); VM.sysWrite(bridgeTarget); VM.sysWrite("\n");
      }         

      if (!bridgeRegistersLocationUpdated) {
	// point registerLocations[] to our callers stackframe
	//
	registerLocations[JTOC] = framePtr.add(JTOC_SAVE_OFFSET).toInt();
	registerLocations[T0]   = framePtr.add(T0_SAVE_OFFSET).toInt();
	registerLocations[T1]   = framePtr.add(T1_SAVE_OFFSET).toInt();
	registerLocations[EBX]  = framePtr.add(EBX_SAVE_OFFSET).toInt();
	
	bridgeRegistersLocationUpdated = true;
      }

      // handle implicit "this" parameter, if any
      //
      if (bridgeParameterIndex == -1) {
	bridgeParameterIndex       += 1;
	bridgeRegisterIndex        += 1;
	bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	
	if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	  VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link GPR this ");
	  VM.sysWrite(bridgeRegisterLocation.add(4));
	  VM.sysWrite(".\n");
	}

	return bridgeRegisterLocation.add(4);
      }
         
      // now the remaining parameters
      //
      while(bridgeParameterIndex < bridgeParameterTypes.length) {
	VM_Type bridgeParameterType = bridgeParameterTypes[bridgeParameterIndex++];
	
	if (bridgeParameterType.isReferenceType()) {
	  bridgeRegisterIndex        += 1;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	  
	  if (bridgeRegisterIndex <= NUM_PARAMETER_GPRS) {
	    if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link GPR parameter ");
	      VM.sysWrite(bridgeRegisterLocation.add(4));
	      VM.sysWrite(".\n");
	    }
	    return bridgeRegisterLocation.add(4);
	  } else {
	    if (bridgeSpilledParameterMappingRequired) {
	      if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
		VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link spilled parameter ");
		VM.sysWrite(bridgeSpilledParamLocation.add(4));
		VM.sysWrite(".\n");
	      }
	      return bridgeSpilledParamLocation.add(4);
	    } else {
	      break;
	    }
	  }
	} else if (bridgeParameterType.isLongType()) {
	  bridgeRegisterIndex        += 2;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(8);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(8);
	} else if (bridgeParameterType.isDoubleType()) {
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(8);
	} else if (bridgeParameterType.isFloatType()) {
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	} else { 
	  // boolean, byte, char, short, int
	  bridgeRegisterIndex        += 1;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	}
      }
    } else {
      // point registerLocations[] to our callers stackframe
      //
      registerLocations[JTOC] = framePtr.add(JTOC_SAVE_OFFSET).toInt();
    }
    
    return VM_Address.zero();
  }

  //
  // Gets the location of the next return address
  // after the current position.
  //  a zero return indicates that no more references exist
  //
  public VM_Address getNextReturnAddressAddress() {
    if (mapId >= 0) {
      if (VM.TraceStkMaps || TRACE_ALL) {
	VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset mapId = ");
	VM.sysWrite(mapId);
	VM.sysWrite(".\n");
      }
      return VM_Address.zero();
    }
    mapOffset = maps.getNextJSRReturnAddr(mapOffset);
    if (VM.TraceStkMaps || TRACE_ALL) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
    }
    return (mapOffset == 0) ? VM_Address.zero() : framePtr.add(mapOffset);
  }

  // cleanup pointers - used with method maps to release data structures
  //    early ... they may be in temporary storage ie storage only used
  //    during garbage collection
  //
  public void cleanupPointers() {
    maps.cleanupPointers();
    maps = null;
    if (mapId < 0)   
      VM_ReferenceMaps.jsrLock.unlock();
    bridgeTarget         = null;
    bridgeParameterTypes = null;
  }

  public int getType() {
    return VM_CompiledMethod.BASELINE;
  }

  // For debugging (used with checkRefMap)
  //
  public int getStackDepth() {
    return maps.getStackDepth(mapId);
  }

  // Iterator state for mapping any stackframe.
  //
  private   int              mapOffset; // current offset in current map
  private   int              mapId;     // id of current map out of all maps
  private   VM_ReferenceMaps maps;      // set of maps for this method

  // Additional iterator state for mapping dynamic bridge stackframes.
  //
  private VM_DynamicLink dynamicLink;                    // place to keep info returned by VM_CompiledMethod.getDynamicLink
  private VM_Method      bridgeTarget;                   // method to be invoked via dynamic bridge (null: current frame is not a dynamic bridge)
  private VM_Method      currentMethod;                  // method for the frame
  private VM_Type[]      bridgeParameterTypes;           // parameter types passed by that method
  private boolean        bridgeParameterMappingRequired; // have all bridge parameters been mapped yet?
  private boolean        bridgeSpilledParameterMappingRequired; // do we need to map spilled params (baseline compiler = no, opt = yes)
  private boolean        bridgeRegistersLocationUpdated; // have the register location been updated
  private int            bridgeParameterInitialIndex;    // first parameter to be mapped (-1 == "this")
  private int            bridgeParameterIndex;           // current parameter being mapped (-1 == "this")
  private int            bridgeRegisterIndex;            // gpr register it lives in
  private VM_Address     bridgeRegisterLocation;         // memory address at which that register was saved
  private VM_Address     bridgeSpilledParamLocation;     // current spilled param location
  private int            bridgeSpilledParamInitialOffset;// starting offset to stack location for param0
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * Registers used by baseline compiler implementation of virtual machine.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
interface VM_BaselineConstants extends VM_Constants {
  
  static final int    WORDSIZE = 4; // bytes
  static final int LG_WORDSIZE = 2; 

  // Dedicated registers.
  //
  static final byte JTOC = EDI;
  static final byte SP   = ESP;
  static final byte PR   = PROCESSOR_REGISTER;
  
  // Volatile (parameter) registers.
  //
  static final byte T0   =  EAX;  // DO NOT CHANGE THIS ASSIGNMENT
  static final byte T1   =  EDX; 
  
  // scratch register
  static final byte S0  =  ECX;

  // Mnemonics corresponding to the above constants.
  // These are some alternate names that can be used in the debugger
  //
  static final String[] RVM_GPR_NAMES =
     {
     "eax", "ecx", "edx", "ebx", "esp", "ebp", "PR", "JT"
     };

  // Constants describing baseline compiler conventions for
  // saving registers in stackframes.
  // 
  static final int STACKFRAME_REG_SAVE_OFFSET	       = STACKFRAME_BODY_OFFSET;
                                        // offset from FP of the saved registers.  
					// Some registers are saved in all baseline
					// frames, and most register as saved in the
					// dynamic bridge frames.
  static final int STACKFRAME_FIRST_PARAMETER_OFFSET  = STACKFRAME_REG_SAVE_OFFSET -8;
  // bridge frames save 3 additional GPRs
  static final int BRIDGE_FRAME_EXTRA_SIZE	       = FPU_STATE_SIZE + 12;

  static final int SAVED_GPRS       = 1; // JTOC is a nonvolatile registers used by baseline compiler
  static final int JTOC_SAVE_OFFSET = STACKFRAME_REG_SAVE_OFFSET;
  static final int EBX_SAVE_OFFSET  = STACKFRAME_REG_SAVE_OFFSET - 4;
  static final int T0_SAVE_OFFSET   = STACKFRAME_FIRST_PARAMETER_OFFSET ;
  static final int T1_SAVE_OFFSET   = STACKFRAME_FIRST_PARAMETER_OFFSET - 4;
  static final int FPU_SAVE_OFFSET  = T1_SAVE_OFFSET - FPU_STATE_SIZE;

}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Handle exception delivery and stack unwinding for methods compiled by 
 * baseline compiler.
 *
 * @author Derek Lieber
 * @date 18 Sep 1998 
 */
class VM_BaselineExceptionDeliverer extends VM_ExceptionDeliverer 
   implements VM_BaselineConstants  {

  /**
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod compiledMethod,
			VM_Address        catchBlockInstructionAddress,
			Throwable         exceptionObject,
			VM_Registers      registers) {
    VM_Address fp     = registers.getInnermostFramePointer();
    VM_Method method = compiledMethod.getMethod();
    VM_Thread myThread = VM_Thread.getCurrentThread();

    // reset sp to "empty expression stack" state
    //
    VM_Address sp = fp.add(VM_Compiler.getEmptyStackOffset(method));
    
    // push exception object as argument to catch block
    //
    sp = sp.sub(4);
    VM_Magic.setMemoryAddress(sp, VM_Magic.objectAsAddress(exceptionObject));
    registers.gprs[SP] = sp.toInt();

    // set address at which to resume executing frame
    registers.ip = catchBlockInstructionAddress;

    // branch to catch block
    //
    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called
    if (VM.VerifyAssertions) VM.assert(registers.inuse == true); 

    registers.inuse = false;

    // 'give back' the portion of the stack we borrowed to run 
    // exception delivery code when invoked for a hardware trap.
    // If this was a straight software trap (athrow) then setting 
    // the stacklimit should be harmless, since the stacklimit should already have exactly
    // the value we are setting it too. 
    if (!myThread.hardwareExceptionRegisters.inuse) {
      myThread.stackLimit = VM_Magic.objectAsAddress(myThread.stack).add(STACK_SIZE_GUARD);
      VM_Processor.getCurrentProcessor().activeThreadStackLimit = myThread.stackLimit;
    }

    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }
   

  /**
   * Unwind a stackframe.
   */
  void unwindStackFrame(VM_CompiledMethod compiledMethod, VM_Registers registers) {
    VM_Method method = compiledMethod.getMethod();
    VM_Address fp     = registers.getInnermostFramePointer();
    if (method.isSynchronized()) { // release the lock, if it is being held
      VM_Address ip = registers.getInnermostInstructionAddress();
      int instr = ip.diff(VM_Magic.objectAsAddress(compiledMethod.getInstructions()));
      int lockOffset = ((VM_BaselineCompiledMethod)compiledMethod).getLockAcquisitionOffset();
      if (instr > lockOffset) { // we actually have the lock, so must unlock it.
	Object lock;
	if (method.isStatic()) {
	  lock = method.getDeclaringClass().getClassForType();
	} else {
	  lock = VM_Magic.addressAsObject(VM_Magic.getMemoryAddress(fp.add(VM_Compiler.getFirstLocalOffset(method))));
	}
	VM_ObjectModel.genericUnlock(lock);
      }
    }
    // Restore nonvolatile registers used by the baseline compiler.
    if (VM.VerifyAssertions) VM.assert(VM_Compiler.SAVED_GPRS == 1);
    registers.gprs[JTOC] = VM_Magic.getMemoryWord(fp.add(VM_Compiler.JTOC_SAVE_OFFSET));
    
    registers.unwindStackFrame();
  }
}


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * 
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

public class VM_Compiler extends VM_BaselineCompiler implements VM_BaselineConstants {

  private final int parameterWords;
  private int firstLocalOffset;

  /**
   * Create a VM_Compiler object for the compilation of method.
   */
  VM_Compiler(VM_BaselineCompiledMethod cm) {
    super(cm);
    stackHeights = new int[bytecodes.length];
    parameterWords = method.getParameterWords() + (method.isStatic() ? 0 : 1); // add 1 for this pointer
  }

  /**
   * The last true local
   */
  static int getEmptyStackOffset (VM_Method m) {
    return getFirstLocalOffset(m) - (m.getLocalWords()<<LG_WORDSIZE) + WORDSIZE;
  }

  /**
   * This is misnamed.  It should be getFirstParameterOffset.
   * It will not work as a base to access true locals.
   * TODO!! make sure it is not being used incorrectly
   */
  static int getFirstLocalOffset (VM_Method method) {
    if (method.getDeclaringClass().isBridgeFromNative())
      return STACKFRAME_BODY_OFFSET - (VM_JNICompiler.SAVED_GPRS_FOR_JNI << LG_WORDSIZE);
    else
      return STACKFRAME_BODY_OFFSET - (SAVED_GPRS << LG_WORDSIZE);
  }
  

  /*
   * implementation of abstract methods of VM_BaselineCompiler
   */

  /*
   * Misc routines not directly tied to a particular bytecode
   */

  /**
   * Emit the prologue for the method
   */
  protected final void emit_prologue() {
    genPrologue();
  }

  /**
   * Emit code to complete the dynamic linking of a
   * prematurely resolved VM_Type.
   * @param dictionaryId of type to link (if necessary)
   */
  protected final void emit_initializeClassIfNeccessary(int dictionaryId) {
    asm.emitMOV_Reg_Imm (T0, dictionaryId);
    asm.emitPUSH_Reg    (T0);
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.initializeClassIfNecessaryMethod.getOffset());
  }

  /**
   * Emit the code for a threadswitch tests (aka a yieldpoint).
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  protected final void emit_threadSwitchTest(int whereFrom) {
    genThreadSwitchTest(whereFrom);
  }

  /**
   * Emit the code to implement the spcified magic.
   * @param magicMethod desired magic
   */
  protected final void emit_Magic(VM_Method magicMethod) {
    genMagic(magicMethod);
  }


  /*
   * Loading constants
   */


  /**
   * Emit code to load the null constant.
   */
  protected final void emit_aconst_null() {
    asm.emitPUSH_Imm(0);
  }

  /**
   * Emit code to load an int constant.
   * @param val the int constant to load
   */
  protected final void emit_iconst(int val) {
    asm.emitPUSH_Imm(val);
  }

  /**
   * Emit code to load a long constant
   * @param val the lower 32 bits of long constant (upper32 are 0).
   */
  protected final void emit_lconst(int val) {
    asm.emitPUSH_Imm(0);  // high part
    asm.emitPUSH_Imm(val);  //  low part
  }

  /**
   * Emit code to load 0.0f
   */
  protected final void emit_fconst_0() {
    asm.emitPUSH_Imm(0);
  }

  /**
   * Emit code to load 1.0f
   */
  protected final void emit_fconst_1() {
    asm.emitPUSH_Imm(0x3f800000);
  }

  /**
   * Emit code to load 2.0f
   */
  protected final void emit_fconst_2() {
    asm.emitPUSH_Imm(0x40000000);
  }

  /**
   * Emit code to load 0.0d
   */
  protected final void emit_dconst_0() {
    asm.emitPUSH_Imm(0x00000000);
    asm.emitPUSH_Imm(0x00000000);
  }

  /**
   * Emit code to load 1.0d
   */
  protected final void emit_dconst_1() {
    asm.emitPUSH_Imm(0x3ff00000);
    asm.emitPUSH_Imm(0x00000000);
  }

  /**
   * Emit code to load a 32 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc(int offset) {
    asm.emitPUSH_RegDisp(JTOC, offset);   
  }

  /**
   * Emit code to load a 64 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc2(int offset) {
    asm.emitPUSH_RegDisp(JTOC, offset+4); // high 32 bits 
    asm.emitPUSH_RegDisp(JTOC, offset);   // low 32 bits
  }


  /*
   * loading local variables
   */


  /**
   * Emit code to load an int local variable
   * @param index the local index to load
   */
  protected final void emit_iload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP,offset);
  }

  /**
   * Emit code to load a long local variable
   * @param index the local index to load
   */
  protected final void emit_lload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset); // high part
    asm.emitPUSH_RegDisp(ESP, offset); // low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to local a float local variable
   * @param index the local index to load
   */
  protected final void emit_fload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp (ESP, offset);
  }

  /**
   * Emit code to load a double local variable
   * @param index the local index to load
   */
  protected final void emit_dload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset); // high part
    asm.emitPUSH_RegDisp(ESP, offset); // low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to load a reference local variable
   * @param index the local index to load
   */
  protected final void emit_aload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset);
  }


  /*
   * storing local variables
   */


  /**
   * Emit code to store an int to a local variable
   * @param index the local index to load
   */
  protected final void emit_istore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset); 
  }

  /**
   * Emit code to store a long to a local variable
   * @param index the local index to load
   */
  protected final void emit_lstore(int index) {
    int offset = localOffset(index+1) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp(ESP, offset); // high part
    asm.emitPOP_RegDisp(ESP, offset); //  low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to store a float to a local variable
   * @param index the local index to load
   */
  protected final void emit_fstore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset);
  }

  /**
   * Emit code to store an double  to a local variable
   * @param index the local index to load
   */
  protected final void emit_dstore(int index) {
    int offset = localOffset(index+1) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp(ESP, offset); // high part
    asm.emitPOP_RegDisp(ESP, offset); //  low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to store a reference to a local variable
   * @param index the local index to load
   */
  protected final void emit_astore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset);
  }


  /*
   * array loads
   */


  /**
   * Emit code to load from an int array
   */
  protected final void emit_iaload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired int array element
  }

  /**
   * Emit code to load from a long array
   */
  protected final void emit_laload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);             // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, WORDSIZE); // load high part of desired long array element
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, 0);        // load low part of desired long array element
  }

  /**
   * Emit code to load from a float array
   */
  protected final void emit_faload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired float array element
  }

  /**
   * Emit code to load from a double array
   */
  protected final void emit_daload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);             // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, WORDSIZE); // load high part of double
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, 0);        // load low part of double
  }

  /**
   * Emit code to load from a reference array
   */
  protected final void emit_aaload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired object array element
  }

  /**
   * Emit code to load from a byte/boolean array
   */
  protected final void emit_baload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                     // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                     // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // complete popping the 2 args
    asm.emitMOVSX_Reg_RegIdx_Byte(T1, S0, T0, asm.BYTE, 0); // load byte and sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                   // push sign extended byte onto stack
  }

  /**
   * Emit code to load from a char array
   */
  protected final void emit_caload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                      // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                      // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                             // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                     // complete popping the 2 args
    asm.emitMOVZX_Reg_RegIdx_Word(T1, S0, T0, asm.SHORT, 0); // load halfword without sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                    // push char onto stack
  }

  /**
   * Emit code to load from a short array
   */
  protected final void emit_saload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                      // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                      // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                             // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                     // complete popping the 2 args
    asm.emitMOVSX_Reg_RegIdx_Word(T1, S0, T0, asm.SHORT, 0); // load halfword sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                    // push sign extended short onto stack
  }


  /*
   * array stores
   */


  /**
   * Emit code to store to an int array
   */
  protected final void emit_iastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the int value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a long array
   */
  protected final void emit_lastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 8);                     // T0 is the array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 12);                    // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitPOP_Reg(T1);                                    // low part of long value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, 0, T1);        // [S0 + T0<<3 + 0] <- T1 store low part into array i.e.  
    asm.emitPOP_Reg(T1);                                    // high part of long value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, WORDSIZE, T1); // [S0 + T0<<3 + 4] <- T1 store high part into array i.e. 
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // remove index and ref from the stack
  }

  /**
   * Emit code to store to a float array
   */
  protected final void emit_fastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the float value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a double array
   */
  protected final void emit_dastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 8);                     // T0 is the array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 12);                    // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitPOP_Reg(T1);                                    // low part of double value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, 0, T1);        // [S0 + T0<<3 + 0] <- T1 store low part into array i.e.  
    asm.emitPOP_Reg(T1);                                    // high part of double value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, WORDSIZE, T1); // [S0 + T0<<3 + 4] <- T1 store high part into array i.e. 
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // remove index and ref from the stack
  }

  /**
   * Emit code to store to a reference array
   */
  protected final void emit_aastore() {
    asm.emitPUSH_RegDisp(SP, 2<<LG_WORDSIZE);        // duplicate array ref
    asm.emitPUSH_RegDisp(SP, 1<<LG_WORDSIZE);        // duplicate object value
    genParameterRegisterLoad(2);                     // pass 2 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.checkstoreMethod.getOffset()); // checkstore(array ref, value)
    if (VM_Collector.NEEDS_WRITE_BARRIER) 
      VM_Barriers.compileArrayStoreBarrier(asm);
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the object value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a byte/boolean array
   */
  protected final void emit_bastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the byte value
    asm.emitMOV_RegIdx_Reg_Byte(S0, T0, asm.BYTE, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }

  /**
   * Emit code to store to a char array
   */
  protected final void emit_castore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the char value
    asm.emitMOV_RegIdx_Reg_Word(S0, T0, asm.SHORT, 0, T1);// store halfword element into array i.e. [S0 +T0] <- T1 (halfword)
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }

  /**
   * Emit code to store to a short array
   */
  protected final void emit_sastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the short value
    asm.emitMOV_RegIdx_Reg_Word(S0, T0, asm.SHORT, 0, T1);// store halfword element into array i.e. [S0 +T0] <- T1 (halfword)
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }


  /*
   * expression stack manipulation
   */


  /**
   * Emit code to implement the pop bytecode
   */
  protected final void emit_pop() {
    asm.emitPOP_Reg(T0);
  }

  /**
   * Emit code to implement the pop2 bytecode
   */
  protected final void emit_pop2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(T0);
  }

  /**
   * Emit code to implement the dup bytecode
   */
  protected final void emit_dup() {
    asm.emitMOV_Reg_RegInd (T0, SP);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup_x1 bytecode
   */
  protected final void emit_dup_x1() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup_x2 bytecode
   */
  protected final void emit_dup_x2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup2 bytecode
   */
  protected final void emit_dup2() {
    asm.emitMOV_Reg_RegDisp (T0, SP, 4);
    asm.emitMOV_Reg_RegInd (S0, SP);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
  }

  /**
   * Emit code to implement the dup2_x1 bytecode
   */
  protected final void emit_dup2_x1() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup2_x2 bytecode
   */
  protected final void emit_dup2_x2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPOP_Reg(JTOC);                  // JTOC is scratch register
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(JTOC);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    // restore JTOC register
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the swap bytecode
   */
  protected final void emit_swap() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
  }


  /*
   * int ALU
   */


  /**
   * Emit code to implement the iadd bytecode
   */
  protected final void emit_iadd() {
    asm.emitPOP_Reg(T0);
    asm.emitADD_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the isub bytecode
   */
  protected final void emit_isub() {
    asm.emitPOP_Reg(T0);
    asm.emitSUB_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the imul bytecode
   */
  protected final void emit_imul() {
    asm.emitPOP_Reg (T0);
    asm.emitIMUL2_Reg_RegInd(T0, SP);
    asm.emitMOV_RegInd_Reg (SP, T0);
  }

  /**
   * Emit code to implement the idiv bytecode
   */
  protected final void emit_idiv() {
    asm.emitMOV_Reg_RegDisp(ECX, SP, 0); // ECX is divisor; NOTE: can't use symbolic registers because of intel hardware requirements
    asm.emitMOV_Reg_RegDisp(EAX, SP, 4); // EAX is dividend
    asm.emitCDQ ();                      // sign extend EAX into EDX
    asm.emitIDIV_Reg_Reg(EAX, ECX);      // compute EAX/ECX - Quotient in EAX, remainder in EDX
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2); // complete popping the 2 values
    asm.emitPUSH_Reg(EAX);               // push result
  }

  /**
   * Emit code to implement the irem bytecode
   */
  protected final void emit_irem() {
    asm.emitMOV_Reg_RegDisp(ECX, SP, 0); // ECX is divisor; NOTE: can't use symbolic registers because of intel hardware requirements
    asm.emitMOV_Reg_RegDisp(EAX, SP, 4); // EAX is dividend
    asm.emitCDQ ();                      // sign extend EAX into EDX
    asm.emitIDIV_Reg_Reg(EAX, ECX);      // compute EAX/ECX - Quotient in EAX, remainder in EDX
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2); // complete popping the 2 values
    asm.emitPUSH_Reg(EDX);               // push remainder
  }

  /**
   * Emit code to implement the ineg bytecode
   */
  protected final void emit_ineg() {
    asm.emitNEG_RegInd(SP); // [SP] <- -[SP]
  }

  /**
   * Emit code to implement the ishl bytecode
   */
  protected final void emit_ishl() {
    asm.emitPOP_Reg(ECX);
    asm.emitSHL_RegInd_Reg(SP, ECX);   
  }

  /**
   * Emit code to implement the ishr bytecode
   */
  protected final void emit_ishr() {
    asm.emitPOP_Reg (ECX);
    asm.emitSAR_RegInd_Reg (SP, ECX);  
  }

  /**
   * Emit code to implement the iushr bytecode
   */
  protected final void emit_iushr() {
    asm.emitPOP_Reg (ECX);
    asm.emitSHR_RegInd_Reg(SP, ECX); 
  }

  /**
   * Emit code to implement the iand bytecode
   */
  protected final void emit_iand() {
    asm.emitPOP_Reg(T0);
    asm.emitAND_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the ior bytecode
   */
  protected final void emit_ior() {
    asm.emitPOP_Reg(T0);
    asm.emitOR_RegInd_Reg (SP, T0);
  }

  /**
   * Emit code to implement the ixor bytecode
   */
  protected final void emit_ixor() {
    asm.emitPOP_Reg(T0);
    asm.emitXOR_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the iinc bytecode
   * @param index index of local
   * @param val value to increment it by
   */
  protected final void emit_iinc(int index, int val) {
    int offset = localOffset(index);
    asm.emitADD_RegDisp_Imm(ESP, offset, val);
  }


  /*
   * long ALU
   */


  /**
   * Emit code to implement the ladd bytecode
   */
  protected final void emit_ladd() {
    asm.emitPOP_Reg(T0);                 // the low half of one long
    asm.emitPOP_Reg(S0);                 // the high half
    asm.emitADD_RegInd_Reg(SP, T0);          // add low halves
    asm.emitADC_RegDisp_Reg(SP, WORDSIZE, S0);   // add high halves with carry
  }

  /**
   * Emit code to implement the lsub bytecode
   */
  protected final void emit_lsub() {
    asm.emitPOP_Reg(T0);                 // the low half of one long
    asm.emitPOP_Reg(S0);                 // the high half
    asm.emitSUB_RegInd_Reg(SP, T0);          // subtract low halves
    asm.emitSBB_RegDisp_Reg(SP, WORDSIZE, S0);   // subtract high halves with borrow
  }

  /**
   * Emit code to implement the lmul bytecode
   */
  protected final void emit_lmul() {
    // 0: JTOC is used as scratch registers (see 14)
    // 1: load value1.low temp0, i.e., save value1.low
    // 2: eax <- temp0 eax is value1.low
    // 3: edx:eax <- eax * value2.low (product of the two low halves)
    // 4: store eax which is  result.low into place --> value1.low is destroyed
    // 5: temp1 <- edx which is the carry of the product of the low halves
    // aex and edx now free of results
    // 6: aex <- temp0 which is still value1.low
    // 7: pop into aex aex <- value2.low  --> value2.low is sort of destroyed
    // 8: edx:eax <- eax * value1.hi  (value2.low * value1.hi)
    // 9: temp1 += aex
    // 10: pop into eax; eax <- value2.hi -> value2.hi is sort of destroyed
    // 11: edx:eax <- eax * temp0 (value2.hi * value1.low)
    // 12: temp1 += eax  temp1 is now result.hi
    // 13: store result.hi
    // 14: restore JTOC
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);
    if (VM.VerifyAssertions) VM.assert(S0 != EDX);
    asm.emitMOV_Reg_RegDisp (JTOC, SP, 8);          // step 1: JTOC is temp0
    asm.emitMOV_Reg_Reg (EAX, JTOC);            // step 2
    asm.emitMUL_Reg_RegInd(EAX, SP);    // step 3
    asm.emitMOV_RegDisp_Reg (SP, 8, EAX);           // step 4
    asm.emitMOV_Reg_Reg (S0, EDX);              // step 5: S0 is temp1
    asm.emitMOV_Reg_Reg (EAX, JTOC);            // step 6
    asm.emitPOP_Reg (EAX);                  // step 7: SP changed!
    asm.emitIMUL1_Reg_RegDisp(EAX, SP, 8);// step 8
    asm.emitADD_Reg_Reg (S0, EAX);      // step 9
    asm.emitPOP_Reg (EAX);                  // step 10: SP changed!
    asm.emitIMUL1_Reg_Reg(EAX, JTOC);    // step 11
    asm.emitADD_Reg_Reg (S0, EAX);      // step 12
    asm.emitMOV_RegDisp_Reg (SP, 4, S0);            // step 13
    // restore JTOC register
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the ldiv bytecode
   */
  protected final void emit_ldiv() {
    // (1) zero check
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);
    asm.emitOR_Reg_RegDisp(T0, SP, 4);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);
    asm.emitINT_Imm(VM_Runtime.TRAP_DIVIDE_BY_ZERO + RVM_TRAP_BASE);	// trap if divisor is 0
    fr1.resolve(asm);
    // (2) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (3) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    // (4) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysLongDivideIPField.getOffset());
    // (5) pop space for arguments
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (6) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (7) pop expression stack
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (8) push results
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the lrem bytecode
   */
  protected final void emit_lrem() {
    // (1) zero check
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);
    asm.emitOR_Reg_RegDisp(T0, SP, 4);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);
    asm.emitINT_Imm(VM_Runtime.TRAP_DIVIDE_BY_ZERO + RVM_TRAP_BASE);	// trap if divisor is 0
    fr1.resolve(asm);
    // (2) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (3) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    // (4) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysLongRemainderIPField.getOffset());
    // (5) pop space for arguments
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (6) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (7) pop expression stack
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (8) push results
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the lneg bytecode
   */
  protected final void emit_lneg() {
    asm.emitNEG_RegDisp(SP, 4);    // [SP+4] <- -[SP+4] or high <- -high
    asm.emitNEG_RegInd(SP);    // [SP] <- -[SP] or low <- -low
    asm.emitSBB_RegDisp_Imm(SP, 4, 0); // [SP+4] += borrow or high += borrow
  }

  /**
   * Emit code to implement the lshsl bytecode
   */
  protected final void emit_lshl() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: pop high half into T1
    // 4: ECX <- JTOC, copy the shift count
    // 5: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 6: branch to step 12 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 7: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 8: T1 <- T0, or replace the high half with the low half.  This accounts for the 32 bit shift
    // 9: shift T1 left by ECX bits
    // 10: T0 <- 0
    // 11: branch to step 14
    // 12: shift left double from T0 into T1 by ECX bits.  T0 is unaltered
    // 13: shift left T0, the low half, also by ECX bits
    // 14: push high half from T1
    // 15: push the low half from T0
    // 16: restore the JTOC
    asm.emitPOP_Reg (JTOC);                 // original shift amount 6 bits
    asm.emitPOP_Reg (T0);                   // pop low half 
    asm.emitPOP_Reg (T1);                   // pop high half
    asm.emitMOV_Reg_Reg (ECX, JTOC);
    asm.emitAND_Reg_Imm (JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitMOV_Reg_Reg (T1, T0);               // low replaces high
    asm.emitSHL_Reg_Reg (T1, ECX);
    asm.emitXOR_Reg_Reg (T0, T0);
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitSHLD_Reg_Reg_Reg(T1, T0, ECX);          // shift high half (step 12)
    asm.emitSHL_Reg_Reg (T0, ECX);                   // shift low half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 14)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the lshr bytecode
   */
  protected final void emit_lshr() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: pop high half into T1
    // 4: ECX <- JTOC, copy the shift count
    // 5: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 6: branch to step 13 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 7: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 8: T0 <- T1, or replace the low half with the high half.  This accounts for the 32 bit shift
    // 9: shift T0 right arithmetic by ECX bits
    // 10: ECX <- 31
    // 11: shift T1 right arithmetic by ECX=31 bits, thus exending the sigh
    // 12: branch to step 15
    // 13: shift right double from T1 into T0 by ECX bits.  T1 is unaltered
    // 14: shift right arithmetic T1, the high half, also by ECX bits
    // 15: push high half from T1
    // 16: push the low half from T0
    // 17: restore JTOC
    asm.emitPOP_Reg (JTOC);                 // original shift amount 6 bits
    asm.emitPOP_Reg (T0);                   // pop low half 
    asm.emitPOP_Reg (T1);                   // pop high
    asm.emitMOV_Reg_Reg (ECX, JTOC);
    asm.emitAND_Reg_Imm (JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitMOV_Reg_Reg (T0, T1);               // replace low with high
    asm.emitSAR_Reg_Reg (T0, ECX);                   // and shift it
    asm.emitMOV_Reg_Imm (ECX, 31);
    asm.emitSAR_Reg_Reg (T1, ECX);                   // set high half
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitSHRD_Reg_Reg_Reg(T0, T1, ECX);          // shift low half (step 13)
    asm.emitSAR_Reg_Reg (T1, ECX);                   // shift high half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 15)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the lushr bytecode
   */
  protected final void emit_lushr() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: ECX <- JTOC, copy the shift count
    // 4: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 5: branch to step 11 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 6: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 7: pop high half into T0 replace the low half with the high 
    //        half.  This accounts for the 32 bit shift
    // 8: shift T0 right logical by ECX bits
    // 9: T1 <- 0                        T1 is the high half
    // 10: branch to step 14
    // 11: pop high half into T1
    // 12: shift right double from T1 into T0 by ECX bits.  T1 is unaltered
    // 13: shift right logical T1, the high half, also by ECX bits
    // 14: push high half from T1
    // 15: push the low half from T0
    // 16: restore JTOC
    asm.emitPOP_Reg(JTOC);                // original shift amount 6 bits
    asm.emitPOP_Reg(T0);                  // pop low half 
    asm.emitMOV_Reg_Reg(ECX, JTOC);
    asm.emitAND_Reg_Imm(JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitPOP_Reg (T0);                   // replace low with high
    asm.emitSHR_Reg_Reg (T0, ECX);      // and shift it (count - 32)
    asm.emitXOR_Reg_Reg (T1, T1);               // high <- 0
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitPOP_Reg (T1);                   // high half (step 11)
    asm.emitSHRD_Reg_Reg_Reg(T0, T1, ECX);          // shift low half
    asm.emitSHR_Reg_Reg (T1, ECX);                   // shift high half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 14)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the land bytecode
   */
  protected final void emit_land() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitAND_RegInd_Reg(SP, T0);
    asm.emitAND_RegDisp_Reg(SP, 4, S0);
  }

  /**
   * Emit code to implement the lor bytecode
   */
  protected final void emit_lor() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitOR_RegInd_Reg(SP, T0);
    asm.emitOR_RegDisp_Reg(SP, 4, S0);
  }

  /**
   * Emit code to implement the lxor bytecode
   */
  protected final void emit_lxor() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitXOR_RegInd_Reg(SP, T0);
    asm.emitXOR_RegDisp_Reg(SP, 4, S0);
  }


  /*
   * float ALU
   */


  /**
   * Emit code to implement the fadd bytecode
   */
  protected final void emit_fadd() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFADD_Reg_RegDisp(FP0, SP, WORDSIZE); // FPU reg. stack += value1
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fsub bytecode
   */
  protected final void emit_fsub() {
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1
    asm.emitFSUB_Reg_RegDisp(FP0, SP, 0);        // FPU reg. stack -= value2
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fmul bytecode
   */
  protected final void emit_fmul() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFMUL_Reg_RegDisp(FP0, SP, WORDSIZE); // FPU reg. stack *= value1
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fdiv bytecode
   */
  protected final void emit_fdiv() {
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1
    asm.emitFDIV_Reg_RegDisp(FP0, SP, 0);        // FPU reg. stack /= value2
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the frem bytecode
   */
  protected final void emit_frem() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2, or a
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1, or b
    asm.emitFPREM ();             // FPU reg. stack <- a%b
    asm.emitFSTP_RegDisp_Reg(SP, WORDSIZE, FP0); // POP FPU reg. stack (results) onto java stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto java stack
    asm.emitPOP_Reg   (T0);           // shrink the stack (T0 discarded)
  }

  /**
   * Emit code to implement the fneg bytecode
   */
  protected final void emit_fneg() {
    asm.emitFLD_Reg_RegInd (FP0, SP); // FPU reg. stack <- value1
    asm.emitFCHS  ();      // change sign to stop of FPU stack
    asm.emitFSTP_RegInd_Reg(SP, FP0); // POP FPU reg. stack onto stack
  }


  /*
   * double ALU
   */


  /**
   * Emit code to implement the dadd bytecode
   */
  protected final void emit_dadd() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFADD_Reg_RegDisp_Quad(FP0, SP, 8);        // FPU reg. stack += value1
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);  // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the dsub bytecode
   */
  protected final void emit_dsub() {
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 8);          // FPU reg. stack <- value1
    asm.emitFSUB_Reg_RegDisp_Quad(FP0, SP, 0);          // FPU reg. stack -= value2
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the dmul bytecode
   */
  protected final void emit_dmul() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);          // FPU reg. stack <- value2
    asm.emitFMUL_Reg_RegDisp_Quad(FP0, SP, 8);          // FPU reg. stack *= value1
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the ddiv bytecode
   */
  protected final void emit_ddiv() {
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 8);          // FPU reg. stack <- value1
    asm.emitFDIV_Reg_RegInd_Quad(FP0, SP);          // FPU reg. stack /= value2
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the drem bytecode
   */
  protected final void emit_drem() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);          // FPU reg. stack <- value2, or a
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 2*WORDSIZE); // FPU reg. stack <- value1, or b
    asm.emitFPREM ();               // FPU reg. stack <- a%b
    asm.emitFSTP_RegDisp_Reg_Quad(SP, 2*WORDSIZE, FP0); // POP FPU reg. stack (result) onto java stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);         // POP FPU reg. stack onto java stack
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
  }

  /**
   * Emit code to implement the dneg bytecode
   */
  protected final void emit_dneg() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP); // FPU reg. stack <- value1
    asm.emitFCHS  ();      // change sign to stop of FPU stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0); // POP FPU reg. stack onto stack
  }


  /*
   * conversion ops
   */


  /**
   * Emit code to implement the i2l bytecode
   */
  protected final void emit_i2l() {
    asm.emitPOP_Reg (EAX);
    asm.emitCDQ ();
    asm.emitPUSH_Reg(EDX);
    asm.emitPUSH_Reg(EAX);
  }

  /**
   * Emit code to implement the i2f bytecode
   */
  protected final void emit_i2f() {
    asm.emitFILD_Reg_RegInd(FP0, SP);
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the i2d bytecode
   */
  protected final void emit_i2d() {
    asm.emitFILD_Reg_RegInd(FP0, SP);
    asm.emitPUSH_Reg(T0);             // grow the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the l2i bytecode
   */
  protected final void emit_l2i() {
    asm.emitPOP_Reg (T0); // low half of the long
    asm.emitPOP_Reg (S0); // high half of the long
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the l2f bytecode
   */
  protected final void emit_l2f() {
    asm.emitFILD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE);                // shrink the stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the l2d bytecode
   */
  protected final void emit_l2d() {
    asm.emitFILD_Reg_RegInd_Quad(FP0, SP);
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the f2i bytecode
   */
  protected final void emit_f2i() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push arg to C function 
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysFloatToIntIPField.getOffset());
    // (4) pop argument;
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the f2l bytecode
   */
  protected final void emit_f2l() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push arg to C function 
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysFloatToLongIPField.getOffset());
    // (4) pop argument;
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 0, T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the f2d bytecode
   */
  protected final void emit_f2d() {
    asm.emitFLD_Reg_RegInd(FP0, SP);
    asm.emitSUB_Reg_Imm(SP, WORDSIZE);                // grow the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the d2i bytecode
   */
  protected final void emit_d2i() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysDoubleToIntIPField.getOffset());
    // (4) pop arguments
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitPOP_Reg(S0); // shrink stack by 1 word
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the d2l bytecode
   */
  protected final void emit_d2l() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysDoubleToLongIPField.getOffset());
    // (4) pop arguments
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 4, T1);
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the d2f bytecode
   */
  protected final void emit_d2f() {
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE);                // shrink the stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the i2b bytecode
   */
  protected final void emit_i2b() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVSX_Reg_Reg_Byte(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }

  /**
   * Emit code to implement the i2c bytecode
   */
  protected final void emit_i2c() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVZX_Reg_Reg_Word(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }

  /**
   * Emit code to implement the i2s bytecode
   */
  protected final void emit_i2s() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVSX_Reg_Reg_Word(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }


  /*
   * comparision ops
   */


  /**
   * Emit code to implement the lcmp bytecode
   */
  protected final void emit_lcmp() {
    asm.emitPOP_Reg(T0);        // the low half of value2
    asm.emitPOP_Reg(S0);        // the high half of value2
    asm.emitPOP_Reg(T1);        // the low half of value1
    asm.emitSUB_Reg_Reg(T1, T0);        // subtract the low half of value2 from
                                // low half of value1, result into T1
    asm.emitPOP_Reg(T0);        // the high half of value 1
    //  pop does not alter the carry register
    asm.emitSBB_Reg_Reg(T0, S0);        // subtract the high half of value2 plus
                                // borrow from the high half of value 1,
                                // result in T0
    asm.emitMOV_Reg_Imm(S0, -1);        // load -1 into S0
    VM_ForwardReference fr1 = asm.forwardJcc(asm.LT); // result negative --> branch to end
    asm.emitMOV_Reg_Imm(S0, 0);        // load 0 into S0
    asm.emitOR_Reg_Reg(T0, T1);        // result 0 
    VM_ForwardReference fr2 = asm.forwardJcc(asm.EQ); // result 0 --> branch to end
    asm.emitMOV_Reg_Imm(S0, 1);        // load 1 into S0
    fr1.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);        // push result on stack
  }

  /**
   * Emit code to implement the fcmpl bytecode
   */
  protected final void emit_fcmpl() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp(FP0, SP, WORDSIZE);          // copy value1 into FPU
    asm.emitFLD_Reg_RegInd(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the fcmpg bytecode
   */
  protected final void emit_fcmpg() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp(FP0, SP, WORDSIZE);          // copy value1 into FPU
    asm.emitFLD_Reg_RegInd(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the dcmpl bytecode
   */
  protected final void emit_dcmpl() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, WORDSIZE*2);        // copy value1 into FPU
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the dcmpg bytecode
   */
  protected final void emit_dcmpg() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, WORDSIZE*2);        // copy value1 into FPU
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }


  /*
   * branching
   */


  /**
   * Emit code to implement the ifeg bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifeq(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the ifne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifne(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the iflt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_iflt(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.LT, bTarget);
  }

  /**
   * Emit code to implement the ifge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifge(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.GE, bTarget);
  }

  /**
   * Emit code to implement the ifgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifgt(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.GT, bTarget);
  }

  /**
   * Emit code to implement the ifle bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifle(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.LE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpeq(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the if_icmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpne(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the if_icmplt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmplt(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.LT, bTarget);
  }

  /**
   * Emit code to implement the if_icmpge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpge(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.GE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpgt(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.GT, bTarget);
  }

  /**
   * Emit code to implement the if_icmple bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmple(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.LE, bTarget);
  }

  /**
   * Emit code to implement the if_acmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpeq(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the if_acmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpne(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the ifnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnull(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the ifnonnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnonnull(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the goto and gotow bytecodes
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_goto(int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    asm.emitJMP_ImmOrLabel(mTarget, bTarget);
  }

  /**
   * Emit code to implement the jsr and jsrw bytecode
   * @param bTarget target bytecode of the jsr
   */
  protected final void emit_jsr(int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    asm.emitCALL_ImmOrLabel(mTarget, bTarget);
  }

  /**
   * Emit code to implement the ret bytecode
   * @param index local variable containing the return address
   */
  protected final void emit_ret(int index) {
    int offset = localOffset(index);
    asm.emitJMP_RegDisp(ESP, offset); 
  }

  /**
   * Emit code to implement the tableswitch bytecode
   * @param defaultval bcIndex of the default target
   * @param low low value of switch
   * @param high high value of switch
   */
  protected final void emit_tableswitch(int defaultval, int low, int high) {
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    int n = high-low+1;                        // n = number of normal cases (0..n-1)
    asm.emitPOP_Reg (T0);                          // T0 is index of desired case
    asm.emitSUB_Reg_Imm(T0, low);                     // relativize T0
    asm.emitCMP_Reg_Imm(T0, n);                       // 0 <= relative index < n
    if (options.EDGE_COUNTERS) {
      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T1, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T1, T1, getEdgeCounterOffset());
      int firstCounter = edgeCounterIdx;
      edgeCounterIdx += (n + 1);

      // Jump around code for default case
      VM_ForwardReference fr = asm.forwardJcc(asm.LLT);
      incEdgeCounter(T1, S0, firstCounter + n);
      asm.emitJMP_ImmOrLabel(mTarget, bTarget);
      fr.resolve(asm);

      // Increment counter for the appropriate case
      incEdgeCounterIdx(T1, S0, T0, firstCounter);
    } else {
      asm.emitJCC_Cond_ImmOrLabel (asm.LGE, mTarget, bTarget);   // if not, goto default case
    }
    asm.emitCALL_Imm(asm.getMachineCodeIndex() + 5 + (n<<LG_WORDSIZE) ); 
    // jump around table, pushing address of 0th delta
    for (int i=0; i<n; i++) {                  // create table of deltas
      int offset = fetch4BytesSigned();
      bTarget = biStart + offset;
      mTarget = bytecodeMap[bTarget];
      // delta i: difference between address of case i and of delta 0
      asm.emitOFFSET_Imm_ImmOrLabel(i, mTarget, bTarget );
    }
    asm.emitPOP_Reg (S0);                          // S0 = address of 0th delta 
    asm.emitADD_Reg_RegIdx (S0, S0, T0, asm.WORD, 0);     // S0 += [S0 + T0<<2]
    asm.emitPUSH_Reg(S0);                          // push computed case address
    asm.emitRET ();                            // goto case
  }
  
  /**
   * Emit code to implement the lookupswitch bytecode.
   * Uses linear search, one could use a binary search tree instead,
   * but this is the baseline compiler, so don't worry about it.
   * 
   * @param defaultval bcIndex of the default target
   * @param npairs number of pairs in the lookup switch
   */
  protected final void emit_lookupswitch(int defaultval, int npairs) {
    if (options.EDGE_COUNTERS) {
      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T1, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T1, T1, getEdgeCounterOffset());
    }

    asm.emitPOP_Reg(T0);
    for (int i=0; i<npairs; i++) {
      int match   = fetch4BytesSigned();
      asm.emitCMP_Reg_Imm(T0, match);
      int offset  = fetch4BytesSigned();
      int bTarget = biStart + offset;
      int mTarget = bytecodeMap[bTarget];
      if (options.EDGE_COUNTERS) {
	// Flip conditions so we can jump over the increment of the taken counter.
	VM_ForwardReference fr = asm.forwardJcc(asm.NE);

	// Increment counter & jump to target
	incEdgeCounter(T1, S0, edgeCounterIdx++);
	asm.emitJMP_ImmOrLabel(mTarget, bTarget);
	fr.resolve(asm);
      } else {
	asm.emitJCC_Cond_ImmOrLabel(asm.EQ, mTarget, bTarget);
      }
    }
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    if (options.EDGE_COUNTERS) {
      incEdgeCounter(T1, S0, edgeCounterIdx++);
    }
    asm.emitJMP_ImmOrLabel(mTarget, bTarget);
  }


  /*
   * returns (from function; NOT ret)
   */


  /**
   * Emit code to implement the ireturn bytecode
   */
  protected final void emit_ireturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T0);
    genEpilogue(4); 
  }

  /**
   * Emit code to implement the lreturn bytecode
   */
  protected final void emit_lreturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T1); // low half
    asm.emitPOP_Reg(T0); // high half
    genEpilogue(8);
  }

  /**
   * Emit code to implement the freturn bytecode
   */
  protected final void emit_freturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitFLD_Reg_RegInd(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE); // pop the stack
    genEpilogue(4);
  }

  /**
   * Emit code to implement the dreturn bytecode
   */
  protected final void emit_dreturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE<<1); // pop the stack
    genEpilogue(8);
  }

  /**
   * Emit code to implement the areturn bytecode
   */
  protected final void emit_areturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T0);
    genEpilogue(4); 
  }

  /**
   * Emit code to implement the return bytecode
   */
  protected final void emit_return() {
    if (method.isSynchronized()) genMonitorExit();
    genEpilogue(0); 
  }


  /*
   * field access
   */


  /**
   * Emit code to implement a dynamically linked getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getstatic(VM_Field fieldRef) {
    emitDynamicLinkingSequence(T0, fieldRef); 
    if (fieldRef.getSize() == 4) { 
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, 0);        // get static field
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, WORDSIZE); // get high part
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, 0);        // get low part
    }
  }

  /**
   * Emit code to implement a getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPUSH_RegDisp(JTOC, fieldOffset);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitPUSH_RegDisp(JTOC, fieldOffset+WORDSIZE); // get high part
      asm.emitPUSH_RegDisp(JTOC, fieldOffset);          // get low part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putstatic(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutstaticBarrier(asm, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, 0);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, 0);        // store low part
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, WORDSIZE); // store high part
    }
  }

  /**
   * Emit code to implement a putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutstaticBarrier(asm, fieldOffset);
    }
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPOP_RegDisp(JTOC, fieldOffset);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitPOP_RegDisp(JTOC, fieldOffset);          // store low part
      asm.emitPOP_RegDisp(JTOC, fieldOffset+WORDSIZE); // store high part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getfield(VM_Field fieldRef) {
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(S0, SP, 0);              // S0 is object reference
      asm.emitMOV_Reg_RegIdx(S0, S0, T0, asm.BYTE, 0); // S0 is field value
      asm.emitMOV_RegDisp_Reg(SP, 0, S0);              // replace reference with value on stack
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitMOV_Reg_RegDisp(S0, SP, 0);                     // S0 is object reference
      asm.emitMOV_Reg_RegIdx(T1, S0, T0, asm.BYTE, WORDSIZE); // T1 is high part of field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T1);                     // replace reference with value on stack
      asm.emitPUSH_RegIdx(S0, T0, asm.BYTE, 0);               // push the low part of field value
    }
  }

  /**
   * Emit code to implement a getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getfield(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);           // T0 is object reference
      asm.emitMOV_Reg_RegDisp(T0, T0, fieldOffset); // T0 is field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T0);           // replace reference with value on stack
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);                    // T0 is object reference
      asm.emitMOV_Reg_RegDisp(T1, T0, fieldOffset+WORDSIZE); // T1 is high part of field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T1);                    // replace reference with high part of value on stack
      asm.emitPUSH_RegDisp(T0, fieldOffset);                 // push low part of field value
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putfield(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutfieldBarrier(asm, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) {// field is one word
      asm.emitMOV_Reg_RegDisp(T1, SP, 0);               // T1 is the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 4);               // S0 is the object reference
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, 0, T1); // [S0+T0] <- T1
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);              // complete popping the value and reference
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitMOV_Reg_RegDisp(JTOC, SP, 0);                          // JTOC is low part of the value to be stored
      asm.emitMOV_Reg_RegDisp(T1, SP, 4);                            // T1 is high part of the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 8);                            // S0 is the object reference
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, 0, JTOC);            // [S0+T0] <- JTOC
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, WORDSIZE, T1);       // [S0+T0+4] <- T1
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                           // complete popping the values and reference
      // restore JTOC
      VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
    }
  }

  /**
   * Emit code to implement a putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putfield(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutfieldBarrier(asm, fieldRef.getOffset());
    }
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);           // T0 is the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 4);           // S0 is the object reference
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset, T0); // [S0+fieldOffset] <- T0
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);          // complete popping the value and reference
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      // TODO!! use 8-byte move if possible
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);                    // T0 is low part of the value to be stored
      asm.emitMOV_Reg_RegDisp(T1, SP, 4);                    // T1 is high part of the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 8);                    // S0 is the object reference
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset, T0);          // store low part
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset+WORDSIZE, T1); // store high part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                   // complete popping the values and reference
    }
  }


  /*
   * method invocation
   */

  /**
   * Emit code to implement a dynamically linked invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokevirtual(VM_Method methodRef) {
    emitDynamicLinkingSequence(T0, methodRef);
    int methodRefparameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int objectOffset = (methodRefparameterWords << 2) - 4;           // object offset into stack
    asm.emitMOV_Reg_RegDisp (T1, SP, objectOffset);                  // S0 has "this" parameter
    VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
    asm.emitMOV_Reg_RegIdx (S0, S0, T0, asm.BYTE, 0);                // S0 has address of virtual method
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_Reg(S0);                                      // call virtual method
    genResultRegisterUnload(methodRef);                    // push return value, if any
  }

  /**
   * Emit code to implement invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokevirtual(VM_Method methodRef) {
    int methodRefparameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int methodRefOffset = methodRef.getOffset();
    int objectOffset = (methodRefparameterWords << 2) - WORDSIZE; // object offset into stack
    asm.emitMOV_Reg_RegDisp (T1, SP, objectOffset);
    VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_RegDisp(S0, methodRefOffset);
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokespecial
   * @param methodRef the referenced method
   * @param targetRef the method to invoke
   */
  protected final void emit_resolved_invokespecial(VM_Method methodRef, VM_Method target) {
    if (target.isObjectInitializer()) {
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(JTOC, target.getOffset());
      genResultRegisterUnload(target);
    } else {
      if (VM.VerifyAssertions) VM.assert(!target.isStatic());
      // invoke via class's tib slot
      int methodRefOffset = target.getOffset();
      asm.emitMOV_Reg_RegDisp (S0, JTOC, target.getDeclaringClass().getTibOffset());
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, methodRefOffset);
      genResultRegisterUnload(methodRef);
    }
  }

  /**
   * Emit code to implement invokespecial
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokespecial(VM_Method methodRef) {
    emitDynamicLinkingSequence(S0, methodRef);
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_RegIdx(JTOC, S0, asm.BYTE, 0);  // call static method
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokestatic(VM_Method methodRef) {
    emitDynamicLinkingSequence(S0, methodRef);
    genParameterRegisterLoad(methodRef, false);          
    asm.emitCALL_RegIdx(JTOC, S0, asm.BYTE, 0); 
    genResultRegisterUnload(methodRef);
  }

  /**
   * Emit code to implement invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokestatic(VM_Method methodRef) {
    int methodOffset = methodRef.getOffset();
    genParameterRegisterLoad(methodRef, false);
    asm.emitCALL_RegDisp(JTOC, methodOffset);
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement the invokeinterface bytecode
   * @param methodRef the referenced method
   * @param count number of parameter words (see invokeinterface bytecode)
   */
  protected final void emit_invokeinterface(VM_Method methodRef, int count) {
    // (1) Emit dynamic type checking sequence if required to do so inline.
    if (VM.BuildForIMTInterfaceInvocation || 
	(VM.BuildForITableInterfaceInvocation && VM.DirectlyIndexedITables)) {
      VM_Method resolvedMethodRef = null;
      try {
	resolvedMethodRef = methodRef.resolveInterfaceMethod(false);
      } catch (VM_ResolutionException e) {
	// actually can't be thrown when we pass false for canLoad.
      }
      if (resolvedMethodRef == null) {
	// might be a ghost ref. Call uncommon case typechecking routine to deal with this
	asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                       // "this" object
	asm.emitPUSH_Imm(methodRef.getDictionaryId());                          // dict id of target
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T1);
	asm.emitPUSH_Reg(S0);
	genParameterRegisterLoad(2);                                            // pass 2 parameter word
	asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unresolvedInvokeinterfaceImplementsTestMethod.getOffset());// check that "this" class implements the interface
      } else {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, methodRef.getDeclaringClass().getTibOffset()); // tib of the interface method
	asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                 // "this" object
	asm.emitPUSH_RegDisp(T0, TIB_TYPE_INDEX << 2);                                // type of the interface method
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T1);
	asm.emitPUSH_Reg(S0);
	genParameterRegisterLoad(2);                                          // pass 2 parameter word
	asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.invokeinterfaceImplementsTestMethod.getOffset());// check that "this" class implements the interface
      }
    }

    // (2) Emit interface invocation sequence.
    if (VM.BuildForIMTInterfaceInvocation) {
      int signatureId = VM_ClassLoader.findOrCreateInterfaceMethodSignatureId(methodRef.getName(), methodRef.getDescriptor());
      int offset      = VM_InterfaceInvocation.getIMTOffset(signatureId);
          
      // squirrel away signature ID
      VM_ProcessorLocalState.emitMoveImmToField(asm, 
						VM_Entrypoints.hiddenSignatureIdField.getOffset(),
						signatureId);

      asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                  // "this" object
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
      if (VM.BuildForIndirectIMT) {
	// Load the IMT Base into S0
	asm.emitMOV_Reg_RegDisp(S0, S0, TIB_IMT_TIB_INDEX << 2);
      }
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, offset);                                             // the interface call
    } else if (VM.BuildForITableInterfaceInvocation && 
	       VM.DirectlyIndexedITables && 
	       methodRef.getDeclaringClass().isResolved()) {
      methodRef = methodRef.resolve();
      VM_Class I = methodRef.getDeclaringClass();
      asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                 // "this" object
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
      asm.emitMOV_Reg_RegDisp (S0, S0, TIB_ITABLES_TIB_INDEX << 2);                     // iTables
      asm.emitMOV_Reg_RegDisp (S0, S0, I.getInterfaceId() << 2);                        // iTable
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, VM_InterfaceInvocation.getITableIndex(I, methodRef) << 2); // the interface call
    } else {
      VM_Class I = methodRef.getDeclaringClass();
      int itableIndex = -1;
      if (false && VM.BuildForITableInterfaceInvocation) {
	// get the index of the method in the Itable
	if (I.isLoaded()) {
	  itableIndex = VM_InterfaceInvocation.getITableIndex(I, methodRef);
	}
      }
      if (itableIndex == -1) {
	// itable index is not known at compile-time.
	// call "invokeInterface" to resolve object + method id into 
	// method address
	int methodRefId = methodRef.getDictionaryId();
	asm.emitPUSH_RegDisp(SP, (count-1)<<LG_WORDSIZE);  // "this" parameter is obj
	asm.emitPUSH_Imm(methodRefId);                 // id of method to call
	genParameterRegisterLoad(2);               // pass 2 parameter words
	asm.emitCALL_RegDisp(JTOC,  VM_Entrypoints.invokeInterfaceMethod.getOffset()); // invokeinterface(obj, id) returns address to call
	asm.emitMOV_Reg_Reg (S0, T0);                      // S0 has address of method
	genParameterRegisterLoad(methodRef, true);
	asm.emitCALL_Reg(S0);                          // the interface method (its parameters are on stack)
      } else {
	// itable index is known at compile-time.
	// call "findITable" to resolve object + interface id into 
	// itable address
	asm.emitMOV_Reg_RegDisp (T0, SP, (count-1) << 2);             // "this" object
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T0);
	asm.emitPUSH_Reg(S0);
	asm.emitPUSH_Imm        (I.getInterfaceId());                // interface id
	genParameterRegisterLoad(2);                                  // pass 2 parameter words
	asm.emitCALL_RegDisp    (JTOC,  VM_Entrypoints.findItableMethod.getOffset()); // findItableOffset(tib, id) returns iTable
	asm.emitMOV_Reg_Reg     (S0, T0);                             // S0 has iTable
	genParameterRegisterLoad(methodRef, true);
	asm.emitCALL_RegDisp    (S0, itableIndex << 2);               // the interface call
      }
    }
    genResultRegisterUnload(methodRef);
  }
 

  /*
   * other object model functions
   */ 


  /**
   * Emit code to allocate a scalar object
   * @param typeRef the VM_Class to instantiate
   */
  protected final void emit_resolved_new(VM_Class typeRef) {
    int instanceSize = typeRef.getInstanceSize();
    int tibOffset = typeRef.getOffset();
    asm.emitPUSH_Imm(instanceSize);            
    asm.emitPUSH_RegDisp (JTOC, tibOffset);       // put tib on stack    
    asm.emitPUSH_Imm(typeRef.hasFinalizer()?1:0); // does the class have a finalizer?
    genParameterRegisterLoad(3);                  // pass 3 parameter words
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.quickNewScalarMethod.getOffset());
    asm.emitPUSH_Reg (T0);
  }

  /**
   * Emit code to dynamically link and allocate a scalar object
   * @param the dictionaryId of the VM_Class to dynamically link & instantiate
   */
  protected final void emit_unresolved_new(int dictionaryId) {
    asm.emitPUSH_Imm(dictionaryId);
    genParameterRegisterLoad(1);           // pass 1 parameter word
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.newScalarMethod.getOffset());
    asm.emitPUSH_Reg (T0);
  }

  /**
   * Emit code to allocate an array
   * @param array the VM_Array to instantiate
   */
  protected final void emit_newarray(VM_Array array) {
    int width      = array.getLogElementSize();
    int tibOffset  = array.getOffset();
    int headerSize = VM_ObjectModel.computeHeaderSize(array);
    // count is already on stack- nothing required
    asm.emitMOV_Reg_RegInd (T0, SP);               // get number of elements
    asm.emitSHL_Reg_Imm (T0, width);              // compute array size
    asm.emitADD_Reg_Imm(T0, headerSize);
    asm.emitPUSH_Reg(T0);      
    asm.emitPUSH_RegDisp(JTOC, tibOffset);        // put tib on stack    
    genParameterRegisterLoad(3);          // pass 3 parameter words
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.quickNewArrayMethod.getOffset());
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to allocate a multi-dimensional array
   * @param typeRef the VM_Array to instantiate
   * @param dimensions the number of dimensions
   * @param dictionaryId, the dictionaryId of typeRef
   */
  protected final void emit_multianewarray(VM_Array typeRef, int dimensions, int dictionaryId) {
    // setup parameters for newarrayarray routine
    asm.emitPUSH_Imm (dimensions);                     // dimension of arays
    asm.emitPUSH_Imm (dictionaryId);                   // type of array elements               
    asm.emitPUSH_Imm ((dimensions + 5)<<LG_WORDSIZE);  // offset to dimensions from FP on entry to newarray 
    // NOTE: 5 extra words- 3 for parameters, 1 for return address on stack, 1 for code technique in VM_Linker
    genParameterRegisterLoad(3);                   // pass 3 parameter words
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.newArrayArrayMethod.getOffset()); 
    for (int i = 0; i < dimensions ; i++) asm.emitPOP_Reg(S0); // clear stack of dimensions (todo use and add immediate to do this)
    asm.emitPUSH_Reg(T0);                              // push array ref on stack
  }

  /**
   * Emit code to implement the arraylength bytecode
   */
  protected final void emit_arraylength() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                   // T0 is array reference
    asm.emitMOV_Reg_RegDisp(T0, T0, VM_ObjectModel.getArrayLengthOffset()); // T0 is array length
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);                   // replace reference with length on stack
  }

  /**
   * Emit code to implement the athrow bytecode
   */
  protected final void emit_athrow() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.athrowMethod.getOffset());
  }

  /**
   * Emit code to implement the checkcast bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this checkcast
   */
  protected final void emit_checkcast(VM_Type typeRef, VM_Method target) {
    asm.emitPUSH_RegInd (SP);                        // duplicate the object ref on the stack
    asm.emitPUSH_Imm(typeRef.getTibOffset());        // JTOC index that identifies klass  
    genParameterRegisterLoad(2);                     // pass 2 parameter words
    asm.emitCALL_RegDisp (JTOC, target.getOffset()); // checkcast(obj, klass-identifier)
  }

  /**
   * Emit code to implement the instanceof bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this instanceof
   */
  protected final void emit_instanceof(VM_Type typeRef, VM_Method target) {
    asm.emitPUSH_Imm(typeRef.getTibOffset());  
    genParameterRegisterLoad(2);          // pass 2 parameter words
    asm.emitCALL_RegDisp(JTOC, target.getOffset());
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the monitorenter bytecode
   */
  protected final void emit_monitorenter() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.lockMethod.getOffset());
  }

  /**
   * Emit code to implement the monitorexit bytecode
   */
  protected final void emit_monitorexit() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unlockMethod.getOffset());  
  }


  //----------------//
  // implementation //
  //----------------//
  
  private final void genPrologue () {
    if (shouldPrint) asm.comment("prologue for " + method);
    if (klass.isBridgeFromNative()) {
      // replace the normal prologue with a special prolog
      VM_JNICompiler.generateGlueCodeForJNIMethod (asm, method, compiledMethod.getId());
      // set some constants for the code generation of the rest of the method
      // firstLocalOffset is shifted down because more registers are saved
      firstLocalOffset = STACKFRAME_BODY_OFFSET - (VM_JNICompiler.SAVED_GPRS_FOR_JNI<<LG_WORDSIZE) ;
    } else {
      /* paramaters are on the stack and/or in registers;  There is space
       * on the stack for all the paramaters;  Parameter slots in the
       * stack are such that the first paramater has the higher address,
       * i.e., it pushed below all the other paramaters;  The return
       * address is the topmost entry on the stack.  The frame pointer
       * still addresses the previous frame.
       * The first word of the header, currently addressed by the stack
       * pointer, contains the return address.
       */

      /* establish a new frame:
       * push the caller's frame pointer in the stack, and
       * reset the frame pointer to the current stack top,
       * ie, the frame pointer addresses directly the word
       * that contains the previous frame pointer.
       * The second word of the header contains the frame
       * point of the caller.
       * The third word of the header contains the compiled method id of the called method.
       */
      asm.emitPUSH_RegDisp   (PR, VM_Entrypoints.framePointerField.getOffset());	// store caller's frame pointer
      VM_ProcessorLocalState.emitMoveRegToField(asm, VM_Entrypoints.framePointerField.getOffset(), SP); // establish new frame
      /*
       * NOTE: until the end of the prologue SP holds the framepointer.
       */
      asm.emitMOV_RegDisp_Imm(SP, STACKFRAME_METHOD_ID_OFFSET, compiledMethod.getId());	// 3rd word of header
      
      /*
       * save registers
       */
      asm.emitMOV_RegDisp_Reg (SP, JTOC_SAVE_OFFSET, JTOC);          // save nonvolatile JTOC register
    
      // establish the JTOC register
      VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());

      int savedRegistersSize   = SAVED_GPRS<<LG_WORDSIZE;	// default
      /* handle "dynamic brige" methods:
       * save all registers except FP, SP, PR, S0 (scratch), and
       * JTOC saved above.
       */
      // TODO: (SJF): When I try to reclaim ESI, I may have to save it here?
      if (klass.isDynamicBridge()) {
	savedRegistersSize += 3 << LG_WORDSIZE;
	asm.emitMOV_RegDisp_Reg (SP, T0_SAVE_OFFSET,  T0); 
	asm.emitMOV_RegDisp_Reg (SP, T1_SAVE_OFFSET,  T1); 
	asm.emitMOV_RegDisp_Reg (SP, EBX_SAVE_OFFSET, EBX); 
	asm.emitFNSAVE_RegDisp  (SP, FPU_SAVE_OFFSET);
	savedRegistersSize += FPU_STATE_SIZE;
      } 

      // copy registers to callee's stackframe
      firstLocalOffset         = STACKFRAME_BODY_OFFSET - savedRegistersSize;
      int firstParameterOffset = (parameterWords << LG_WORDSIZE) + WORDSIZE;
      genParameterCopy(firstParameterOffset, firstLocalOffset);

      int emptyStackOffset = firstLocalOffset - (method.getLocalWords() << LG_WORDSIZE) + WORDSIZE;
      asm.emitADD_Reg_Imm (SP, emptyStackOffset);		// set aside room for non parameter locals
      /*
       * generate stacklimit check
       */
      if (isInterruptible) {
	// S0<-limit
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
						  VM_Entrypoints.activeThreadStackLimitField.getOffset());

	asm.emitSUB_Reg_Reg (S0, SP);                                   	// space left
	asm.emitADD_Reg_Imm (S0, method.getOperandWords() << LG_WORDSIZE); 	// space left after this expression stack
	VM_ForwardReference fr = asm.forwardJcc(asm.LT);	// Jmp around trap if OK
	asm.emitINT_Imm ( VM_Runtime.TRAP_STACK_OVERFLOW + RVM_TRAP_BASE );	// trap
	fr.resolve(asm);
      } else {
	// TODO!! make sure stackframe of uninterruptible method doesn't overflow guard page
      }

      if (method.isSynchronized()) genMonitorEnter();

      genThreadSwitchTest(VM_Thread.PROLOGUE);

      asm.emitNOP();                                      // mark end of prologue for JDP
    }
  }
  
  private final void genEpilogue (int bytesPopped) {
    if (klass.isBridgeFromNative()) {
      // pop locals and parameters, get to saved GPR's
      asm.emitADD_Reg_Imm(SP, (this.method.getLocalWords() << LG_WORDSIZE));
      VM_JNICompiler.generateEpilogForJNIMethod(asm, this.method);
    } else if (klass.isDynamicBridge()) {
      // we never return from a DynamicBridge frame
      asm.emitINT_Imm(0xFF);
    } else {
      // normal method
      asm.emitADD_Reg_Imm     (SP, fp2spOffset(0) - bytesPopped);      // SP becomes frame pointer
      asm.emitMOV_Reg_RegDisp (JTOC, SP, JTOC_SAVE_OFFSET);            // restore nonvolatile JTOC register
      asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset()); // discard frame
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE);	 // return to caller- pop parameters from stack
    }
  }
   
  private final void genMonitorEnter () {
    if (method.isStatic()) {
      if (VM.writingBootImage) {
	VM.deferClassObjectCreation(klass);
      } else {
	klass.getClassForType();
      }
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (T0, JTOC, tibOffset);	           // T0 = tib for klass
      asm.emitMOV_Reg_RegInd (T0, T0);		                   // T0 = VM_Class for klass
      asm.emitPUSH_RegDisp(T0, VM_Entrypoints.classForTypeField.getOffset()); // push java.lang.Class object for klass
    } else {
      asm.emitPUSH_RegDisp(ESP, localOffset(0));	                   // push "this" object
    }
    genParameterRegisterLoad(1);			           // pass 1 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.lockMethod.getOffset());  
    lockOffset = asm.getMachineCodeIndex();                       // after this instruction, the method has the monitor
  }
  
  private final void genMonitorExit () {
    if (method.isStatic()) {
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (T0, JTOC, tibOffset);                   // T0 = tib for klass
      asm.emitMOV_Reg_RegInd (T0, T0);                             // T0 = VM_Class for klass
      asm.emitPUSH_RegDisp(T0, VM_Entrypoints.classForTypeField.getOffset()); // push java.lang.Class object for klass
    } else {
      asm.emitPUSH_RegDisp(ESP, localOffset(0));                    // push "this" object
    }
    genParameterRegisterLoad(1); // pass 1 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unlockMethod.getOffset());  
  }
  
  private final void genBoundsCheck (VM_Assembler asm, byte indexReg, byte arrayRefReg ) { 
    if (options.ANNOTATIONS &&
	method.queryAnnotationForBytecode(biStart, VM_Method.annotationBoundsCheck)) {
      return;
    }
    asm.emitCMP_RegDisp_Reg(arrayRefReg,
                            VM_ObjectModel.getArrayLengthOffset(), indexReg);  // compare index to array length
    VM_ForwardReference fr = asm.forwardJcc(asm.LGT);                     // Jmp around trap if index is OK
    
    // "pass" index param to C trap handler
    VM_ProcessorLocalState.emitMoveRegToField(asm, 
                                              VM_Entrypoints.arrayIndexTrapParamField.getOffset(),
                                              indexReg);

    asm.emitINT_Imm(VM_Runtime.TRAP_ARRAY_BOUNDS + RVM_TRAP_BASE );	  // trap
    fr.resolve(asm);
  }

  /**
   * Emit a conditional branch on the given condition and bytecode target.
   * The caller has just emitted the instruction sequence to set the condition codes.
   */
  private final void genCondBranch(byte cond, int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    if (options.EDGE_COUNTERS) {
      // Allocate 2 counters, taken and not taken
      int entry = edgeCounterIdx;
      edgeCounterIdx += 2;

      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T0, T0, getEdgeCounterOffset());

      // Flip conditions so we can jump over the increment of the taken counter.
      VM_ForwardReference notTaken = asm.forwardJcc(asm.flipCode(cond));

      // Increment taken counter & jump to target
      incEdgeCounter(T0, T1, entry + VM_EdgeCounts.TAKEN);
      asm.emitJMP_ImmOrLabel(mTarget, bTarget);

      // Increment not taken counter
      notTaken.resolve(asm);
      incEdgeCounter(T0, T1, entry + VM_EdgeCounts.NOT_TAKEN);
    } else {
      asm.emitJCC_Cond_ImmOrLabel(cond, mTarget, bTarget);
    }
  }


  private final void incEdgeCounter(byte counters, byte scratch, int counterIdx) {
    asm.emitMOV_Reg_RegDisp(scratch, counters, counterIdx<<2);
    asm.emitINC_Reg(scratch);
    asm.emitAND_Reg_Imm(scratch, 0x7fffffff); // saturate at max int;
    asm.emitMOV_RegDisp_Reg(counters, counterIdx<<2, scratch);
  }

  private final void incEdgeCounterIdx(byte counters, byte scratch, byte idx, int counterIdx) {
    asm.emitMOV_Reg_RegIdx(scratch, counters, idx, asm.WORD, counterIdx<<2);
    asm.emitINC_Reg(scratch);
    asm.emitAND_Reg_Imm(scratch, 0x7fffffff); // saturate at max int;
    asm.emitMOV_RegIdx_Reg(counters, idx, asm.WORD, counterIdx<<2, scratch);
  }


  /** 
   * Copy a single floating-point double parameter from operand stack into fp register stack.
   * Assumption: method to be called has exactly one parameter.
   * Assumption: parameter type is double.
   * Assumption: parameter is on top of stack.
   * Also, this method is only called before generation of a call
   * to doubleToInt() or doubleToLong()
   */
  private final void genParameterRegisterLoad () {
    if (0 < NUM_PARAMETER_FPRS) {
      asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    }
  }
   
  /** 
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of a "helper" method call.
   * Assumption: no floating-point parameters.
   * @param params number of parameter words (including "this" if any).
   */
  private final void genParameterRegisterLoad (int params) {
    if (VM.VerifyAssertions) VM.assert(0 < params);
    if (0 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T0, SP, (params-1) << LG_WORDSIZE);
    }
    if (1 < params && 1 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T1, SP, (params-2) << LG_WORDSIZE);
    }
  }
   
  /** 
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of an explicit method call.
   * @param method is the method to be called.
   * @param hasThisParameter is the method virtual?
   */
  private final void genParameterRegisterLoad (VM_Method method, boolean hasThisParam) {
    int max = NUM_PARAMETER_GPRS + NUM_PARAMETER_FPRS;
    if (max == 0) return; // quit looking when all registers are full
    int gpr = 0;  // number of general purpose registers filled
    int fpr = 0;  // number of floating point  registers filled
    byte  T = T0; // next GPR to get a parameter
    int params = method.getParameterWords() + (hasThisParam ? 1 : 0);
    int offset = (params-1) << LG_WORDSIZE; // stack offset of first parameter word
    if (hasThisParam) {
      if (gpr < NUM_PARAMETER_GPRS) {
	asm.emitMOV_Reg_RegDisp(T, SP, offset);
	T = T1; // at most 2 parameters can be passed in general purpose registers
	gpr++;
	max--;
      }
      offset -= WORDSIZE;
    }
    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      if (max == 0) return; // quit looking when all registers are full
      VM_Type t = types[i];
      if (t.isLongType()) {
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_Reg_RegDisp(T, SP, offset); // lo register := hi mem (== hi order word)
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  max--;
	  if (gpr < NUM_PARAMETER_GPRS) {
	    asm.emitMOV_Reg_RegDisp(T, SP, offset - WORDSIZE);  // hi register := lo mem (== lo order word)
	    gpr++;
	    max--;
	  }
	}
	offset -= 2*WORDSIZE;
      } else if (t.isFloatType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  asm.emitFLD_Reg_RegDisp(FP0, SP, offset);
	  fpr++;
	  max--;
	}
	offset -= WORDSIZE;
      } else if (t.isDoubleType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, offset - WORDSIZE);
	  fpr++;
	  max--;
	}
	offset -= 2*WORDSIZE;
      } else { // t is object, int, short, char, byte, or boolean
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_Reg_RegDisp(T, SP, offset);
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  max--;
	}
	offset -= WORDSIZE;
      }
    }
    if (VM.VerifyAssertions) VM.assert(offset == - WORDSIZE);
  }
   
  /** 
   * Store parameters into local space of the callee's stackframe.
   * Taken: srcOffset - offset from frame pointer of first parameter in caller's stackframe.
   *        dstOffset - offset from frame pointer of first local in callee's stackframe
   * Assumption: although some parameters may be passed in registers,
   * space for all parameters is layed out in order on the caller's stackframe.
   */
  private final void genParameterCopy (int srcOffset, int dstOffset) {
    int gpr = 0;  // number of general purpose registers unloaded
    int fpr = 0;  // number of floating point registers unloaded
    byte  T = T0; // next GPR to get a parameter
    if (!method.isStatic()) { // handle "this" parameter
      if (gpr < NUM_PARAMETER_GPRS) {
	asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);
	T = T1; // at most 2 parameters can be passed in general purpose registers
	gpr++;
      } else { // no parameters passed in registers
	asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
      }
      srcOffset -= WORDSIZE;
      dstOffset -= WORDSIZE;
    }
    VM_Type [] types     = method.getParameterTypes();
    int     [] fprOffset = new     int [NUM_PARAMETER_FPRS]; // to handle floating point parameters in registers
    boolean [] is32bit   = new boolean [NUM_PARAMETER_FPRS]; // to handle floating point parameters in registers
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);    // hi mem := lo register (== hi order word)
	  T = T1;                                       // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  if (gpr < NUM_PARAMETER_GPRS) {
	    asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);  // lo mem := hi register (== lo order word)
	    gpr++;
	  } else {
	    asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset); // lo mem from caller's stackframe
	    asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  }
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // hi mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // lo mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else if (t.isFloatType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  fprOffset[fpr] = dstOffset;
	  is32bit[fpr]   = true;
	  fpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else if (t.isDoubleType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  fprOffset[fpr] = dstOffset;
	  is32bit[fpr]   = false;
	  fpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // hi mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // lo mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else { // t is object, int, short, char, byte, or boolean
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      }
    }
    for (int i=fpr-1; 0<=i; i--) { // unload the floating point register stack (backwards)
      if (is32bit[i]) {
	asm.emitFSTP_RegDisp_Reg(SP, fprOffset[i], FP0);
      } else {
	asm.emitFSTP_RegDisp_Reg_Quad(SP, fprOffset[i], FP0);
      }
    }
  }
   
  /** 
   * Push return value of method from register to operand stack.
   */
  private final void genResultRegisterUnload (VM_Method method) {
    VM_Type t = method.getReturnType();
    if (t.isVoidType()) return;
    if (t.isLongType()) {
      asm.emitPUSH_Reg(T0); // high half
      asm.emitPUSH_Reg(T1); // low half
    } else if (t.isFloatType()) {
      asm.emitSUB_Reg_Imm  (SP, 4);
      asm.emitFSTP_RegInd_Reg(SP, FP0);
    } else if (t.isDoubleType()) {
      asm.emitSUB_Reg_Imm  (SP, 8);
      asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
    } else { // t is object, int, short, char, byte, or boolean
      asm.emitPUSH_Reg(T0);
    }
  }
  
  /**
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  private final void genThreadSwitchTest (int whereFrom) {
    if (!isInterruptible) {
      return;
    } else if (VM.BuildForDeterministicThreadSwitching) {
      // decrement the deterministic thread switch count field in the
      // processor object
      VM_ProcessorLocalState.emitDecrementField(asm, 
                                                VM_Entrypoints.deterministicThreadSwitchCountField.getOffset());
      VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);                  // if not, skip
      
      // reset the count.
      VM_ProcessorLocalState.emitMoveImmToField(asm,VM_Entrypoints.deterministicThreadSwitchCountField.getOffset(),
						VM.deterministicThreadSwitchInterval);

      if (whereFrom == VM_Thread.PROLOGUE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromPrologueMethod.getOffset()); 
      } else if (whereFrom == VM_Thread.BACKEDGE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromBackedgeMethod.getOffset()); 
      } else { // EPILOGUE
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromEpilogueMethod.getOffset()); 
      }
      fr1.resolve(asm);
    } else {
      // thread switch requested ??
      VM_ProcessorLocalState.emitCompareFieldWithImm(asm, 
                                                     VM_Entrypoints.threadSwitchRequestedField.getOffset(),
                                                     0);
      VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);                    // if not, skip
      if (whereFrom == VM_Thread.PROLOGUE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromPrologueMethod.getOffset()); 
      } else if (whereFrom == VM_Thread.BACKEDGE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromBackedgeMethod.getOffset()); 
      } else { // EPILOGUE
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromEpilogueMethod.getOffset()); 
      }
      fr1.resolve(asm);
    }
  }

  private final void genMagic (VM_Method m) {
    VM_Atom methodName = m.getName();

    if (methodName == VM_MagicNames.attempt) {
      // attempt gets called with four arguments
      //   base
      //   offset
      //   oldVal
      //   newVal
      // returns ([base+offset] == oldVal)
      // if ([base+offset] == oldVal) [base+offset] := newVal
      // (operation on memory is atomic)
      asm.emitPOP_Reg (T1);            // newVal
      asm.emitPOP_Reg (EAX);           // oldVal (EAX is implicit arg to LCMPXCNG
      asm.emitPOP_Reg (S0);            // S0 = offset
      asm.emitADD_Reg_RegInd(S0, SP);  // S0 += base
      if (VM.BuildForSingleVirtualProcessor) {
	asm.emitMOV_RegInd_Reg (S0, T1);       // simply a store on uniprocessor (need not be atomic or cmp/xchg)
	asm.emitMOV_RegInd_Imm (SP, 1);        // 'push' true (overwriting base)
      } else {
	asm.emitLockNextInstruction();
	asm.emitCMPXCHG_RegInd_Reg (S0, T1);   // atomic compare-and-exchange
	asm.emitMOV_RegInd_Imm (SP, 0);        // 'push' false (overwriting base)
	VM_ForwardReference fr = asm.forwardJcc(asm.NE); // skip if compare fails
	asm.emitMOV_RegInd_Imm (SP, 1);        // 'push' true (overwriting base)
	fr.resolve(asm);
      }
      return;
    }
    
    if (methodName == VM_MagicNames.invokeMain) {
      // invokeMain gets "called" with two arguments:
      //   String[] mainArgs       // the arguments to the main method
      //   INSTRUCTION[] mainCode  // the code for the main method
      asm.emitPOP_Reg (S0);            // 
      genParameterRegisterLoad(1); // pass 1 parameter word	
      asm.emitCALL_Reg(S0);            // branches to mainCode with mainArgs on the stack
      return;
    }
    
    if (methodName == VM_MagicNames.saveThreadState) {
      int offset = VM_Entrypoints.saveThreadStateInstructionsField.getOffset();
      genParameterRegisterLoad(1); // pass 1 parameter word
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }

    if (methodName == VM_MagicNames.threadSwitch) {
      int offset = VM_Entrypoints.threadSwitchInstructionsField.getOffset();
      genParameterRegisterLoad(2); // pass 2 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }         
         
    if (methodName == VM_MagicNames.restoreHardwareExceptionState) {
      int offset = VM_Entrypoints.restoreHardwareExceptionStateInstructionsField.getOffset();
      genParameterRegisterLoad(1); // pass 1 parameter word
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }

    if (methodName == VM_MagicNames.invokeClassInitializer) {
      asm.emitPOP_Reg (S0);
      asm.emitCALL_Reg(S0); // call address just popped
      return;
    }
    
    /*
     * sysCall0, sysCall1, sysCall2, sysCall3 and sysCall4 return
     * an integer (32 bits).
     *
     *	hi mem
     *	  branch address	<- SP
     *
     * before call to C
     *  hi mem
     *	  branch address
     *	  saved ebx
     *	  saved pr
     *	  saved jtoc		<- SP
     */
    if (methodName == VM_MagicNames.sysCall0) {
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
       asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall1) {
      asm.emitPOP_Reg(S0);	// first and only argument
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(S0);	// push arg on stack
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(S0);	// pop the argument 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall2) {
      // C require its arguments reversed
      asm.emitPOP_Reg(T1);	// second arg
      asm.emitPOP_Reg(S0);	// first arg
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(T1);	// reorder arguments for C 
      asm.emitPUSH_Reg(S0);	// reorder arguments for C
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);	// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall3) {
      // C require its arguments reversed
      asm.emitMOV_Reg_RegInd(T0, SP);			// load 3rd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 3rd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, ESI);
      asm.emitMOV_RegInd_Reg(SP, JTOC);			// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 3*WORDSIZE); // fourth arg on stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall4) {
      // C require its arguments reversed
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 3rd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 3th arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 3*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 3*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, ESI);	
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, JTOC);	// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 4*WORDSIZE); // fifth arg on stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*4);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }
    
    /*
     * sysCall_L_0  returns a long and takes no arguments
     */
    if (methodName == VM_MagicNames.sysCall_L_0) {
      asm.emitMOV_Reg_Reg(T0, SP);
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitCALL_RegInd(T0);	// first arg on stack is address to call
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T1);	// store return value: hi half
      asm.emitPUSH_Reg(T0);	// low half
      return;
    }
    
    /*
     * sysCall_L_I  returns a long and takes an integer argument
     */
    if (methodName == VM_MagicNames.sysCall_L_I) {
      asm.emitPOP_Reg(S0);	// the one integer argument
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(S0);	// push arg on stack
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(S0);	// pop the argument 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T1);	// store return value: hi half
      asm.emitPUSH_Reg(T0);	// low half
      return;
    }
    
    if (methodName == VM_MagicNames.sysCallAD) {  // address, double
      // C require its arguments reversed
      asm.emitMOV_Reg_RegInd(T0, SP);			// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, ESI);	
      asm.emitMOV_RegInd_Reg(SP, JTOC);			// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 3*WORDSIZE); // 4th word on orig. stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }

    /*
     * A special version of sysCall2, for invoking sigWait and allowing
     * collection of the frame making the call.  The signature of the
     * magic is shared between powerPC and intel to simplify the caller.
     * The signature is
     * address to "call"
     * toc or undefined for intel
     * address of a lock/barrier which will be passed to sigWait
     * value to store into the lock/barrier, also passed to sigWait.  
     * the VM_Register object of the executing thread.
     *
     * The magic stores the current ip/fp into the VM_Register, to
     * allow collection of this thread from the current frame and below.
     * It then reverses the order of the two parameters on the stack to
     * conform to C calling convention, and finally invokes sigwait.
     *
     * stack:
     *	low memory
     *		ip -- address of sysPthreadSigWait in sys.C
     *		toc --
     *          p1 -- address of lockword
     *		p2 -- value to store in lockword
     *          address of VM_Register object for this thread
     *	high mem
     * This to be invoked from baseline code only.
     */
    if (methodName == VM_MagicNames.sysCallSigWait) {

      int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
      int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
      int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();

      asm.emitMOV_Reg_RegInd(T0, SP);	                // T0 <- context register obj @
      asm.emitLEA_Reg_RegDisp(S0, SP, fp2spOffset(0));  // compute FP
      asm.emitMOV_RegDisp_Reg(T0, fpOffset, S0);	// store fp in context
      asm.emitCALL_Imm (asm.getMachineCodeIndex() + 5);
      asm.emitPOP_Reg(T1);				// T1 <- IP
      asm.emitMOV_RegDisp_Reg(T0, ipOffset, T1);	// store ip in context
      asm.emitMOV_Reg_RegDisp(T0, T0, gprsOffset);	// T0 <- grps array @
      asm.emitMOV_Reg_RegDisp(T1, SP, WORDSIZE);	// second arg
      asm.emitMOV_Reg_RegDisp(S0, SP, 2*WORDSIZE);	// first arg
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- [sysPthreadSigWait @]
      asm.emitADD_Reg_Imm(T0, 4*WORDSIZE);
      asm.emitPUSH_Reg(JTOC);	// save JTOC aka EDI
      asm.emitPUSH_Reg(T1);	// reorder arguments for C 
      asm.emitPUSH_Reg(S0);	// reorder arguments for C
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);	// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore JTOC
      asm.emitADD_Reg_Imm(SP, WORDSIZE*4);	// pop all but last
      asm.emitMOV_RegInd_Reg(SP, T0);	// overwrite last with return value

      return;
    }
    
    if (methodName == VM_MagicNames.getFramePointer) {
      asm.emitLEA_Reg_RegDisp(S0, SP, fp2spOffset(0));
      asm.emitPUSH_Reg       (S0);
      return;
    }
    
    if (methodName == VM_MagicNames.getCallerFramePointer) {
      asm.emitPOP_Reg(T0);                                       // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_FRAME_POINTER_OFFSET); // Caller FP
      return;
    }

    if (methodName == VM_MagicNames.setCallerFramePointer) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_FRAME_POINTER_OFFSET, T0); // [S0+SFPO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getCompiledMethodID) {
      asm.emitPOP_Reg(T0);                                   // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_METHOD_ID_OFFSET); // Callee CMID
      return;
    }

    if (methodName == VM_MagicNames.setCompiledMethodID) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_METHOD_ID_OFFSET, T0); // [S0+SMIO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getReturnAddress) {
      asm.emitPOP_Reg(T0);                                        // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_RETURN_ADDRESS_OFFSET); // Callee return address
      return;
    }
    
    if (methodName == VM_MagicNames.setReturnAddress) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_RETURN_ADDRESS_OFFSET, T0); // [S0+SRAO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getTocPointer ||
	methodName == VM_MagicNames.getJTOC ) {
      asm.emitPUSH_Reg(JTOC);
      return;
    }
    
    if (methodName == VM_MagicNames.getThreadId) {
      VM_ProcessorLocalState.emitPushField(asm,VM_Entrypoints.threadIdField.getOffset());
      return;
    }
       
    // set the Thread id register (not really a register)
    if (methodName == VM_MagicNames.setThreadId) {
      VM_ProcessorLocalState.emitPopField(asm,VM_Entrypoints.threadIdField.getOffset()); 
      return;
    }
    
    // get the processor register (PR)
    if (methodName == VM_MagicNames.getProcessorRegister) {
      asm.emitPUSH_Reg(PR);
      return;
    }  

    // set the processor register (PR)
    if (methodName == VM_MagicNames.setProcessorRegister) {
      asm.emitPOP_Reg(PR);
      return;
    }
    
    // Get the value in ESI 
    if (methodName == VM_MagicNames.getESIAsProcessor) {
      asm.emitPUSH_Reg(ESI);
      return;
    }  

    // Set the value in ESI
    if (methodName == VM_MagicNames.setESIAsProcessor) {
      asm.emitPOP_Reg(ESI);
      return;
    }
  
    if (methodName == VM_MagicNames.getIntAtOffset ||
	methodName == VM_MagicNames.getObjectAtOffset ||
	methodName == VM_MagicNames.getObjectArrayAtOffset ||
	methodName == VM_MagicNames.prepare) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 0); // pushes [T0+S0]
      return;
    }
    
    if (methodName == VM_MagicNames.getByteAtOffset) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitMOV_Reg_RegIdx_Byte(T0, T0, S0, asm.BYTE, 0); // load and zero extend byte [T0+S0]
      asm.emitPUSH_Reg (T0);
      return;
    }
    
    if (methodName == VM_MagicNames.setIntAtOffset ||
	methodName == VM_MagicNames.setObjectAtOffset ) {
      asm.emitPOP_Reg(T0);                   // value
      asm.emitPOP_Reg(S0);                   // offset
      asm.emitPOP_Reg(T1);                   // obj ref
      asm.emitMOV_RegIdx_Reg(T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- T0
      return;
    }
    
    if (methodName == VM_MagicNames.setByteAtOffset) {
      asm.emitPOP_Reg(T0);                   // value
      asm.emitPOP_Reg(S0);                   // offset
      asm.emitPOP_Reg(T1);                   // obj ref
      asm.emitMOV_RegIdx_Reg_Byte(T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- (byte) T0
      return;
    }
    
    if (methodName == VM_MagicNames.getLongAtOffset) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 4); // pushes [T0+S0+4]
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 0); // pushes [T0+S0]
      return;
    }
    
    if (methodName == VM_MagicNames.setLongAtOffset) {
      asm.emitMOV_Reg_RegInd (T0, SP);		// value high
      asm.emitMOV_Reg_RegDisp(S0, SP, +8 );	// offset
      asm.emitMOV_Reg_RegDisp(T1, SP, +12);	// obj ref
      asm.emitMOV_RegIdx_Reg (T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- T0
      asm.emitMOV_Reg_RegDisp(T0, SP, +4 );	// value low
      asm.emitMOV_RegIdx_Reg (T1, S0, asm.BYTE, 4, T0); // [T1+S0+4] <- T0
      asm.emitADD_Reg_Imm    (SP, WORDSIZE * 4); // pop stack locations
      return;
    }
    
    if (methodName == VM_MagicNames.getMemoryWord) {
      asm.emitPOP_Reg(T0);	// address
      asm.emitPUSH_RegInd(T0); // pushes [T0+0]
      return;
    }

    if (methodName == VM_MagicNames.getMemoryAddress) {
      asm.emitPOP_Reg(T0);	// address
      asm.emitPUSH_RegInd(T0); // pushes [T0+0]
      return;
    }
    
    if (methodName == VM_MagicNames.setMemoryWord) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // address
      asm.emitMOV_RegInd_Reg(S0,T0); // [S0+0] <- T0
      return;
    }

    if (methodName == VM_MagicNames.setMemoryAddress) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // address
      asm.emitMOV_RegInd_Reg(S0,T0); // [S0+0] <- T0
      return;
    }
    
    if (methodName == VM_MagicNames.objectAsAddress         ||
	methodName == VM_MagicNames.addressAsByteArray      ||
	methodName == VM_MagicNames.addressAsIntArray       ||
	methodName == VM_MagicNames.addressAsObject         ||
	methodName == VM_MagicNames.addressAsObjectArray    ||
	methodName == VM_MagicNames.addressAsType           ||
	methodName == VM_MagicNames.objectAsType            ||
	methodName == VM_MagicNames.objectAsShortArray      ||
	methodName == VM_MagicNames.objectAsByteArray       ||
	methodName == VM_MagicNames.objectAsIntArray       ||
	methodName == VM_MagicNames.pragmaNoOptCompile      ||
	methodName == VM_MagicNames.addressAsThread         ||
	methodName == VM_MagicNames.objectAsThread          ||
	methodName == VM_MagicNames.objectAsProcessor       ||
//-#if RVM_WITH_JIKESRVM_MEMORY_MANAGERS
	methodName == VM_MagicNames.addressAsBlockControl   ||
	methodName == VM_MagicNames.addressAsSizeControl    ||
	methodName == VM_MagicNames.addressAsSizeControlArray   ||
//-#if RVM_WITH_CONCURRENT_GC
	methodName == VM_MagicNames.threadAsRCCollectorThread ||
//-#endif
//-#endif
	methodName == VM_MagicNames.threadAsCollectorThread ||
	methodName == VM_MagicNames.addressAsRegisters      ||
	methodName == VM_MagicNames.addressAsStack          ||
	methodName == VM_MagicNames.floatAsIntBits          ||
	methodName == VM_MagicNames.intBitsAsFloat          ||
	methodName == VM_MagicNames.doubleAsLongBits        ||
	methodName == VM_MagicNames.longBitsAsDouble)
      {
	// no-op (a type change, not a representation change)
	return;
      }
    
    // code for      VM_Type VM_Magic.getObjectType(Object object)
    if (methodName == VM_MagicNames.getObjectType) {
      asm.emitPOP_Reg (T0);			          // object ref
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
      asm.emitPUSH_RegDisp(S0, TIB_TYPE_INDEX<<LG_WORDSIZE); // push VM_Type slot of TIB
      return;
    }
    
    if (methodName == VM_MagicNames.getArrayLength) {
      asm.emitPOP_Reg(T0);			// object ref
      asm.emitPUSH_RegDisp(T0, VM_ObjectModel.getArrayLengthOffset()); 
      return;
    }
    
    if (methodName == VM_MagicNames.sync) {  // nothing required on IA32
      return;
    }
    
    if (methodName == VM_MagicNames.isync) { // nothing required on IA32
      return;
    }
    
    // baseline compiled invocation only: all paramaters on the stack
    // hi mem
    //      Code
    //      GPRs
    //      FPRs
    //      Spills
    // low-mem
    if (methodName == VM_MagicNames.invokeMethodReturningVoid) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningInt) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningLong) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0); // high half
      asm.emitPUSH_Reg(T1); // low half
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningFloat) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitSUB_Reg_Imm  (SP, 4);
      asm.emitFSTP_RegInd_Reg(SP, FP0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningDouble) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitSUB_Reg_Imm  (SP, 8);
      asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningObject) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0);
      return;
    }                 

    // baseline invocation
    // one paramater, on the stack  -- actual code
    if (methodName == VM_MagicNames.dynamicBridgeTo) {
      if (VM.VerifyAssertions) VM.assert(klass.isDynamicBridge());

      // save the branch address for later
      asm.emitPOP_Reg (S0);		// S0<-code address

      asm.emitADD_Reg_Imm(SP, fp2spOffset(0) - 4); // just popped 4 bytes above.

      // restore FPU state
      asm.emitFRSTOR_RegDisp(SP, FPU_SAVE_OFFSET);

      // restore GPRs
      asm.emitMOV_Reg_RegDisp (T0,  SP, T0_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (T1,  SP, T1_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (JTOC,  SP, JTOC_SAVE_OFFSET); 

      // pop frame
      asm.emitPOP_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset()); // FP<-previous FP 

      // branch
      asm.emitJMP_Reg (S0);
      return;
    }
                                                  
    if (methodName == VM_MagicNames.returnToNewStack) {
      // SP gets frame pointer for new stack
      asm.emitPOP_Reg (SP);	

      // restore nonvolatile JTOC register
      asm.emitMOV_Reg_RegDisp (JTOC, SP, JTOC_SAVE_OFFSET);

      // discard current stack frame
      asm.emitPOP_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset());

      // return to caller- pop parameters from stack
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE);	 
      return;
    }

    if (methodName == VM_MagicNames.roundToZero) {
      // Store the FPU Control Word to a JTOC slot
      asm.emitFNSTCW_RegDisp(JTOC, VM_Entrypoints.FPUControlWordField.getOffset());
      // Set the bits in the status word that control round to zero.
      // Note that we use a 32-bit OR, even though we only care about the
      // low-order 16 bits
      asm.emitOR_RegDisp_Imm(JTOC,VM_Entrypoints.FPUControlWordField.getOffset(), 0x00000c00);
      // Now store the result back into the FPU Control Word
      asm.emitFLDCW_RegDisp(JTOC,VM_Entrypoints.FPUControlWordField.getOffset());
      return;
    }
    if (methodName == VM_MagicNames.clearFloatingPointState) {
      // Clear the hardware floating-point state
      asm.emitFNINIT();
      return;
    }
    
    if (methodName == VM_MagicNames.clearThreadSwitchBit) { // nothing to do
      // ignore obsolete magic
      return;
    }

    if (methodName == VM_MagicNames.getTime) {
      VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: getTime in " + method + "\n");
      asm.emitMOV_RegInd_Imm(SP, 0);  // TEMP!! for now, return 0
      return;
    }

    if (methodName == VM_MagicNames.getTimeBase) {
      VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: getTimeBase in " + method + "\n");
      asm.emitMOV_RegInd_Imm(SP, 0);  // TEMP!! for now, return 0
      return;
    }


    if (methodName == VM_MagicNames.addressFromInt ||
	methodName == VM_MagicNames.addressToInt) {
	// no-op
	return;
    }

    if (methodName == VM_MagicNames.addressAdd) {
	asm.emitPOP_Reg(T0);
	asm.emitADD_RegInd_Reg(SP, T0);
	return;
    }

    if (methodName == VM_MagicNames.addressSub ||
	methodName == VM_MagicNames.addressDiff) {
	asm.emitPOP_Reg(T0);
	asm.emitSUB_RegInd_Reg(SP, T0);
	return;
    }

    if (methodName == VM_MagicNames.addressZero) {
	asm.emitPUSH_Imm(0);
	return;
    }

    if (methodName == VM_MagicNames.addressMax) {
	asm.emitPUSH_Imm(-1);
	return;
    }

    if (methodName == VM_MagicNames.addressLT) {
	generateAddrComparison(asm.LT);
	return;
    }
    if (methodName == VM_MagicNames.addressLE) {
	generateAddrComparison(asm.LE);
	return;
    }
    if (methodName == VM_MagicNames.addressGT) {
	generateAddrComparison(asm.GT);
	return;
    }
    if (methodName == VM_MagicNames.addressGE) {
	generateAddrComparison(asm.GE);
	return;
    }
    if (methodName == VM_MagicNames.addressEQ) {
	generateAddrComparison(asm.EQ);
	return;
    }
    if (methodName == VM_MagicNames.addressNE) {
	generateAddrComparison(asm.NE);
	return;
    }
    if (methodName == VM_MagicNames.addressIsZero) {
	asm.emitPUSH_Imm(0);
	generateAddrComparison(asm.EQ);
	return;
    }
    if (methodName == VM_MagicNames.addressIsMax) {
	asm.emitPUSH_Imm(-1);
	generateAddrComparison(asm.EQ);
	return;
    }
						     
    VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: " + methodName + " in " + method + "\n");
    asm.emitINT_Imm(0xFF); // trap
    
  }

  private void generateAddrComparison(byte comparator) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    asm.emitSET_Cond_Reg_Byte(comparator, T0);
    asm.emitMOVZX_Reg_Reg_Byte(T0, T0);   // Clear upper 3 bytes
    asm.emitPUSH_Reg(T0);
  }

  // Offset of Java local variable (off stack pointer)
  // assuming ESP is still positioned as it was at the 
  // start of the current bytecode (biStart)
  private final int localOffset  (int local) {
    return (stackHeights[biStart] - local)<<LG_WORDSIZE;
  }

  // Translate a FP offset into an SP offset 
  // assuming ESP is still positioned as it was at the 
  // start of the current bytecode (biStart)
  private final int fp2spOffset(int offset) {
    int offsetToFrameHead = (stackHeights[biStart] << LG_WORDSIZE) - firstLocalOffset;
    return offsetToFrameHead + offset;
  }
  
  private void emitDynamicLinkingSequence(byte reg, VM_Field fieldRef) {
    emitDynamicLinkingSequence(reg, fieldRef.getDictionaryId(), 
			       VM_Entrypoints.fieldOffsetsField.getOffset(),
			       VM_Entrypoints.resolveFieldMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(byte reg, VM_Method methodRef) {
    emitDynamicLinkingSequence(reg, methodRef.getDictionaryId(), 
			       VM_Entrypoints.methodOffsetsField.getOffset(),
			       VM_Entrypoints.resolveMethodMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(byte reg, int memberId,
					  int tableOffset,
					  int resolverOffset) {
    int memberOffset = memberId << 2;
    int retryLabel = asm.getMachineCodeIndex();            // branch here after dynamic class loading
    asm.emitMOV_Reg_RegDisp (reg, JTOC, tableOffset);      // reg is offsets table
    asm.emitMOV_Reg_RegDisp (reg, reg, memberOffset);      // reg is offset of member, or 0 if member's class isn't loaded
    asm.emitTEST_Reg_Reg    (reg, reg);                    // reg ?= 0, is field's class loaded?
    VM_ForwardReference fr = asm.forwardJcc(asm.NE);       // if so, skip call instructions
    asm.emitPUSH_Imm(memberId);                            // pass member's dictId
    genParameterRegisterLoad(1);                           // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, resolverOffset);            // does class loading as sideffect
    asm.emitJMP_Imm (retryLabel);                          // reload reg with valid value
    fr.resolve(asm);                                       // come from Jcc above.
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generate inline machine instructions for special methods that cannot be implemented
 * in java bytecodes. These instructions are generated whenever we encounter an 
 * "invokestatic" bytecode that calls a method with a signature of 
 * the form "static native VM_Magic.xxx(...)".
 * 23 Jan 1998 Derek Lieber
 *
 * NOTE: when adding a new "methodName" to "generate()", be sure to also consider
 * how it affects the values on the stack and update "checkForActualCall()" accordingly.
 * If no call is actually generated, the map will reflect the status of the 
 * locals (including parameters) at the time of the call but nothing on the 
 * operand stack for the call site will be mapped.
 *
 * @author Janice Shepherd
 * @date 7 Jul 1998 
 */
class VM_MagicCompiler implements VM_BaselineConstants
   {
   //-----------//
   // interface //
   //-----------//

   // Generate inline code sequence for specified method.
   // Taken:    compiler we're generating code with
   //           method whose name indicates semantics of code to be generated
   // Returned: nothing
   //
   static void
   generateInlineCode(VM_Compiler compiler, VM_Method methodToBeCalled)
       {
       //!!TODO
       VM.sysWrite("VM_MagicCompiler.java: no magic for " + methodToBeCalled + "\n");
       if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
       }

     // Indicate if specified VM_Magic method causes a frame to be created on the runtime stack.
     // Taken:   VM_Method of the magic method being called
     // Returned: true if method causes a stackframe to be created
     //
     public static boolean
     checkForActualCall(VM_Method methodToBeCalled)
        {
        VM_Atom methodName = methodToBeCalled.getName();
        return methodName == VM_MagicNames.invokeMain                  ||
               methodName == VM_MagicNames.invokeClassInitializer      ||
               methodName == VM_MagicNames.invokeMethodReturningVoid   ||
               methodName == VM_MagicNames.invokeMethodReturningInt    ||
               methodName == VM_MagicNames.invokeMethodReturningLong   ||
               methodName == VM_MagicNames.invokeMethodReturningFloat  ||
               methodName == VM_MagicNames.invokeMethodReturningDouble ||
               methodName == VM_MagicNames.invokeMethodReturningObject;
       }
   }
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * Helper routine to pull the parameters to multianewarray off the
 * Java expression stack maintained by the baseline compiler and 
 * pass them to VM_Runtime.buildMultiDimensionalArray.
 * 
 * TODO: There is only 1 line of platform dependent code here; refactor?
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Tony Cocchi 
 * @author Derek Lieber
 */
class VM_MultianewarrayHelper {

  /**
   * Allocate something like "new Foo[cnt0][cnt1]...[cntN-1]",
   *                      or "new int[cnt0][cnt1]...[cntN-1]".
   * @param numDimensions number of array dimensions
   * @param dictionaryId  type of array (VM_TypeDictionary id)
   * @param argOffset     position of word *above* `cnt0' argument within caller's frame
   *                      This is used to access the number of elements to 
   *                      be allocated for each dimension.
   * See also: bytecode 0xc5 ("multianewarray") in VM_Compiler
   */
  static Object newArrayArray (int numDimensions, int dictionaryId, int argOffset)
    throws VM_ResolutionException, 
	   NegativeArraySizeException, 
	   OutOfMemoryError {
    // fetch number of elements to be allocated for each array dimension
    //
    int[] numElements = new int[numDimensions];
    VM.disableGC();
    VM_Address argp = VM_Magic.getFramePointer().add(argOffset);
    for (int i = 0; i < numDimensions; ++i) {
	argp = argp.sub(4);
	numElements[i] = VM_Magic.getMemoryWord(argp);
    }
    VM.enableGC();
    
    // validate arguments
    //
    for (int i = 0; i < numDimensions; ++i)
      if (numElements[i] < 0) throw new NegativeArraySizeException();
    
    // create array
    //
    return VM_Runtime.buildMultiDimensionalArray(numElements, 0, VM_TypeDictionary.getValue(dictionaryId).asArray());
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Class called from baseline compiler to generate architecture specific
 * write barrier for generational garbage collectors.  For baseline 
 * compiled methods, the write barrier calls methods of VM_WriteBarrier.
 *
 * @author Steve Blackburn for Jeff Stylos (UMass)
 * @author Stephen Smith
 */
class VM_Barriers implements VM_BaselineConstants {

  static void compileArrayStoreBarrier (VM_Assembler asm) {
    // on entry java stack contains ...|target_array_ref|array_index|ref_to_store|
    // SP -> ref_to_store, SP+8 -> target_ref

    asm.emitPUSH_RegDisp(SP, 8);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 4)
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.arrayStoreWriteBarrierMethod.getOffset());
  }

  static void compilePutfieldBarrier (VM_Assembler asm, int fieldOffset) {
    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref

    asm.emitPUSH_RegDisp(SP, 4);
    asm.emitPUSH_Imm(fieldOffset);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.resolvedPutfieldWriteBarrierMethod.getOffset());
  }

  static void compileUnresolvedPutfieldBarrier (VM_Assembler asm, int fieldID) {
    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref
    
    asm.emitPUSH_RegDisp(SP, 4);
    asm.emitPUSH_Imm(fieldID);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.unresolvedPutfieldWriteBarrierMethod.getOffset());
  }

  // currently do not have a "write barrier for putstatic, emit nothing, for now...
  // (the collectors still scan all of statics/jtoc during each GC)
  //
  static void compilePutstaticBarrier (VM_Assembler asm, int fieldOffset) {
  }
  static void compileUnresolvedPutstaticBarrier(VM_Assembler asm, int fieldOffset) {
  }


  /**
   * (Taken from VM_Compiler.java)
   *
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of a helper method call.
   * Assumption: no floating-point parameters.
   * @param params number of parameter words (including "this" if any).
   */
  private final static void genParameterRegisterLoad (VM_Assembler asm, int params){
    if (VM.VerifyAssertions) VM.assert(0 < params);
    if (0 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T0, SP, (params-1) << LG_WORDSIZE);
    }
    if (1 < params && 1 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T1, SP, (params-2) << LG_WORDSIZE);
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Contains architecture-specific helper functions for BURS.
 * 
 * @author Dave Grove
 * @author Stephen Fink
 */
abstract class OPT_BURS_Helpers extends OPT_PhysicalRegisterTools
  implements OPT_Operators, OPT_PhysicalRegisterConstants {
  
  // Generic helper functions.
  // Defined here to allow us to use them in the arch-specific
  // helper functions which are the bulk of this file.

  // returns the given operand as a register
  final OPT_RegisterOperand R(OPT_Operand op) {
    return (OPT_RegisterOperand) op;
  }

  // returns the given operand as an integer constant
  final OPT_IntConstantOperand I(OPT_Operand op) {
    return (OPT_IntConstantOperand) op;
  }
   
  // returns the given operand as a long constant
  final OPT_LongConstantOperand L(OPT_Operand op) {
    return (OPT_LongConstantOperand) op;
  }

  // returns the integer value of the given operand
  final int IV(OPT_Operand op) {
    return I(op).value;
  }

  // Cost functions better suited to grammars with multiple non-termials
  final int ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost) {
    return ADDRESS_EQUAL(store, load, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost, int falseCost) {
    if (Store.getAddress(store).similar(Load.getAddress(load)) &&
	Store.getOffset(store).similar(Load.getOffset(load))) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  final int ARRAY_ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost) {
    return ARRAY_ADDRESS_EQUAL(store, load, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int ARRAY_ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost, int falseCost) {
    if (AStore.getArray(store).similar(ALoad.getArray(load)) &&
	AStore.getIndex(store).similar(ALoad.getIndex(load))) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  final int FITS(OPT_Operand op, int numBits, int trueCost) {
    return FITS(op, numBits, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int FITS(OPT_Operand op, int numBits, int trueCost, int falseCost) {
    if(op.isIntConstant() && OPT_Bits.fits(IV(op),numBits)) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  // can an IV be the scale in a LEA instruction?
  final int LEA_SHIFT(OPT_Operand op, int trueCost) {
    return LEA_SHIFT(op, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int LEA_SHIFT(OPT_Operand op, int trueCost, int falseCost) {
    if (op.isIntConstant()) {
      int val = IV(op);
      if (val >=0 && val <= 3) {
	return trueCost;
      }
    }
    return falseCost;
  }
  final byte LEA_SHIFT(OPT_Operand op) {
    switch (IV(op)) {
    case 0: return B_S;
    case 1: return W_S;
    case 2: return DW_S;
    case 3: return QW_S;
    default:
      throw new OPT_OptimizingCompilerException("bad val for LEA shift "+op);
    }
  }

  final int isFPC_ONE(OPT_Instruction s, int trueCost) {
    return isFPC_ONE(s, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int isFPC_ONE(OPT_Instruction s, int trueCost, int falseCost) {
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      return fc.value == 1.0f ? trueCost : falseCost;
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      return dc.value == 1.0 ? trueCost : falseCost;
    }
  }
  final int isFPC_ZERO(OPT_Instruction s, int trueCost) {
    return isFPC_ZERO(s, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int isFPC_ZERO(OPT_Instruction s, int trueCost, int falseCost) {
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      return fc.value == 0.0f ? trueCost : falseCost;
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      return dc.value == 0.0 ? trueCost : falseCost;
    }
  }

  // 
  // Begin IA32 specific helper functions.
  // 
  final OPT_IA32ConditionOperand COND(OPT_ConditionOperand op) {
    return new OPT_IA32ConditionOperand(op);
  }

  // word size for memory operands
  static final byte B  = 0x01;  // byte (8 bits)
  static final byte W  = 0x02;  // word (16 bits)
  static final byte DW = 0x04;  // doubleword (32 bits)
  static final byte QW = 0x08;  // quadword (64 bits)

  static final byte B_S  = 0x00;  // byte (8*2^0 bits)
  static final byte W_S  = 0x01;  // word (8*2^116 bits)
  static final byte DW_S = 0x02;  // doubleword (8*2^2 bits)
  static final byte QW_S = 0x03;  // quadword (8*2^3 bits)

  // Get particular physical registers
  OPT_Register getEAX () {
    return getIR().regpool.getPhysicalRegisterSet().getEAX();
  }
  OPT_Register getECX () {
    return getIR().regpool.getPhysicalRegisterSet().getECX();
  }
  OPT_Register getEDX () {
    return getIR().regpool.getPhysicalRegisterSet().getEDX();
  }
  OPT_Register getEBX () {
    return getIR().regpool.getPhysicalRegisterSet().getEBX();
  }
  OPT_Register getESP () {
    return getIR().regpool.getPhysicalRegisterSet().getESP();
  }
  OPT_Register getEBP () {
    return getIR().regpool.getPhysicalRegisterSet().getEBP();
  }
  OPT_Register getESI () {
    return getIR().regpool.getPhysicalRegisterSet().getESI();
  }
  OPT_Register getEDI () {
    return getIR().regpool.getPhysicalRegisterSet().getEDI();
  }
  OPT_Register getFPR (int n) {
    return getIR().regpool.getPhysicalRegisterSet().getFPR(n);
  }

  OPT_Operand myFP0() {
    return new OPT_BURSManagedFPROperand(0);
  }
  OPT_Operand myFP1() {
    return new OPT_BURSManagedFPROperand(1);
  }

  // support to remember an address being computed in a subtree
  private static final class AddrStackElement {
    OPT_RegisterOperand base;
    OPT_RegisterOperand index;
    byte scale;
    int displacement;
    AddrStackElement next;
    AddrStackElement(OPT_RegisterOperand b,
		     OPT_RegisterOperand i,
		     byte s, int d,
		     AddrStackElement n) {
      base = b;
      index = i;
      scale = s;
      displacement = d;
      next = n;
    }
  }
  private AddrStackElement AddrStack;
  final void pushAddress(OPT_RegisterOperand base,
			 OPT_RegisterOperand index,
			 byte scale,
			 int disp) {
    AddrStack = new AddrStackElement(base, index, scale, disp, AddrStack);
  }
  final void augmentAddress(OPT_Operand op) {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to augment");
    if (op.isRegister()) {
      OPT_RegisterOperand rop = op.asRegister();
      if (AddrStack.base == null) {
	AddrStack.base = rop;
      } else if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = rop;
      } else {
	throw new OPT_OptimizingCompilerException("three base registers in address");
      }
    } else {
      int disp = ((OPT_IntConstantOperand)op).value;
      AddrStack.displacement += disp;
    }
  }
  final void combineAddresses() {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to combine");
    AddrStackElement tmp = AddrStack;
    AddrStack = AddrStack.next;
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "only 1 address to combine");
    if (tmp.base != null) {
      if (AddrStack.base == null) {
	AddrStack.base = tmp.base;
      } else if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = tmp.base;
      } else {
	throw new OPT_OptimizingCompilerException("three base registers in address");
      }
    }
    if (tmp.index != null) {
      if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = tmp.index;
	AddrStack.scale = tmp.scale;
      } else if (AddrStack.base == null && tmp.scale == (byte)0) {
	AddrStack.base = tmp.base;
      } else {
	throw new OPT_OptimizingCompilerException("two scaled registers in address");
      }
    }
    AddrStack.displacement += tmp.displacement;
  }
  final OPT_MemoryOperand consumeAddress(byte size, 
					 OPT_LocationOperand loc,
					 OPT_Operand guard) {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to consume");
    OPT_MemoryOperand mo = 
      new OPT_MemoryOperand(AddrStack.base, AddrStack.index, AddrStack.scale,
			    AddrStack.displacement, size, loc, guard);
    AddrStack = AddrStack.next;
    return mo;
  }

  // support to remember a memory operand computed in a subtree
  private static final class MOStackElement {
    OPT_MemoryOperand mo;
    MOStackElement next;
    MOStackElement(OPT_MemoryOperand m, 
		   MOStackElement n) {
      mo = m;
      next = n;
    }
  }
  private MOStackElement MOStack;
  final void pushMO(OPT_MemoryOperand mo) {
    MOStack = new MOStackElement(mo, MOStack);
  }
  final OPT_MemoryOperand consumeMO() {
    if (VM.VerifyAssertions) VM.assert(MOStack != null, "No memory operand to consume");
    OPT_MemoryOperand mo = MOStack.mo;
    MOStack = MOStack.next;
    return mo;
  }


  // Construct a memory operand for the effective address of the 
  // load instruction
  final OPT_MemoryOperand MO_L(OPT_Instruction s, byte size) {
    return MO(Load.getAddress(s), Load.getOffset(s), size, 
	      Load.getLocation(s), Load.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // store instruction
  final OPT_MemoryOperand MO_S(OPT_Instruction s, byte size) {
    return MO(Store.getAddress(s), Store.getOffset(s), size, 
	      Store.getLocation(s), Store.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array load instruction
  final OPT_MemoryOperand MO_AL(OPT_Instruction s, byte scale, byte size) {
    return MO_ARRAY(ALoad.getArray(s), ALoad.getIndex(s), scale, size, 
		    ALoad.getLocation(s), ALoad.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array store instruction
  final OPT_MemoryOperand MO_AS(OPT_Instruction s, byte scale, byte size) {
    return MO_ARRAY(AStore.getArray(s), AStore.getIndex(s), scale, size, 
		    AStore.getLocation(s), AStore.getGuard(s));
  }

  // Construct a memory operand for the effective address of the 
  // load instruction 
  final OPT_MemoryOperand MO_L(OPT_Instruction s, byte size, int disp) {
    return MO(Load.getAddress(s), Load.getOffset(s), size, disp,
	      Load.getLocation(s), Load.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // store instruction
  final OPT_MemoryOperand MO_S(OPT_Instruction s, byte size, int disp) {
    return MO(Store.getAddress(s), Store.getOffset(s), size, disp,
	      Store.getLocation(s), Store.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array load instruction
  final OPT_MemoryOperand MO_AL(OPT_Instruction s, byte scale, byte size, int disp) {
    return MO_ARRAY(ALoad.getArray(s), ALoad.getIndex(s), scale, size, disp,
		    ALoad.getLocation(s), ALoad.getGuard(s));
  }
  // Construct a memory operand for the effective address of the array store instruction
  final OPT_MemoryOperand MO_AS(OPT_Instruction s, byte scale, byte size, int disp) {
    return MO_ARRAY(AStore.getArray(s), AStore.getIndex(s), scale, size, disp,
		    AStore.getLocation(s), AStore.getGuard(s));
  }

  final OPT_MemoryOperand MO(OPT_Operand base, OPT_Operand offset, 
			     byte size, OPT_LocationOperand loc,
			     OPT_Operand guard) {
    if (base instanceof OPT_IntConstantOperand) {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+IV(offset), size, loc, guard);
      } else {
	return MO_BD(offset, IV(base), size, loc, guard);
      }
    } else {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_BD(base, IV(offset), size, loc, guard);
      } else {
	return MO_BI(base, offset, size, loc, guard);
      }
    }
  }

  final OPT_MemoryOperand MO_ARRAY(OPT_Operand base, 
				   OPT_Operand index, 
				   byte scale, byte size, 
				   OPT_LocationOperand loc,
				   OPT_Operand guard) {
    if (index instanceof OPT_IntConstantOperand) {
      return MO_BD(base, IV(index)<<scale, size, loc, guard);
    } else {
      return MO_BIS(base, index, scale, size, loc, guard);
    }
  }


  final OPT_MemoryOperand MO(OPT_Operand base, OPT_Operand offset, 
			     byte size, int disp,
			     OPT_LocationOperand loc,
			     OPT_Operand guard) {
    if (base instanceof OPT_IntConstantOperand) {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+IV(offset)+disp, size, loc, guard);
      } else {
	return MO_BD(offset, IV(base)+disp, size, loc, guard);
      }
    } else {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_BD(base, IV(offset)+disp, size, loc, guard);
      } else {
	return MO_BID(base, offset, disp, size, loc, guard);
      }
    }
  }

  final OPT_MemoryOperand MO_ARRAY(OPT_Operand base, 
				   OPT_Operand index, 
				   byte scale, byte size, 
				   int disp,
				   OPT_LocationOperand loc,
				   OPT_Operand guard) {
    if (index instanceof OPT_IntConstantOperand) {
      return MO_BD(base, (IV(index)<<scale)+disp, size, loc, guard);
    } else {
      return new OPT_MemoryOperand(R(base), R(index), scale, 
				   disp, size, loc, guard);
    }
  }

 
  final OPT_MemoryOperand MO_B(OPT_Operand base, byte size, 
			       OPT_LocationOperand loc,
			       OPT_Operand guard) {
    return OPT_MemoryOperand.B(R(base), size, loc, guard);
  }

  final OPT_MemoryOperand MO_BI(OPT_Operand base, 
				OPT_Operand index, 
				byte size, OPT_LocationOperand loc,
				OPT_Operand guard) {
    return OPT_MemoryOperand.BI(R(base), R(index), size, loc, guard);
  }

  final OPT_MemoryOperand MO_BD(OPT_Operand base, int disp, 
				byte size, OPT_LocationOperand loc,
				OPT_Operand guard) {
    return OPT_MemoryOperand.BD(R(base), disp, size, loc, guard);
  }

  final OPT_MemoryOperand MO_BID(OPT_Operand base, 
				 OPT_Operand index, 
				 int disp, byte size, 
				 OPT_LocationOperand loc,
				 OPT_Operand guard) {
    return OPT_MemoryOperand.BID(R(base), R(index), disp, size, loc, guard);
  }

  final OPT_MemoryOperand MO_BIS(OPT_Operand base, 
				 OPT_Operand index, 
				 byte scale, byte size, 
				 OPT_LocationOperand loc,
				 OPT_Operand guard) {
    return OPT_MemoryOperand.BIS(R(base), R(index), scale, size, loc, guard);
  }

  final OPT_MemoryOperand MO_D(int disp, 
			       byte size, OPT_LocationOperand loc,
			       OPT_Operand guard) {
    return OPT_MemoryOperand.D(disp, size, loc, guard);
  }



  final OPT_MemoryOperand MO_MC(OPT_Instruction s) {
    OPT_Operand base = Binary.getVal1(s);
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      int offset = fc.index << 2;
      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
      if (base instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+offset, DW, loc, TG());
      } else {
	return MO_BD(Binary.getVal1(s), offset, DW, loc, TG());
      }
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      int offset = dc.index << 2;
      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
      if (base instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+offset, QW, loc, TG());
      } else {
	return MO_BD(Binary.getVal1(s), offset, QW, loc, TG());
      }
    }
  }

  final OPT_Operand MO_CONV(OPT_BURS burs, byte size) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    return new OPT_StackLocationOperand(true, offset, size);
  }

  final void STORE_LONG_FOR_CONV(OPT_BURS burs, OPT_Operand op) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    if (op instanceof OPT_RegisterOperand) {
      OPT_RegisterOperand hval = R(op);
      OPT_RegisterOperand lval = R(burs.ir.regpool.getSecondReg(hval.register));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset+4, DW), hval));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset, DW), lval));
    } else {
      OPT_LongConstantOperand val = L(op);
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset+4, DW), I(val.upper32())));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset, DW), I(val.lower32())));
    }
  }      

  // condition code state
  private OPT_ConditionOperand cc;
  void pushCOND(OPT_ConditionOperand c) {
    if (VM.VerifyAssertions) VM.assert(cc == null);
    cc = c ;
  }
  OPT_ConditionOperand consumeCOND() {
    OPT_ConditionOperand ans = cc;
    if (VM.VerifyAssertions) {
      VM.assert(cc != null);
      cc = null;
    }
    return ans;
  }

  // emit code to load 32 bits form a given jtoc offset
  private OPT_MemoryOperand loadFromJTOC(OPT_BURS burs, int offset) {
    OPT_LocationOperand loc = new OPT_LocationOperand(offset);
    OPT_Operand guard = TG();
    if (burs.ir.options.FIXED_JTOC) {
      return OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(offset).toInt(),
				 (byte)4, loc, guard);
    } else {
      OPT_Operand jtoc = 
	OPT_MemoryOperand.BD(R(burs.ir.regpool.getPhysicalRegisterSet().getPR()),
			     VM_Entrypoints.jtocField.getOffset(), 
			     (byte)4, null, TG());
      OPT_RegisterOperand regOp = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, regOp, jtoc));
      return OPT_MemoryOperand.BD(regOp.copyD2U(), offset, (byte)4, loc, guard);
    }
  }

  /*
   * IA32-specific emit rules that are complex 
   * enough that we didn't want to write them in the LIR2MIR.rules file.
   * However, all expansions in this file are called during BURS and
   * thus are constrained to generate nonbranching code (ie they can't
   * create new basic blocks and/or do branching).
   *
   */

  /**
   * Emit code to get a caught exception object into a register
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void GET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForCaughtException();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, DW);
    burs.append(MIR_Move.mutate(s, IA32_MOV, Nullary.getResult(s), sl));
  }


  /**
   * Emit code to move a value in a register to the stack location
   * where a caught exception object is expected to be.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void SET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager. allocateSpaceForCaughtException();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, DW);
    OPT_RegisterOperand obj = (OPT_RegisterOperand)CacheOp.getRef(s);
    burs.append(MIR_Move.mutate(s, IA32_MOV, sl, obj));
  }


  /**
   * Expansion of INT_2LONG
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param value the second operand
   */
  final void INT_2LONG(OPT_BURS burs, OPT_Instruction s,
		       OPT_RegisterOperand result,
		       OPT_Operand value) {
    OPT_Register hr = result.register;
    OPT_Register lr = burs.ir.regpool.getSecondReg(hr);
    burs.append(MIR_Move.create(IA32_MOV, R(lr), value));
    burs.append(MIR_Move.create(IA32_MOV, R(hr), R(lr)));
    burs.append(MIR_BinaryAcc.create(IA32_SAR, R(hr), I(31)));
  }

  /**
   * Expansion of FLOAT_2INT and DOUBLE_2INT, using the FIST instruction.
   * This expansion does some boolean logic and conditional moves in order
   * to avoid changing the floating-point rounding mode or inserting
   * branches.  Other expansions are possible, and may be better?
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param value the second operand
   */
  final void FPR_2INT(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_MemoryOperand M;

    // Step 1: Get value to be converted into myFP0
    //         and in 'strict' IEEE mode.
    if (value instanceof OPT_MemoryOperand) {
      // value is in memory, all we have to do is load it
      burs.append(MIR_Move.create(IA32_FLD, myFP0(), value));
    } else {
      // sigh.  value is an FP register. Unfortunately,
      // SPECjbb requires some 'strict' FP semantics.  Naturally, we don't
      // normally implement strict semantics, but we try to slide by in
      // order to pass the benchmark.  
      // In order to pass SPECjbb, it turns out we need to enforce 'strict'
      // semantics before doing a particular f2int conversion.  To do this
      // we must have a store/load sequence to cause IEEE rounding.
      if (value instanceof OPT_BURSManagedFPROperand) {
	if (VM.VerifyAssertions) VM.assert(value.similar(myFP0()));
	burs.append(MIR_Move.create(IA32_FSTP, MO_CONV(burs, DW), value));
	burs.append(MIR_Move.create(IA32_FLD, myFP0(), MO_CONV(burs, DW)));
      } else {
	burs.append(MIR_Move.create(IA32_FMOV, MO_CONV(burs, DW), value));
	burs.append(MIR_Move.create(IA32_FLD, myFP0(), MO_CONV(burs, DW)));
      }
    }

    // FP Stack: myFP0 = value 
    burs.append(MIR_Move.create(IA32_FIST, MO_CONV(burs, DW),  myFP0()));
    // MO_CONV now holds myFP0 converted to an integer (round-toward nearest)
    // FP Stack: myFP0 == value

    // isPositive == 1 iff 0.0 < value
    // isNegative == 1 iff 0.0 > value
    OPT_Register one        = burs.ir.regpool.getInteger();
    OPT_Register isPositive = burs.ir.regpool.getInteger();
    OPT_Register isNegative = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(one), I(1)));
    burs.append(MIR_Move.create(IA32_MOV, R(isPositive), I(0)));
    burs.append(MIR_Move.create(IA32_MOV, R(isNegative), I(0)));
    burs.append(MIR_Nullary.create(IA32_FLDZ, myFP0()));
    // FP Stack: myFP0 = 0.0; myFP1 = value 
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    burs.append(MIR_CondMove.create(IA32_CMOV, R(isPositive), R(one),
                                    OPT_IA32ConditionOperand.LLT()));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(isNegative), R(one),
                                    OPT_IA32ConditionOperand.LGT()));

    burs.append(MIR_Move.create(IA32_FILD, myFP0(), MO_CONV(burs, DW)));
    // FP Stack: myFP0 = round(value), myFP1 = value

    // addee      = 1 iff round(x) < x
    // subtractee = 1 iff round(x) > x
    OPT_Register addee      = burs.ir.regpool.getInteger();
    OPT_Register subtractee = burs.ir.regpool.getInteger();
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    burs.append(MIR_Move.create(IA32_MOV, R(addee) , I(0)));
    burs.append(MIR_Move.create(IA32_MOV, R(subtractee) , I(0)));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(addee), R(one),
                                    OPT_IA32ConditionOperand.LLT()));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(subtractee), R(one),
                                    OPT_IA32ConditionOperand.LGT()));
    
    // Now a little tricky part.
    // We will add 1 iff isNegative and x > round(x)
    // We will subtract 1 iff isPositive and x < round(x)
    burs.append(MIR_BinaryAcc.create(IA32_AND, R(addee), R(isNegative)));
    burs.append(MIR_BinaryAcc.create(IA32_AND, R(subtractee), R(isPositive)));
    burs.append(MIR_Move.create(IA32_MOV, result.copy(), MO_CONV(burs, DW)));
    burs.append(MIR_BinaryAcc.create(IA32_ADD, result.copy(), R(addee)));
    burs.append(MIR_BinaryAcc.create(IA32_SUB, result.copy(), R(subtractee)));

    // Acquire the JTOC in a register
    OPT_Register jtoc = null;
    if (!burs.ir.options.FIXED_JTOC) {
      jtoc = burs.ir.regpool.getInteger();
      burs.append(MIR_Move.create(IA32_MOV, 
				  R(jtoc), 
				  MO_BD(R(burs.ir.regpool.getPhysicalRegisterSet().getPR()),
					VM_Entrypoints.jtocField.getOffset(), DW, null, null)));
    }

    // Compare myFP0 with (double)Integer.MAX_VALUE
    if (burs.ir.options.FIXED_JTOC) {
      M = OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(VM_Entrypoints.maxintField.getOffset()).toInt(),
			      QW, null, null);
    } else {
      M = OPT_MemoryOperand.BD(R(jtoc), VM_Entrypoints.maxintField.getOffset(), QW, null, null);
    }
    burs.append(MIR_Move.create(IA32_FLD, myFP0(), M));
    // FP Stack: myFP0 = (double)Integer.MAX_VALUE; myFP1 = value
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    // If MAX_VALUE < value, then result := MAX_INT
    OPT_Register maxInt = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(maxInt), I(Integer.MAX_VALUE)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(maxInt), 
                                    OPT_IA32ConditionOperand.LLT()));
    
    // Compare myFP0 with (double)Integer.MIN_VALUE
    if (burs.ir.options.FIXED_JTOC) {
      M = OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(VM_Entrypoints.minintField.getOffset()).toInt(),
			      QW, null, null);
    } else {
      M = OPT_MemoryOperand.BD(R(jtoc), VM_Entrypoints.minintField.getOffset(), QW, null, null);
    }
    burs.append(MIR_Move.create(IA32_FLD, myFP0(), M));
    // FP Stack: myFP0 = (double)Integer.MIN_VALUE; myFP1 = value
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    // If MIN_VALUE > value, then result := MIN_INT
    OPT_Register minInt = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(minInt), I(Integer.MIN_VALUE)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(minInt), 
                                    OPT_IA32ConditionOperand.LGT()));
    
    // Set condition flags: set PE iff myFP0 is a NaN
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP0()));
    // FP Stack: back to original level (all BURS managed slots freed)
    // If FP0 was classified as a NaN, then result := 0
    OPT_Register zero = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(zero), I(0)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(zero),
				    OPT_IA32ConditionOperand.PE()));
    
  }

  /**
   * Emit code to move 64 bits from FPRs to GPRs
   */
  final void FPR2GPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, QW);
    OPT_StackLocationOperand sl1 = new OPT_StackLocationOperand(true, offset+4, DW);
    OPT_StackLocationOperand sl2 = new OPT_StackLocationOperand(true, offset, DW);
    burs.append(MIR_Move.create(IA32_FMOV, sl, Unary.getVal(s)));
    OPT_RegisterOperand i1 = Unary.getResult(s);
    OPT_RegisterOperand i2 = R(burs.ir.regpool.getSecondReg(i1.register));
    burs.append(MIR_Move.create(IA32_MOV, i1, sl1));
    burs.append(MIR_Move.mutate(s, IA32_MOV, i2, sl2));
  }


  /**
   * Emit code to move 64 bits from GPRs to FPRs
   */
  final void GPR2FPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, QW);
    OPT_StackLocationOperand sl1 = new OPT_StackLocationOperand(true, offset+4, DW);
    OPT_StackLocationOperand sl2 = new OPT_StackLocationOperand(true, offset, DW);
    OPT_Operand i1, i2;
    OPT_Operand val = Unary.getVal(s);
    if (val instanceof OPT_RegisterOperand) {
      OPT_RegisterOperand rval = (OPT_RegisterOperand)val;
      i1 = val;
      i2 = R(burs.ir.regpool.getSecondReg(rval.register));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)val;
      i1 = I(rhs.upper32());
      i2 = I(rhs.lower32());
    }      
    burs.append(MIR_Move.create(IA32_MOV, sl1, i1));
    burs.append(MIR_Move.create(IA32_MOV, sl2, i2));
    burs.append(MIR_Move.mutate(s, IA32_FMOV, Unary.getResult(s), sl));
  }

  /**
   * Expansion of ROUND_TO_ZERO.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void ROUND_TO_ZERO(OPT_BURS burs, OPT_Instruction s) {
    // load the JTOC into a register
    OPT_RegisterOperand PR = R(burs.ir.regpool.getPhysicalRegisterSet().
                               getPR());
    OPT_Operand jtoc = OPT_MemoryOperand.BD(PR, VM_Entrypoints.jtocField.getOffset(), 
                                            DW, null, null);
    OPT_RegisterOperand regOp = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Move.create(IA32_MOV, regOp, jtoc));

    // Store the FPU Control Word to a JTOC slot
    OPT_MemoryOperand M = OPT_MemoryOperand.BD
      (regOp.copyRO(), VM_Entrypoints.FPUControlWordField.getOffset(), W, null, null);
    burs.append(MIR_UnaryNoRes.create(IA32_FNSTCW, M));
    // Set the bits in the status word that control round to zero.
    // Note that we use a 32-bit and, even though we only care about the
    // low-order 16 bits
    burs.append(MIR_BinaryAcc.create(IA32_OR, M.copy(), I(0x00000c00)));
    // Now store the result back into the FPU Control Word
    burs.append(MIR_Nullary.mutate(s,IA32_FLDCW, M.copy()));
    return;
  }


  /**
   * Expansion of INT_DIV and INT_REM
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   * @param isDiv true for div, false for rem
   */
  final void INT_DIVIDES(OPT_BURS burs, OPT_Instruction s,
			 OPT_RegisterOperand result,
			 OPT_Operand val1,
			 OPT_Operand val2,
			 boolean isDiv) {
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), val1));
    burs.append(MIR_ConvertDW2QW.create(IA32_CDQ, R(getEDX()), R(getEAX())));
    if (val2 instanceof OPT_IntConstantOperand) {
      OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, temp, val2));
      val2 = temp;
    }
    burs.append(MIR_Divide.mutate(s, IA32_IDIV, R(getEDX()), R(getEAX()), 
				  val2, GuardedBinary.getGuard(s)));
    if (isDiv) {
      burs.append(MIR_Move.create(IA32_MOV, result.copyD2D(), R(getEAX())));
    } else {
      burs.append(MIR_Move.create(IA32_MOV, result.copyD2D(), R(getEDX())));
    }      
  }


  /**
   * Expansion of LONG_ADD_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_ADD(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_ADC, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) {
	burs.append(MIR_BinaryAcc.mutate(s, IA32_ADD, R(lhsReg), I(high)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lowlhsReg), I(low)));
	burs.append(MIR_BinaryAcc.mutate(s, IA32_ADC, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_SUB_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_SUB(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) {
	burs.append(MIR_BinaryAcc.mutate(s, IA32_SUB, R(lhsReg), I(high)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lowlhsReg), I(low)));
	burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_MUL_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_MUL(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    // In general, (a,b) * (c,d) = (l(a imul d)+l(b imul c)+u(b mul d), l(b mul d))
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      OPT_Register tmp = burs.ir.regpool.getInteger();
      burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), R(lowrhsReg)));
      burs.append(MIR_Move.create(IA32_MOV, R(tmp), R(rhsReg)));
      burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
      burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), R(lowlhsReg)));
      burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowrhsReg)));
      burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();

      // We only have to handle those cases that OPT_Simplifier wouldn't get.  
      // OPT_Simplifier catches 
      // high   low
      //    0     0  (0L)
      //    0     1  (1L)
      //   -1    -1 (-1L)
      // So, the possible cases we need to handle here:
      //   -1     0 
      //   -1     1
      //   -1     *
      //    0    -1
      //    0     *
      //    1    -1
      //    1     0 
      //    1     1
      //    1     *
      //    *    -1
      //    *     0
      //    *     1
      //    *     *
      // (where * is something other than -1,0,1)
      if (high == -1) {
	if (low == 0) {
	  // -1, 0
	  // CLAIM: (x,y) * (-1,0) = (-y,0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // -1, 1
	  // CLAIM: (x,y) * (-1,1) = (x-y,y)
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lhsReg), R(lowlhsReg)));
	} else {
	  // -1, *
	  // CLAIM: (x,y) * (-1, z) = (l(x imul z)-y+u(y mul z)+, l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else if (high == 0) {
	if (low == -1) {
	  // 0, -1
	  // CLAIM: (x,y) * (0,-1) = (u(y mul -1)-x, l(y mul -1))
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(-1)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(getEDX()), R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(getEDX())));
	} else {
	  // 0, *
	  // CLAIM: (x,y) * (0,z) = (l(x imul z)+u(y mul z), l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else if (high == 1) {
	if (low == -1) {
	  // 1, -1
	  // CLAIM: (x,y) * (1,-1) = (-x+y+u(y mul -1), l(y mul -1))
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(-1)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	} else if (low == 0) {
	  // 1, 0 
	  // CLAIM: (x,y) * (1,0) = (y,0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // 1, 1
	  // CLAIM: (x,y) * (1,1)  = (x+y,y)
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	} else {
	  // 1, *
	  // CLAIM: (x,y) * (1,z) = (l(x imul z)+y+u(y mul z), l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else {
	if (low == -1) {
	  // *, -1
	  // CLAIM: (x,y) * (z,-1) = (-x+l(y imul z)+u(y mul -1), l(y mul -1))
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	} else if (low == 0) {
	  // *,  0
	  // CLAIM: (x,y) * (z,0) = (l(y imul z),0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // *, 1
	  // CLAIM: (x,y) * (z,1) = (l(y imul z)+x,y)	
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), I(high)));
	  burs.append(MIR_Move.create(IA32_ADD, R(lhsReg), R(tmp)));
	} else {
	  // *, * (sigh, can't do anything interesting...)
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      }
    }
  }


  /**
   * Expansion of LONG_NEG_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   */
  final void LONG_NEG(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
    burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lowlhsReg)));
    burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), I(0)));
  }


  /**
   * Expansion of LONG_AND
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_AND(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_AND, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_AND, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x &= 0 ==> x = 0
	burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
      } else if (low == -1) { // x &= 0xffffffff ==> x = x ==> nop
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_AND, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x &= 0 ==> x = 0
	burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(0)));
      } else if (high == -1) { // x &= 0xffffffff ==> x = x ==> nop
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_AND, R(lhsReg), I(high)));
      }
    }	
  }


  /**
   * Expansion of LONG_OR
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_OR(OPT_BURS burs, OPT_Instruction s,
		     OPT_RegisterOperand result,
		     OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_OR, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_OR, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x |= 0 ==> x = x ==> nop
      } else if (low == -1) { // x |= 0xffffffff ==> x = 0xffffffff
	burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(-1)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_OR, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x |= 0 ==> x = x ==> nop
      } else if (high == -1) { // x |= 0xffffffff ==> x = 0xffffffff
	burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(-1)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_OR, R(lhsReg), I(high)));
      }
    }	
  }


  /**
   * Expansion of LONG_XOR
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_XOR(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_XOR, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x ^= 0 ==> x = x ==> nop
      } else if (low == -1) { // x ^= 0xffffffff ==> x = ~x
	burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lowlhsReg)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x ^= 0 ==> x = x ==> nop
      } else if (high == -1) { // x ^= 0xffffffff ==> x = ~x
	burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lhsReg)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_NOT
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   */
  final void LONG_NOT(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lowlhsReg)));
    burs.append(MIR_UnaryAcc.mutate(s, IA32_NOT, R(lhsReg)));
  }


  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * Moves first value into fp0,
   * accumulates second value into fp0 using op,
   * moves fp0 into result.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_MOV_OP_MOV(OPT_BURS burs, OPT_Instruction s,
			   OPT_Operator op,
			   OPT_Operand result,
			   OPT_Operand val1,
			   OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s, op, D(getFPR(0)), val2));
    burs.append(MIR_Move.create(IA32_FMOV, result, D(getFPR(0))));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * Moves first value into fp0,
   * accumulates second value into fp0 using op.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_MOV_OP(OPT_BURS burs, OPT_Instruction s,
			   OPT_Operator op,
			   OPT_Operand val1,
			   OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s, op, D(getFPR(0)), val2));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * apply op to val1 and val2
   * move val1 to result using movop
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param movop the move op to use
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_OP_MOV(OPT_BURS burs, OPT_Instruction s,
		       OPT_Operator op,
		       OPT_Operator movop,
		       OPT_Operand result,
		       OPT_Operand val1,
		       OPT_Operand val2) {
    burs.append(MIR_BinaryAcc.mutate(s, op, val1, val2));
    burs.append(MIR_Move.create(movop, result, val1.copy()));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * apply op to val1 and val2.
   * NOTE: either val1 or val2 must be either FPR0 or ST(0)!
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_OP(OPT_BURS burs, OPT_Instruction s,
		   OPT_Operator op,
		   OPT_Operand val1,
		   OPT_Operand val2) {
    burs.append(MIR_BinaryAcc.mutate(s, op, val1, val2));
  }

  /**
   * Expansion of FP_REM 
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_REM(OPT_BURS burs, OPT_Instruction s,
		    OPT_Operand val1,
		    OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(1)), val2));
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s,IA32_FPREM, D(getFPR(0)), D(getFPR(1))));
  }
  /**
   * Expansion of FP_REM
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param val the operand to divide with fp0 to get a remainder
   */
  final void FP_REM(OPT_BURS burs, OPT_Instruction s,
		    OPT_Operand val) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(1)), val));
    burs.append(MIR_BinaryAcc.mutate(s,IA32_FPREM, D(getFPR(0)), D(getFPR(1))));
  }


  /**
   * Expansion of BOOLEAN_CMP
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result operand
   * @param val1   the first value
   * @param val2   the second value
   * @param cond   the condition operand
   */
  final void BOOLEAN_CMP(OPT_BURS burs, OPT_Instruction s,
			 OPT_Operand res, 
			 OPT_Operand val1,
			 OPT_Operand val2,
			 OPT_ConditionOperand cond) {
    burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    OPT_RegisterOperand temp = burs.ir.regpool.makeTemp(VM_Type.BooleanType);
    burs.append(CPOS(s, MIR_Set.create(IA32_SET$B, temp, COND(cond))));
    burs.append(MIR_Unary.mutate(s, IA32_MOVZX$B, res, temp.copyD2U()));
  }


  /**
   * Expansion of a special case of BOOLEAN_CMP when the 
   * condition registers have already been set by the previous
   * ALU op.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result operand
   * @param cond   the condition operand
   */
  final void BOOLEAN_CMP(OPT_BURS burs, OPT_Instruction s,
			 OPT_Operand res, 
			 OPT_ConditionOperand cond) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTemp(VM_Type.BooleanType);
    burs.append(CPOS(s, MIR_Set.create(IA32_SET$B, temp, COND(cond))));
    burs.append(MIR_Unary.mutate(s, IA32_MOVZX$B, res, temp.copyD2U()));
  }


  /**
   * Generate a compare and branch sequence.
   * Used in the expansion of trees where INT_IFCMP is a root
   * 
   * @param burs and OPT_BURS object
   * @param s the ifcmp instruction 
   * @param val1 the first value operand
   * @param val2 the second value operand
   * @param cond the condition operand
   */
  final void IFCMP(OPT_BURS burs, OPT_Instruction s,
		   OPT_Operand val1, OPT_Operand val2,
		   OPT_ConditionOperand cond) {
    burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    burs.append(MIR_CondBranch.mutate(s, IA32_JCC, COND(cond),
				      IfCmp.getTarget(s), 
				      IfCmp.getBranchProfile(s)));
  }


  /**
   * Generate the compare portion of a conditional move.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param val1 the first value to compare
   * @param val2 the second value to compare
   */
  final void CMOV_CMP(OPT_BURS burs, OPT_Instruction s,
		      OPT_Operand val1, OPT_Operand val2) {
    if (val1.isRegister() && val1.asRegister().register.isFloatingPoint()) {
      if (VM.VerifyAssertions) {
        VM.assert(val2.isRegister());
        VM.assert(val2.asRegister().register.isFloatingPoint());
      }
      burs.append(CPOS(s, MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1)));
      burs.append(CPOS(s, MIR_Compare.create(IA32_FCOMI, D(getFPR(0)), val2)));
    } else {
      burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    }
  }

  /**
   * Generate the move portion of a conditional move.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result of the conditional move
   * @param cond the condition operand
   * @param trueVal the value to move to result if cond is true
   * @param falseVal the value to move to result if cond is not true
   */
  final void CMOV_MOV(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_ConditionOperand cond,
		      OPT_Operand trueValue,
		      OPT_Operand falseValue) {
    OPT_Operator movop, cmovop;
    if (result.type.isDoubleType() || result.type.isFloatType()) {
      movop = IA32_FMOV;
      cmovop = IA32_FCMOV;
    } else {
      movop = IA32_MOV;
      cmovop = IA32_CMOV;
    }

    if (result.similar(trueValue)) {
      // in this case, only need a conditional move for the false branch.
      burs.append(MIR_CondMove.mutate(s, cmovop, result,
				      asReg(burs, s, movop, falseValue),
				      COND(cond.flipCode())));
    } else if (result.similar(falseValue)) {
      // in this case, only need a conditional move for the true branch.
      burs.append(MIR_CondMove.mutate(s, cmovop, result, 
				      asReg(burs, s, movop, trueValue),
				      COND(cond)));
    } else {
      // need to handle both possible assignments. Unconditionally
      // assign one value then conditionally assign the other.
      if (falseValue.isRegister()) {
	burs.append(CPOS(s,MIR_Move.create(movop, result, trueValue)));
	burs.append(MIR_CondMove.mutate(s, cmovop, result.copy(), 
					falseValue,
					COND(cond.flipCode())));
      } else {
	burs.append(CPOS(s,MIR_Move.create(movop, result, falseValue)));
	burs.append(MIR_CondMove.mutate(s, cmovop, result.copy(), 
					asReg(burs, s, movop, trueValue),
					COND(cond)));
      }
    }
  }

  // move op into a register operand if it isn't one already.
  private OPT_Operand asReg(OPT_BURS burs, OPT_Instruction s, 
			    OPT_Operator movop, OPT_Operand op) {
    if (op.isRegister()) return op;
    OPT_RegisterOperand tmp = burs.ir.regpool.makeTemp(op);
    burs.append(CPOS(s, MIR_Move.create(movop, tmp, op)));
    return tmp.copy();
  }


  /**
   * Expand a prologue by expanding out longs into pairs of ints
   */
  void PROLOGUE(OPT_BURS burs, OPT_Instruction s) {
    int numFormals = Prologue.getNumberOfFormals(s);
    int numLongs = 0;
    for (int i=0; i<numFormals; i++) {
      if (Prologue.getFormal(s, i).type == VM_Type.LongType) numLongs ++;
    }
    if (numLongs != 0) {
      OPT_Instruction s2 = Prologue.create(IR_PROLOGUE, numFormals+numLongs);
      for (int sidx=0, s2idx=0; sidx<numFormals; sidx++) {
	OPT_RegisterOperand sForm = Prologue.getFormal(s, sidx);
	if (sForm.type == VM_Type.LongType) {
	  sForm.type = VM_Type.IntType;
	  Prologue.setFormal(s2, s2idx++, sForm);
          OPT_Register r2 = burs.ir.regpool.getSecondReg(sForm.register);
	  Prologue.setFormal(s2, s2idx++, R(r2));
          sForm.register.clearType();
          sForm.register.setInteger();
          r2.clearType();
          r2.setInteger();
	} else {
	  Prologue.setFormal(s2, s2idx++, sForm);
	}
      }									     
      burs.append(s2);
    } else {
      burs.append(s);
    }
  }

  /**
   * Expansion of CALL.
   * Expand longs registers into pairs of int registers.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param address the operand containing the target address
   */
  final void CALL(OPT_BURS burs, 
		  OPT_Instruction s,
		  OPT_Operand address) {
    OPT_RegisterPool regpool = burs.ir.regpool;

    // Step 1: Find out how many parameters we're going to have.
    int numParams = Call.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (Call.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be.
    OPT_RegisterOperand result = Call.getResult(s);
    OPT_RegisterOperand result2 = null;
    if (result != null && result.type == VM_Type.LongType) {
      result.type = VM_Type.IntType;
      result2 = R(regpool.getSecondReg(result.register));
    }
    
    // Step 3: Mutate the Call to an MIR_Call.
    // Note MIR_Call and Call have a different number of fixed 
    // arguments, so some amount of copying is required. 
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = Call.getParam(s, i);
    }
    MIR_Call.mutate(s, IA32_CALL, result, result2, 
		    address, Call.getMethod(s),
		    numParams + longParams);
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      if (param instanceof OPT_RegisterOperand) {
	MIR_Call.setParam(s, mirCallIdx++, param);
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
                            L(regpool.getSecondReg(rparam.register)));
        }
      } else if (param instanceof OPT_LongConstantOperand) {
	OPT_LongConstantOperand val = (OPT_LongConstantOperand)param;
	MIR_Call.setParam(s, mirCallIdx++, I(val.upper32()));
	MIR_Call.setParam(s, mirCallIdx++, I(val.lower32()));
      } else {
	MIR_Call.setParam(s, mirCallIdx++, param);
      }
    }

    // emit the call instruction.
    burs.append(s);
  }

  /**
   * Expansion of SYSCALL.
   * Expand longs registers into pairs of int registers.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param address the operand containing the target address
   */
  final void SYSCALL(OPT_BURS burs, OPT_Instruction s, OPT_Operand address) {
    OPT_RegisterPool regpool = burs.ir.regpool;
    burs.ir.setHasSysCall(true);

    // Step 1: Find out how many parameters we're going to have.
    int numParams = CallSpecial.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (CallSpecial.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be.
    OPT_RegisterOperand result = CallSpecial.getResult(s);
    OPT_RegisterOperand result2 = null;
    // NOTE: C callee returns longs little endian!
    if (result != null && result.type == VM_Type.LongType) {
      result.type = VM_Type.IntType;
      result2 = result;
      result = R(regpool.getSecondReg(result.register));
    }
    
    // Step 3: Mutate the CallSpecial to an MIR_Call.
    // Note MIR_Call and CallSpecial have a different number of fixed 
    // arguments, so some amount of copying is required. 
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = CallSpecial.getParam(s, i);
    }
    MIR_Call.mutate(s, IA32_SYSCALL, result, result2, 
		    address, (OPT_MethodOperand)CallSpecial.getMethod(s),
		    numParams + longParams);
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      if (param instanceof OPT_RegisterOperand) {
	// NOTE: longs passed little endian to C callee!
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
                            L(regpool.getSecondReg(rparam.register)));
        }
	MIR_Call.setParam(s, mirCallIdx++, param);
      } else if (param instanceof OPT_LongConstantOperand) {
	long value = ((OPT_LongConstantOperand)param).value; 
	int valueHigh = (int)(value >> 32);
	int valueLow = (int)(value & 0xffffffff);
	// NOTE: longs passed little endian to C callee!
	MIR_Call.setParam(s, mirCallIdx++, I(valueLow));
	MIR_Call.setParam(s, mirCallIdx++, I(valueHigh));
      } else {
	MIR_Call.setParam(s, mirCallIdx++, param);
      }
    }

    // emit the call instruction.
    burs.append(s);
  }

  /**
   * Expansion of LOWTABLESWITCH.  
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void LOWTABLESWITCH(OPT_BURS burs, OPT_Instruction s) {
    // (1) We're changing index from a U to a DU.
    //     Inject a fresh copy instruction to make sure we aren't
    //     going to get into trouble (if someone else was also using index).
    OPT_RegisterOperand newIndex = burs.ir.regpool.makeTempInt(); 
    burs.append(MIR_Move.create(IA32_MOV, newIndex, LowTableSwitch.getIndex(s))); 
    int number = LowTableSwitch.getNumberOfTargets(s);
    OPT_Instruction s2 = CPOS(s,MIR_LowTableSwitch.create(MIR_LOWTABLESWITCH, newIndex, number*2));
    for (int i=0; i<number; i++) {
      MIR_LowTableSwitch.setTarget(s2,i,LowTableSwitch.getTarget(s,i));
      MIR_LowTableSwitch.setBranchProfile(s2,i,LowTableSwitch.getBranchProfile(s,i));
    }
    burs.append(s2);
  }

  /**
   * Expansion of RESOLVE.  Dynamic link point.
   * Build up MIR instructions for Resolve.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void RESOLVE(OPT_BURS burs, 
		     OPT_Instruction s) {
    OPT_Operand target = loadFromJTOC(burs, VM_Entrypoints.optResolveMethod.getOffset());
    burs.append(CPOS(s, MIR_Call.mutate0(s, CALL_SAVE_VOLATILE, 
					 null, null,  target, 
					 OPT_MethodOperand.STATIC(VM_Entrypoints.optResolveMethod))));
  }
  /**
   * Expansion of TRAP_IF, with an int constant as the second value.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void TRAP_IF_IMM(OPT_BURS burs, OPT_Instruction s) {
    OPT_RegisterOperand gRes = TrapIf.getGuardResult(s);
    OPT_RegisterOperand v1 =  (OPT_RegisterOperand)TrapIf.getVal1(s);
    OPT_IntConstantOperand v2 = (OPT_IntConstantOperand)TrapIf.getVal2(s);
    OPT_ConditionOperand cond = TrapIf.getCond(s);
    OPT_TrapCodeOperand tc = TrapIf.getTCode(s);

    // A slightly ugly matter, but we need to deal with combining
    // the two pieces of a long register from a LONG_ZERO_CHECK.  
    // A little awkward, but probably the easiest workaround...
    if (tc.getTrapCode() == VM_Runtime.TRAP_DIVIDE_BY_ZERO &&
        v1.type == VM_Type.LongType) {
      OPT_RegisterOperand rr = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, rr, v1.copy()));
      burs.append(MIR_BinaryAcc.create(IA32_OR, rr.copy(), 
				       R(burs.ir.regpool.getSecondReg
					 (v1.register))));
      v1 = rr.copyD2U();
    } 

    // emit the trap instruction
    burs.append(MIR_TrapIf.mutate(s, IA32_TRAPIF, gRes, v1, v2, COND(cond),
                                  tc));
  }


  /**
   * This routine expands an ATTEMPT instruction 
   * into an atomic compare exchange.
   *
   * @param burs     an OPT_BURS object
   * @param result   the register operand that is set to 0/1 as a result of the attempt
   * @param mo       the address at which to attempt the exchange
   * @param oldValue the old value at the address mo
   * @param newValue the new value at the address mo
   */
  void ATTEMPT(OPT_BURS burs, 
	       OPT_RegisterOperand result,
	       OPT_MemoryOperand mo,
	       OPT_Operand oldValue,
	       OPT_Operand newValue) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
    OPT_RegisterOperand temp2 = burs.ir.regpool.makeTemp(result);
    burs.append(MIR_Move.create(IA32_MOV, temp, newValue));
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), oldValue));
    burs.append(MIR_CompareExchange.create(IA32_LOCK_CMPXCHG, R(getEAX()), 
					   mo, (OPT_RegisterOperand)temp.copy())); 
    burs.append(MIR_Set.create(IA32_SET$B, temp2, OPT_IA32ConditionOperand.EQ()));
    // need to zero-extend the result of the set
    burs.append(MIR_Unary.create(IA32_MOVZX$B, result, temp2.copy()));
  }


  /**
   * This routine expands the compound pattern
   * IFCMP(ATTEMPT, ZERO) into an atomic compare/exchange 
   * followed by a branch on success/failure
   * of the attempted atomic compare/exchange.
   *
   * @param burs     an OPT_BURS object
   * @param mo       the address at which to attempt the exchange
   * @param oldValue the old value at the address mo
   * @param newValue the new value at the address mo
   * @param cond     the condition to branch on
   * @param target   the branch target
   * @param bp       the branch profile information
   */
  void ATTEMPT_IFCMP(OPT_BURS burs, 
		     OPT_MemoryOperand mo,
		     OPT_Operand oldValue,
		     OPT_Operand newValue,
		     OPT_ConditionOperand cond,
		     OPT_BranchOperand target,
		     OPT_BranchProfileOperand bp) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Move.create(IA32_MOV, temp, newValue));
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), oldValue));
    burs.append(MIR_CompareExchange.create(IA32_LOCK_CMPXCHG, R(getEAX()), 
					   mo, (OPT_RegisterOperand)temp.copy())); 
    burs.append(MIR_CondBranch.create(IA32_JCC, COND(cond), target, bp));
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Handles the conversion from LIR to MIR of operators whose 
 * expansion requires the introduction of new control flow (new basic blocks).
 *
 * @author Dave Grove
 * @modified Peter Sweeney
 */
abstract class OPT_ComplexLIR2MIRExpansion extends OPT_IRTools {

  /**
   * Converts the given IR to low level IA32 IR.
   *
   * @param ir IR to convert
   */
  public static void convert(OPT_IR ir) {
    OPT_Instruction nextInstr;
    for (OPT_Instruction s = ir.firstInstructionInCodeOrder();
	 s != null; s = nextInstr) {
      switch (s.getOpcode()) {
      case LONG_SHL_ACC_opcode:
	nextInstr = long_shl(s, ir);
	break;
      case LONG_SHR_ACC_opcode:
	nextInstr = long_shr(s, ir);
	break;
      case LONG_USHR_ACC_opcode:
	nextInstr = long_ushr(s, ir);
	break;
      case LONG_IFCMP_opcode:
	{
	  OPT_Operand val2 = IfCmp.getVal2(s);
	  if (val2 instanceof OPT_RegisterOperand) {
	    nextInstr = long_ifcmp(s, ir);
	  } else {
	    nextInstr = long_ifcmp_imm(s, ir);
	  }
	}
	break;
      case LONG_CMP_opcode:
	nextInstr = threeValueLongCmp(s, ir);
	break;
      case FLOAT_IFCMPL_opcode:
      case FLOAT_IFCMPG_opcode:
      case DOUBLE_IFCMPL_opcode:
      case DOUBLE_IFCMPG_opcode:
	nextInstr = fp_ifcmp(s, ir);
	break;
      case FLOAT_CMPL_opcode:
      case FLOAT_CMPG_opcode:
      case DOUBLE_CMPL_opcode:
      case DOUBLE_CMPG_opcode:
	nextInstr = threeValueFPCmp(s, ir);
	break;
      case YIELDPOINT_PROLOGUE_opcode:
      case YIELDPOINT_EPILOGUE_opcode:
      case YIELDPOINT_BACKEDGE_opcode:
	nextInstr = yield_point(s,ir);
	break;
      default:
	nextInstr = s.nextInstructionInCodeOrder();
	break;
      }
    }
    OPT_DefUse.recomputeSpansBasicBlock(ir);
  }


  private static OPT_Instruction long_shl(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(hval), R(lval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SHL, R(hval), I(shift)));
	MIR_Move.mutate(s, IA32_MOV, R(lval), I(0));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHLD, R(hval), R(lval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SHL, R(lval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();

      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(hval), R(lval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(hval), R(ecx)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), I(0)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHLD, R(hval), R(lval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(lval), R(ecx)));
    }
    return nextInstr;
  }


  private static OPT_Instruction long_shr(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SAR, R(lval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SAR, R(hval), I(31));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SAR, R(hval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();
      
      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(lval), R(ecx)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(hval), I(31)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(hval), R(ecx)));
    }
    return nextInstr;
  }

  
  private static OPT_Instruction long_ushr(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SHR, R(lval), I(shift)));
	MIR_Move.mutate(s, IA32_MOV, R(hval), I(0));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SHR, R(hval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();
      
      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHR, R(lval), R(ecx)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(hval), I(0)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHR, R(hval), R(ecx)));
    }
    return nextInstr;
  }


  private static OPT_Instruction long_ifcmp(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_ConditionOperand cond = IfCmp.getCond(s);
    OPT_Register xh = ((OPT_RegisterOperand)IfCmp.getVal1(s)).register;
    OPT_Register xl = ir.regpool.getSecondReg(xh);
    OPT_RegisterOperand yh = (OPT_RegisterOperand)IfCmp.getClearVal2(s);
    OPT_RegisterOperand yl = R(ir.regpool.getSecondReg(yh.register));
    basic_long_ifcmp(s, ir, cond, xh, xl, yh, yl);
    return nextInstr;
  }


  private static OPT_Instruction long_ifcmp_imm(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_ConditionOperand cond = IfCmp.getCond(s);
    OPT_Register xh = ((OPT_RegisterOperand)IfCmp.getVal1(s)).register;
    OPT_Register xl = ir.regpool.getSecondReg(xh);
    OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)IfCmp.getVal2(s);
    int low = rhs.lower32();
    int high = rhs.upper32();
    OPT_IntConstantOperand yh = I(high);
    OPT_IntConstantOperand yl = I(low);
    
    if (cond.isEQUAL() || cond.isNOT_EQUAL()) {
      // tricky... ((xh^yh)|(xl^yl) == 0) <==> (lhll == rhrl)!!
      OPT_Register th = ir.regpool.getInteger();
      OPT_Register tl = ir.regpool.getInteger();
      if (high == 0) {
	if (low == 0) { // 0,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // 0,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(tl), R(xh)));
	} else { // 0,*
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(tl), R(xh)));
	}
      } else if (high == -1) {
	if (low == 0) { // -1,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // -1,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	} else { // -1,*
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	}
      } else { 
	if (low == 0) { // *,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // *,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	} else { // neither high nor low is special
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	}
      }
      MIR_CondBranch.mutate(s, IA32_JCC, 
			    new OPT_IA32ConditionOperand(cond),
			    IfCmp.getTarget(s),
			    IfCmp.getBranchProfile(s));
      return nextInstr;
    } else {
      // pick up a few special cases where the sign of xh is sufficient
      if (rhs.value == 0L) {
	if (cond.isLESS()) {
	  // xh < 0 implies true
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.LT(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	} else if (cond.isGREATER_EQUAL()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.GE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	}
      } else if (rhs.value == -1L) {
	if (cond.isLESS_EQUAL()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(-1)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.LE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	} else if (cond.isGREATER()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.GE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	}
      }

      basic_long_ifcmp(s, ir, cond, xh, xl, yh, yl);
      return nextInstr;
    }
  }


  private static void basic_long_ifcmp(OPT_Instruction s, OPT_IR ir, 
				       OPT_ConditionOperand cond, 
				       OPT_Register xh, 
				       OPT_Register xl, 
				       OPT_Operand yh, 
				       OPT_Operand yl) {
    if (cond.isEQUAL() || cond.isNOT_EQUAL()) {
      OPT_RegisterOperand th = ir.regpool.makeTempInt();
      OPT_RegisterOperand tl = ir.regpool.makeTempInt();
      // tricky... ((xh^yh)|(xl^yl) == 0) <==> (lhll == rhrl)!!
      s.insertBefore(MIR_Move.create(IA32_MOV, th, R(xh)));
      s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, th.copyD2D(), yh));
      s.insertBefore(MIR_Move.create(IA32_MOV, tl, R(xl)));
      s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, tl.copyD2D(), yl));
      s.insertBefore(MIR_BinaryAcc.create(IA32_OR, th.copyD2D(), tl.copyD2U()));
      MIR_CondBranch.mutate(s, IA32_JCC, 
			    new OPT_IA32ConditionOperand(cond),
			    IfCmp.getTarget(s),
			    IfCmp.getBranchProfile(s));
    } else {
      // Do the naive thing and generate multiple compare/branch implementation.
      OPT_IA32ConditionOperand cond1;
      OPT_IA32ConditionOperand cond2;
      OPT_IA32ConditionOperand cond3;
      if (cond.isLESS()) {
	cond1 = OPT_IA32ConditionOperand.LT();
	cond2 = OPT_IA32ConditionOperand.GT();
	cond3 = OPT_IA32ConditionOperand.LLT();
      } else if (cond.isGREATER()) {
	cond1 = OPT_IA32ConditionOperand.GT();
	cond2 = OPT_IA32ConditionOperand.LT();
	cond3 = OPT_IA32ConditionOperand.LGT();
      } else if (cond.isLESS_EQUAL()) {
	cond1 = OPT_IA32ConditionOperand.LT();
	cond2 = OPT_IA32ConditionOperand.GT();
	cond3 = OPT_IA32ConditionOperand.LLE();
      } else if (cond.isGREATER_EQUAL()) {
	cond1 = OPT_IA32ConditionOperand.GT();
	cond2 = OPT_IA32ConditionOperand.LT();
	cond3 = OPT_IA32ConditionOperand.LGE();
      } else {
	// I don't think we use the unsigned compares for longs,
	// so defer actually implementing them until we find a test case. --dave
	cond1 = cond2 = cond3 = null;
	OPT_OptimizingCompilerException.TODO();
      }

      OPT_BasicBlock myBlock = s.getBasicBlock();
      OPT_BasicBlock test2Block = myBlock.createSubBlock(s.bcIndex, ir, 0.25f);
      OPT_BasicBlock falseBlock = myBlock.splitNodeAt(s, ir);
      OPT_BasicBlock trueBlock = IfCmp.getTarget(s).target.getBasicBlock();
      
      falseBlock.recomputeNormalOut(ir);
      myBlock.insertOut(test2Block);
      myBlock.insertOut(falseBlock);
      myBlock.insertOut(trueBlock);
      test2Block.insertOut(falseBlock);
      test2Block.insertOut(trueBlock);
      ir.cfg.linkInCodeOrder(myBlock, test2Block);
      ir.cfg.linkInCodeOrder(test2Block, falseBlock);
      
      s.remove();
      
      myBlock.appendInstruction(MIR_Compare.create(IA32_CMP, R(xh), yh));
      myBlock.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						       cond1, trueBlock.makeJumpTarget(), new OPT_BranchProfileOperand(),
						       cond2, falseBlock.makeJumpTarget(), new OPT_BranchProfileOperand()));
      test2Block.appendInstruction(MIR_Compare.create(IA32_CMP, R(xl), yl));
      test2Block.appendInstruction(MIR_CondBranch.create(IA32_JCC, cond3, trueBlock.makeJumpTarget(), new OPT_BranchProfileOperand()));
    }
  }


  // the fcmoi/fcmoip was generated by burs
  // we do the rest of the expansion here because in some
  // cases we must remove a trailing goto, and we 
  // can't do that in burs!
  private static OPT_Instruction fp_ifcmp(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Operator op = s.operator();
    OPT_BranchOperand testFailed;
    OPT_BasicBlock bb = s.getBasicBlock();
    OPT_Instruction lastInstr = bb.lastRealInstruction();
    if (lastInstr.operator() == IA32_JMP) {
      // We're in trouble if there is another instruction between s and lastInstr!
      if (VM.VerifyAssertions) VM.assert(s.nextInstructionInCodeOrder() == lastInstr);
      // Set testFailed to target of GOTO
      testFailed = MIR_Branch.getTarget(lastInstr);
      nextInstr = lastInstr.nextInstructionInCodeOrder();
      lastInstr.remove();
    } else {
      // Set testFailed to label of next (fallthrough basic block)
      testFailed = bb.nextBasicBlockInCodeOrder().makeJumpTarget();
    }

    OPT_ConditionOperand c = IfCmp.getCond(s);
    OPT_BranchOperand target = IfCmp.getTarget(s);
    boolean UeqL = (op == DOUBLE_IFCMPL) || (op == FLOAT_IFCMPL);
    boolean UeqG = (op == DOUBLE_IFCMPG) || (op == FLOAT_IFCMPG);
    OPT_BranchOperand unorderedTarget;
    if (c.value == OPT_ConditionOperand.EQUAL || 
	(UeqL && (c.value == OPT_ConditionOperand.GREATER || 
		  c.value == OPT_ConditionOperand.GREATER_EQUAL)) || 
	(UeqG && (c.value == OPT_ConditionOperand.LESS || 
		  c.value == OPT_ConditionOperand.LESS_EQUAL))) {
      unorderedTarget = (OPT_BranchOperand)testFailed.copy();
    } else {
      unorderedTarget = (OPT_BranchOperand)target.copy();
    }
    
    // IMPORTANT: FCOMI only sets 3 of the 6 bits in EFLAGS, so
    // we can't just translate the condition operand as if it 
    // were an integer compare.
    // FCMOI sets ZF, PF, and CF as follows: 
    // Compare Results      ZF     PF      CF
    // left > right          0      0       0
    // left < right          0      0       1
    // left == right         1      0       0
    // UNORDERED             1      1       1
    if (c.isEQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2, 
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.EQ(),  // ZF == 1
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, testFailed));
    } else if (c.isNOT_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.EQ(),  // ZF == 1
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else if (c.isLESS()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LLT(), // CF == 1
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, testFailed));
    } else if (c.isGREATER()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LGT(), // ZF == 0 and CF == 0
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP,testFailed));
    } else if (c.isLESS_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LGT(), // ZF == 0 and CF == 0
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else if (c.isGREATER_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LLT(), // CF == 1
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else {
      throw new OPT_OptimizingCompilerException("Unexpected fp compare operation" + c.toString());
    }
    s.remove();
    return nextInstr;
  }


  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static OPT_Instruction threeValueFPCmp (OPT_Instruction s, OPT_IR ir) {
    // IMPORTANT: FCOMI only sets 3 of the 6 bits in EFLAGS, so 
    // we can't quite just translate the condition operand as if it 
    // were an integer compare.
    // FCMOI sets ZF, PF, and CF as follows: 
    // Compare Results      ZF     PF      CF
    // left > right          0      0       0
    // left < right          0      0       1
    // left == right         1      0       0
    // UNORDERED             1      1       1
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand two = (OPT_RegisterOperand) Binary.getClearVal2(s);
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s.remove();
    OPT_BasicBlock oneBlock = BB1.createSubBlock(0, ir, .33f);
    OPT_BasicBlock zeroBlock = BB1.createSubBlock(0, ir, .34f);
    OPT_BasicBlock minusOneBlock = BB1.createSubBlock(0, ir, .33f);
    OPT_BasicBlock unorderedBlock = oneBlock;
    if ((s.operator == DOUBLE_CMPL) || (s.operator == FLOAT_CMPL)) {
      unorderedBlock = minusOneBlock;
    }

    OPT_Register FP0 = ir.regpool.getPhysicalRegisterSet().getFPR(0);
    BB1.appendInstruction(MIR_Move.create(IA32_FMOV, R(FP0), one));
    BB1.appendInstruction(MIR_Compare.create(IA32_FCOMI, R(FP0), two));
    BB1.appendInstruction(MIR_CondBranch2.create(IA32_JCC2,
						 OPT_IA32ConditionOperand.PE(),  // PF == 1	
						 unorderedBlock.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.01f),
						 OPT_IA32ConditionOperand.EQ(),  // ZF == 1
						 zeroBlock.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.33f)));
    BB1.appendInstruction(MIR_CondBranch.create(IA32_JCC,
						OPT_IA32ConditionOperand.LLT(), // CF == 1
						minusOneBlock.makeJumpTarget(),
						new OPT_BranchProfileOperand(0.33f)));
    
    oneBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(1)));
    oneBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));

    zeroBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(0)));
    zeroBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));

    minusOneBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(-1)));
    minusOneBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));


    // fix CFG
    BB1.insertOut(oneBlock);
    BB1.insertOut(zeroBlock);
    BB1.insertOut(minusOneBlock);
    oneBlock.insertOut(BB6);
    zeroBlock.insertOut(BB6);
    minusOneBlock.insertOut(BB6);

    ir.cfg.linkInCodeOrder(BB1, oneBlock);
    ir.cfg.linkInCodeOrder(oneBlock, zeroBlock);
    ir.cfg.linkInCodeOrder(zeroBlock, minusOneBlock);
    ir.cfg.linkInCodeOrder(minusOneBlock, BB6);
    return nextInstr;
  }

  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static OPT_Instruction threeValueLongCmp (OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand lone = L(ir.regpool.getSecondReg(one.register));
    OPT_Operand two = Binary.getClearVal2(s);
    OPT_Operand ltwo;
    if (two instanceof OPT_RegisterOperand) {
      ltwo = L(ir.regpool.getSecondReg(((OPT_RegisterOperand)two).register));
    } else {
      OPT_LongConstantOperand tmp = (OPT_LongConstantOperand)two;
      two = I(tmp.upper32());
      ltwo = I(tmp.lower32());
    }
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s = s.remove();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB4 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB5 = BB1.createSubBlock(0, ir);
    BB1.appendInstruction(MIR_Compare.create(IA32_CMP, one, two));
    BB1.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						 OPT_IA32ConditionOperand.LT(),
						 BB4.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f),
						 OPT_IA32ConditionOperand.GT(),
						 BB5.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f)));
    BB2.appendInstruction(MIR_Compare.create(IA32_CMP, lone, ltwo));
    BB2.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						 OPT_IA32ConditionOperand.LLT(),
						 BB4.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f),
						 OPT_IA32ConditionOperand.LGT(),
						 BB5.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f)));
    BB3.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(0)));
    BB3.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));
    BB4.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(-1)));
    BB4.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));
    BB5.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(1)));
    // fix CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB4);
    BB1.insertOut(BB5);
    BB2.insertOut(BB3);
    BB2.insertOut(BB4);
    BB2.insertOut(BB5);
    BB3.insertOut(BB6);
    BB4.insertOut(BB6);
    BB5.insertOut(BB6);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);
    ir.cfg.linkInCodeOrder(BB4, BB5);
    ir.cfg.linkInCodeOrder(BB5, BB6);
    return nextInstr;
  }


  /*
   * This routine expands a yield_point instruction.
   * Split the yield point's basic block just after the yield point instruction.
   * Create a new yield point basic block that jumps to the thread switch code
   *   and then jumps to the split basic block.
   * Before the yield point, test if thread switch flag is on.
   *  Mutate yield point to a conditional jump if true to yield point 
   *  basic block. 
   * If options.FIXED_JTOC, then we can delay the yieldpoint expansion
   * until final mir expansion, since we can expand it without impacting
   * register allocation.
   */
  private static OPT_Instruction yield_point(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    if (ir.options.FIXED_JTOC) return nextInstr; // defer expansion until later

    s.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(0)));
    
    // get the correct method to be called for a thread switch
    VM_Method meth = null;
    if (s.getOpcode() == YIELDPOINT_PROLOGUE_opcode) {
      meth = VM_Entrypoints.optThreadSwitchFromPrologueMethod;
    } else if (s.getOpcode() == YIELDPOINT_EPILOGUE_opcode) {
      meth = VM_Entrypoints.optThreadSwitchFromEpilogueMethod;
    } else { 
      meth = VM_Entrypoints.optThreadSwitchFromBackedgeMethod;
    }

    // split the basic block after the yieldpoint
    OPT_BasicBlock thisBlock = s.getBasicBlock();
    OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(s,ir);
    
    // create a basic block at the end of the IR to hold the yieldpoint   
    OPT_BasicBlock yieldpoint = thisBlock.createSubBlock(s.bcIndex, ir, .00001f);
    thisBlock.insertOut(yieldpoint);
    yieldpoint.insertOut(nextBlock);
    ir.cfg.addLastInCodeOrder(yieldpoint);
    
    int offset = meth.getOffset();
    OPT_Operand jtoc = 
      OPT_MemoryOperand.BD(R(ir.regpool.getPhysicalRegisterSet().getPR()),
			   VM_Entrypoints.jtocField.getOffset(), 
			   (byte)4, null, TG());
    OPT_RegisterOperand regOp = ir.regpool.makeTempInt();
    yieldpoint.appendInstruction(MIR_Move.create(IA32_MOV, regOp, jtoc));
    OPT_Operand target =
      OPT_MemoryOperand.BD(regOp.copyD2U(), offset, (byte)4, 
			   new OPT_LocationOperand(offset), TG());
    
    // call thread switch
    OPT_Instruction call = 
      MIR_Call.create0(CALL_SAVE_VOLATILE, null, null, target, 
		       OPT_MethodOperand.STATIC(meth));
    call.markAsNonPEI();
    call.copyPosition(s);
    yieldpoint.appendInstruction(call);
    yieldpoint.appendInstruction(MIR_Branch.create(IA32_JMP,
						   nextBlock.makeJumpTarget())); 
    
    // Check to see if threadSwitch requested
    OPT_Register PR = ir.regpool.getPhysicalRegisterSet().getPR();
    int tsr = VM_Entrypoints.threadSwitchRequestedField.getOffset();
    OPT_MemoryOperand M = OPT_MemoryOperand.BD(R(PR),tsr,(byte)4,null,null);
    OPT_Instruction compare = MIR_Compare.create(IA32_CMP, M, I(0));
    s.insertBefore(compare);
    MIR_CondBranch.mutate(s, IA32_JCC, OPT_IA32ConditionOperand.NE(),
			  yieldpoint.makeJumpTarget(),
			  OPT_BranchProfileOperand.unlikely());
    return nextInstr;
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * <ul>
 * <li> Convert instructions with 3-operand binary ALU operators to use 
 *      2-operand ALU operators. 
 * <li> Convert instructions with 2-operand unary ALU operators to use 
 *      1-operand ALU operators.
 * </ul>
 * 
 * <pre>
 * In the most general case, we must do the following:
 *
 *  op r100 = r200, r300      =====>    move r100 = r200
 *                                      op   r100 <-- r300
 *
 * but there are several easy cases where we can avoid the move
 * 
 *  op r100 = r100, r300      =====>    op   r100 <-- r300
 *                                          
 *  op r100 = r200, r100      =====>    op   r100 <-- r200
 *  (if op is commutative) 
 *
 * but, we must be careful in this case. If r100 spans a basic block,
 * then we are better doing the following (since it will break the
 * BURS expression tree _after_ op).
 *
 *  op r100 = r200, r100      =====>    move rtemp = r200
 *  (if op is non commutative)          op   rtemp <-- r100
 *                                      move r100  = rtemp 
 *
 * We also keep our eyes open for the special (somewhat common) case 
 * of one of the uses being the last use of a temporary.  When this happens
 * we can sometimes avoid inserting a move at all. When this happens, we 
 * rewrite:
 * 
 *  op r100 = r200, r300     =====>    op r200 <-- r300
 * and replace all uses of r100 with r200. 
 *
 * We aren't doing a full live analysis, but the following conditions
 * covers the cases where it is critical to get this right:
 *  (a) r100 is ssa
 *  (b) r100 does not span a basic block
 *  (c) r200 does not span a basic block
 *  (d) this instruction is the last use of r200
 * These conditions are designed to be cheap to verify and 
 * cover those cases where it is advantegous from BURS's perspective to
 * coalesce the registers to avoid the move instruction.
 * 
 * If we are in the following very similar case:
 *  op r100 = r200, r300     =====>      op r200 <-- r300
 *                                           move r100 = r200
 *  (1) r200 does not span a basic block
 *  (2) this instruction is the last use of r200
 * then we want the move instruction here (but after op), because 
 * merging registers r100 and r200 would force BURS to break its 
 * exprssion trep _before_ op since r200 would now span a basic block 
 * (since r100 spans a basic block).
 * We depend on the register allocator to later coalesce r100 and r200,
 * since they are not simultaneously live.
 * Ditto (5) and (6) on r300 if op is commutative and r200 doesn't work out.
 * 
 * </pre>
 * @author Dave Grove
 */
final class OPT_ConvertALUOperators extends OPT_CompilerPhase 
  implements OPT_Operators {

  private static final boolean OPTIMIZE = true;

  final String getName() { return "ConvertALUOps"; }
  final OPT_CompilerPhase newExecution(OPT_IR ir) { return this; }
  final boolean printingEnabled (OPT_Options options, boolean before) {
    return false;
  }

  final void perform(OPT_IR ir) { 
    // Calling OPT_Simplifier.simplify ensures that the instruction is 
    // in normalized form. This reduces the number of cases we have to 
    // worry about (and does last minute constant folding on the off 
    // chance we've missed an opportunity...)
    // BURS assumes that this has been done, so we must do it even if
    // OPTIMIZE is false.
    for (OPT_InstructionEnumeration instrs = ir.forwardInstrEnumerator();
	 instrs.hasMoreElements();) {
      OPT_Instruction s = instrs.next(); 
      OPT_Simplifier.simplify(s);
    }

    if (OPTIMIZE) {
      // Compute simple ssa, u/d chains, spansBasicBlock
      // to catch some additional cases where we don't have to insert moves
      OPT_DefUse.computeDU(ir);
      OPT_DefUse.recomputeSSA(ir);
      OPT_DefUse.recomputeSpansBasicBlock(ir);
      for (OPT_Register reg = ir.regpool.getFirstRegister(); 
	   reg != null; 
	   reg = reg.getNext()) {
	markDead(reg);
      }
    }

    // Reverse pass over instructions supports simple live analysis.
    for (OPT_Instruction next, s = ir.lastInstructionInCodeOrder(); 
	 s != null; 
	 s = next) {
      next = s.prevInstructionInCodeOrder();
      
      switch(s.getOpcode()) {
      case BOOLEAN_NOT_opcode: unary(s, BOOLEAN_NOT_ACC, ir); break;

      case INT_ADD_opcode: commutative(s, INT_ADD_ACC, ir); break;
      case INT_SUB_opcode: noncommutative(s, INT_SUB_ACC, ir); break;
      case INT_MUL_opcode: commutative(s, INT_MUL_ACC, ir); break;
      case INT_SHL_opcode: noncommutative(s, INT_SHL_ACC, ir); break;
      case INT_SHR_opcode: noncommutative(s, INT_SHR_ACC, ir); break;
      case INT_USHR_opcode: noncommutative(s, INT_USHR_ACC, ir); break;
      case INT_AND_opcode: commutative(s, INT_AND_ACC, ir); break;
      case INT_OR_opcode: commutative(s, INT_OR_ACC, ir); break;
      case INT_XOR_opcode: commutative(s, INT_XOR_ACC, ir); break;
      case INT_NEG_opcode: unary(s, INT_NEG_ACC, ir); break;
      case INT_NOT_opcode: unary(s, INT_NOT_ACC, ir); break;

      case LONG_ADD_opcode: commutative(s, LONG_ADD_ACC, ir); break;
      case LONG_SUB_opcode: noncommutative(s, LONG_SUB_ACC, ir); break;
      case LONG_MUL_opcode: commutative(s, LONG_MUL_ACC, ir); break;
      case LONG_SHL_opcode: noncommutative(s, LONG_SHL_ACC, ir); break;
      case LONG_SHR_opcode: noncommutative(s, LONG_SHR_ACC, ir); break;
      case LONG_USHR_opcode: noncommutative(s, LONG_USHR_ACC, ir); break;
      case LONG_AND_opcode: commutative(s, LONG_AND_ACC, ir); break;
      case LONG_OR_opcode: commutative(s, LONG_OR_ACC, ir); break;
      case LONG_XOR_opcode: commutative(s, LONG_XOR_ACC, ir); break;
      case LONG_NEG_opcode: unary(s, LONG_NEG_ACC, ir); break;
      case LONG_NOT_opcode: unary(s, LONG_NOT_ACC, ir); break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case FLOAT_ADD_opcode: s.operator = FP_ADD; break;
      case DOUBLE_ADD_opcode: s.operator = FP_ADD; break;
      case FLOAT_SUB_opcode: s.operator = FP_SUB; break;
      case DOUBLE_SUB_opcode: s.operator = FP_SUB; break;
      case FLOAT_MUL_opcode: s.operator = FP_MUL; break;
      case DOUBLE_MUL_opcode: s.operator = FP_MUL; break;
      case FLOAT_DIV_opcode: s.operator = FP_DIV; break;
      case DOUBLE_DIV_opcode: s.operator = FP_DIV; break; 
      case FLOAT_REM_opcode: s.operator = FP_REM; break;
      case DOUBLE_REM_opcode: s.operator = FP_REM; break;
      case FLOAT_NEG_opcode: s.operator = FP_NEG; break;
      case DOUBLE_NEG_opcode: s.operator = FP_NEG; break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case INT_COND_MOVE_opcode: s.operator = CMOV; break;
      case REF_COND_MOVE_opcode: s.operator = CMOV; break;
      case FLOAT_COND_MOVE_opcode: s.operator = CMOV; break;
      case DOUBLE_COND_MOVE_opcode: s.operator = CMOV; break;
      case LONG_COND_MOVE_opcode: OPT_OptimizingCompilerException.TODO(); break;
      case GUARD_COND_MOVE_opcode: OPT_OptimizingCompilerException.TODO(); break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case INT_2FLOAT_opcode: s.operator = INT_2FP; break;
      case INT_2DOUBLE_opcode: s.operator = INT_2FP; break;
      case LONG_2FLOAT_opcode: s.operator = LONG_2FP; break;
      case LONG_2DOUBLE_opcode: s.operator = LONG_2FP; break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case REF_LOAD_opcode: s.operator = INT_LOAD; break;
      case REF_STORE_opcode: s.operator = INT_STORE; break;
      case REF_ALOAD_opcode: s.operator = INT_ALOAD; break;
      case REF_ASTORE_opcode: s.operator = INT_ASTORE; break;
      case REF_MOVE_opcode: s.operator = INT_MOVE; break;
      case REF_IFCMP_opcode: s.operator = INT_IFCMP; break;
      }

      if (OPTIMIZE) {
	// update liveness 
	for (OPT_OperandEnumeration defs = s.getPureDefs();
	     defs.hasMoreElements();) {
	  OPT_Operand op = defs.next();
	  if (op.isRegister()) {
	    markDead(op.asRegister().register);
	  }
	}
	for (OPT_OperandEnumeration uses = s.getUses(); // includes def/uses
	     uses.hasMoreElements();) {
	  OPT_Operand op = uses.next();
	  if (op.isRegister()) {
	    markLive(op.asRegister().register);
	  }
	}
      }
    }
  }

  private void commutative(OPT_Instruction s, OPT_Operator opCode, OPT_IR ir) {
    OPT_RegisterOperand result = Binary.getClearResult(s);
    OPT_Operand op1 = Binary.getClearVal1(s);
    OPT_Operand op2 = Binary.getClearVal2(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op2);
      return;
    }
    if (result.similar(op2)) {
      OPT_DefUse.removeUse(op2.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op1);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
      if (op2.isRegister()) {
	OPT_RegisterOperand rop2 = op2.asRegister();
	if (!rop2.register.spansBasicBlock() && isDead(rop2.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop2);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop2);
	    OPT_DefUse.mergeRegisters(ir, rop2.register, result.register);
	    rop2.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop2, op1);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop2);
	    OPT_DefUse.recordDefUse(rop2);
	    BinaryAcc.mutate(s, opCode, rop2, op1);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop2.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh, need some kind of move instruction
    OPT_Instruction move =   
      Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
    OPT_DefUse.updateDUForNewInstruction(move);
    s.insertBefore(move);
    OPT_DefUse.removeDef(result);
    OPT_DefUse.recordDefUse(result);
    if (op1.isRegister()) {
      OPT_DefUse.removeUse(op1.asRegister());
    }
    BinaryAcc.mutate(s, opCode, result, op2);
  }    

  private void noncommutative(OPT_Instruction s, OPT_Operator opCode, 
			      OPT_IR ir) {
    OPT_RegisterOperand result = Binary.getClearResult(s);
    OPT_Operand op1 = Binary.getClearVal1(s);
    OPT_Operand op2 = Binary.getClearVal2(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op2);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh need some move instructions after all.
    if (result.similar(op2)) {
      OPT_RegisterOperand tmp = ir.gc.temps.makeTemp(op1);
      OPT_Instruction move = 
	Move.create(getMoveOp(tmp.type), tmp.copyRO(), op1.copy());
      s.insertBefore(move);
      OPT_DefUse.updateDUForNewInstruction(move);
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(tmp);
      if (op1.isRegister()) {
	OPT_DefUse.removeUse(op1.asRegister());
      }
      BinaryAcc.mutate(s, opCode, tmp, op2);
      move = Move.create(getMoveOp(tmp.type), result.copyRO(), tmp.copyRO());
      s.insertAfter(move);
      OPT_DefUse.updateDUForNewInstruction(move);
    } else {
      OPT_Instruction move =   
	Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
      OPT_DefUse.updateDUForNewInstruction(move);
      s.insertBefore(move);
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      if (op1.isRegister()) {
	OPT_DefUse.removeUse(op1.asRegister());
      }
      BinaryAcc.mutate(s, opCode, result, op2);
    }
  }

  private void unary(OPT_Instruction s, OPT_Operator opCode, OPT_IR ir) {
    OPT_RegisterOperand result = Unary.getClearResult(s);
    OPT_Operand op1 = Unary.getClearVal(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      UnaryAcc.mutate(s, opCode, result);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    UnaryAcc.mutate(s, opCode, rop1);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    UnaryAcc.mutate(s, opCode, rop1);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh, need the move instruction before op.
    OPT_Instruction move =   
      Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
    OPT_DefUse.updateDUForNewInstruction(move);
    s.insertBefore(move);
    OPT_DefUse.removeDef(result);
    OPT_DefUse.recordDefUse(result);
    if (op1.isRegister()) {
      OPT_DefUse.removeUse(op1.asRegister());
    }
    UnaryAcc.mutate(s, opCode, result);
  }

  private static OPT_Operator getMoveOp(VM_Type t) {
    OPT_Operator op = OPT_IRTools.getMoveOp(t);
    if (op == REF_MOVE) { 
      return INT_MOVE;
    } else {
      return op;
    }
  }

  // Use the scratch field of the register to record 
  // dead/live for local live analysis.
  private static void markDead(OPT_Register r) {
    r.scratch = 0;
  }
  private static void markLive(OPT_Register r) {
    r.scratch = 1;
  }
  private static boolean isDead(OPT_Register r) {
    return r.scratch == 0;
  }
  private static boolean isLive(OPT_Register r) {
    return r.scratch == 1;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Normalize the use of constants in the LIR
 * to match the patterns supported in LIR2MIR.rules
 *
 * @author Dave Grove
 */
abstract class OPT_NormalizeConstants implements OPT_Operators {

  /**
   * Only thing we do for IA32 is to restrict the usage of 
   * String, Float, and Double constants.  The rules are prepared 
   * to deal with everything else.
   * 
   * @param ir IR to normalize
   */
  static void perform(OPT_IR ir) { 
    for (OPT_Instruction s = ir.firstInstructionInCodeOrder(); 
	 s != null; 
	 s = s.nextInstructionInCodeOrder()) {

      // Get 'large' constants into a form the the BURS rules are 
      // prepared to deal with.
      // Constants can't appear as defs, so only scan the uses.
      //
      int numUses = s.getNumberOfUses();
      if (numUses > 0) {
        int numDefs = s.getNumberOfDefs();
        for (int idx = numDefs; idx < numUses + numDefs; idx++) {
          OPT_Operand use = s.getOperand(idx);
          if (use != null) {
            if (use instanceof OPT_StringConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.JavaLangStringType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_StringConstantOperand sc = (OPT_StringConstantOperand)use;
              int offset = sc.index << 2;
              if (offset == 0)
                throw new OPT_OptimizingCompilerException("String constant w/o valid JTOC offset");
              OPT_LocationOperand loc = new OPT_LocationOperand(offset);
	      s.insertBefore(Load.create(INT_LOAD, rop, jtoc, new OPT_IntConstantOperand(offset), loc));
	      s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_DoubleConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.DoubleType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)use.copy();
              if (dc.index == 0) {
                dc.index = VM_Statics.findOrCreateDoubleLiteral(VM_Magic.doubleAsLongBits(dc.value));
              }
	      s.insertBefore(Binary.create(MATERIALIZE_FP_CONSTANT, rop, jtoc, dc));
              s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_FloatConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.FloatType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)use.copy();
              if (fc.index  == 0) {
                fc.index = VM_Statics.findOrCreateFloatLiteral(VM_Magic.floatAsIntBits(fc.value));
              }
	      s.insertBefore(Binary.create(MATERIALIZE_FP_CONSTANT, rop, jtoc, fc));
              s.putOperand(idx, rop.copyD2U());
	    } else if (use instanceof OPT_NullConstantOperand) {
	      s.putOperand(idx, new OPT_IntConstantOperand(0));
	    }
	  }
        }
      }
    }
  }

  /**
   * IA32 supports 32 bit int immediates, so nothing to do.
   */
  static OPT_Operand asImmediateOrReg (OPT_Operand addr, 
				       OPT_Instruction s, 
				       OPT_IR ir) {
    return addr;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.*;
import java.util.*;
import java.lang.reflect.*;

/**
 *  <P> Generates the assembler that is used by the optimizing compiler,
 * using a combination of the tables describing the low-level
 * instruction formats and operators used by the opt compiler, and the
 * interface of the low-level assembler that understands how to
 * generate IA32 opcodes given specific operands.  Essentially, the
 * opt assembler becomes a rather large piece of impedence-matching
 * code that decodes the OPT_Instructions and OPT_Operators understood
 * by the opt compiler to determine what is the appropriate IA32
 * machine code to emit.  </P>
 *
 *  <P> In order for this to work, both the optimizing compiler tables and
 * the VM_Assembler must use stylized formats.  On the optimizing
 * com[piler side, the major stylization is that the low-level
 * operators that represent assembly code must correspond directly to
 * the official IA32 assembler pneumonics; i.e. since there is an ADD
 * assembler pneumonic in the Intel assembly specification, there must
 * be a correponding IA32_ADD operator in the opt compiler tables.
 * The stylization of the VM_Assembler side is more thoroughgoing, and
 * the reader is referred to the VM_Assembler header comments for a
 * definition. </P>
 *
 *  <P> Given these stylizations, GenerateAssembler reads the set of
 * assembler pneumonics supported by the VM_Assembler using reflection
 * to examinme its stylized method signitures.  GenerateAssembler also
 * reads the set of IA32 operators that the opt compiler defines,
 * using the helper classes OPT_InstructionFormatTable and
 * OPT_OperatorFormatTable.  It then, for each operator, generates a
 * handler method to call the appropriate VM_Assembler emit method
 * given an OPT_Instruction.  The VM_Assembler will have a family of
 * emit methods named for each opcode, each such emit method takes a 
 * specific set of operand addressing modes and sizes.  The handler
 * methods that the GenerateAssembler emits examine the operands to an
 * OPT_Instruction, and determine which VM_Assembler method to call
 * for the operand addressing modes and sizes that it finds.
 * GenerateAssembler also generates a top-level dispatch method that
 * examines the operator and calls the appropriate handler. </P>
 *
 *  <P> GenerateAssembler generates the opt assembler as part of the
 * normal build process; this poses a slight problem in that it needs
 * to examine the VM_Assembler via reflection to generate the
 * OPT_Assembler, but that is not possible until the VM sources
 * (including, of course, the OPT_Assembler) have been compiled.  The
 * current hack to get around this is to compile the VM_Assembler in
 * advance, and read the resulting class file.  This utilizies some
 * supporting files to make the VM_Assembler compile in isolation.
 * This is the purpose of the .fake files in the optimizing compiler's
 * assembler directory. </P>
 *
 * @see OPT_InstructionFormatTables
 * @see OPT_OperatorFormatTables
 * @see OPT_AssemblerBase
 * @see OPT_Instruction
 * @see OPT_Assembler
 * @see VM_Assembler
 *
 * @author Julian Dolby 
 */
public class GenerateAssembler {

    /** Global flag controlling printing of debugging information */
    static final boolean DEBUG = false;

    /** Global reference to the assembler being generated */
    static FileWriter out;

    /**
     * Write a single string to the assembler source file.
     * @param String s  The string to be written
     */
    private static void emit(String s) {
	try {
	    out.write(s, 0, s.length());
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit(-1);
	}
    }

    /**
     * Write tabification to the assembler source file.  This is used 
     * to make the generates source more readable by identing it.
     * @param int level  The level of indentation to generate
     */
    private static void emitTab(int level) {
	for(int i = 0; i < level; i++) emit("  ");
    }

    /**
     *  Global reference to the OPT_InstructionFormatTables class that 
     * contains descriptions of each optimizing compiler instruction
     * format that sis visible to the assembler (i.e. the MIR_* 
     * instruction formats.
     *
     * @see OPT_InstructionFormatTables
     */
    private static Class formats;

    /**
     *  Load the instruction format table, and throw up if that is
     * not possible.
     */
    static {
	try {
	    formats = Class.forName("OPT_InstructionFormatTables");
	} catch (ClassNotFoundException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }

    /**
     *  Global reference to the opcode argument table for the current
     * opcode being processed.  This table is null unless some of the
     * operands in the OPT_Instruction are to ignored when generating
     * code for the opcode.  Ignoring arguments is an ad-hock special
     * case that is controlled by the global opcodeArgTable.
     */
    static int[] currentOpcodeArgTable;

    /**
     *  Global reference to the table of symbolic names of the arguments
     * to the current MIR_ instruction format.  This information is read
     * from the OPT_InstructionFormatTables
     */
    static String[] currentOpcodeSymbolicNames;

    /**
     *  The current IA32 opcode being processed.  This is the name of
     * IA32 instruction.  Typically, it is the name of the opt compiler
     * IA32_* opcode as well, but there are exceptions in that multiple
     * IA32_* opcodes can map to the same IA32 instruction
     */
    static String currentOpcode;

    /**
     *  The instruction format for the IA32_* opt compiler opcode(s)
     * being processed. 
     *
     */
    static String currentFormat;

    /**
     *  Global table mapping opt compiler IA32_* opcodes to arrays
     * listing the set of OPT_Instruction operands that are to be used
     * as arguments to the IA32 architecture instruction.  This is used
     * when an instruction has extra operands that are not used in
     * assembly (e.g. CALL) has mappings only for such instructions.
     */
    static Hashtable opcodeArgTables;

    /**
     *  Initialize the opcodeArgTables table
     */
    static {
	opcodeArgTables = new Hashtable();
	opcodeArgTables.put("CALL", new int[]{2});
	opcodeArgTables.put("INT", new int[]{1});
	opcodeArgTables.put("CDQ", new int[]{0});
	opcodeArgTables.put("DIV", new int[]{1,2});
	opcodeArgTables.put("IDIV", new int[]{1,2});
	opcodeArgTables.put("MUL", new int[]{1,2});
	opcodeArgTables.put("IMUL1", new int[]{1,2});
	opcodeArgTables.put("DIV", new int[]{1,2});
	opcodeArgTables.put("IDIV", new int[]{1,2});
	opcodeArgTables.put("SET", new int[]{1,0});
	opcodeArgTables.put("CMPXCHG", new int[]{1,2});
	opcodeArgTables.put("FCMOV", new int[]{2,0,1});
	opcodeArgTables.put("CMOV", new int[]{2,0,1});
    }

    /**
     *  Set the current opcode.  This sets four global fields:
     * the currentOpcode, the currentOpcodeArgTable, the currentFormat
     * and the currentOpcodeSymbolicNames.
     *
     * @param opcode  The IA32 architecture opcode to make the current opcode
     */
    static void setCurrentOpcode(String opcode) {
	try {
	    currentOpcode = opcode;
	    currentOpcodeArgTable = (int[]) opcodeArgTables.get( opcode );
	    currentFormat = OPT_OperatorFormatTables.getFormat( opcode );
	    Field f = formats.getDeclaredField(currentFormat+"ParameterNames");
	    currentOpcodeSymbolicNames = (String[]) f.get( null );
	} catch (Throwable e) {
	    System.err.println("Cannot handle VM_Assembler opcode " + opcode);
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }

    /**
     * Constant representing immediate arguments to VM_Assembler calls
     */
    static final int Immediate = 0;
    /**
     * Constant representing register arguments to VM_Assembler calls.
     * This covers the cases when a register is encoded into the mod/rm
     * byte; the VM_Assembler handles the detais of generating either 
     * the reg bits of the mod/rm byte or encoding a register as mod 11.
     */
    static final int Register = 1;
    /**
     * Constant representing condition arguments to VM_Assembler calls.
     * Such operands are not arguments to the ultimate IA32 machine 
     * code instruction, but they are used to calculate the opcode that
     * is generated.
     */
    static final int Condition = 2;
    /**
     * Constant representing arguments to VM_Assembler calls that use the
     * scaled-index-base (SIB) addressing mode in the special way that uses
     * neither a base not an index to generate an absolute address
     */
    static final int Absolute = 3;
    /**
     * Constant representing IA32 memory operands that use register-
     * displacement addressing mode (usually mod bits 01 and 10) arguments 
     * to VM_Assembler calls.  The VM_Assembler takes care of choosing the
     * right mode for the size of the displacement, so this one mode
     * covers two of the four addressing modes the IA32 has.  The
     * VM_Assembler also handles the special cases in which this mode
     * requires weird SIB bytes.
     */
    static final int RegisterDisplacement = 4;
    /**
     * Constant representing arguments to VM_Assembler calls that use the
     * scaled-index-base (SIB) addressing mode in the special way that does
     * not use a base register.  The OPT_Assembler simply assumes it has
     * an [index < < scale + disp] addressing mode, and the VM_Assembler takes
     * care of generating the special mod/rm that causes the base register
     * to be ignored.
     */
    static final int RegisterOffset = 5;
    /**
     * Constant representing scaled-index-base (SIB) mode arguments to 
     * VM_Assembler calls.
     */
    static final int RegisterIndexed = 6;
    /**
     * Constant representing register-indirect arguments to VM_Assembler 
     * calls.  This mode handles what is (usually) mod 00 in the mod/rm
     * byte.
     */
    static final int RegisterIndirect = 7;
    /**
     * Constant representing labels used as branch targets.  While code
     * is being generated, the machine code offset for a forward branch
     * cannot, in general, be computed as the target code has not been
     * generated yet.  The OPT_Assembler uses synthetic code offsets,
     * based upon the order of OPT_Instructions in the code being 
     * compiled, to communicate forward branch targets to the 
     * VM_Assembler.  These synthetic offsets are passed to the
     * VM_Assembler where it expected Label arguments.
     */
    static final int Label = 8;
    /**
     * Constant representing arguments to VM_Assembler calls in which
     * it may be either a backward branch target (resolved to an
     * immediate being the exact branch displacement) or a forward
     * branch (which will be a synthetic Label).
     */
    static final int LabelOrImmediate = 9;

    /**
     * How many different sizes of instruction operand are there, not
     * counting the standard double word.
     */
    static final int SIZES = 3;
    /**
     * Constant representing instructions that operate upon bytes
     */
    static final int Byte = 10;
    /**
     * Constant representing instructions that operate upon words (16 bits)
     */
    static final int Word = 11;
    /**
     * Constant representing instructions that operate upon quad words (64 bits)
     */
    static final int Quad = 12;

    /**
     *  This array denotes all possible encodings in a VM_Assembler emitter
     * function.  It includes all possible operand types and all possible
     * instruction sizes.  For all of the constants corresponding to a 
     * possible operand type or instruction size, the corresponding entry
     * is this table holds the string that the VM_Assembler uses to denote
     * that operand type or instruction size.
     *
     * This table is used when parsing a VM_Assembler emitter name to create 
     * a descriptor that denotes the operand size and types of the given
     * emitter in terms of the constants.
     *
     * This table is also used when generating the OPT_Assembler emitter
     * functions to allow the generator to pick which queries to use to
     * dispatch an OPT_Instruction to the appropriate VM_Assembler emitter.
     */
    static final String[] encoding = 
    {"Imm",		// encoding[Immediate]
     "Reg",		// encoding[Register]
     "Cond",		// encoding[Condition]
     "Abs",		// encoding[Absolute]
     "RegDisp",		// encoding[RegisterDisplacement]
     "RegOff",		// encoding[RegisterOffset]
     "RegIdx",		// encoding[RegisterIndexed]
     "RegInd",		// encoding[RegisterIndirect]
     "Label",		// encoding[Label]
     "ImmOrLabel",	// encoding[LabelOrImmediate]
     "Byte",
     "Word",
     "Quad"};

    /**
     * For a given string representing a valid operand encoding for the 
     * VM_Assembler, return the corresponding OPT_Assembler constant.  This
     * function only looks for encodings of operand types, and will not
     * accept strings that correspond to size encodings.
     *
     * @param str A valid VM_Assembler encoding of operand type
     * @return The OPT_Assembler constant corresponding to str, or -1 if none
     *
     */
    private static int getEncoding(String str) {
	for(int i = 0; i < encoding.length - SIZES; i++)
	    if (encoding[i].equals(str))
		return i;

	return -1;
    }

    /**
     * For a given string representing a valid size encoding for the 
     * VM_Assembler, return the corresponding OPT_Assembler constant.  This
     * function only looks for encodings of sizes, and will not accept 
     * strings that correspond to operand types.
     *
     * @param str A valid VM_Assembler encoding of operand size
     * @return The OPT_Assembler constant corresponding to str, or -1 if none
     *
     */
    private static int getSize(String str) {
	for(int i = encoding.length - SIZES; i < encoding.length; i++)
	    if (encoding[i].equals(str))
		return i;

	return -1;
    }

    /**
     * For a given operand number, return a string which is a valid Java
     * expression for reading that operand out of the current instruction.
     * This function uses the currentOpcodSymbolicNames table to determine
     * the appropriate accessor (e.g. getValue if the current name is Value),
     * and it uses the currentOpcodeArgTable (in cases where it has an
     * entry for the kind of instruction being processed) to determine which
     * operand in OPT_Instruction corresponds to operand sought.
     *
     * @param op  The operand number sought.
     * @return A Java expression for adcessing the requested operand.
     */
    private static String getOperand(int op) {
	try {
	    if (currentOpcodeArgTable == null)
		return currentFormat + ".get" + currentOpcodeSymbolicNames[op] + "(inst)";
	    else
		return currentFormat + ".get" + currentOpcodeSymbolicNames[currentOpcodeArgTable[op]] + "(inst)";
	} catch (ArrayIndexOutOfBoundsException e) {
	    System.err.println(currentOpcode + ": cannot access operand " + op  + ":");
	    for(int i = 0; i < currentOpcodeSymbolicNames.length; i++)
		System.err.println( currentOpcodeSymbolicNames[i] );
	    System.exit( -1 );
	    return null;
	}
    }

    /**
     * Given an operand number and an encoding, generate a test to
     * determine whether the given operand matches the encoding.  That
     * is, generate code to the OPT_Assembler that examines a given operand
     * of the current OPT_Instruction, and determines whether it is of
     * the type encoded by the given encoding.  This is used to generate the
     * if statements of the dispatch functions for each opt compiler opcode.
     *
     * @param argNumber The argument to examine
     * @param argEncoding The encoding for which to check 
     */
    private static void emitTest(int argNumber, int argEncoding) {   
	if (argEncoding < encoding.length - SIZES)
	    emit("is" + encoding[argEncoding] + "(" + getOperand(argNumber) + ")");
	else
	    emit("is" + encoding[argEncoding] + "(inst)");
    }

    /**
     * Generate code to verify that a given operand matches a given encoding.
     * Since the IA32 architecture is not exactly orthogonal (please note
     * the charitable understatement), there are cases when the opt assembler
     * can determine the VM_Assembler emitter to call without looking at
     * all (or, in some cases, any) of the arguments of the OPT_Instruction.
     * An example is the ENTER instruction that only takes one immediate
     * parameter, so the opt assembler could simply call that VM_Assembler
     * emiiter without checking that argument is really an immediate. In 
     * such cases, the opt assembler generates guarded tests that verify 
     * that OPT_Instruction operand actually matches the required encoding.
     * This function emits such tests to the assembler being generated.
     *
     * @param argNumber The argument to examine
     * @param argEncoding The encoding for which to check
     * @param level current level for generating pretty, tabified output
     */
    private static void emitVerify(int argNumber, int argEncoding, int level) {   
	emitTab(level);
	emit("if (VM.VerifyAssertions && !");
	emitTest(argNumber, argEncoding);
	emit(") VM.assert(false, inst.toString());\n");
    }

    /**
     * Generate code to fetch all the arguments needed for a given operand
     * number and encoding.  The different argument encodings of the
     * VM_Assembler need different arguments to be passed to the emitter
     * function.  For instance, a register-displacement mode operand
     * needs to be given a base register and an immediate displacement.
     * This function generates the appropriate arguments given the
     * operand number and encoding; that is, it generates reads of the
     * appropriate OPT_Instruction argument and fetches of the appropriate
     * pieces of information from the operand.
     * 
     * @param argNumber The argument being generated.
     * @param argEcoding The encoding to use.
     */
    private static void emitArgs(int argNumber, int argEncoding) {
	String op = getOperand(argNumber);
	if (argEncoding == LabelOrImmediate)
	    emit("getImm(" + op + "), getLabel(" + op + ")");
    	else if (argEncoding == RegisterDisplacement)
	    emit("getBase(" + op + "), getDisp(" + op + ")");
	else if (argEncoding == Absolute)
	    emit("getDisp(" + op + ")");
	else if (argEncoding == RegisterOffset)
	    emit("getIndex(" + op + "), getScale(" + op + 
		 "), getDisp(" + op + ")");
	else if (argEncoding == RegisterIndexed)
	    emit("getBase(" + op + "), getIndex(" + op + 
		 "), getScale(" + op + "), getDisp(" + op + ")");
	else if (argEncoding == RegisterIndirect)
	    emit("getBase(" + op + ")");
	else 
	    emit("get" + encoding[argEncoding] + "(" + op + ")");
    }

    /**
     *  This exception class is used to indicate that GenerateAssembler
     * found an emit* method in the vM_Assembler that it does not 
     * understand. To generate the OPT_Assembler for a given 
     * IA32 OPT_Operator, GenerateAssembler looks at all of the emit* 
     * methods for the corresponding IA32 opcode in the VM_Assembler.  It 
     * parses each name to determine what kinds of operands it expects and
     * what size operands it uses; this requires the emit* methods to
     * have stylized names (see the header comment of VM_Assembler for 
     * details).  If an emit* method name does not have the stylized 
     * format required, GenerateAssembler will throw a BadEmitMethod
     * exception and abort.
     */
    static class BadEmitMethod extends RuntimeException {

	/**
	 *  Create a BadEmitMethod exception indicating that 
	 * GenerateAssembler cannot understand the code portion
	 * of the method name methodName.
	 *
	 * @param methodName The method name causing trouble
	 * @param code The portion of methodName that does not parse
	 */
	BadEmitMethod(String methodName, String code) {
	    super("cannot interpret method " + methodName + "(" + code + ")");
	}

    }

    /**
     *  An EmitterDescriptor represents a single emit method from the
     * VM_Assembler: it explicitly represents the types of operands the
     * method expects, their number, and the size of the data it uses.
     * When GenerateAssembler encounters an emit* method from the 
     * VM_Assembler, it creates an EmitterDescriptor for it.  Based upon 
     * the stlyized form the method name is required to have, the
     * EmitterDexcriptor represents information about its arguments. This 
     * information is stored in terms of the GenerateAssembler constants 
     * that represent operand type and size.
     * <P>
     * The EmitterDescriptor class encapsulates the logic for parsing the 
     * stylized emit* method names that the VM_Assembler has, and turning
     * them into the explicit representation that GenerateAssembler uses.  
     * If parsing a name fails, a {@link GenerateAssembler.BadEmitMethod} 
     * runtime exception is thrown and assembler generation is aborted.
     * <P>
     * <HR>
     * <EM>See the descriptions of the GenerateAssembler constants:</EM>
     * <DL>
     * <DT> <EM>Operand types</EM>
     * <DI> 
     *  <UL>
     *   <LI> {@link #Immediate}
     *   <LI> {@link #Label}
     *   <LI> {@link #LabelOrImmediate}
     *   <LI> {@link #Absolute}
     *   <LI> {@link #Register}
     *   <LI> {@link #RegisterIndirect}
     *   <LI> {@link #RegisterOffset}
     *   <LI> {@link #RegisterIndexed}
     *  </UL>
     * <DT> <EM>Data size</EM>
     *  <UL>
     *   <LI> {@link #Byte}
     *   <LI> {@link #Word}
     *   <LI> {@link #Quad}
     *  </UL>
     * </DL>
     */
    static class EmitterDescriptor {
	private int size;
	private int count;
	private final int args[];

	/**
	 * Create an EmitterDescriptor for the given methodName.  This 
	 * conmstructor creates a descriptor that represents explicitly 
	 * the types and size of the operands of the given emit* method.
	 * This constructor encapsulate the logic to parse the given
	 * method name into the appropriate explicit representation.
	 */
	EmitterDescriptor(String methodName) {
	    StringTokenizer toks = new StringTokenizer(methodName, "_");
	    toks.nextElement(); // first element is emitXXX;
	    args = new int[ toks.countTokens() ];
	    this.size = 0;
	    this.count = 0;
	    for(int i = 0; i < args.length; i++) {
		String cs = toks.nextToken();
		int code = getEncoding(cs);
		int size = GenerateAssembler.getSize(cs);

		if (DEBUG) {
		    System.err.println(methodName + "[" + i + "] is " + code + "," + size + " for " + cs);
		}

		if (code != -1)
		    args[count++] = code;
		else if (size != -1)
		    this.size = size;
		else
		    throw new BadEmitMethod( methodName, cs );
	    }
	}

	/**
	 *  This method checks whether the emit* method represented by
	 * this EmitterDescriptor expects the argument type represented
	 * by enc as its argument'th operand.  If enc is an operand type
	 * encoding, this method checks wether the given argument is of
	 * the appropriate type.  If enc is an operand size encoding,
	 * the argument parameter is ignored, and this method checks
	 * whether the emit* method represented operates upon data of
	 * the desired size.
	 * <P>
	 * <EM>See the descriptions of the GenerateAssembler constants:</EM>
	 * <DL>
	 * <DT> <EM>Operand types</EM>
	 * <DI> 
	 *  <UL>
	 *   <LI> {@link #Immediate}
	 *   <LI> {@link #Label}
	 *   <LI> {@link #LabelOrImmediate}
	 *   <LI> {@link #Absolute}
	 *   <LI> {@link #Register}
	 *   <LI> {@link #RegisterIndirect}
	 *   <LI> {@link #RegisterOffset}
	 *   <LI> {@link #RegisterIndexed}
	 *  </UL>
	 * <DT> <EM>Data size</EM>
	 *  <UL>
	 *   <LI> {@link #Byte}
	 *   <LI> {@link #Word}
	 *   <LI> {@link #Quad}
	 *  </UL>
	 * </DL>
	 * <P>
	 * @param argument The operand number examined 
	 * @param enc The argument type queried, as encoded as one of
	 *    the operand type constants used throughout 
	 *    GenerateAssembler.
	 *
	 * @return True if this method expects an argument type encoded
	 *    by enc as its argument'th operand, and false otherwise.
	 */
	boolean argMatchesEncoding(int argument, int enc) {
	    if (enc < encoding.length - SIZES)
		return (count > argument) && args[argument] == enc;
	    else
		return size == enc;
	}

	/**
	 * Access the array that stores the encodings of the arguments
	 * to the emit method represented by this EmitterDescriptor.
	 *
	 * @return the array of argument encodings
	 */
	int[] getArgs() { return args; }

	/**
	 * Access the data size operated upon by emit method represented 
	 * by this EmitterDescriptor.
	 *
	 * @return data size for this descriptor
	 */
	int getSize() { return size; }

	/**
	 * Access the number of operands operated upon by emit method 
	 * represented by this EmitterDescriptor.
	 *
	 * @return number of operands for this descriptor
	 */
	int getCount() { return count; }

	public String toString() {
	    StringBuffer s = new StringBuffer();
	    s.append ("ed:");
	    for(int i = 0; i < count; i++)
		s.append(" " + encoding[args[i]]);
	    if (size != 0) s.append(" (" + encoding[size] + ")");
	    return s.toString();
	}
    }

    /**
     *  An EmitterSet represents a set of emit methods from the
     * VM_Assembler for the same IA32 assembler opcode.  These sets
     * are used when generating the do<opcode> method for a given IA32
     * opcde: first an EmitterSet of all the VM_Assembler emit methods
     * for that opcode is built, and then the do method is recursively
     * generated by emitting operand type and size tests that
     * partition the set of emitters into two smaller sets.  This
     * continues until the set is a singleton
     */
    static class EmitterSet {

	/**
	 *  The VM_Assembler emit methods that this set represents.
	 * This is a set of EmitterDescriptor objects.
	 */
	private final Set emitters = new HashSet();

	/**
	 * Print this EmitterSet readably.
	 * @return a string describing this EmitterSet
	 */
	public String toString() {
	    StringBuffer s = new StringBuffer();
	    s.append("Emitter Set of:\n");
	    Iterator i = emitters.iterator();
	    while (i.hasNext()) 
		s.append(i.next().toString() + "\n");
	    
	    s.append("-------------\n");
	    return s.toString();
	}

	/**
	 *  Test whethe rthis EmitterSet as exactly one element.
	 * @return true if this EmitterSet as exactly one element.
	 */
	boolean isSingleton() {
	    return  (emitters.size() == 1);
	}

	/**
	 *  Insert an EmitterDescriptor into this set
	 * @param ed the EmitterDescriptor to insert
	 */
	void add(EmitterDescriptor ed) {
	    emitters.add( ed );
	}

	/**
	 *  Count how many of the emit represented by this set match a
	 * given operand type and size encoding.  This method is used
	 * (via getEncodingSplit) while recursively partitioning a
	 * given EmitterSet to determine how evenly (or even whether)
	 * a given operand type and size splits this set.
	 *
	 * @see #getEncodingSplit
	 *
	 * @param n the operand being examined
	 * @param code the operand type or size code being considered
	 * @return the number of emit methods of which the specified
	 *         operand type matches the specified one.  */
	private int countEncoding(int n, int code) {
	    Iterator i = emitters.iterator();
	    int count = 0;
	    while (i.hasNext())
		if (((EmitterDescriptor)i.next()).argMatchesEncoding(n, code))
		    count++;
	    return count;
	}

	/**
	 *  Return the difference between the number of emit methods
	 * in this set that match a given operand type and size for a
	 * given operand, and the number of those that do not. This
	 * method is used while recursively partitioning a given
	 * EmitterSet to determine how evenly (or even whether) a
	 * given operand type and size splits this set.
	 *
	 * @param n the operand being examined
	 * @param code the operand type or size code being considered
	 * @return the different between matching and non-matching
	 *         emit method in this set.  */
	private int getEncodingSplit(int n, int code) {
	    int count = countEncoding(n, code);
	    return Math.abs( (emitters.size() - count) - count );
	}

	/**
	 * This class is used just to communicate the two results of
	 * searching for the best split for a given set: the chosen
	 * operand type or size, and the chosen operand nummber.  This
	 * class is basically to avoid writing the slew of required
	 * type casts that a generic pair would need given Java's
	 * primitive type system.
	 *
	 * @see #makeSplit
	 * @see #split
	 */
	static class SplitRecord {
	    /**
	     * The operand number to be split.
	     */
	    int argument;

	    /**
	     * The operand type or size test on which to split.
	     */
	    int test;

	    /**
	     * Make s split record to communicate the results of
	     * searching for the best operand to split.
	     *
	     * argument The operand number to be split.
	     * test The operand type or size test on which to split.
	     */
	    SplitRecord(int argument, int test) {
		this.argument = argument;
		this.test = test;
	    }
	}

	/**
	 * This method uses a SplitRecord as the criertion to
	 * partition the given EmitterSet into two subsets.
	 *
	 * @param split the plit record dicatating how to split
	 */
	private EmitterSet[] makeSplit(SplitRecord split) {
	    int arg = split.argument;
	    int test = split.test;
	    EmitterSet yes = new EmitterSet();
	    EmitterSet no = new EmitterSet();
	    Iterator i = emitters.iterator();
	    while (i.hasNext()) {
		EmitterDescriptor ed = (EmitterDescriptor) i.next();
		if (ed.argMatchesEncoding(arg, test))
		    yes.add( ed );
		else
		    no.add( ed );
	    }

	    return new EmitterSet[]{yes, no};
	}

	/**
	 *  Find the best operand type or size and operand number to
	 * partition this EmitterSet.  This method searches across all
	 * possible ways of splitting this set--all possible operand
	 * types and sizes, and all possible operands--to determine
	 * which one splits the set most evenly.  
	 *
	 * @return a SplitRecord representing the most-even split
	 */
	SplitRecord split() {
	    int splitArg = -1;
	    int splitTest = -1;
	    int splitDiff = 1000;
	    for(int arg = 0; arg < 4; arg++) {
		for (int test = 0; test < encoding.length; test++) {
		    int c = getEncodingSplit(arg, test);
		    if (c == 0)
			return new SplitRecord(arg, test);
		    else if (c < splitDiff) {
			splitArg = arg;
			splitTest = test;
			splitDiff = c;
		    }
		}
	    }

	    return new SplitRecord(splitArg, splitTest);
	}

	/**
	 *  Emit the Java code to call a particular emit method for a
	 * particular opcode.  This method takes representations of
	 * the opcode and operands of a given emit method, and
	 * generates the appropriate Java source code to call it.  It
	 * synthesizes the encoded emit method name, and uses emitArgs
	 * to pass all the required arguments.
	 *
	 * @see #emitArgs
	 *
	 * @param opcode the IA32 opcode of the emit method
	 * @param args the encoding of each operand to the emit method
	 * @param count the number of operands
	 * @param level the level of tabbing for pretty output
	 */
	private void emitEmitCall(String opcode, int[] args, int count, int level, int size) {
	    emitTab(level);
	    emit("emit" + opcode);
	    for(int i = 0; i < count; i++)
		emit("_" + encoding[args[i]]);
	    if (size != 0) emit("_" + encoding[size]);

	    if (count == 0)
		emit("();\n");
	    else {
		emit("(");
		for(int i = 0; i < count; i++) {
		    emit("\n");
		    emitTab(level+1);
		    emitArgs(i, args[i]);
		    if (i == count-1)
			emit(");\n");
		    else
			emit(",");
		}
	    }
	}

	/**
	 *  Write the Java code required for error checking and
	 * calling the emit method represented by a singleton
	 * EmitterSet.  A singleton EmiiterSet will typically be the
	 * result of a series of splits of bigger sets, where the
	 * splits represent emitted queries of operand types and
	 * sizes.  (See emitSet) However, there may be cases when some
	 * operand has only one possible options, so the splitting
	 * will not have generated any tests for it.  In this case, we
	 * will emit assertions that guarantee the operand is of the
	 * expected type.  Note that the answers to queries alrrready
	 * performed by splitting are known to be fine, so no
	 * additional error checking is needed for cases they cover.
	 *
	 * @see #emitSet
	 *
	 * @param opcode the IA32 opcode to generate
	 * @param testsPerformed the set of queries already performed
	 *        by splitting.  
	 * @param level level of indentation for prett printing */
	private void emitSingleton(String opcode, boolean[][] testsPerformed, int level) {
	    EmitterDescriptor ed = 
		(EmitterDescriptor) emitters.iterator().next();

	    int[] args = ed.getArgs();
	    int count = ed.getCount();
	    for(int i = 0; i < count; i++) 
		if (! testsPerformed[i][args[i]])
		    emitVerify(i, args[i], level);

	    int size = ed.getSize();
	    if (size != 0) {
		boolean needed = true;

		for(int i = 0; i < count; i++) 
		    if (testsPerformed[i][size])
			needed = false;
		    
		if (needed)
		    emitVerify(0, size, level);

		if (size == Byte)
		    for(int i = 0; i < count; i++) 
			if (args[i] == Register)
			    if (currentOpcode.indexOf("MOVZX") == -1 &&
				currentOpcode.indexOf("MOVSX") == -1)
			    {
				emitTab(level);
				emit("if (VM.VerifyAssertions && !(");
				emitArgs(i, Register);
				emit(" < 4)) VM.assert(false, inst.toString());\n");
			    }
		
	    }

	    emitEmitCall(opcode, args, count, level, ed.getSize());
	}

	/**
	 *  Emit Java code for deciding which emit method in the given
	 * set applies to an OPT_Instruction, and then calling the
	 * apprpriate method.  The method essentially works by
	 * recursively parititioning the given set into two smaller
	 * pieces until it finds a set with only one element.  On each
	 * partition, this method generates code for the appropriate
	 * operand type or size query, and then calls itself
	 * recursively on the two sets resulting from the partition.
	 *
	 * This method uses split to determine what test to apply, and
	 * emitSingleton when it encounteres a singleton set.
	 *
	 * Note that the testsPerformed parameter is not needed to do
	 * the recursive splitting; this is passed to emitSingleton to
	 * help it generate appropriate error checking for operands.
	 *
	 * @see #split
	 * @see #emitSingleton
	 *
	 * @param opcode the IA32 opcode being generated
	 * @param testsPerformed the set of tests already performed
	 * @param level the indentation level for pretty printing
	 *
	 */
	private void emitSet(String opcode, boolean[][] testsPerformed, int level) {
	    if (emitters.isEmpty()) {
		// do nothing
	    } else if (isSingleton())
		emitSingleton(opcode, testsPerformed, level);
	    else {
		SplitRecord rec = split();

		if (DEBUG) {
		    for(int i = 0; i < level; i++) System.err.print("  ");
		    System.err.println("split of " + opcode + "[" + rec.argument + "] for " + encoding[rec.test]);
		}

		if (testsPerformed[rec.argument][rec.test] == true) {
		    System.err.println("repeated split of " + opcode + "[" + rec.argument + "] for " + encoding[rec.test]);
		    System.err.println( this );
		    System.exit( -1 );
		}

		testsPerformed[rec.argument][rec.test] = true;
		EmitterSet[] splits = makeSplit(rec);
		emitTab(level);	emit("if (");
		emitTest( rec.argument, rec.test );
		emit(") {\n");
		splits[0].emitSet(opcode, testsPerformed, level+1);
		emit("\n"); emitTab(level); emit("} else {\n");
		splits[1].emitSet(opcode, testsPerformed, level+1);
		emitTab(level); emit("}\n");
		testsPerformed[rec.argument][rec.test] = false;
	    }
	}
    }

    /**
     * the Class object of the VM_Assembler.  This is used for
     * reflective inquiries about emit methods.
     *
     * @see #main
     */
    static Class lowLevelAsm;

    /**
     * Computes the set of emit methods in the VM_Assembler for a
     * given IA32 opcode.
     *
     * @param emitters the set of all emit methods
     * @param opcode the opcode being examined
     */
    private static EmitterSet 
	buildSetForOpcode(Method[] emitters, String opcode)
    {
	EmitterSet s = new EmitterSet();
	for(int i = 0; i < emitters.length; i++) {
	    Method m = emitters[i];
	    if (m.getName().startsWith("emit" + opcode + "_") 
		                    ||
		m.getName().equals("emit" + opcode))
	    {
		s.add(new EmitterDescriptor(m.getName()));
	    }
	}

	return s;
    }

    /**
     * the set of IA32 opcodes to ignore.  Some opcode are not used by
     * the opt compiler (NOP is a good example) but may be present in
     * the VM_Assembler if other compilers use them.  We keep an
     * explicit list of such opcodes to ignore.
     */
    private static Set excludedOpcodes;

    /**
     *  Initialize the set of opcodes to ignore 
     *
     * @see #excludedOpcodes
     */
    static {
	excludedOpcodes = new HashSet();
	excludedOpcodes.add("FSAVE");
	excludedOpcodes.add("FNSTSW");
	excludedOpcodes.add("FUCOMPP");
	excludedOpcodes.add("SAHF");
	excludedOpcodes.add("NOP");
	excludedOpcodes.add("RDTSC");
	excludedOpcodes.add("ENTER");
    }

    /**
     * Compute the set of all IA32 opcodes that have emit methods in
     * the VM_Assembler.  This method uses the stylized form of all
     * emit method names in the VM_Assembler to extract the opcode of
     * each one.  It returns a set of all such distinct names, as a
     * set of Strings.
     *
     * @param emitters the set of all emit methods in the VM_Assembler
     * @return the set of all opcodes handled by the VM_Assembler
     */
    private static Set getOpcodes(Method[] emitters) {
	Set s = new HashSet();
	for(int i = 0; i < emitters.length; i++) {
	    String name = emitters[i].getName();
	    if (DEBUG) System.out.println(name);
	    if (name.startsWith("emit")) {
		int posOf_ = name.indexOf('_');
		if (posOf_ != -1) {
		    String opcode = name.substring(4, posOf_);
		    if (! excludedOpcodes.contains(opcode)) s.add( opcode );
		} else {
		    String opcode = name.substring(4);
		    // make sure it is an opcode
		    if (opcode.equals(opcode.toUpperCase(Locale.getDefault())))
			if (! excludedOpcodes.contains(opcode))
			    s.add( opcode );
		}
	    }
	}

	return s;
    }

    /**
     * returns a list of all IA32_ opt compiler operators that do not
     * correspond to real IA32 opcodes handled by the assembler.
     * These are all supposed to have been removed by the time the
     * assembler is called, so the assembler actually seeing such an
     * opcode is an internal compiler error.  This set is used during
     * generating of error checking code.
     *
     * @param emittedOpcodes the set of IA32 opcodes the assembler
     * understands. 
     * @return the set of IA32 opt operators that the assembler does
     * not understand.
     */
    private static Set getErrorOpcodes(Set emittedOpcodes) {
	Iterator e = OPT_OperatorFormatTables.getOpcodes();
	Set errorOpcodes = new HashSet();
	while (e.hasNext()) {
	    String opcode = (String) e.next();
	    if (! emittedOpcodes.contains(opcode))
		errorOpcodes.add( opcode );
	}

	return errorOpcodes;
    }

    /**
     * Given an IA32 opcode, return the set of opt compiler IA32_
     * operators that translate to it.  There is, by and large, a
     * one-to-one mapping in each each IA332_ opt operator represents
     * an IA32 opcde, so this method might seem useless.  However,
     * there are some special cases, notably for operand size.  In
     * this case, an opt operator of the form ADD$B would mean use the
     * ADD IA32 opcode with a byte operand size.  
     */
    private static Set getMatchingOperators(String lowLevelOpcode) {
	Iterator e = OPT_OperatorFormatTables.getOpcodes();
	Set matchingOperators = new HashSet();
	while (e.hasNext()) {
	    String o = (String) e.next();
	    if (o.equals(lowLevelOpcode) || o.startsWith(lowLevelOpcode+"$"))
		matchingOperators.add( o );
	}

	return matchingOperators;
    }

    /**
     * Generate an assembler for the opt compiler
     */
    public static void main(String[] args) {
	try {
	    out = new FileWriter(System.getProperty("generateToDir") + "/OPT_Assembler.java");
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}

	try {
	    lowLevelAsm = Class.forName("VM_Assembler");
	} catch (ClassNotFoundException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}

	emit("import instructionFormats.*;\n\n");
	emit("\n\n");

	emit("/**\n");
	emit(" *  This class is the automatically-generated assembler for\n");
	emit(" * the optimizing compiler.  It consists of methods that\n");
	emit(" * understand the possible operand combinations of each\n");
	emit(" * instruction type, and how to translate those operands to\n");
	emit(" * calls to the VM_Assember low-level emit method\n");
	emit(" *\n");
	emit(" * @see GenerateAssembler\n");
	emit(" *\n");
	emit(" * @author Julian Dolby\n");
	emit(" * @author {@link GenerateAssembler}\n");
	emit(" */\n");
	emit("class OPT_Assembler extends OPT_AssemblerBase {\n\n");

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  This class requires no special construction;\n");
	emitTab(1);emit(" * this constructor simply invokes the\n");
	emitTab(1);emit(" * constructor for VM_Assembler\n");
	emitTab(1);emit(" *\n");
	emitTab(1);emit(" * @see VM_Assembler\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("OPT_Assembler(int bcSize, boolean print) {\n");
	emitTab(2);   emit("super(bcSize, print);\n");
	emitTab(1); emit("}");
	emit("\n\n");

	Method[] emitters = lowLevelAsm.getDeclaredMethods();
	Set opcodes = getOpcodes(emitters);

	Iterator i = opcodes.iterator();
	while (i.hasNext()) {
	    String opcode = (String) i.next();
	    setCurrentOpcode( opcode );
	    emitTab(1);emit("/**\n");
	    emitTab(1);emit(" *  Emit the given instruction, assuming that\n");
	    emitTab(1);emit(" * it is a " + currentFormat + " instruction\n");
	    emitTab(1);emit(" * and has a " + currentOpcode + " operator\n");
	    emitTab(1);emit(" *\n");
	    emitTab(1);emit(" * @param inst the instruction to assemble\n");
	    emitTab(1);emit(" */\n");
	    emitTab(1);
	    emit("private void do" + opcode + "(OPT_Instruction inst) {\n");
	    EmitterSet emitter = buildSetForOpcode(emitters, opcode);
	    boolean[][] tp = new boolean[4][ encoding.length ];
	    emitter.emitSet(opcode, tp, 2);
	    emitTab(1);
	    emit("}\n\n");
	}

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  The number of instructions emitted so far\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("private int instructionCount = 0;\n\n");

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  Assemble the given instruction\n");
	emitTab(1);emit(" *\n");
	emitTab(1);emit(" * @param inst the instruction to assemble\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("void doInst(OPT_Instruction inst) {\n");
	emitTab(2);    emit("resolveForwardReferences(++instructionCount);\n");
	emitTab(2);    emit("switch (inst.getOpcode()) {\n");

	Set emittedOpcodes = new HashSet();

	i = opcodes.iterator();
	while (i.hasNext()) {
	    String opcode = (String) i.next();
	    Iterator operators = getMatchingOperators( opcode ).iterator();
	    while ( operators.hasNext() ) {
		Object operator = operators.next();
		emitTab(3); 
		emittedOpcodes.add( operator );
		emit("case IA32_" + operator + "_opcode:\n");
	    }
	    emitTab(4);    emit("do" + opcode + "(inst);\n");
	    emitTab(4);    emit("break;\n");
	}

	// Kludge for IA32_LOCK which needs to call emitLockNextInstruction
	emittedOpcodes.add("LOCK");
	emitTab(3);    emit("case IA32_LOCK_opcode:\n");
	emitTab(4);    emit("emitLockNextInstruction();\n");
	emitTab(4);    emit("break;\n");

	// Kludge for PATCH_POINT 
	emittedOpcodes.add("LOCK");
	emitTab(3);    emit("case IG_PATCH_POINT_opcode:\n");
	emitTab(4);    emit("emitPatchPoint();\n");
	emitTab(4);    emit("break;\n");
	
	Set errorOpcodes = getErrorOpcodes( emittedOpcodes );
	if (! errorOpcodes.isEmpty()) {
	    i = errorOpcodes.iterator();
	    while (i.hasNext()) {
		emitTab(3); 
		emit("case IA32_" + i.next() + "_opcode:\n");
	    }
	    emitTab(4); emit("throw new OPT_OptimizingCompilerException(inst + \" has unimplemented IA32 opcode (check excludedOpcodes)\");\n");
	}
	
	emitTab(2);    emit("}\n");
	emitTab(2);    emit("inst.setmcOffset( mi );\n");
	emitTab(1); emit("}\n\n");
	
	emit("\n}\n");

	try {
	    out.close();
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 *  This class provides support functionality used by the generated
 * OPT_Assembler; it handles basic impedance-matching functionality
 * such as determining which addressing mode is suitable for a given
 * OPT_IA32MemoryOperand.  This class also provides some boilerplate
 * methods that do not depend on how instructions sould actually be
 * assembled, like the top-level generateCode driver.  This class is
 * not meant to be used in isolation, but rather to provide support
 * from the OPT_Assembler.
 *
 * @author Julian Dolby
 */
abstract class OPT_AssemblerBase 
    extends VM_Assembler 
    implements OPT_Operators, VM_Constants, OPT_PhysicalRegisterConstants
{
    /**
     *  This class requires no particular construction behavior; this
     * constructor simply calls super.
     *
     * @see VM_Assembler
     */
    OPT_AssemblerBase(int bytecodeSize, boolean shouldPrint) {
	super(bytecodeSize, shouldPrint);
    }

    /**
     *  Is the given operand an immediate?  In the IA32 assembly, one
     * cannot specify floating-point constants, so the possible
     * immediates we may see are OPT_IntegerConstants and
     * OPT_TrapConstants (a trap constant really is an integer), and
     * jump targets for which the exact offset is known.
     *
     * @see #getImm
     *
     * @param op the operand being queried
     * @return true if op represents an immediate
     */
    static boolean isImm(OPT_Operand op) {
	return 
	    (op instanceof OPT_IntConstantOperand)
	                   ||
	    (op instanceof OPT_TrapCodeOperand)
	                   ||
	    (op instanceof OPT_BranchOperand 
                           &&
	     op.asBranch().target.getmcOffset() >= 0);
    }

    /**
     *  Return the IA32 ISA encoding of the immediate value
     * represented by the the given operand.  This method assumes the
     * operand is an immediate and will likely throw a
     * ClassCastException if this not the case.  It treats
     * OPT_BranchOperands somewhat differently than isImm does: in
     * case a branch target is not resolved, it simply returns a wrong
     * answer and trusts the caller to ignore it. This behavior
     * simplifies life when generating code for ImmOrLabel operands.
     *
     * @see #isImm
     *
     * @param op the operand being queried
     * @return the immediate value represented by the operand
     */
    static int getImm(OPT_Operand op) {
	if (op instanceof OPT_BranchOperand) {
	    // used by ImmOrLabel stuff
	    return op.asBranch().target.getmcOffset();
	}
	else if (op instanceof OPT_TrapCodeOperand) 
	    return ((OPT_TrapCodeOperand)op).getTrapCode() 
		                  +
		   VM_TrapConstants.RVM_TRAP_BASE;
	else
	    return op.asIntConstant().value;
    }

    /**
     *  Is the given operand a register operand?
     *
     * @see #getReg
     *
     * @param op the operand being queried
     * @return true if op is an OPT_RegisterOperand
     */
    static boolean isReg(OPT_Operand op) {
	return (op instanceof OPT_RegisterOperand);
    }

    /**
     *  Return the machine-level register number corresponding to a
     * given OPT_Register.  The optimizing compiler has its own notion
     * of register numbers, which is not the same as the numbers used
     * by the IA32 ISA.  This method takes an optimizing compiler
     * register and translates it into the appropriate machine-level
     * encoding.  This method is not applied directly to operands, but
     * rather to register objects.
     *
     * @see #getReg
     * @see #getBase
     * @see #getIndex
     *
     * @param reg the register being queried
     * @return the 3 bit machine-level encoding of reg
     */
    static private byte getMachineRegister(OPT_Register reg) {
	int type = OPT_PhysicalRegisterSet.getPhysicalRegisterType(reg);
	if (type == INT_REG) 
	    return (byte) (reg.number - FIRST_INT);
	else if (type == DOUBLE_REG)
	    return (byte) (reg.number - FIRST_DOUBLE);
	else
	    throw new OPT_OptimizingCompilerException("unexpected register type " + type);
    }
	
    /**
     *  Given a register operand, return the 3 bit IA32 ISA encoding
     * of that register.  This function translates an optimizing
     * compiler register operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a register operand, and will blow up if it is not;
     * use isReg to check operands passed to this method.
     *
     * @see #isReg 
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of op
     */
    static byte getReg(OPT_Operand op) {
	return getMachineRegister( op.asRegister().register );
    }

    /**
     *  Given a memory operand, return the 3 bit IA32 ISA encoding
     * of its base regsiter.  This function translates the optimizing
     * compiler register operand representing the base of the given
     * memory operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a memory operand, and will blow up if it is not;
     * one should confirm an operand really has a base register before
     * invoking this method on it.
     *
     * @see #isRegDisp
     * @see #isRegIdx
     * @see #isRegInd
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of the base register of op
     */
    static byte getBase(OPT_Operand op) {
	return getMachineRegister( ((OPT_MemoryOperand)op).base.register );
    }

    /**
     *  Given a memory operand, return the 3 bit IA32 ISA encoding
     * of its index regsiter.  This function translates the optimizing
     * compiler register operand representing the index of the given
     * memory operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a memory operand, and will blow up if it is not;
     * one should confirm an operand really has an index register before
     * invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of the index register of op
     */
    static byte getIndex(OPT_Operand op) {
	return getMachineRegister(((OPT_MemoryOperand)op).index.register);
    }

    /**
     *  Given a memory operand, return the 2 bit IA32 ISA encoding
     * of its scale, suitable for passing to the VM_Assembler to mask
     * into a SIB byte.  This function assumes its operand is a memory
     * operand, and will blow up if it is not; one should confirm an
     * operand really has a scale before invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the IA32 ISA encoding of the scale of op
     */
    static short getScale(OPT_Operand op) {
	return ((OPT_MemoryOperand)op).scale;
    }

    /**
     *  Given a memory operand, return the 2 bit IA32 ISA encoding
     * of its scale, suitable for passing to the VM_Assembler to mask
     * into a SIB byte.  This function assumes its operand is a memory
     * operand, and will blow up if it is not; one should confirm an
     * operand really has a scale before invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the IA32 ISA encoding of the scale of op
     */
    static int getDisp(OPT_Operand op) {
	return ((OPT_MemoryOperand)op).disp;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-displacement mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-displacement mode.  That is, does it have a non-zero
     * displacement and a base register, but no scale and no index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-displacement mode
     */
    static boolean isRegDisp(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base != null) &&
		(mop.index == null) &&
		(mop.disp != 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * absolute mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * absolute address mode.  That is, does it have a non-zero
     * displacement, but no scale, no scale and no index register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as absolute mode
     */
    static boolean isAbs(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base == null) &&
		(mop.index == null) &&
		(mop.disp != 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-indirect mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-displacement mode.  That is, does it have a base
     * register, but no displacement, no scale and no index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-indirect mode
     */
    static boolean isRegInd(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base != null) &&
		(mop.index == null) &&
		(mop.disp == 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-offset mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-offset mode.  That is, does it have a non-zero
     * displacement, a scale parameter and an index register, but no
     * base register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-offset mode
     */
    static boolean isRegOff(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base == null) &&
		(mop.index != null);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * the full glory of scaled-index-base addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * SIB mode.  That is, does it have a non-zero
     * displacement, a scale parameter, a base register and an index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as SIB mode
     */
    static boolean isRegIdx(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) 
	    return !(isAbs(op) || isRegInd(op) || isRegDisp(op) || isRegOff(op));
	else
	    return false;
    }

    /**
     *  Return the condition bits of a given optimizing compiler
     * condition operand.  This method returns the IA32 ISA bits
     * representing a given condition operand, suitable for passing to
     * the VM_Assembler to encode into the opcode of a SET, Jcc or
     * CMOV instruction.  This being IA32, there are of course
     * exceptions in the binary encoding of conditions (see FCMOV),
     * but the VM_Assembler handles that.  This function assumes its
     * argument is an OPT_IA32ConditionOperand, and will blow up if it
     * is not.
     *
     * @param op the operand being queried
     * @return the bits that (usually) represent the given condition
     * in the IA32 ISA */
    static byte getCond(OPT_Operand op) {
	return ((OPT_IA32ConditionOperand)op).value;
    }

    /**
     *  Is the given operand an IA32 condition operand?
     *
     * @param op the operand being queried
     * @return true if op is an IA32 condition operand
     */
    static boolean isCond(OPT_Operand op) {
	return (op instanceof OPT_IA32ConditionOperand);
    }

    /**
     *  Return the label representing the target of the given branch
     * operand.  These labels are used to represent branch targets
     * that have not yet been assembled, and so cannot be given
     * concrete machine code offsets.  All instructions are nunbered
     * just prior to assembly, and these numbers are used as labels.
     * This method also returns 0 (not a valid label) for int
     * constants to simplify generation of branches (the branch
     * generation code will ignore this invalid label; it is used to
     * prevent type exceptions).  This method assumes its operand is a
     * branch operand (or an int) and will blow up if it is not.
     *
     * @param op the branch operand being queried
     * @return the label representing the branch target
     */
    static int getLabel(OPT_Operand op) {
	if (op instanceof OPT_IntConstantOperand)
	    // used by ImmOrLabel stuff
	    return 0;
	else {
	    if (op.asBranch().target.getmcOffset() < 0)
		return - op.asBranch().target.getmcOffset();
	    else
		return -1;
	}
    }

    /**
     *  Is the given operand a branch target that requires a label?
     *
     * @see #getLabel
     *
     * @param op the operand being queried
     * @return true if it represents a branch requiring a label target
     */
    static boolean isLabel(OPT_Operand op) {
	return (op instanceof OPT_BranchOperand
		                &&
		op.asBranch().target.getmcOffset() < 0);
    }
    
    /**
     *  Is the given operand a branch target?
     *
     * @see #getLabel
     * @see #isLabel
     *
     * @param op the operand being queried
     * @return true if it represents a branch target
     */
    static boolean isImmOrLabel(OPT_Operand op) {
	return (isImm(op) || isLabel(op));
    }

    /**
     *  Does the given instruction operate upon byte-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a byte.  This does not work for the
     * size-converting moves (MOVSX and MOVZX), and those instructions
     * use the operator convention that $b on the end of the operator
     * name means operate upon byte data.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon byte data
     */
    static boolean isByte(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$b") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 1);
	}

	return false;
    }

    /**
     *  Does the given instruction operate upon word-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a word.  This does not work for the
     * size-converting moves (MOVSX and MOVZX), and those instructions
     * use the operator convention that $w on the end of the operator
     * name means operate upon word data.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon word data
     */
    static boolean isWord(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$w") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 2);
	}

	return false;
    }

    /**
     *  Does the given instruction operate upon quad-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a byte.  This method also recognizes 
     * the operator convention that $q on the end of the operator
     * name means operate upon quad data; no operator currently uses
     * this convention.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon quad data
     */
    static boolean isQuad(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$q") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 8);
	}

	return false;
    }

  /**
   * Debugging support (return a printable representation of the machine code).
   *
   * @param instr, an integer to be interpreted as a PowerPC instruction
   * @param offset the mcoffset (in bytes) of the instruction
   *
   */
  public static String disasm(int instr, int offset) {
    OPT_OptimizingCompilerException.TODO("OPT_Assembler: disassembler");
    return null;
  }

  /**
   * generate machine code into ir.machinecode
   * @param ir the IR to generate
   * @return   the number of machinecode instructions generated
   */
  public static final int generateCode(OPT_IR ir, boolean shouldPrint) {
      int count = 0;

      for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	   p != null; p = p.nextInstructionInCodeOrder()) 
      {
	  p.setmcOffset( - ++count);
      }
      
      OPT_Assembler asm = new OPT_Assembler(count, shouldPrint);

      for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	   p != null; p = p.nextInstructionInCodeOrder()) 
      {
	  asm.doInst(p);
      }
      
    ir.MIRInfo.machinecode = asm.getMachineCodes();

    return ir.MIRInfo.machinecode.length;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Final acts of MIR expansion for the IA32 architecture.
 * Things that are expanded here (immediately before final assembly)
 * should only be those sequences that cannot be expanded earlier
 * due to difficulty in keeping optimizations from interfering with them.
 *
 * One job of this phase is to handle the expansion of the remains of
 * table switch.  The code looks like a mess (which it is), but there
 * is little choice for relocatable IA32 code that does this.  And the
 * details of this code are shared with the baseline compiler and
 * dependent in detail on the VM_Assembler (see {@link
 * VM_Assembler#emitOFFSET_Imm_ImmOrLabel}).  If you want to mess with
 * it, you will probably need to mess with them as well.
 *
 * @author Dave Grove
 * @author Stephen Fink
 * @author Julian Dolby
 * @modified Peter Sweeney 
 */
class OPT_FinalMIRExpansion extends OPT_IRTools {

  /**
   * @param ir the IR to expand
   * @return return value is garbage for IA32
   */
  public final static int expand(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (OPT_Instruction next, p = ir.firstInstructionInCodeOrder();
         p != null;
         p = next) {
      next = p.nextInstructionInCodeOrder();
      p.setmcOffset(-1);
      p.scratchObject = null; 

      switch (p.getOpcode()) {
      case MIR_LOWTABLESWITCH_opcode:
	{
	  // split the basic block after the MIR_LOWTABLESWITCH
	  OPT_BasicBlock thisBlock = p.getBasicBlock();
	  OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(p,ir);
	  nextBlock.firstInstruction().setmcOffset(-1); 

	  // place offset data table after call so that call pushes
	  // the base address of this table onto the stack
	  int NumTargets = MIR_LowTableSwitch.getNumberOfTargets(p);
	  for (int i = 0; i < NumTargets; i++) {
	    thisBlock.appendInstruction(MIR_CaseLabel.create
					(IA32_OFFSET, I(i), 
					 MIR_LowTableSwitch.getClearTarget(p, i)));
	  }
	  // calculate address to which to jump, and store it
	  // on the top of the stack
	  OPT_Register regS = MIR_LowTableSwitch.getIndex(p).register;
	  nextBlock.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(regS), I(2)));
	  nextBlock.appendInstruction(MIR_BinaryAcc.create
				      (IA32_ADD, R(regS), 
				       OPT_MemoryOperand.I(R(phys.getESP()),
							   (byte)4,null,null)));
	  nextBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(regS), 
						      OPT_MemoryOperand.I
						      (R(regS),(byte)4,null,
						       null)));
	  nextBlock.appendInstruction(MIR_BinaryAcc.create
				      (IA32_ADD, 
				       OPT_MemoryOperand.I(R(phys.getESP()),
							   (byte)4,null,null),
				       R(regS))); 
	  // ``return'' to mangled return address
	  nextBlock.appendInstruction(MIR_Return.create(IA32_RET, I(0), null, null));

	  // CALL next block to push pc of next ``instruction'' onto stack
	  MIR_Call.mutate0(p, IA32_CALL, null, null, nextBlock.makeJumpTarget(), null);
	}
	break;
	  
      case IA32_TEST_opcode: 
	// don't bother telling rest of compiler that memory operand
	// must be first; we can just commute it here.
	if (MIR_Test.getVal2(p).isMemory()) {
	  OPT_Operand tmp = MIR_Test.getClearVal1(p);
	  MIR_Test.setVal1(p, MIR_Test.getClearVal2(p));
	  MIR_Test.setVal2(p, tmp);
	}
	break;

      case NULL_CHECK_opcode:
	{
	  // mutate this into a TRAPIF, and then fall through to the the
	  // TRAP_IF case. 
	  OPT_Operand ref = NullCheck.getRef(p);
	  MIR_TrapIf.mutate(p,IA32_TRAPIF,null,ref.copy(),I(0),
			    OPT_IA32ConditionOperand.EQ(),
			    OPT_TrapCodeOperand.NullPtr());
	} 
	// There is no break statement here on purpose!
      case IA32_TRAPIF_opcode: 
	{
	  // split the basic block right before the IA32_TRAPIF
	  OPT_BasicBlock thisBlock = p.getBasicBlock();
	  OPT_BasicBlock trap = thisBlock.createSubBlock(p.bcIndex,ir,0f);
	  OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(p,ir);
	  OPT_TrapCodeOperand tc = MIR_TrapIf.getClearTrapCode(p);
	  p.remove();
	  nextBlock.firstInstruction().setmcOffset(-1); 

	  // add code to thisBlock to conditionally jump to trap
	  OPT_Instruction cmp = MIR_Compare.create(IA32_CMP, 
						   MIR_TrapIf.getVal1(p), 
						   MIR_TrapIf.getVal2(p));
	  if (p.isMarkedAsPEI()) {
	    // The trap if was explictly marked, which means that it has 
	    // a memory operand into which we've folded a null check.
	    // Actually need a GC map for both the compare and the INT.
	    cmp.markAsPEI();
	    cmp.copyPosition(p);
	    ir.MIRInfo.gcIRMap.insertTwin(p, cmp);
	  }
	  thisBlock.appendInstruction(cmp);
	  thisBlock.appendInstruction(MIR_CondBranch.create
				      (IA32_JCC, MIR_TrapIf.getCond(p), 
				       trap.makeJumpTarget(), null));

	  // add block at end to hold trap instruction, and 
	  // insert trap sequence
	  ir.cfg.addLastInCodeOrder(trap);
	  if (tc.isArrayBounds()) {
	    // attempt to store index expression in processor object for 
	    // C trap handler
	    OPT_Operand index = MIR_TrapIf.getVal2(p);
	    if (!(index instanceof OPT_RegisterOperand ||
		  index instanceof OPT_IntConstantOperand)) {
	      index = I(0xdeadbeef); // index was spilled, and 
	      // we can't get it back here.
	    }
	    OPT_MemoryOperand mo = 
	      OPT_MemoryOperand.BD(R(phys.getPR()),
				   VM_Entrypoints.arrayIndexTrapParamField.getOffset(),
				   (byte)4, 
				   null, 
				   null);
	    trap.appendInstruction(MIR_Move.create(IA32_MOV, mo, 
						   index.copy()));
	  }
	  // NOTE: must make p the trap instruction: it is the GC point!
	  // IMPORTANT: must also inform the GCMap that the instruction has 
	  // been moved!!!
	  trap.appendInstruction(MIR_Trap.mutate(p, IA32_INT, null, tc));
	  ir.MIRInfo.gcIRMap.moveToEnd(p);

	  if (tc.isStackOverflow()) {
	    // only stackoverflow traps resume at next instruction.
	    trap.appendInstruction(MIR_Branch.create
				   (IA32_JMP, nextBlock.makeJumpTarget()));
	  }
	}
	break;

      case IA32_FMOV_ENDING_LIVE_RANGE_opcode:
	OPT_Operand result = MIR_Move.getResult(p);
	OPT_Operand value = MIR_Move.getValue(p);
	if (result.isRegister() && value.isRegister()) {
	  if (result.similar(value)) {
	    // eliminate useless move
	    p.remove(); 
	  } else {
	    int i = phys.getFPRIndex(result.asRegister().register);   
	    int j = phys.getFPRIndex(value.asRegister().register);   
	    if (i == 0) {
	      MIR_XChng.mutate(p, IA32_FXCH, result, value);
	    } else if (j == 0) {
	      MIR_XChng.mutate(p, IA32_FXCH, value, result);
	    } else {
	      expandFmov(p,phys);
	    }
	  }
	} else {
	  expandFmov(p,phys);
	}
	break;

      case DUMMY_DEF_opcode:
      case DUMMY_USE_opcode:
      case REQUIRE_ESP_opcode:
      case ADVISE_ESP_opcode:
	p.remove();
	break;

      case IA32_FMOV_opcode:
	expandFmov(p,phys);
	break;

      case IA32_FCLEAR_opcode:
	expandFClear(p,ir);
	break;

      case IA32_JCC2_opcode:
	p.insertBefore(MIR_CondBranch.create
		       (IA32_JCC, MIR_CondBranch2.getCond1(p), 
			MIR_CondBranch2.getTarget1(p), 
			MIR_CondBranch2.getBranchProfile1(p)));
	MIR_CondBranch.mutate(p, IA32_JCC, MIR_CondBranch2.getCond2(p),
			      MIR_CondBranch2.getTarget2(p),
			      MIR_CondBranch2.getBranchProfile2(p));
	break;

      case CALL_SAVE_VOLATILE_opcode:
	p.operator=IA32_CALL;
	break;

      case IA32_LOCK_CMPXCHG_opcode:
	p.insertBefore(MIR_Empty.create(IA32_LOCK));
	p.operator=IA32_CMPXCHG;
	break;

      case YIELDPOINT_PROLOGUE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromPrologueMethod);
	break;

      case YIELDPOINT_EPILOGUE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromEpilogueMethod);
	break;

      case YIELDPOINT_BACKEDGE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromBackedgeMethod);
	break;

      case IR_ENDPROLOGUE_opcode:
	// Remember where the end of prologue is for jdp
	p.remove();
	ir.MIRInfo.instAfterPrologue = next;
	break;

      }
    }
    return 0;
  }

  /**
   * expand an FCLEAR pseudo-insruction using FFREEs.
   *
   * @param s the instruction to expand
   * @param phys controlling physical register set
   */
  private static void expandFClear(OPT_Instruction s, OPT_IR ir) {
    int nSave = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
    int fpStackHeight = ir.MIRInfo.fpStackHeight;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (int i=nSave; i<fpStackHeight; i++) {
      OPT_Register f = phys.getFPR(i);
      s.insertBefore(MIR_UnaryAcc.create(IA32_FFREE,D(f)));
    }

    // Remove the FCLEAR.
    s.remove();
  }

  /**
   * expand an FMOV pseudo-insruction.
   *
   * @param s the instruction to expand
   * @param phys controlling physical register set
   */
  private static void expandFmov(OPT_Instruction s, 
                                 OPT_PhysicalRegisterSet phys) {
    OPT_Operand result = MIR_Move.getResult(s); 
    OPT_Operand value = MIR_Move.getValue(s); 

    if (result.isRegister() && value.isRegister()) {
      if (result.similar(value)) {
        // eliminate useless move
        s.remove(); 
      } else { 
        int i = phys.getFPRIndex(result.asRegister().register);   
        int j = phys.getFPRIndex(value.asRegister().register);   
        if (j == 0) {
          // We have FMOV Fi, F0
          // Expand as:
          //        FST F(i)  (copy F0 to F(i))
          MIR_Move.mutate(s,IA32_FST,D(phys.getFPR(i)),D(phys.getFPR(0)));
        } else {
          // We have FMOV Fi, Fj
          // Expand as:
          //        FLD Fj  (push Fj on FP stack).
          //        FSTP F(i+1)  (copy F0 to F(i+1) and then pop register stack)
          s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));

          MIR_Move.mutate(s,IA32_FSTP,D(phys.getFPR(i+1)),D(phys.getFPR(0)));
        }

      }
    } else if (value instanceof OPT_MemoryOperand) {
      if (result instanceof OPT_MemoryOperand) {
        // We have FMOV M1, M2
        // Expand as:
        //        FLD M1   (push M1 on FP stack).
        //        FSTP M2  (copy F0 to M2 and pop register stack)
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,result,D(phys.getFPR(0)));
      } else {
        // We have FMOV Fi, M
        // Expand as:
        //        FLD M    (push M on FP stack).
        //        FSTP F(i+1)  (copy F0 to F(i+1) and pop register stack)
        if (VM.VerifyAssertions) VM.assert(result.isRegister());
        int i = phys.getFPRIndex(result.asRegister().register);   
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,D(phys.getFPR(i+1)),D(phys.getFPR(0)));
      }
    } else {
      // We have FMOV M, Fi
      if (VM.VerifyAssertions) VM.assert(value.isRegister());
      if (VM.VerifyAssertions) 
        VM.assert(result instanceof OPT_MemoryOperand);
      int i = phys.getFPRIndex(value.asRegister().register);   
      if (i!=0) {
        // Expand as:
        //        FLD Fi    (push Fi on FP stack).
        //        FSTP M    (store F0 in M and pop register stack);
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,result,D(phys.getFPR(0)));
      } else {
        // Expand as:
        //        FST M    (store F0 in M);
        MIR_Move.mutate(s,IA32_FST,result,value);
      }
    }
  }

  private static void expandYieldpoint(OPT_Instruction s,
				       OPT_IR ir,
				       VM_Method meth) {
    if (VM.VerifyAssertions) VM.assert(ir.options.FIXED_JTOC);

    // split the basic block after the yieldpoint, create a new
    // block at the end of the IR to hold the yieldpoint,
    // remove the yieldpoint (to prepare to out it in the new block at the end)
    OPT_BasicBlock thisBlock = s.getBasicBlock();
    OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(s,ir);
    OPT_BasicBlock yieldpoint = thisBlock.createSubBlock(s.bcIndex, ir, 0);
    thisBlock.insertOut(yieldpoint);
    yieldpoint.insertOut(nextBlock);
    ir.cfg.addLastInCodeOrder(yieldpoint);
    s.remove();
    
    // change thread switch instruction into call to thread switch routine
    // NOTE: must make s the call instruction: it is the GC point!
    //       must also inform the GCMap that s has been moved!!!
    int offset = meth.getOffset();
    OPT_LocationOperand loc = new OPT_LocationOperand(offset);
    OPT_Operand guard = TG();
    OPT_Operand target = 
      OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(offset).toInt(), (byte)4, loc, guard);
    MIR_Call.mutate0(s, CALL_SAVE_VOLATILE, null, null, target, 
		     OPT_MethodOperand.STATIC(meth));
    yieldpoint.appendInstruction(s);
    ir.MIRInfo.gcIRMap.moveToEnd(s);

    yieldpoint.appendInstruction(MIR_Branch.create(IA32_JMP,
						   nextBlock.makeJumpTarget())); 
    
    // Check to see if threadSwitch requested
    OPT_Register PR = ir.regpool.getPhysicalRegisterSet().getPR();
    int tsr = VM_Entrypoints.threadSwitchRequestedField.getOffset();
    OPT_MemoryOperand M = OPT_MemoryOperand.BD(R(PR),tsr,(byte)4,null,null);
    thisBlock.appendInstruction(MIR_Compare.create(IA32_CMP, M, I(0)));
    thisBlock.appendInstruction(MIR_CondBranch.create(IA32_JCC, OPT_IA32ConditionOperand.NE(),
						      yieldpoint.makeJumpTarget(),
						      OPT_BranchProfileOperand.never()));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An FPR register that BURS is managing.
 * Created by a fld, and then eventually
 * deallocated with some popping alu/store.
 *
 * @author Dave Grove
 */
public final class OPT_BURSManagedFPROperand extends OPT_Operand {
  int regNum;

  OPT_BURSManagedFPROperand(int r) {
    regNum = r;
  }

  /**
   * Returns a copy of the current operand.
   */
  OPT_Operand copy() { 
    return new OPT_BURSManagedFPROperand(regNum);
  }

  /**
   * Returns if this operand is the 'same' as another operand.
   *
   * @param op other operand
   */
  boolean similar(OPT_Operand op) {
    return (op instanceof OPT_BURSManagedFPROperand) && 
      ((OPT_BURSManagedFPROperand)op).regNum == regNum;
  }

  // Returns the string representation of this operand.
  public String toString() {
    return "ST("+regNum+")";
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$
/**
 * An IA32 condition operand
 *
 * @author Dave Grove
 */
public final class OPT_IA32ConditionOperand extends OPT_Operand 
  implements VM_AssemblerConstants {
  
  /**
   * Value of this operand (one of the ConditionCode constants operands 
   * defined in VM_AssemblerConstants)
   */
  byte value;

  /**
   * Returns a copy of the current operand.
   */
  OPT_Operand copy() { 
    return new OPT_IA32ConditionOperand(value);
  }

  /**
   * Returns if this operand is the 'same' as another operand.
   *
   * @param op other operand
   */
  boolean similar(OPT_Operand op) {
    return (op instanceof OPT_IA32ConditionOperand) && 
      ((OPT_IA32ConditionOperand)op).value == value;
  }

  /**
   * flip the direction of the condition (return this, mutated to flip value)
   */
  OPT_IA32ConditionOperand flipCode() { 
    switch (value) {
    case O:   value =  NO; break;
    case NO:  value =   O; break;
    case LLT: value = LGE; break;
    case LGE: value = LLT; break;
    case EQ:  value =  NE; break;
    case NE:  value =  EQ; break;
    case LLE: value = LGT; break;
    case LGT: value = LLE; break;
    case S:   value =  NS; break;
    case NS:  value =   S; break;
    case PE:  value =  PO; break;
    case PO:  value =  PE; break;
    case LT:  value =  GE; break;
    case GE:  value =  LT; break;
    case LE:  value =  GT; break;
    case GT:  value =  LE; break;
    default:
      OPT_OptimizingCompilerException.UNREACHABLE();
    }
    return this;
  }

  /**
   * change the condition when operands are flipped 
   * (return this mutated to change value)
   */
  OPT_IA32ConditionOperand flipOperands() {
    switch (value) {
    case LLT: value = LGT; break;
    case LGE: value = LLE; break;
    case LLE: value = LGE; break;
    case LGT: value = LLT; break;
    case LT:  value =  GT; break;
    case GE:  value =  LE; break;
    case LE:  value =  GE; break;
    case GT:  value =  LT; break;
    default:
      OPT_OptimizingCompilerException.TODO();
    }
    return this;
  }      

  /**
   * Construct the IA32 Condition Operand that corresponds to the 
   * argument ConditionOperand
   */
  OPT_IA32ConditionOperand(OPT_ConditionOperand c) {
    translate(c);
  }

  static OPT_IA32ConditionOperand EQ() {
    return new OPT_IA32ConditionOperand(EQ);
  }
  static OPT_IA32ConditionOperand NE() {
    return new OPT_IA32ConditionOperand(NE);
  }
  static OPT_IA32ConditionOperand LT() {
    return new OPT_IA32ConditionOperand(LT);
  }
  static OPT_IA32ConditionOperand LE() {
    return new OPT_IA32ConditionOperand(LE);
  }
  static OPT_IA32ConditionOperand GT() {
    return new OPT_IA32ConditionOperand(GT);
  }
  static OPT_IA32ConditionOperand GE() {
    return new OPT_IA32ConditionOperand(GE);
  }
  static OPT_IA32ConditionOperand O() {
    return new OPT_IA32ConditionOperand(O);
  }
  static OPT_IA32ConditionOperand NO() {
    return new OPT_IA32ConditionOperand(NO);
  }
  static OPT_IA32ConditionOperand LGT() {
    return new OPT_IA32ConditionOperand(LGT);
  }
  static OPT_IA32ConditionOperand LLT() {
    return new OPT_IA32ConditionOperand(LLT);
  }
  static OPT_IA32ConditionOperand LGE() {
    return new OPT_IA32ConditionOperand(LGE);
  }
  static OPT_IA32ConditionOperand LLE() {
    return new OPT_IA32ConditionOperand(LLE);
  }
  static OPT_IA32ConditionOperand PE() {
    return new OPT_IA32ConditionOperand(PE);
  }
  static OPT_IA32ConditionOperand PO() {
    return new OPT_IA32ConditionOperand(PO);
  }

  private OPT_IA32ConditionOperand(byte c) {
    value = c;
  }

  // translate from OPT_ConditionOperand: used during LIR => MIR translation
  void translate(OPT_ConditionOperand c) {
     switch(c.value) {
     case OPT_ConditionOperand.EQUAL:         value =  EQ; break;
     case OPT_ConditionOperand.NOT_EQUAL:     value =  NE; break;
     case OPT_ConditionOperand.LESS:          value =  LT; break;
     case OPT_ConditionOperand.LESS_EQUAL:    value =  LE; break;
     case OPT_ConditionOperand.GREATER:       value =  GT; break;
     case OPT_ConditionOperand.GREATER_EQUAL: value =  GE; break;
     case OPT_ConditionOperand.OVERFLOW:      value =   O; break;
     case OPT_ConditionOperand.NOT_OVERFLOW:  value =  NO; break;
     case OPT_ConditionOperand.HIGHER:        value = LGT; break;
     case OPT_ConditionOperand.LOWER:         value = LLT; break;
     case OPT_ConditionOperand.HIGHER_EQUAL:  value = LGE; break;
     case OPT_ConditionOperand.LOWER_EQUAL:   value = LLE; break;
     default:
       OPT_OptimizingCompilerException.TODO();
     }
  }

  // Returns the string representation of this operand.
  public String toString() {
    return CONDITION[value];
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Pool of symbolic registers.
 * Intel specific implementation where JTOC is stored in the processor object
 * and accessed through the processor register.  
 * 
 * @see OPT_Register
 * 
 * @author Peter Sweeney
 * @author Stephen Fink
 */
class OPT_RegisterPool extends OPT_GenericRegisterPool implements OPT_Operators {

  /**
   * Initializes a new register pool for the method meth.
   * 
   * @param meth the VM_Method of the outermost method
   */
  OPT_RegisterPool(VM_Method meth) {
    super(meth);
  }

  /**
   * Inject an instruction to load the JTOC from
   * the processor register and return an OPT_RegisterOperand
   * that contains the result of said load.
   * 
   * @param  ir  the containing IR
   * @param s    the instruction to insert the load operand before
   * @return     a register operand that holds the JTOC
   */ 
  public OPT_Operand makeJTOCOp(OPT_IR ir, OPT_Instruction s) {
    if (ir.options.FIXED_JTOC) {
      VM_Address jtoc = VM_Magic.getTocPointer();
      return new OPT_IntConstantOperand(jtoc.toInt());
    } else {
      OPT_RegisterOperand res = ir.regpool.makeTemp
	(OPT_ClassLoaderProxy.IntArrayType);
      s.insertBefore(Unary.create(GET_JTOC, res, 
                                  OPT_IRTools.
                                  R(ir.regpool.getPhysicalRegisterSet().
                                    getPR())));
      return res.copyD2U();
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This class implements the machine-specific magics for the opt compiler.
 *
 * @see OPT_GenerateMagic for the machine-independent magics
 * 
 * @author Dave Grove
 */
class OPT_GenerateMachineSpecificMagic implements OPT_Operators, VM_Constants {

  /**
   * "Semantic inlining" of methods of the VM_Magic class.
   * Based on the methodName, generate a sequence of opt instructions
   * that implement the magic, updating the stack as neccessary
   *
   * @param bc2ir the bc2ir object generating the ir containing this magic
   * @param gc == bc2ir.gc
   * @param meth the VM_Method that is the magic method
   */
  static void generateMagic(OPT_BC2IR bc2ir, 
			    OPT_GenerationContext gc, 
			    VM_Method meth) 
    throws OPT_MagicNotImplementedException {

    VM_Atom methodName = meth.getName();
    OPT_PhysicalRegisterSet phys = gc.temps.getPhysicalRegisterSet();

    if (methodName == VM_MagicNames.getESIAsProcessor) {
      OPT_RegisterOperand rop = gc.temps.makePROp();
      bc2ir.markGuardlessNonNull(rop);
      bc2ir.push(rop);
    } else if (methodName == VM_MagicNames.setESIAsProcessor) {
      OPT_Operand val = bc2ir.popRef();
      if (val instanceof OPT_RegisterOperand) {
	bc2ir.appendInstruction(Move.create(REF_MOVE, 
					    gc.temps.makePROp(), 
					    val));
      } else {
	String msg = " Unexpected operand VM_Magic.setProcessorRegister";
	throw OPT_MagicNotImplementedException.UNEXPECTED(msg);
      }
    }else if (methodName == VM_MagicNames.getFramePointer) {
      gc.allocFrame = true;
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      VM_Field f = VM_Entrypoints.processorFPField;
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      bc2ir.appendInstruction(GetField.create(GETFIELD, val, pr.copy(), 
					      new OPT_LocationOperand(f), 
					      new OPT_TrueGuardOperand()));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.getJTOC || 
	       methodName == VM_MagicNames.getTocPointer) {
      VM_Type t = (methodName == VM_MagicNames.getJTOC ? OPT_ClassLoaderProxy.IntArrayType : VM_Type.AddressType);
      OPT_RegisterOperand val = gc.temps.makeTemp(t);
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      if (gc.options.FIXED_JTOC) {
        VM_Address jtoc = VM_Magic.getTocPointer();
        OPT_IntConstantOperand I = new OPT_IntConstantOperand(jtoc.toInt());
        bc2ir.appendInstruction(Move.create(REF_MOVE, val, I));
      } else {
        bc2ir.appendInstruction(Unary.create(GET_JTOC, val, pr.copy()));
      }
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.isync) {
      // nothing required on Intel
    } else if (methodName == VM_MagicNames.sync) {
      // nothing required on Intel
    } else if (methodName == VM_MagicNames.getThreadId) {
      OPT_RegisterOperand val = gc.temps.makeTempInt();
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, pr.copy(),
					  new
                                          OPT_IntConstantOperand(VM_Entrypoints.threadIdField.getOffset()),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.getCallerFramePointer) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCallerFramePointer) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getCompiledMethodID) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTempInt();
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCompiledMethodID) {
      OPT_Operand val = bc2ir.popInt();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getReturnAddress) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_RETURN_ADDRESS_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setReturnAddress) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_RETURN_ADDRESS_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.sysCall0) {
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, null));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_0) {
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, null));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_I) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, null, p1));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall1) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, null, p1));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall2) {
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, null, 
                                                  p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCallAD) {
      OPT_Operand p2 = bc2ir.popDouble();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, null,
                                                  p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall3) {
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create3(SYSCALL, op0, ip, null,
						  p1, p2, p3));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall4) {
      OPT_Operand p4 = bc2ir.popInt();
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create4(SYSCALL, op0, ip, null,
						  p1, p2, p3, p4));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.roundToZero) {
      bc2ir.appendInstruction(Empty.create(ROUND_TO_ZERO));
    } else if (methodName == VM_MagicNames.clearFloatingPointState) {
      bc2ir.appendInstruction(Empty.create(CLEAR_FLOATING_POINT_STATE));
    } else {
      // Distinguish between magics that we know we don't implement
      // (and never plan to implement) and those (usually new ones) 
      // that we want to be warned that we don't implement.
      String msg = " Magic method not implemented: " + meth;
      if (methodName == VM_MagicNames.returnToNewStack || 
	  methodName == VM_MagicNames.pragmaNoOptCompile) {
	throw OPT_MagicNotImplementedException.EXPECTED(msg);
      } else {
	throw OPT_MagicNotImplementedException.UNEXPECTED(msg);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import  java.util.Vector;

/**
 * This class specifies the order in which OPT_CompilerPhases are
 * executed in the target-specific backend of the optimzing compiler.
 * The methods LIR2MIR, MIROptimizations, and MIR2MC each specify the
 * elements that make up the main compilation stages.
 *
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind */
class OPT_MIROptimizationPlanner extends OPT_OptimizationPlanner {

  /**
   * Initialize the "master plan" for the IA32 backend of the opt compiler.
   */
  static void intializeMasterPlan(Vector temp) {
    LIR2MIR(temp);
    MIROptimizations(temp);
    MIR2MC(temp);
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert LIR to IA32 MIR.
   *
   * @param p the plan under construction
   */
  private static void LIR2MIR(Vector p) {
    composeComponents(p, "Convert LIR to MIR", new Object[] {
      // Split very large basic blocks into smaller ones.
      new OPT_SplitBasicBlock(), 
      // Optional printing of final LIR
      new OPT_IRPrinter("Final LIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_FINAL_LIR;
        }
      }, 
      // Change operations that split live ranges to moves
      new OPT_MutateSplits(),
      // Instruction Selection
      new OPT_ConvertLIRtoMIR(), 
      // For now, always print the Initial MIR
      new OPT_IRPrinter("Initial MIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_MIR;
        }
      }
    });
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed on IA32 MIR.
   *
   * @param p the plan under construction
   */
  private static void MIROptimizations(Vector p) {
    // NullCheck combining and validation operand removal.
    addComponent(p, new OPT_NullCheckCombining());

    // Register Allocation
    composeComponents(p, "Register Mapping", new Object[] {
      new OPT_MIRSplitRanges(),
      // MANDATORY: Expand calling convention
      new OPT_ExpandCallingConvention(),
      // MANDATORY: Insert defs/uses due to floating-point stack
      new OPT_ExpandFPRStackConvention(),
      // MANDATORY: Perform Live analysis and create GC maps
      new OPT_LiveAnalysis(true, false),
      // MANDATORY: Perform register allocation
      new OPT_RegisterAllocator(),
      // MANDATORY: Add prologue and epilogue
      new OPT_PrologueEpilogueCreator(),
    });
    // Peephole branch optimizations
    addComponent(p, new OPT_MIRBranchOptimizations(1));
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert IA32 MIR into
   * ready-to-execute machinecode (and associated mapping tables).
   *
   * @param p the plan under construction
   */
  private static void MIR2MC(Vector p) {
    // MANDATORY: Final assembly
    addComponent(p, new OPT_ConvertMIRtoMC());
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Enumeration;

/**
 * This class contains IA32 calling conventions
 * The two public methods are:
 *  (1) expandCallingConventions(OPT_IR) which is called by the 
 *  register allocator immediately before allocation to make manifest the 
 *  use of registers by the calling convention.
 *  (2) expandSysCall(OPT_Instruction, OPT_IR) which is called to expand 
 *  a SYSCALL HIR instruction into the appropriate sequence of 
 *  LIR instructions.
 *
 * TODO: Much of this code could still be factored out as
 * architecture-independent.
 *
 * @author Dave Grove
 * @author Stephen Fink
 */
final class OPT_CallingConvention extends OPT_IRTools
  implements OPT_Operators,
	     OPT_PhysicalRegisterConstants {

  /**
   * Size of a word, in bytes
   */
  private static final int WORDSIZE = 4;

  /**
   * Expand calling conventions to make physical registers explicit in the
   * IR when required for calls, returns, and the prologue.
   */
  public static void expandCallingConventions(OPT_IR ir)  {
    // expand each call and return instruction
    for (OPT_Instruction inst = ir.firstInstructionInCodeOrder(); 
         inst != null; inst = inst.nextInstructionInCodeOrder()) {
      if (inst.isCall()) {
        callExpand(inst, ir);
      } else if (inst.isReturn()) {
        returnExpand(inst, ir);
      }
    }

    // expand the prologue instruction
    expandPrologue(ir);
  }

  /**
   * Expand the calling convention for a particular call instruction
   */
  private static void callExpand(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    boolean isSysCall = call.operator() == IA32_SYSCALL;

    // 0. Handle the parameters
    int parameterBytes = isSysCall ? expandParametersToSysCall(call,ir) : 
      expandParametersToCall(call,ir);

    // 1. Clear the floating-point stack if dirty.
    if (call.operator != CALL_SAVE_VOLATILE) {
      int FPRRegisterParams= countFPRParams(call);
      FPRRegisterParams = Math.min(FPRRegisterParams, 
				   phys.getNumberOfFPRParams());
      call.insertBefore(MIR_UnaryNoRes.create(IA32_FCLEAR,
					      I(FPRRegisterParams)));
    }
    
    // 2. Move the return value into a register
    expandResultOfCall(call,ir);
    
    // 3. If this is an interface invocation, set up the hidden parameter
    //    in the processor object to hold the interface signature id.
    if (VM.BuildForIMTInterfaceInvocation) {
      if (MIR_Call.hasMethod(call)) {
	OPT_MethodOperand mo = MIR_Call.getMethod(call);
        if (mo.isInterface()) {
          int signatureId = VM_ClassLoader.
            findOrCreateInterfaceMethodSignatureId(mo.method.getName(), 
                                                   mo.method.getDescriptor());
          OPT_MemoryOperand M = OPT_MemoryOperand.BD
            (R(phys.getPR()), VM_Entrypoints.hiddenSignatureIdField.getOffset(), 
             (byte)WORDSIZE, null, null);
          call.insertBefore(MIR_Move.create(IA32_MOV,M,I(signatureId)));
        }
      }
    }

    // 4. ESP must be parameterBytes before call, will be at either parameterBytes
    //    or 0 afterwards depending on whether or it is an RVM method or a sysCall.
    call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes)));
    call.insertAfter(MIR_UnaryNoRes.create(ADVISE_ESP, I(isSysCall?parameterBytes:0)));
  }

  /**
   * Expand the calling convention for a particular return instruction
   */
  private static void returnExpand(OPT_Instruction ret, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    if (MIR_Return.hasVal(ret)) {
      OPT_Operand symb1 = MIR_Return.getClearVal(ret);
      MIR_Return.setVal(ret, null);
      VM_Type type = symb1.getType();
      if (type.isFloatType() || type.isDoubleType()) {
        OPT_Register r = phys.getReturnFPR();
        OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
	ret.insertBefore(MIR_Move.create(IA32_FMOV, rOp, symb1));
	MIR_Return.setVal(ret, rOp.copyD2U());
      } else {
        OPT_Register r = phys.getFirstReturnGPR();
        OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
	ret.insertBefore(MIR_Move.create(IA32_MOV, rOp, symb1));
	MIR_Return.setVal(ret, rOp.copyD2U());
      }
    }

    if (MIR_Return.hasVal2(ret)) {
      OPT_Operand symb2 = MIR_Return.getClearVal2(ret);
      MIR_Return.setVal2(ret,null);
      VM_Type type = symb2.getType();
      OPT_Register r = phys.getSecondReturnGPR();
      OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
      ret.insertBefore(MIR_Move.create(IA32_MOV, rOp, symb2));
      MIR_Return.setVal2(ret, rOp.copyD2U());
    }

    // Clear the floating-point stack if dirty.
    int nSave=0;
    if (MIR_Return.hasVal(ret)) {
      OPT_Operand symb1 = MIR_Return.getClearVal(ret);
      VM_Type type = symb1.getType();
      if (type.isFloatType() || type.isDoubleType()) {
	nSave=1;
      }
    }
    ret.insertBefore(MIR_UnaryNoRes.create(IA32_FCLEAR,I(nSave)));

    // Set the first 'Val' in the return instruction to hold an integer
    // constant which is the number of words to pop from the stack while 
    // returning from this method.
    MIR_Return.setPopBytes(ret, I(ir.incomingParameterBytes()));
  }

  /**
   * Explicitly copy the result of a call instruction from the result
   * register to the appropriate symbolic register,
   * as defined by the calling convention.
   */
  private static void expandResultOfCall(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // copy the first result parameter
    if (MIR_Call.hasResult(call)) {
      OPT_RegisterOperand result1 = MIR_Call.getClearResult(call);
      MIR_Call.setResult(call,null);
      if (result1.type.isFloatType() || result1.type.isDoubleType()) {
        OPT_Register r = phys.getReturnFPR();
        OPT_RegisterOperand physical = 
	  new OPT_RegisterOperand(r, result1.type);
        OPT_Instruction tmp = MIR_Move.create(IA32_FMOV, result1, physical);
        call.insertAfter(tmp);
        MIR_Call.setResult(call, null);
      } else {
        // first GPR result register
        OPT_Register r = phys.getFirstReturnGPR();
        OPT_RegisterOperand physical = 
	  new OPT_RegisterOperand(r, result1.type);
        OPT_Instruction tmp = MIR_Move.create(IA32_MOV, result1, physical);
        call.insertAfter(tmp);
        MIR_Call.setResult(call, null);
      }
    }

    // copy the second result parameter
    if (MIR_Call.hasResult2(call)) {
      OPT_RegisterOperand result2 = MIR_Call.getClearResult2(call);
      MIR_Call.setResult2(call,null);
      // second GPR result register
      OPT_Register r = phys.getSecondReturnGPR();
      OPT_RegisterOperand physical = 
	new OPT_RegisterOperand(r, result2.type);
      OPT_Instruction tmp = MIR_Move.create(IA32_MOV, result2, physical);
      call.insertAfter(tmp);
      MIR_Call.setResult2(call, null);
    }
  }


  /**
   * Explicitly copy parameters to a call into the appropriate physical
   * registers as defined by the calling convention.
   *
   * Note: Assumes that ESP points to the word before the slot where the
   * first parameter should be stored.
   */
  private static int expandParametersToCall(OPT_Instruction call, OPT_IR ir) {
    int nGPRParams = 0;
    int nFPRParams = 0;

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    // count the number FPR parameters in a pre-pass
    int FPRRegisterParams= countFPRParams(call);
    FPRRegisterParams = Math.min(FPRRegisterParams, 
                                 phys.getNumberOfFPRParams());

    // offset, in bytes, from the SP, for the next parameter slot on the
    // stack
    int parameterBytes = 0;
    OPT_Register ESP = phys.getESP();

    // Require ESP to be at bottom of frame before a call,
    call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(0)));

    // walk over each parameter
    // must count then before we start nulling them out!
    int numParams = MIR_Call.getNumberOfParams(call); 
    int nParamsInRegisters = 0;
    for (int i = 0; i < numParams;  i++) {
      OPT_Operand param = MIR_Call.getClearParam(call,i);
      MIR_Call.setParam(call,i,null);
      VM_Type paramType = param.getType();
      if (paramType.isFloatType() || paramType.isDoubleType()) {
	nFPRParams++;
	int size = paramType.isFloatType() ? 4 : 8;
	parameterBytes -= size;
	if (nFPRParams > phys.getNumberOfFPRParams()) {
	  // pass the FP parameter on the stack
	  OPT_Operand M =
	    new OPT_StackLocationOperand(false, parameterBytes, size);
	  call.insertBefore(MIR_Move.create(IA32_FMOV, M, param));
	} else {
	  // Pass the parameter in a register.
	  // Note that if k FPRs are passed in registers, 
	  // the 1st goes in F(k-1),
	  // the 2nd goes in F(k-2), etc...
	  OPT_Register phy = 
	    phys.getFPRParam(FPRRegisterParams - nFPRParams);
	  OPT_RegisterOperand real = new OPT_RegisterOperand(phy, paramType);
	  call.insertBefore(MIR_Move.create(IA32_FMOV, real, param));
	  // Record that the call now has a use of the real register.
	  MIR_Call.setParam(call,nParamsInRegisters++,real.copy());
	}
      } else {
	nGPRParams++;
	parameterBytes -= 4;
	if (nGPRParams > phys.getNumberOfGPRParams()) {
	  // Too many parameters to pass in registers.  Write the
	  // parameter into the appropriate stack frame location.
	  call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes + 4)));
	  call.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, param));
	} else {
	  // Pass the parameter in a register.
	  OPT_Register phy = phys.getGPRParam(nGPRParams-1);
	  OPT_RegisterOperand real = new OPT_RegisterOperand(phy, paramType);
	  call.insertBefore(MIR_Move.create(IA32_MOV, real, param));
	  // Record that the call now has a use of the real register.
	  MIR_Call.setParam(call,nParamsInRegisters++,real.copy());       
	}
      }
    }
    return parameterBytes;
  }

  /**
   * Save and restore all nonvolatile registers around a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * Side effect: changes the operator of the call instruction to
   * IA32_CALL.
   *
   * @param call the sys call
   */
  static void saveNonvolatilesAroundSysCall(OPT_Instruction call, OPT_IR ir) {
    saveNonvolatilesBeforeSysCall(call, ir); 
    restoreNonvolatilesAfterSysCall(call, ir);
    call.operator = IA32_CALL;
  }

  /**
   * Save all nonvolatile registers before a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * @param call the sys call
   */
  static void saveNonvolatilesBeforeSysCall(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    OPT_Instruction result = null;

    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    // get the offset into the stack frame of where to stash the first
    // nonvolatile for this case.
    int location = sm.getOffsetForSysCall();
    
    // save each non-volatile
    for (Enumeration e = phys.enumerateNonvolatileGPRs();
         e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      OPT_Operand M = 
	new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
      call.insertBefore(MIR_Move.create(IA32_MOV, M, R(r)));
      location += WORDSIZE;
    }
    
    // save the processor register
    OPT_Register PR = phys.getPR();
    OPT_Operand M = 
      new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
    call.insertBefore(MIR_Move.create(IA32_MOV, M, R(PR)));
  }
  /**
   * Restore all nonvolatile registers after a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * @param call the sys call
   */
  static void restoreNonvolatilesAfterSysCall(OPT_Instruction call,
                                              OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    
    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    // get the offset into the stack frame of where to stash the first
    // nonvolatile for this case.
    int location = sm.getOffsetForSysCall();
    
    // restore each non-volatile
    for (Enumeration e = phys.enumerateNonvolatileGPRs();
         e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      OPT_Operand M = 
	new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
      call.insertAfter(MIR_Move.create(IA32_MOV, R(r), M));
      location += WORDSIZE;
    }
    
    // restore the processor register
    OPT_Register PR = phys.getPR();
    OPT_Operand M = 
      new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
    call.insertAfter(MIR_Move.create(IA32_MOV, R(PR), M));
  }

  /**
   * Explicitly copy parameters to a system call into the appropriate physical
   * registers as defined by the calling convention.  Note that for a system
   * call (ie., a call to C), the order of parameters on the stack is
   * <em> reversed </em> compared to the normal RVM calling convention
   *
   * TODO: much of this code is exactly the same as in expandParametersToCall().
   *       factor out the common code.
   *
   * Note: Assumes that ESP points to the word before the slot where the
   * first parameter should be stored.
   */
  private static int expandParametersToSysCall(OPT_Instruction call, OPT_IR ir){
    int nGPRParams = 0;
    int nFPRParams = 0;
    int parameterBytes = 0;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    OPT_Register ESP = phys.getESP();
    // count the number FPR parameters in a pre-pass
    int FPRRegisterParams= countFPRParams(call);
    FPRRegisterParams = Math.min(FPRRegisterParams, phys.getNumberOfFPRParams());
    
    // walk over the parameters in reverse order
    // NOTE: All params to syscall are passed on the stack!
    int numParams = MIR_Call.getNumberOfParams(call); 
    for (int i = numParams-1; i >=0;  i--) {
      OPT_Operand param = MIR_Call.getClearParam(call,i);
      MIR_Call.setParam(call,i,null);
      VM_Type paramType = param.getType();
      if (paramType.isFloatType() || paramType.isDoubleType()) {
	nFPRParams++;
	int size = paramType.isFloatType() ? 4 : 8;
	parameterBytes -= size;
	OPT_Operand M = 
	  new OPT_StackLocationOperand(false, parameterBytes, size);
	call.insertBefore(MIR_Move.create(IA32_FMOV, M, param));
      } else {
	nGPRParams++;
	parameterBytes -= 4;
	call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes + 4)));
	call.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, param));
      }
    }
    return parameterBytes;
  }

  /**
   * We have to save/restore the non-volatile registers around syscalls,
   * to protect ourselves from malicious C compilers and Linux kernels.
   * 
   * Although the register allocator is not yet ready to insert these
   * spills, allocate space on the stack in preparation.
   *
   * For now, we naively save/restore all nonvolatiles.
   */
  public static void allocateSpaceForSysCall(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    
    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    sm.allocateSpaceForSysCall(nToSave);
  }

  /**
   * Calling convention to implement calls to native (C) routines 
   * using the Linux linkage conventions.
   */
  public static void expandSysCall(OPT_Instruction s, OPT_IR ir) {
    
    // Determine the address of the method to call.
    OPT_RegisterOperand ip = null;
    if (CallSpecial.getMethod(s) != null) {
      OPT_SysMethodOperand sysM = 
	(OPT_SysMethodOperand)CallSpecial.getClearMethod(s);
      OPT_RegisterOperand t1 = 
	OPT_ConvertToLowLevelIR.getStatic(s, ir, VM_Entrypoints.the_boot_recordField);
      ip = OPT_ConvertToLowLevelIR.getField(s, ir, t1, sysM.ip);
    } else {
      ip = (OPT_RegisterOperand)CallSpecial.getClearAddress(s);
    }

    // Allocate space to save non-volatiles.
    allocateSpaceForSysCall(ir);
    
    // Make sure we allocate enough space for the parameters to this call.
    int numberParams = CallSpecial.getNumberOfParams(s);
    int parameterWords = 0;
    for (int i = 0; i < numberParams; i++) {
      parameterWords++;
      OPT_Operand op = CallSpecial.getParam(s, i);
      if (op instanceof OPT_RegisterOperand) {
        OPT_RegisterOperand reg = (OPT_RegisterOperand)op;
        if ((reg.type == VM_Type.LongType) || (reg.type == VM_Type.DoubleType))
          parameterWords++;
      } else if ((op instanceof OPT_LongConstantOperand) || 
                 (op instanceof OPT_DoubleConstantOperand)) {
        parameterWords++;
      }
    }
    // allocate space for each parameter, plus one word on the stack to
    // hold the address of the callee.
    ir.stackManager.allocateParameterSpace((1 + parameterWords)*4);
                                                   
    // Convert to a SYSCALL instruction with a null method operand.
    CallSpecial.mutate0(s, SYSCALL, CallSpecial.getClearResult(s), ip, null);
  }

  /**
   * Count the number of FPR parameters in a call instruction.
   */
  private static int countFPRParams(OPT_Instruction call) {
    int result = 0;
    // walk over the parameters 
    int numParams = MIR_Call.getNumberOfParams(call); 
    for (int i = 0; i <numParams;  i++) {
      OPT_Operand param = MIR_Call.getParam(call,i);
      if (param.isRegister()) {
        OPT_RegisterOperand symb = (OPT_RegisterOperand)param;
        if (symb.type.isFloatType() || symb.type.isDoubleType()) {
          result++;
        }
      }
    }
    return result;
  }
  /**
   * Count the number of FPR parameters in a prologue instruction.
   */
  private static int countFPRParamsInPrologue(OPT_Instruction p) {
    int result = 0;
    // walk over the parameters 
    for (OPT_OperandEnumeration e = p.getDefs(); e.hasMoreElements(); ) {
      OPT_Operand param = (OPT_Operand)e.nextElement();
      if (param.isRegister()) {
        OPT_RegisterOperand symb = (OPT_RegisterOperand)param;
        if (symb.type.isFloatType() || symb.type.isDoubleType()) {
          result++;
        }
      }
    }
    return result;
  }

  /**
   * Expand the prologue instruction.
   */
  private static void expandPrologue(OPT_IR ir) {
    if (ir.options.SIMPLE_OPT) {
      // set up register lists for dead code elimination.
      OPT_DefUse.computeDU(ir);
    }
    
    OPT_Instruction p = ir.firstInstructionInCodeOrder().
      nextInstructionInCodeOrder();
    if (VM.VerifyAssertions) VM.assert(p.operator == IR_PROLOGUE);
    OPT_Instruction start = p.nextInstructionInCodeOrder();
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    int gprIndex = 0; 
    int fprIndex = 0; 
    int paramByteOffset = ir.incomingParameterBytes() + 8;

    // count the number of FPR params in a pre-pass
    int FPRRegisterParams= countFPRParamsInPrologue(p);
    FPRRegisterParams = Math.min(FPRRegisterParams, phys.getNumberOfFPRParams());
    ir.MIRInfo.fpStackHeight = Math.max(ir.MIRInfo.fpStackHeight, FPRRegisterParams);

    // deal with each parameter
    for (OPT_OperandEnumeration e = p.getDefs(); e.hasMoreElements(); ) {
      OPT_RegisterOperand symbOp = (OPT_RegisterOperand)e.nextElement();
      VM_Type rType = symbOp.type;
      if (rType.isFloatType() || rType.isDoubleType()) {
	int size = rType.isFloatType() ? 4 : 8;
	paramByteOffset -= size;
        // if optimizing, only define the register if it has uses
        if (!ir.options.SIMPLE_OPT || symbOp.register.useList != null) {
          if (fprIndex < phys.getNumberOfFPRParams()) {
            // insert a MOVE symbolic register = parameter
            // Note that if k FPRs are passed in registers, 
            // the 1st goes in F(k-1),
            // the 2nd goes in F(k-2), etc...
            OPT_Register param = 
	      phys.getFPRParam(FPRRegisterParams - fprIndex - 1);
            start.insertBefore(MIR_Move.create(IA32_FMOV,symbOp.copyRO(),
					       D(param)));
          } else {
	    OPT_Operand M = 
	      new OPT_StackLocationOperand(true, paramByteOffset, size);
	    start.insertBefore(MIR_Move.create(IA32_FMOV, symbOp.copyRO(), M));
          }
        }
        fprIndex++;
      } else {
        // if optimizing, only define the register if it has uses
	paramByteOffset -= 4;
        if (!ir.options.SIMPLE_OPT || symbOp.register.useList != null) {
          // t is object, 1/2 of a long, int, short, char, byte, or boolean
          if (gprIndex < phys.getNumberOfGPRParams()) {
	    // to give the register allocator more freedom, we
	    // insert two move instructions to get the physical into
	    // the symbolic.  First a move from the physical to a fresh temp 
	    // before start and second a move from the temp to the
	    // 'real' parameter symbolic after start.
	    OPT_RegisterOperand tmp = ir.gc.temps.makeTemp(rType);
            OPT_Register param = phys.getGPRParam(gprIndex);
	    OPT_RegisterOperand pOp = new OPT_RegisterOperand(param, rType);
            start.insertBefore(OPT_PhysicalRegisterTools.makeMoveInstruction(tmp,pOp));
	    OPT_Instruction m2 = OPT_PhysicalRegisterTools.makeMoveInstruction(symbOp.copyRO(),tmp.copyD2U());
	    start.insertBefore(m2);
	    start = m2;
          } else {
	    OPT_Operand M = 
	      new OPT_StackLocationOperand(true, paramByteOffset, 4);
	    start.insertBefore(MIR_Move.create(IA32_MOV, symbOp.copyRO(), M));
          }
        }
        gprIndex++;
      }
    }

    if (VM.VerifyAssertions) VM.assert(paramByteOffset == 8, "pb = "+paramByteOffset);
    
    // Now that we've made the calling convention explicit in the prologue,
    // set IR_PROLOGUE to have no defs.
    p.replace(Prologue.create(IR_PROLOGUE, 0));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$ 

import java.util.Enumeration;
import instructionFormats.*;

/**
 * At the beginning of each basic block, the register allocator expects
 * all floating-point stack locations to be available, and named
 * FPi, 0 < i < 7
 *
 * <p>However, BURS may consume FP stack locations by inserting instructions
 * that push or pop the floating-point stack.  This phase inserts dummy
 * definitions and uses to indicate when symbolic FP registers are not
 * available for register allocation since BURS has consumed a stack slot.
 *
 * For example,
 * <pre>
 *    FLD t1
 *    ...
 *    FSTP M, t1
 * </pre>
 *
 * will be modified by this phase to indicate that FP6 is not available
 * for allocation in the interval:
 *
 * <pre>
 *   DUMMY_DEF FP6
 *   FLD t1
 *   .....
 *   FSTP M, t1
 *   DUMMY_USE FP6
 * </pre>
 *
 * <p> Additionally, by convention, we will always clear the
 * floating-point stack when delivering an exception.  To model this, we
 * insert dummy defs and uses for each floating-point register at the
 * beginning of each catch block.
 *
 * @author Stephen Fink
 */

final class OPT_ExpandFPRStackConvention extends OPT_CompilerPhase
implements OPT_Operators{

  // The number of FPRs available for allocation.
  // Normally 7: we reserve one for final MIR expansion.
  int NUM_ALLOCATABLE_FPR = 7;

  boolean printingEnabled (OPT_Options options, boolean before) {
    return  options.PRINT_CALLING_CONVENTIONS && !before;
  }

  final boolean shouldPerform(OPT_Options options) { 
    return true; 
  }

  final String getName() { 
    return "Expand Calling Convention"; 
  }

  /**
   * Insert the needed dummy defs and uses.
   */
  final void perform(OPT_IR ir)  {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (Enumeration b = ir.getBasicBlocks(); b.hasMoreElements(); ) {
      OPT_BasicBlock bb = (OPT_BasicBlock)b.nextElement();
      
      if (bb instanceof OPT_ExceptionHandlerBasicBlock) {
        // clear all floating-point state at the entry to a catch block
        for (int i=0; i<NUM_ALLOCATABLE_FPR; i++) {
          OPT_Register fpr = phys.getFPR(i);
          bb.prependInstruction(MIR_UnaryNoRes.create(DUMMY_USE,
                                                      OPT_IRTools.D(fpr)));
          bb.prependInstruction(MIR_Nullary.create(DUMMY_DEF,
                                                   OPT_IRTools.D(fpr)));
        }
      }
      
      // The following holds the floating point stack offset from its
      // 'normal' position.
      int fpStackOffset = 0;

      for (Enumeration inst = bb.forwardInstrEnumerator(); 
           inst.hasMoreElements();) {
        OPT_Instruction s = (OPT_Instruction)inst.nextElement();
        if (s.operator().isFpPop()) {
          // A pop instruction 'ends' a dummy live range.
          OPT_Register fpr = phys.getFPR(NUM_ALLOCATABLE_FPR-fpStackOffset);
          s.insertAfter(MIR_UnaryNoRes.create(DUMMY_USE,OPT_IRTools.D(fpr)));
          fpStackOffset--;
        } else if (s.operator().isFpPush()) {
          fpStackOffset++;
          OPT_Register fpr = phys.getFPR(NUM_ALLOCATABLE_FPR-fpStackOffset);
          s.insertBefore(MIR_Nullary.create(DUMMY_DEF,OPT_IRTools.D(fpr)));
        }
        if (VM.VerifyAssertions) VM.assert(fpStackOffset >= 0);
        if (VM.VerifyAssertions) VM.assert(fpStackOffset <
                                           NUM_ALLOCATABLE_FPR);
      }
    }
  } 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import instructionFormats.*;

/**
 * This class splits live ranges for certain special cases to ensure
 * correctness during IA32 register allocation.
 *
 * @author Stephen Fink
 */
class OPT_MIRSplitRanges extends OPT_CompilerPhase 
implements OPT_Operators {

  /**
   * Should this phase be performed?
   * @param options controlling compiler options
   * @return true or false
   */
  final boolean shouldPerform (OPT_Options options) {
    return true;
  }

  /**
   * Return the name of this phase
   * @return "Live Range Splitting"
   */
  final String getName () {
    return "MIR Range Splitting"; 
  }

  public boolean printingEnabled(OPT_Options options, boolean before) {
    return false;
  } 

  /**
   * The main method.
   * 
   * We split live ranges for registers around PEIs which have catch
   * blocks.  Suppose we have a
   * PEI s which uses a symbolic register r1.  We must ensure that after
   * register allocation, r1 is NOT assigned to a scratch location in s,
   * since this would mess up code in the catch block tghat uses r1.
   *
   * So, instead, we introduce a new temporary r2 which holds the value of
   * r1.  The live range for r2 spans only the instruction s.  Later, we
   * will ensure that r2 is never spilled.
   * 
   * TODO: This could be implemented more efficiently.
   *
   * @param ir the governing IR
   */
  final public void perform (OPT_IR ir) {

    java.util.HashMap newMap = new java.util.HashMap(5);

    for (Enumeration be = ir.getBasicBlocks(); be.hasMoreElements(); ) {
      OPT_BasicBlock bb = (OPT_BasicBlock)be.nextElement();
      for (OPT_InstructionEnumeration ie  = bb.forwardInstrEnumerator(); 
           ie.hasMoreElements(); ) {
        OPT_Instruction s = ie.next();

        // clear the cache of register assignments
        newMap.clear();

        // Split live ranges at PEIs and a few special cases to 
        // make sure we can pin values that must be in registers.
        // NOTE: Any operator that is an IA32 special case that must have
        //       a particular operand in a register must be mentioned both
        //       here and in OPT_RegisterRestrictions!
        if (s.isPEI() && s.operator != IR_PROLOGUE) {
          if (bb.hasApplicableExceptionalOut(s) ||
              !OPT_RegisterRestrictions.SCRATCH_IN_PEI) {
            splitAllLiveRanges(s, newMap, ir, false);
          }
        }

        // handle special cases for IA32
        //  (1) Some operands must be in registers
        switch (s.getOpcode()) {
          case MIR_LOWTABLESWITCH_opcode:
            {
              OPT_RegisterOperand rOp = MIR_LowTableSwitch.getIndex(s);
              OPT_RegisterOperand temp = findOrCreateTemp(rOp, newMap, ir);
              // NOTE: Index as marked as a DU because LowTableSwitch is 
              //       going to destroy the value in the register.
              //       By construction (see ConvertToLowLevelIR), no one will
              //       every read the value computed by a LowTableSwitch.
              //       Therefore, don't insert a move instruction after the
              //       LowTableSwitch (which would cause IR verification 
              //       problems anyways, since LowTableSwitch is a branch).
              insertMoveBefore(temp, rOp.copyRO(), s); // move r into 'temp' before s
              rOp.register = temp.register;
            }
            break;
        }
      }
    }
  }

  /**
   * Split the live ranges of all register operands of an instruction
   * @param s      the instruction to process
   * @param map a mapping from symbolics to temporaries
   * @param ir  the containing IR
   * @param rootOnly only consider root operands?
   */
  private static void splitAllLiveRanges(OPT_Instruction s, 
                                         java.util.HashMap newMap,
                                         OPT_IR ir,
					 boolean rootOnly) {
    // walk over each USE
    for (OPT_OperandEnumeration u = rootOnly?s.getRootUses():s.getUses(); 
	 u.hasMoreElements(); ) {
      OPT_Operand use = u.next();
      if (use.isRegister()) {
	OPT_RegisterOperand rUse = use.asRegister();
	OPT_RegisterOperand temp = findOrCreateTemp(rUse, newMap, ir);
	// move 'use' into 'temp' before s
	insertMoveBefore(temp, rUse.copyRO(), s);
      }
    }
    // walk over each DEF (by defintion defs == root defs)
    for (OPT_OperandEnumeration d = s.getDefs(); d.hasMoreElements(); ) {
      OPT_Operand def = d.next();
      if (def.isRegister()) {
	OPT_RegisterOperand rDef = def.asRegister();
	OPT_RegisterOperand temp = findOrCreateTemp(rDef ,newMap, ir);
	// move 'temp' into 'r' after s
	insertMoveAfter(rDef.copyRO(), temp, s);
      }
    }
    // Now go back and replace the registers.
    for (OPT_OperandEnumeration ops = rootOnly?s.getRootOperands():s.getOperands(); 
	 ops.hasMoreElements(); ) {
      OPT_Operand op = ops.next();
      if (op.isRegister()) {
	OPT_RegisterOperand rOp = op.asRegister();
	OPT_Register r = rOp.register;
	OPT_Register newR = (OPT_Register)newMap.get(r); 
	if (newR != null) {
	  rOp.register = newR;
	}
      }
    }
  }

  /**
   * Find or create a temporary register to cache a symbolic register.
   *
   * @param r the symbolic register
   * @param map a mapping from symbolics to temporaries
   * @param ir the governing IR
   */
  private static OPT_RegisterOperand findOrCreateTemp(OPT_RegisterOperand rOp,
						      java.util.HashMap map,
						      OPT_IR ir) {
    OPT_Register tReg = (OPT_Register)map.get(rOp.register);
    if (tReg == null) {
      OPT_RegisterOperand tOp = ir.regpool.makeTemp(rOp.type);
      map.put(rOp.register, tOp.register);
      return tOp;
    } else {
      return new OPT_RegisterOperand(tReg, rOp.type);
    }
  }

  /**
   * Insert an instruction to move r1 into r2 before instruction s
   */
  private static void insertMoveBefore(OPT_RegisterOperand r2, 
				       OPT_RegisterOperand r1,
                                       OPT_Instruction s) {
    OPT_Instruction m = OPT_PhysicalRegisterTools.makeMoveInstruction(r2,r1);
    s.insertBefore(m);
  }
  /**
   * Insert an instruction to move r1 into r2 after instruction s
   */
  private static void insertMoveAfter(OPT_RegisterOperand r2, 
				      OPT_RegisterOperand r1,
				      OPT_Instruction s) {
    OPT_Instruction m = OPT_PhysicalRegisterTools.makeMoveInstruction(r2,r1);
    s.insertAfter(m);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class provides utilities to record defs and uses of physical
 * registers by IR operators.
 *
 * @author Stephen Fink
 * @author Dave Grove
 */
class OPT_PhysicalDefUse {

  // constants used to encode defs/uses of physical registers
  final static int mask             = 0x0000;  // empty mask
  final static int maskAF           = 0x0001;
  final static int maskCF           = 0x0002;
  final static int maskOF           = 0x0004;
  final static int maskPF           = 0x0008;
  final static int maskSF           = 0x0010;
  final static int maskZF           = 0x0020;
  final static int maskC0           = 0x0040;
  final static int maskC1           = 0x0080;
  final static int maskC2           = 0x0100;
  final static int maskC3           = 0x0200;
  final static int maskPR           = 0x0400;
  // Meta mask for the enumeration.
  private final static int maskHIGH = 0x0400;
  private final static int maskALL  = 0x07FF;
  
  final static int maskCF_OF = maskCF | maskOF;
  final static int maskCF_PF_ZF = maskCF | maskPF | maskZF;
  final static int maskCF_OF_PF_SF_ZF = maskCF | maskOF | maskPF | maskSF | 
                                        maskZF;
  final static int maskAF_OF_PF_SF_ZF = maskAF | maskOF | maskPF | maskSF | 
                                        maskZF;
  final static int maskAF_CF_OF_PF_SF_ZF = maskAF | maskCF | maskOF |
                                           maskPF | maskSF | maskZF;
  final static int maskC0_C1_C2_C3 = maskC0 | maskC1 | maskC2 | maskC3;
  final static int maskcallDefs = maskAF_CF_OF_PF_SF_ZF;
  final static int maskcallUses = mask;
  final static int maskIEEEMagicUses = mask;
  final static int maskTSPUses = mask;
  final static int maskTSPDefs = maskAF_CF_OF_PF_SF_ZF | maskPR;


  /**
   * @return whether or not an OPT_Operator uses the EFLAGS
   */
  static boolean usesEFLAGS(OPT_Operator op) {
    return (op.implicitUses & maskAF_CF_OF_PF_SF_ZF) != 0;
  }
			      
  /**
   * @return whether or not an OPT_Operator uses the EFLAGS
   */
  static boolean definesEFLAGS(OPT_Operator op) {
    return (op.implicitDefs & maskAF_CF_OF_PF_SF_ZF) != 0;
  }
			      

  /**
   * @return a string representation of the physical registers encoded by
   * an integer
   */
  static String getString(int code) {
    if (code == mask) return "";
    if (code == maskAF_CF_OF_PF_SF_ZF) return " AF CF OF PF SF ZF";
    // Not a common case, construct it...
    String s = "";
    if ((code & maskAF) != 0) s += " AF";
    if ((code & maskCF) != 0) s += " CF";
    if ((code & maskOF) != 0) s += " OF";
    if ((code & maskPF) != 0) s += " PF";
    if ((code & maskZF) != 0) s += " ZF";
    if ((code & maskC0) != 0) s += " CO";
    if ((code & maskC1) != 0) s += " C1";
    if ((code & maskC2) != 0) s += " C2";
    if ((code & maskC3) != 0) s += " C3";
    if ((code & maskPR) != 0) s += " PR";
    return s;
  }

  /**
   * @param code an integer that encodes a set of physical registers
   * @param ir the governing IR
   * @return an enumeration of the physical registers embodied by a code
   */
  static PDUEnumeration enumerate(int code, OPT_IR ir) {
    return new PDUEnumeration(code,ir);
  }

  /**
   * @param ir the governing IR
   * @return an enumeration of all physical registers that code be 
   *         implicitly defed/used
   */
  static PDUEnumeration enumerateAllImplicitDefUses(OPT_IR ir) {
    return new PDUEnumeration(maskALL,ir);
  }

  /**
   * A class to enumerate physical registers based on a code.
   */
  static final class PDUEnumeration implements Enumeration {
    private int code;
    private int curMask;
    private OPT_PhysicalRegisterSet phys;
    
    PDUEnumeration(int c, OPT_IR ir) {
      phys = ir.regpool.getPhysicalRegisterSet();
      code = c;
      curMask = maskHIGH;
    }

    public boolean hasMoreElements() {
      return code != 0;
    }

    public Object nextElement() {
      while (true) {
	int curBit = code & curMask;
	code -= curBit;
	curMask = curMask >> 1;
	if (curBit != 0) return getReg(curBit, phys);
      }
    }

    // artifically make static to enable scalar replacement of 
    // enumeration object without requiring this method to be inlined.
    private static OPT_Register getReg(int m, OPT_PhysicalRegisterSet phys) {
      switch(m) {
      case maskAF: return phys.getAF();
      case maskCF: return phys.getCF();
      case maskOF: return phys.getOF();
      case maskPF: return phys.getPF();
      case maskSF: return phys.getSF();
      case maskZF: return phys.getZF();
      case maskC0: return phys.getC0();
      case maskC1: return phys.getC1();
      case maskC2: return phys.getC2();
      case maskC3: return phys.getC3();
      case maskPR: return phys.getPR();
      }
      OPT_OptimizingCompilerException.UNREACHABLE();
      return null; // placate jikes.
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class holds constants that describe IA32 physical register set.
 *
 * @author Stephen Fink
 */
interface OPT_PhysicalRegisterConstants extends VM_RegisterConstants {

  // Types of values stored in physical registers; 
  // These affect instruction selection for accessing
  // the data
  static final byte INT_VALUE= 0;
  static final byte DOUBLE_VALUE = 1;
  static final byte FLOAT_VALUE = 2;
  static final byte CONDITION_VALUE = 3;
  
  // There are different types of hardware registers, so we define
  // the following register classes:
  // NOTE: they must be in consecutive ordering
  // TODO: Kill this?
  static final byte INT_REG = 0;
  static final byte DOUBLE_REG = 1;
  static final byte SPECIAL_REG = 2;
  static final byte NUMBER_TYPE = 3;

  // Derived constants for use by the register pool.
  // In the register pool, the physical registers are assigned integers
  // based on these constants.
  static final int FIRST_INT = 0;
  static final int FIRST_DOUBLE = NUM_GPRS;
  static final int FIRST_SPECIAL = NUM_GPRS + NUM_FPRS;

  // special intel registers or register sub-fields.
  static final int NUM_SPECIALS = 10;
  static final int AF = FIRST_SPECIAL + 0;      // AF bit of EFLAGS
  static final int CF = FIRST_SPECIAL + 1;      // CF bit of EFLAGS
  static final int OF = FIRST_SPECIAL + 2;      // OF bit of EFLAGS
  static final int PF = FIRST_SPECIAL + 3;      // PF bit of EFLAGS
  static final int SF = FIRST_SPECIAL + 4;      // SF bit of EFLAGS
  static final int ZF = FIRST_SPECIAL + 5;      // ZF bit of EFLAGS
  static final int C0 = FIRST_SPECIAL + 6;      // FP status bit
  static final int C1 = FIRST_SPECIAL + 7;      // FP status bit
  static final int C2 = FIRST_SPECIAL + 8;      // FP status bit
  static final int C3 = FIRST_SPECIAL + 9;      // FP status bit
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class represents a set of OPT_Registers corresponding to the
 * IA32 register set.
 *
 * @author Stephen Fink
 */
final class OPT_PhysicalRegisterSet extends OPT_GenericPhysicalRegisterSet
implements VM_RegisterConstants, OPT_PhysicalRegisterConstants {

  /**
   * This array holds a pool of objects representing physical registers
   */
  private OPT_Register[] reg = new OPT_Register[getSize()];

  /**
   * Cache the set of volatile registers for efficiency
   */
  private OPT_BitSet volatileSet;

  /**
   * Cache the set of floating-point registers for efficiency
   */
  private OPT_BitSet fpSet;

  /**
   * Return the total number of physical registers.
   */
  static final int getSize() {
    return NUM_GPRS + NUM_FPRS + NUM_SPECIALS;
  }

  /**
   * Return the total number of physical registers.
   */
  final int getNumberOfPhysicalRegisters() {
    return getSize();
  }

  /**
   * Return the total number of nonvolatile GPRs.
   */
  static final int getNumberOfNonvolatileGPRs() {
    return NUM_NONVOLATILE_GPRS;
  }

  /**
   * Return the total number of GPRs that may hold parameters.
   */
  static final int getNumberOfGPRParams() {
    return NUM_PARAMETER_GPRS;
  }

  /**
   * Return the total number of FPRs that may hold parameters.
   */
  static final int getNumberOfFPRParams() {
    return NUM_PARAMETER_FPRS;
  }


  /**
   * Return the (zero-based indexed) nth GPR that may hold a parameter.
   */
  final OPT_Register getGPRParam(int n) {
    if (VM.VerifyAssertions) VM.assert(n < 2);
    if (n==0) {
      return getEAX();
    } else {
      return getEDX();
    }
  }

  /**
   * Return the (zero-based indexed) nth FPR that may hold a parameter.
   */
  final OPT_Register getFPRParam(int n) {
    return getFPR(VOLATILE_FPRS[n]);
  }

  /**
   * Return the (zero-based indexed) nth GPR that may hold a return value.
   */
  OPT_Register getReturnGPR(int n) {
    if (VM.VerifyAssertions) VM.assert(n < 2);
    if (n==0) {
      return getEAX();
    } else {
      return getEDX();
    }
  }

  /**
   * Constructor: set up a pool of physical registers.
   */
  OPT_PhysicalRegisterSet() {

    // 1. Create all the physical registers in the pool.
    for (int i = 0; i < reg.length ; i++) {
      OPT_Register r = new OPT_Register(i);
      r.setPhysical();
      reg[i] = r;
    }

    // 2. Set the 'integer' attribute on each GPR
    for (int i = FIRST_INT; i < FIRST_DOUBLE; i++) {
      reg[i].setInteger();
    }

    // 3. Set the 'double' attribute on each FPR
    for (int i = FIRST_DOUBLE; i < FIRST_SPECIAL; i++) {
      reg[i].setDouble();
    }

    // 4. set up the volatile GPRs
    for (Enumeration e = enumerateVolatileGPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setVolatile();
    }

    // 5. set up the non-volatile GPRs
    for (Enumeration e = enumerateNonvolatileGPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setNonVolatile();
    }

    // 6. set properties on some special registers
    reg[AF].setSpansBasicBlock();
    reg[CF].setSpansBasicBlock();
    reg[OF].setSpansBasicBlock();
    reg[PF].setSpansBasicBlock();
    reg[SF].setSpansBasicBlock();
    reg[ZF].setSpansBasicBlock();
    reg[C0].setSpansBasicBlock();
    reg[C1].setSpansBasicBlock();
    reg[C2].setSpansBasicBlock();
    reg[C3].setSpansBasicBlock();
    reg[PROCESSOR_REGISTER].setSpansBasicBlock();

    // 7. set up the volatile FPRs
    for (Enumeration e = enumerateVolatileFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setVolatile();
    }

    // 8. set up the non-volatile FPRs
    for (Enumeration e = enumerateNonvolatileFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setNonVolatile();
    }

    // 9. Cache the volatile registers for efficiency
    volatileSet = new OPT_BitSet(this);
    for (Enumeration e = enumerateVolatiles(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      volatileSet.add(r);
    }

    // 10. Cache the FPRs for efficiency
    fpSet = new OPT_BitSet(this);
    for (Enumeration e = enumerateFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      fpSet.add(r);
    }

    // Note no registers are excluded from live analysis (as is done for PPC)

  }

  /**
   * Is a particular register subject to allocation?
   */
  boolean isAllocatable(OPT_Register r) {
    return (r.number < FIRST_SPECIAL && r != getPR() && r != getESP());
  }


  /**
   * @return the processor register
   */
  OPT_Register getPR() {
    return getGPR(PROCESSOR_REGISTER);
  }

  /**
   * @return the frame pointer register
   */
  OPT_Register getFP() {
    throw new OPT_OptimizingCompilerException("Framepointer is not a register on IA32");
  }

  /**
   * @return the EAX register
   */
  OPT_Register getEAX() {
    return getGPR(EAX);
  }

  /**
   * @return the ECX register
   */
  OPT_Register getECX() {
    return getGPR(ECX);
  }

  /**
   * @return the EDX register
   */
  OPT_Register getEDX() {
    return getGPR(EDX);
  }

  /**
   * @return the EBX register
   */
  OPT_Register getEBX() {
    return getGPR(EBX);
  }

  /**
   * @return the ESP register
   */
  OPT_Register getESP() {
    return getGPR(ESP);
  }

  /**
   * @return the EBP register
   */
  OPT_Register getEBP() {
    return getGPR(EBP);
  }

  /**
   * @return the ESI register
   */
  OPT_Register getESI() {
    return getGPR(ESI);
  }

  /**
   * @return the EDI register
   */
  OPT_Register getEDI() {
    return getGPR(EDI);
  }


  /**
   * @return a register representing the AF bit of the EFLAGS register.
   */
  OPT_Register getAF() {
    return reg[AF];
  }

  /**
   * @return a register representing the CF bit of the EFLAGS register.
   */
  OPT_Register getCF() {
    return reg[CF];
  }

  /**
   * @return a register representing the OF bit of the EFLAGS register.
   */
  OPT_Register getOF() {
    return reg[OF];
  }

  /**
   * @return a register representing the PF bit of the EFLAGS register.
   */
  OPT_Register getPF() {
    return reg[PF];
  }

  /**
   * @return a register representing the SF bit of the EFLAGS register.
   */
  OPT_Register getSF() {
    return reg[SF];
  }

  /**
   * @return a register representing the ZF bit of the EFLAGS register.
   */
  OPT_Register getZF() {
    return reg[ZF];
  }

  /**
   * @return a register representing the C0 floating-point status bit
   */
  OPT_Register getC0() {
    return reg[C0];
  }

  /**
   * @return a register representing the C1 floating-point status bit
   */
  OPT_Register getC1() {
    return reg[C1];
  }

  /**
   * @return a register representing the C2 floating-point status bit
   */
  OPT_Register getC2() {
    return reg[C2];
  }

  /**
   * @return a register representing the C3 floating-point status bit
   */
  OPT_Register getC3() {
    return reg[C3];
  }

  /**
   * @return the nth physical GPR 
   */
  OPT_Register getGPR(int n) {
    return reg[FIRST_INT+n];
  }

  /**
   * @return the index into the GPR set corresponding to a given register.
   *
   * PRECONDITION: r is a physical GPR
   */
  static int getGPRIndex(OPT_Register r) {
    return r.number - FIRST_INT;
  }

  /**
   * @return the first GPR register used to hold a return value
   */
  OPT_Register getFirstReturnGPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_GPRS > 0);
    return getEAX();
  }

  /**
   * @return the second GPR register used to hold a return value
   */
  OPT_Register getSecondReturnGPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_GPRS > 1);
    return getEDX();
  }

  /**
   * @return the FPR register used to hold a return value
   */
  OPT_Register getReturnFPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_FPRS == 1);
    return getFPR(0);
  }

  /**
   * @return the nth physical FPR 
   */
  OPT_Register getFPR(int n) {
    return reg[FIRST_DOUBLE + n];
  }

  /**
   * @return the index into the GPR set corresponding to a given register.
   *
   * PRECONDITION: r is a physical GPR
   */
  static int getFPRIndex(OPT_Register r) {
    return r.number - FIRST_DOUBLE;
  }

  /**
   * @return the nth physical register in the pool. 
   */
  OPT_Register get(int n) {
    return reg[n];
  }

  /**
   * Given a symbolic register, return a code that gives the physical
   * register type to hold the value of the symbolic register.
   * @param r a symbolic register
   * @return one of INT_REG, DOUBLE_REG 
   */
  static final int getPhysicalRegisterType(OPT_Register r) {
    if (r.isInteger() || r.isLong()) {
      return INT_REG;
    } else if (r.isFloatingPoint()) {
      return DOUBLE_REG;
    } else {
      throw new OPT_OptimizingCompilerException("getPhysicalRegisterType "
                                                + " unexpected " + r);
    }
  }

  /**
   * Register names for each class. used in printing the IR
   */
  private static final String registerName[] = new String[getSize()];
  static {
    String regName[] = registerName;
    for (int i = 0; i < NUM_GPRS; i++)
      regName[i + FIRST_INT] = GPR_NAMES[i];
    for (int i = 0; i < NUM_FPRS; i++)
      regName[i + FIRST_DOUBLE] = FPR_NAMES[i];
    regName[PROCESSOR_REGISTER] = "PR";
    regName[AF] = "AF";
    regName[CF] = "CF";
    regName[OF] = "OF";
    regName[PF] = "PF";
    regName[SF] = "SF";
    regName[ZF] = "ZF";
  }

  /**
   * Get the register name for a register with a particular number in the
   * pool
   */
  static String getName(int number) {
    return registerName[number];
  }
  /**
   * Get the spill size for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  static int getSpillSize(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }
  /**
   * Get the required spill alignment for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG,  SPECIAL_REG
   */
  static int getSpillAlignment(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }

  /**
   * Enumerate all the physical registers in this set.
   */
  Enumeration enumerateAll() {
    return new RangeEnumeration(0,getSize()-1);
  }

  /**
   * Enumerate all the GPRs in this set.
   */
  Enumeration enumerateGPRs() {
    return new RangeEnumeration(FIRST_INT,FIRST_DOUBLE-1);
  }

  /**
   * Enumerate all the GPRs in this set.
   */
  Enumeration enumerateFPRs() {
    return new RangeEnumeration(FIRST_DOUBLE,FIRST_SPECIAL-1);
  }

  /**
   * Enumerate all the volatile GPRs in this set.
   */
  Enumeration enumerateVolatileGPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_VOLATILE_GPRS ];
    for(int i = 0; i < NUM_VOLATILE_GPRS; i++)
      r[i] = getGPR(VOLATILE_GPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /**
   * Enumerate all the nonvolatile GPRs in this set.
   */
  Enumeration enumerateNonvolatileGPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_NONVOLATILE_GPRS ];
    for(int i = 0; i < NUM_NONVOLATILE_GPRS; i++)
      r[i] = getGPR(NONVOLATILE_GPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }
  /**
   * Enumerate all the volatile FPRs in this set.
   */
  Enumeration enumerateVolatileFPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_VOLATILE_FPRS ];
    for(int i = 0; i < NUM_VOLATILE_FPRS; i++)
      r[i] = getFPR(VOLATILE_FPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /**
   * Enumerate all the nonvolatile FPRs in this set.
   */
  Enumeration enumerateNonvolatileFPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_NONVOLATILE_FPRS ];
    for(int i = 0; i < NUM_NONVOLATILE_FPRS; i++)
      r[i] = getFPR(NONVOLATILE_FPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /** 
   * Enumerate the volatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateVolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateVolatileGPRs();
      case DOUBLE_REG:
        return enumerateVolatileFPRs();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException("Unsupported volatile type");
    }
  }

  /**
   * Enumerate all the volatile physical registers
   */
  Enumeration enumerateVolatiles() {
    Enumeration e1 = enumerateVolatileGPRs();
    Enumeration e2 = enumerateVolatileFPRs();
    return new OPT_CompoundEnumerator(e1, e2);
  }

  /**
   * @return the set of volatile physical registers
   */
  OPT_BitSet getVolatiles() {
    return volatileSet;
  }

  /**
   * @return the set of FPR physical registers
   */
  OPT_BitSet getFPRs() {
    return fpSet;
  }

  /** 
   * Enumerate the nonvolatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateNonvolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateNonvolatileGPRs();
      case DOUBLE_REG:
        return enumerateNonvolatileFPRs();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException
          ("Unsupported non-volatile type");
    }
  }
  /** 
   * Enumerate the nonvolatile physical registers of a given class,
   * backwards
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateNonvolatilesBackwards(int regClass) {
    return new OPT_ReverseEnumerator(enumerateNonvolatiles(regClass));
  }


  /**
   * An enumerator for use by the physical register utilities.
   */
  class PhysicalRegisterEnumeration implements Enumeration {
    private int start;
    private int end;
    private int index;
    private OPT_Register r[];
    PhysicalRegisterEnumeration(OPT_Register[] r) {
      this.r = r;
      this.index = 0;
    }
    public Object nextElement() {
      return r[index++];
    }
    public boolean hasMoreElements() {
      return (index < r.length);
    }
  }
  /**
   * An enumerator for use by the physical register utilities.
   */
  class RangeEnumeration implements Enumeration {
    private int start;
    private int end;
    private int index;
    private int exclude = -1; // an index in the register range to exclude
    RangeEnumeration(int start, int end) {
      this.start = start;
      this.end = end;
      this.index = start;
    }
    RangeEnumeration(int start, int end, int exclude) {
      this.start = start;
      this.end = end;
      this.exclude = exclude;
      this.index = start;
    }
    public Object nextElement() {
      if (index == exclude) index++;
      return reg[index++];
    }
    public boolean hasMoreElements() {
      if (index == exclude) index++;
      return (index <= end);
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This abstract class provides a set of useful methods for
 * manipulating physical registers for an IR.
 *
 * @author Jong-Deok Choi
 * @author Dave Grove
 * @author Mauricio Serrano
 * @author John Whaley
 * @author Stephen Fink
 */
abstract class OPT_PhysicalRegisterTools extends
OPT_GenericPhysicalRegisterTools{

  /**
   * Return the governing IR.
   */
  abstract OPT_IR getIR();

  /**
   * Create an MIR instruction to move rhs into lhs
   */
  static OPT_Instruction makeMoveInstruction(OPT_RegisterOperand lhs, 
                                             OPT_RegisterOperand rhs) {
    if (rhs.register.isInteger() || rhs.register.isLong()) {
      if (VM.VerifyAssertions) 
	VM.assert(lhs.register.isInteger() || lhs.register.isLong());
      return MIR_Move.create(IA32_MOV, lhs, rhs);
    } else if (rhs.register.isDouble() || rhs.register.isFloat()) {
      if (VM.VerifyAssertions) 
	VM.assert(lhs.register.isDouble() || lhs.register.isFloat());
      return MIR_Move.create(IA32_FMOV, lhs, rhs);
    } else {
      OPT_OptimizingCompilerException.TODO("OPT_PhysicalRegisterTools.makeMoveInstruction");
      return null;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import instructionFormats.*;

/**
 * @author Stephen Fink
 */
final class OPT_RegisterPreferences extends OPT_GenericRegisterPreferences
implements OPT_Operators {

  /**
   * Set up register preferences based on instructions in an IR.
   */
  void initialize(OPT_IR ir) {

    for (Enumeration e = ir.forwardInstrEnumerator(); 
         e.hasMoreElements();) {
      OPT_Instruction s = (OPT_Instruction)e.nextElement();
      switch (s.operator.opcode) {
        case IA32_MOV_opcode:
          // add affinities produced by MOVE instructions
          OPT_Operand result = MIR_Move.getResult(s);
          OPT_Operand value = MIR_Move.getValue(s);
          if (result.isRegister() && value.isRegister()) {
            OPT_Register r1 = result.asRegister().register;
            OPT_Register r2 = value.asRegister().register;
            addAffinity(1,r1,r2);
          }
          break;
        default:
          break;
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Iterator;
import instructionFormats.*;
import java.util.ArrayList;
import java.util.Enumeration;

/**
 * An instance of this class encapsulates restrictions on register
 * assignment.
 * 
 * @author Stephen Fink
 */
final class OPT_RegisterRestrictions extends OPT_GenericRegisterRestrictions implements OPT_Operators, OPT_PhysicalRegisterConstants {

  /**
   * Allow scratch registers in PEIs?
   */
  final static boolean SCRATCH_IN_PEI = true;

  /**
   * Default Constructor
   */
  OPT_RegisterRestrictions(OPT_PhysicalRegisterSet phys) {
    super(phys);
  }

  /**
   * Add architecture-specific register restrictions for a basic block.
   * Override as needed.
   *
   * @param bb the basic block 
   * @param symbolics the live intervals for symbolic registers on this
   * block
   */
  void addArchRestrictions(OPT_BasicBlock bb, ArrayList symbolics) {
    // If there are any registers used in catch blocks, we want to ensure
    // that these registers are not used or evicted from scratch registers
    // at a relevant PEI, so that the assumptions of register homes in the
    // catch block remain valid.  For now, we do this by forcing any
    // register used in such a PEI as not spilled.  TODO: relax this
    // restriction for better code.
    for (OPT_InstructionEnumeration ie = bb.forwardInstrEnumerator();
         ie.hasMoreElements(); ) {
      OPT_Instruction s = ie.next();
      if (s.isPEI() && s.operator != IR_PROLOGUE) {
        if (bb.hasApplicableExceptionalOut(s) || !SCRATCH_IN_PEI) {
          for (Enumeration e = s.getOperands(); e.hasMoreElements(); ) {
            OPT_Operand op = (OPT_Operand)e.nextElement();
            if (op != null && op.isRegister()) {
              noteMustNotSpill(op.asRegister().register);
              handle8BitRestrictions(s);
            }
          }
        }
      }

      // handle special cases 
      switch (s.getOpcode()) {
        case MIR_LOWTABLESWITCH_opcode:
          {
            OPT_RegisterOperand op = MIR_LowTableSwitch.getIndex(s);
            noteMustNotSpill(op.register);
          }
          break;
        case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
          {
            OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
            if (MIR_Unary.getVal(s).isRegister()) {
              OPT_RegisterOperand val = MIR_Unary.getVal(s).asRegister();
              restrictTo8Bits(val.register);
            }
          }
          break;
        case IA32_SET$B_opcode:
          { 
            if (MIR_Set.getResult(s).isRegister()) {
              OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
              restrictTo8Bits(op.register);
            }
          }
          break;

        default:
          handle8BitRestrictions(s);
          break;
      }
    }
    for (OPT_InstructionEnumeration ie = bb.forwardInstrEnumerator();
         ie.hasMoreElements(); ) {
      OPT_Instruction s = ie.next();
      if (s.operator == IA32_FNINIT) {
        // No floating point register survives across an FNINIT
        for (Iterator sym = symbolics.iterator(); sym.hasNext(); ) {
          OPT_LiveIntervalElement symb = (OPT_LiveIntervalElement) sym.next();
          if (symb.getRegister().isFloatingPoint()) {
            if (contains(symb,s.scratch)) {
              addRestrictions(symb.getRegister(),phys.getFPRs());
            }
          }
        }
      } else if (s.operator == IA32_FCLEAR) {
        // Only some FPRs survive across an FCLEAR
        for (Iterator sym = symbolics.iterator(); sym.hasNext(); ) {
          OPT_LiveIntervalElement symb = (OPT_LiveIntervalElement) sym.next();
          if (symb.getRegister().isFloatingPoint()) {
            if (contains(symb,s.scratch)) {
              int nSave = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
              for (int i = nSave; i < NUM_FPRS; i++) {
                addRestriction(symb.getRegister(), phys.getFPR(i));
              }
            }
          }
        }
      }
    }
  }

  /**
   * Does instruction s contain an 8-bit memory operand?
   */
  final boolean has8BitMemoryOperand(OPT_Instruction s) {
    for (OPT_OperandEnumeration me = s.getMemoryOperands(); 
         me.hasMoreElements(); ) {
      OPT_MemoryOperand mop = (OPT_MemoryOperand)me.next();
      if (mop.size == 1) {
        return true;
      }
    }
    return false;
  }
  /**
   * Ensure that if an operand has an 8 bit memory operand that
   * all of its register operands are in 8 bit registers.
   * @param s the instruction to restrict
   */
  final void handle8BitRestrictions(OPT_Instruction s) {
    for (OPT_OperandEnumeration me = s.getMemoryOperands(); 
         me.hasMoreElements(); ) {
      OPT_MemoryOperand mop = (OPT_MemoryOperand)me.next();
      if (mop.size == 1) {
        for (OPT_OperandEnumeration e2 = s.getRootOperands(); 
             e2.hasMoreElements(); ) {
          OPT_Operand rootOp = e2.next();
          if (rootOp.isRegister()) {
            restrictTo8Bits(rootOp.asRegister().register);
          }
        }
      }
    }
  }


  /**
   * Ensure that a particular register is only assigned to AL, BL, CL, or
   * DL, since these are the only 8-bit registers we normally address.
   */
  final void restrictTo8Bits(OPT_Register r) {
    OPT_Register ESP = phys.getESP();        
    OPT_Register EBP = phys.getEBP();        
    OPT_Register ESI = phys.getESI();        
    OPT_Register EDI = phys.getEDI();        
    addRestriction(r,ESP);
    addRestriction(r,EBP);
    addRestriction(r,ESI);
    addRestriction(r,EDI);
  }

  /**
   * Given symbolic register r that appears in instruction s, does the
   * architecture demand that r be assigned to a physical register in s?
   */
  static boolean mustBeInRegister(OPT_Register r, OPT_Instruction s) {
    switch (s.getOpcode()) {
      case IA32_SHRD_opcode: case IA32_SHLD_opcode:
        {
          OPT_RegisterOperand op = MIR_DoubleShift.getSource(s);
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_FCOMI_opcode: case IA32_FCOMIP_opcode:
        {
          OPT_Operand op = MIR_Compare.getVal2(s);
          if (!(op instanceof OPT_BURSManagedFPROperand)) {
            if (op.asRegister().register == r) return true;
          }
        }
        break;
      case IA32_IMUL2_opcode:
        { 
          OPT_RegisterOperand op = MIR_BinaryAcc.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case MIR_LOWTABLESWITCH_opcode:
        {
          OPT_RegisterOperand op = MIR_LowTableSwitch.getIndex(s);
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_CMOV_opcode: case IA32_FCMOV_opcode:
        {
          OPT_RegisterOperand op = MIR_CondMove.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
        {
          OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_MOVZX$W_opcode: case IA32_MOVSX$W_opcode:
        { 
          OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_SET$B_opcode:
        { 
          if (MIR_Set.getResult(s).isRegister()) {
            OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
            if (op.asRegister().register == r) return true;
          }
        }
        break;
      case IA32_TEST_opcode:
        {
          // at least 1 of the two operands must be in a register
          if (!MIR_Test.getVal2(s).isConstant()) {
            if (MIR_Test.getVal1(s).isRegister()) {
              if (MIR_Test.getVal1(s).asRegister().register == r) return true;
            } else if (MIR_Test.getVal2(s).isRegister()) {
              if (MIR_Test.getVal2(s).asRegister().register == r) return true;
            }
          }
        }
        break;

      default:
        break;
    }
    return false;
  }

  /**
   * Can physical register r hold an 8-bit value?
   */
  private boolean okFor8(OPT_Register r) {
    OPT_Register ESP = phys.getESP();        
    OPT_Register EBP = phys.getEBP();        
    OPT_Register ESI = phys.getESI();        
    OPT_Register EDI = phys.getEDI();        
    return (r!=ESP && r!=EBP && r!=ESI && r!=EDI);
  }

  /**
   * Is it forbidden to assign symbolic register symb to physical register r
   * in instruction s?
   */
  boolean isForbidden(OPT_Register symb, OPT_Register r,
                             OPT_Instruction s) {

    // Look at 8-bit restrictions.
    switch (s.operator.opcode) {
      case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
        {
          if (MIR_Unary.getVal(s).isRegister()) {
            OPT_RegisterOperand val = MIR_Unary.getVal(s).asRegister();
            if (val.register == symb) {
              return !okFor8(r);
            }
          }
        }
        break;
      case IA32_SET$B_opcode:
        { 
          if (MIR_Set.getResult(s).isRegister()) {
            OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
            if (op.asRegister().register == symb) {
              return !okFor8(r);
            }
          }
        }
        break;
     }

    if (has8BitMemoryOperand(s)) {
      return !okFor8(r);
    }

    // Otherwise, it's OK.
    return false;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
///$Id$

import instructionFormats.*;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.HashSet;
import java.util.HashMap;

/**
 * Class to manage the allocation of the "compiler-specific" portion of 
 * the stackframe.  This class holds only the architecture-specific
 * functions.
 * <p>
 *
 * @author Stephen Fink
 * @author Dave Grove
 * @author Mauricio J. Serrano
 * @author Julian Dolby
 */
final class OPT_StackManager extends OPT_GenericStackManager
implements OPT_Operators {


  /**
   * A frame offset for 108 bytes of stack space to store the 
   * floating point state in the SaveVolatile protocol.
   */
  private int fsaveLocation;

  /**
   * We allow the stack pointer to float from its normal position at the
   * bottom of the frame.  This field holds the 'current' offset of the
   * SP.
   */
  private int ESPOffset = 0;

  /**
   * Should we allow the stack pointer to float in order to avoid scratch
   * registers in move instructions.  Note: as of Feb. 02, we think this
   * is a bad idea.
   */
  private static boolean FLOAT_ESP = false;

  /**
   * Return the size of the fixed portion of the stack.
   * (in other words, the difference between the framepointer and
   * the stackpointer after the prologue of the method completes).
   * @return size in bytes of the fixed portion of the stackframe
   */
  final int getFrameFixedSize() {
    return frameSize-WORDSIZE;
  }

  /**
   * Return the size of a type of value, in bytes.
   * NOTE: For the purpose of register allocation, a FLOAT_VALUE is 64 bits!
   *
   * @param type one of INT_VALUE, FLOAT_VALUE, or DOUBLE_VALUE
   */
  private static byte getSizeOfType(byte type) {
    switch(type) {
      case INT_VALUE:
        return (byte)(WORDSIZE);
      case FLOAT_VALUE: case DOUBLE_VALUE:
        return (byte)(2 * WORDSIZE);
      default:
        OPT_OptimizingCompilerException.TODO("getSizeOfValue: unsupported");
        return 0;
    }
  }

  /**
   * Return the move operator for a type of value.
   *
   * @param type one of INT_VALUE, FLOAT_VALUE, or DOUBLE_VALUE
   */
  private static OPT_Operator getMoveOperator(byte type) {
    switch(type) {
      case INT_VALUE:
        return IA32_MOV;
      case DOUBLE_VALUE:
      case FLOAT_VALUE:
        return IA32_FMOV;
      default:
        OPT_OptimizingCompilerException.TODO("getMoveOperator: unsupported");
        return null;
    }
  }

  /**
   * Allocate a new spill location and grow the
   * frame size to reflect the new layout.
   *
   * @param type the type to spill
   * @return the spill location
   */
  final int allocateNewSpillLocation(int type) {

    // increment by the spill size
    spillPointer += OPT_PhysicalRegisterSet.getSpillSize(type);

    if (spillPointer + WORDSIZE > frameSize) {
      frameSize = spillPointer + WORDSIZE;
    }
    return spillPointer;
  }


  /**
   * Insert a spill of a physical register before instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location, as an offset from the frame
   * pointer
   */
  final void insertSpillBefore(OPT_Instruction s, OPT_Register r,
                               byte type, int location) {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operator move = getMoveOperator(type);
    byte size = getSizeOfType(type);
    OPT_RegisterOperand rOp;
    switch(type) {
      case FLOAT_VALUE: rOp = F(r); break;
      case DOUBLE_VALUE: rOp = D(r); break;
      default: rOp = R(r); break;
    }
    OPT_StackLocationOperand spill = 
      new OPT_StackLocationOperand(true, -location, size);
    s.insertBefore(MIR_Move.create(move, spill, rOp));
  }

  /**
   * Insert a load of a physical register from a spill location before 
   * instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location
   */
  final void insertUnspillBefore(OPT_Instruction s, OPT_Register r, 
                                 byte type, int location) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operator move = getMoveOperator(type);
    byte size = getSizeOfType(type);
    OPT_RegisterOperand rOp;
    switch(type) {
      case FLOAT_VALUE: rOp = F(r); break;
      case DOUBLE_VALUE: rOp = D(r); break;
      default: rOp = R(r); break;
    }
    OPT_StackLocationOperand spill = 
      new OPT_StackLocationOperand(true, -location, size);
    s.insertBefore(MIR_Move.create(move, rOp, spill ));
  }

  /**
   * Compute the number of stack words needed to hold nonvolatile
   * registers.
   *
   * Side effects: 
   * <ul>
   * <li> updates the VM_OptCompiler structure 
   * <li> updates the <code>frameSize</code> field of this object
   * <li> updates the <code>frameRequired</code> field of this object
   * </ul>
   */
  void computeNonVolatileArea() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    if (ir.compiledMethod.isSaveVolatile()) {
      // Record that we use every nonvolatile GPR
      int numGprNv = phys.getNumberOfNonvolatileGPRs();
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);

      // set the frame size
      frameSize += numGprNv * WORDSIZE;
      frameSize = align(frameSize, STACKFRAME_ALIGNMENT);

      // TODO!!
      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)0);

      // Record that we need a stack frame.
      setFrameRequired();

      // Grab 108 bytes (same as 27 4-byte spills) in the stack
      // frame, as a place to store the floating-point state with FSAVE
      for (int i=0; i<27; i++) {
        fsaveLocation = allocateNewSpillLocation(INT_REG);
      }

      // Map each volatile register to a spill location.
      int i = 0;
      for (Enumeration e = phys.enumerateVolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        saveVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }

      // Map each non-volatile register to a spill location.
      i=0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        nonVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }

      // Set the offset to find non-volatiles.
      int gprOffset = getNonvolatileGPROffset(0);
      ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);

    } else {
      // Count the number of nonvolatiles used. 
      int numGprNv = 0;
      int i = 0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs();
           e.hasMoreElements(); ) {
        OPT_Register r = (OPT_Register)e.nextElement();
        if (r.isTouched() ) {
          // Note that as a side effect, the following call bumps up the
          // frame size.
          nonVolatileGPRLocation[i++] = allocateNewSpillLocation(INT_REG);
          numGprNv++;
        }
      }
      // Update the VM_OptCompiledMethod object.
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);
      if (numGprNv > 0) {
        int gprOffset = getNonvolatileGPROffset(0);
        ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);
        // record that we need a stack frame
        setFrameRequired();
      } else {
        ir.compiledMethod.setUnsignedNonVolatileOffset(0);
      }

      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)0);

    }
  }



  /**
   * Clean up some junk that's left in the IR after register allocation,
   * and add epilogue code.
   */ 
  void cleanUpAndInsertEpilogue() {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    OPT_Instruction inst = ir.firstInstructionInCodeOrder().getNext();
    for (; inst != null; inst = inst.nextInstructionInCodeOrder()) {
      switch (inst.getOpcode()) {
        case IA32_MOV_opcode:
          // remove frivolous moves
          OPT_Operand result = MIR_Move.getResult(inst);
          OPT_Operand val = MIR_Move.getValue(inst);
          if (result.similar(val)) {
            inst = inst.remove();
          }
          break;
        case IA32_FMOV_opcode:
          // remove frivolous moves
          result = MIR_Move.getResult(inst);
          val = MIR_Move.getValue(inst);
          if (result.similar(val)) {
            inst = inst.remove();
          }
          break;
        case IA32_RET_opcode:
          if (frameIsRequired()) {
            insertEpilogue(inst);
          }
        default:
          break;
      }
    }
    // now that the frame size is fixed, fix up the spill location code
    rewriteStackLocations();
  }

  /**
   * Insert an explicit stack overflow check in the prologue <em>after</em>
   * buying the stack frame.
   * SIDE EFFECT: mutates the plg into a trap instruction.  We need to
   * mutate so that the trap instruction is in the GC map data structures.
   *
   * @param plg the prologue instruction
   */
  private void insertNormalStackOverflowCheck(OPT_Instruction plg) {
    if (!ir.method.isInterruptible()) {
      plg.remove();
      return;
    }

    if (ir.compiledMethod.isSaveVolatile()) {
      return;
    }

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register PR = phys.getPR();
    OPT_Register ESP = phys.getESP();
    OPT_MemoryOperand M = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.activeThreadStackLimitField.getOffset(), 
			   (byte)WORDSIZE, null, null);

    //    Trap if ESP <= active Thread Stack Limit
    MIR_TrapIf.mutate(plg,IA32_TRAPIF,null,R(ESP),M,
                      OPT_IA32ConditionOperand.LE(),
                      OPT_TrapCodeOperand.StackOverflow());
  }

  /**
   * Insert an explicit stack overflow check in the prologue <em>before</em>
   * buying the stack frame.
   * SIDE EFFECT: mutates the plg into a trap instruction.  We need to
   * mutate so that the trap instruction is in the GC map data structures.
   *
   * @param plg the prologue instruction
   */
  private void insertBigFrameStackOverflowCheck(OPT_Instruction plg) {
    if (!ir.method.isInterruptible()) {
      plg.remove();
      return;
    }

    if (ir.compiledMethod.isSaveVolatile()) {
      return;
    }

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register PR = phys.getPR();
    OPT_Register ESP = phys.getESP();
    OPT_Register ECX = phys.getECX();

    //    ECX := active Thread Stack Limit
    OPT_MemoryOperand M = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.activeThreadStackLimitField.getOffset(), 
			   (byte)WORDSIZE, null, null);
    plg.insertBefore(MIR_Move.create(IA32_MOV, R(ECX), M));

    //    ECX += frame Size
    int frameSize = getFrameFixedSize();
    plg.insertBefore(MIR_BinaryAcc.create(IA32_ADD, R(ECX), I(frameSize)));
    //    Trap if ESP <= ECX
    MIR_TrapIf.mutate(plg,IA32_TRAPIF,null,R(ESP),R(ECX),
                      OPT_IA32ConditionOperand.LE(),
                      OPT_TrapCodeOperand.StackOverflow());
  }

  /**
   * Insert the prologue for a normal method.  
   *
   * Assume we are inserting the prologue for method B called from method
   * A.  
   *    <ul>
   *    <li> Perform a stack overflow check.
   *    <li> Store a back pointer to A's frame
   *    <li> Store B's compiled method id
   *    <li> Adjust frame pointer to point to B's frame
   *    <li> Save any used non-volatile registers
   *    </ul>
   */
  void insertNormalPrologue() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register ESP = phys.getESP(); 
    OPT_Register PR = phys.getPR();
    OPT_MemoryOperand fpHome = 
      OPT_MemoryOperand.BD(R(PR),
			   VM_Entrypoints.framePointerField.getOffset(),
			   (byte)WORDSIZE, null, null);

    // inst is the instruction immediately after the IR_PROLOGUE
    // instruction
    OPT_Instruction inst = ir.firstInstructionInCodeOrder().getNext().getNext();
    OPT_Instruction plg = inst.getPrev();

    int frameFixedSize = getFrameFixedSize();
    ir.compiledMethod.setFrameFixedSize(frameFixedSize);

    // I. Buy a stackframe (including overflow check)
    // NOTE: We play a little game here.  If the frame we are buying is
    //       very small (less than 256) then we can be sloppy with the 
    //       stackoverflow check and actually allocate the frame in the guard
    //       region.  We'll notice when this frame calls someone and take the
    //       stackoverflow in the callee. We can't do this if the frame is too big, 
    //       because growing the stack in the callee and/or handling a hardware trap 
    //       in this frame will require most of the guard region to complete.
    //       See libjvm.C.
    if (frameFixedSize >= 256) {
      // 1. Insert Stack overflow check.  
      insertBigFrameStackOverflowCheck(plg);

      // 2. Save caller's frame pointer
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, fpHome));

      // 3. Set my frame pointer to current value of stackpointer
      inst.insertBefore(MIR_Move.create(IA32_MOV, fpHome.copy(), R(ESP)));

      // 4. Store my compiled method id
      int cmid = ir.compiledMethod.getId();
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, I(cmid)));
    } else {
      // 1. Save caller's frame pointer
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, fpHome));

      // 2. Set my frame pointer to current value of stackpointer
      inst.insertBefore(MIR_Move.create(IA32_MOV, fpHome.copy(), R(ESP)));

      // 3. Store my compiled method id
      int cmid = ir.compiledMethod.getId();
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, I(cmid)));

      // 4. Insert Stack overflow check.  
      insertNormalStackOverflowCheck(plg);
    }

    // II. Save any used volatile and non-volatile registers
    if (ir.compiledMethod.isSaveVolatile())  {
      saveVolatiles(inst);
      saveFloatingPointState(inst);
    }
    saveNonVolatiles(inst);
    inst.insertBefore(Empty.create(IR_ENDPROLOGUE));
  }

  /**
   * Insert code into the prologue to save any used non-volatile
   * registers.  
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    // Save each non-volatile GPR used by this method. 
    int n = nNonvolatileGPRS - 1;
    for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
         e.hasMoreElements() && n >= 0 ; n--) {
      OPT_Register nv = (OPT_Register)e.nextElement();
      int offset = getNonvolatileGPROffset(n);
      OPT_Operand M = new OPT_StackLocationOperand(true, -offset, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, M, R(nv)));
    }
  }

  /**
   * Insert code before a return instruction to restore the nonvolatile 
   * registers.
   *
   * @param inst the return instruction
   */
  private void restoreNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    int n = nNonvolatileGPRS - 1;
    for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
         e.hasMoreElements() && n >= 0 ; n--) {
      OPT_Register nv = (OPT_Register)e.nextElement();
      int offset = getNonvolatileGPROffset(n);
      OPT_Operand M = new OPT_StackLocationOperand(true, -offset, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, R(nv), M));
    }
  }

  /**
   * Insert code into the prologue to save the floating point state.
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveFloatingPointState(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = new OPT_StackLocationOperand(true, -fsaveLocation, 4);
    inst.insertBefore(MIR_FSave.create(IA32_FNSAVE, M));
  }

  /**
   * Insert code into the epilogue to restore the floating point state.
   *
   * @param inst the return instruction after the epilogue.  
   */
  private void restoreFloatingPointState(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = new OPT_StackLocationOperand(true, -fsaveLocation, 4);
    inst.insertBefore(MIR_FSave.create(IA32_FRSTOR, M));
  }

  /**
   * Insert code into the prologue to save all volatile
   * registers.  
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // Save each GPR. 
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      OPT_Operand M = new OPT_StackLocationOperand(true, -location, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, M, R(r)));
    }
  }
  /**
   * Insert code before a return instruction to restore the volatile 
   * and volatile registers.
   *
   * @param inst the return instruction
   */
  private void restoreVolatileRegisters(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // Restore every GPR
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs(); 
         e.hasMoreElements(); i++){
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      OPT_Operand M = new OPT_StackLocationOperand(true, -location, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, R(r), M));
    }
  }
  /**
   * Insert the epilogue before a particular return instruction.
   *
   * @param ret the return instruction.
   */
  private void insertEpilogue(OPT_Instruction ret) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet(); 
    OPT_Register ESP = phys.getESP(); 
    OPT_Register PR = phys.getPR();

    // 1. Restore any saved registers
    if (ir.compiledMethod.isSaveVolatile())  {
      restoreVolatileRegisters(ret);
      restoreFloatingPointState(ret);
    }
    restoreNonVolatiles(ret);

    // 2. Restore caller's stackpointer and framepointer
    int frameSize = getFrameFixedSize();
    ret.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(frameSize)));
    OPT_MemoryOperand fpHome = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.framePointerField.getOffset(),
			   (byte)WORDSIZE, null, null);
    ret.insertBefore(MIR_Nullary.create(IA32_POP, fpHome));
  }

  /**
   * In instruction s, replace all appearances of a symbolic register 
   * operand with uses of the appropriate spill location, as cached by the
   * register allocator.
   *
   * @param s the instruction to mutate.
   * @param symb the symbolic register operand to replace
   */
  void replaceOperandWithSpillLocation(OPT_Instruction s, 
                                               OPT_RegisterOperand symb) {

    // Get the spill location previously assigned to the symbolic
    // register.
    int location = OPT_RegisterAllocatorState.getSpill(symb.register);

    // Create a memory operand M representing the spill location.
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = null;
    int type = phys.getPhysicalRegisterType(symb.register);
    int size = phys.getSpillSize(type);

    M = new OPT_StackLocationOperand(true, -location, (byte)size);

    // replace the register operand with the memory operand
    s.replaceOperand(symb,M);
  }

  /**
   * Does a memory operand hold a symbolic register?
   */
  private boolean hasSymbolicRegister(OPT_MemoryOperand M) {
    if (M.base != null && !M.base.register.isPhysical()) return true;
    if (M.index != null && !M.index.register.isPhysical()) return true;
    return false;
  }

  /**
   * Is s a MOVE instruction that can be generated without resorting to
   * scratch registers?
   */
  private boolean isScratchFreeMove(OPT_Instruction s) {
    if (s.operator() != IA32_MOV) return false;

    // if we don't allow ESP to float, we will always use scratch
    // registers in these move instructions.
    if (!FLOAT_ESP) return false;

    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand value = MIR_Move.getValue(s);

    // We need scratch registers for spilled registers that appear in
    // memory operands.
    if (result.isMemory()) {
      OPT_MemoryOperand M = result.asMemory();
      if (hasSymbolicRegister(M)) return false;
      // We will perform this transformation by changing the MOV to a PUSH
      // or POP.  Note that IA32 cannot PUSH/POP 8-bit quantities, so
      // disable the transformation for that case.  Also, (TODO), our
      // assembler does not emit the prefix to allow 16-bit push/pops, so
      // disable these too.  What's left?  32-bit only.
      if (M.size != 4) return false;
    }
    if (value.isMemory()) {
      OPT_MemoryOperand M = value.asMemory();
      if (hasSymbolicRegister(M)) return false;
      // We will perform this transformation by changing the MOV to a PUSH
      // or POP.  Note that IA32 cannot PUSH/POP 8-bit quantities, so
      // disable the transformation for that case.  Also, (TODO), our
      // assembler does not emit the prefix to allow 16-bit push/pops, so
      // disable these too.  What's left?  32-bit only.
      if (M.size != 4) return false;
    }
    // If we get here, all is kosher.
    return true;
  }

  /**
   * Given symbolic register r in instruction s, do we need to ensure that
   * r is in a scratch register is s (as opposed to a memory operand)
   */
  boolean needScratch(OPT_Register r, OPT_Instruction s) {
    // We never need a scratch register for a floating point value in an
    // FMOV instruction.
    if (r.isFloatingPoint() && s.operator==IA32_FMOV) return false;

    // Some MOVEs never need scratch registers
    if (isScratchFreeMove(s)) return false;

    // If s already has a memory operand, it is illegal to introduce
    // another.
    if (s.hasMemoryOperand()) return true;

    // Check the architecture restrictions.
    if (getRestrictions().mustBeInRegister(r,s)) return true;
    
    // Otherwise, everything is OK.
    return false;
  }

  /**
   * Before instruction s, insert code to adjust ESP so that it lies at a
   * particular offset from its usual location.
   */
  private void moveESPBefore(OPT_Instruction s, int desiredOffset) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet(); 
    OPT_Register ESP = phys.getESP(); 
    int delta = desiredOffset - ESPOffset;
    if (delta != 0) {
      if (canModifyEFLAGS(s)) {
	s.insertBefore(MIR_BinaryAcc.create(IA32_ADD, R(ESP), I(delta)));
      } else {
	OPT_MemoryOperand M = 
	  OPT_MemoryOperand.BD(R(ESP),delta, (byte)4, null, null); 
	s.insertBefore(MIR_Lea.create(IA32_LEA, R(ESP), M));
      }
      ESPOffset = desiredOffset;
    }
  }
  private boolean canModifyEFLAGS(OPT_Instruction s) {
    if (OPT_PhysicalDefUse.usesEFLAGS(s.operator()))
      return false;
    if (OPT_PhysicalDefUse.definesEFLAGS(s.operator()))
      return true;
    if (s.operator == BBEND) return true;
    return canModifyEFLAGS(s.nextInstructionInCodeOrder());
  }

  /**
   * Attempt to rewrite a move instruction to a NOP.
   *
   * @return true iff the transformation applies
   */
  private boolean mutateMoveToNop(OPT_Instruction s) {
    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand val = MIR_Move.getValue(s);
    if (result.isStackLocation() && val.isStackLocation()) {
      if (result.similar(val)) {
        Empty.mutate(s,NOP);
        return true;
      }
    }
    return false;
  }

  /**
   * Rewrite a move instruction if it has 2 memory operands.
   * One of the 2 memory operands must be a stack location operand.  Move
   * the SP to the appropriate location and use a push or pop instruction.
   */
  private void rewriteMoveInstruction(OPT_Instruction s) {
    // first attempt to mutate the move into a noop
    if (mutateMoveToNop(s)) return;

    OPT_Register ESP = ir.regpool.getPhysicalRegisterSet().getESP();
    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand val = MIR_Move.getValue(s);
    if (result instanceof OPT_StackLocationOperand) {
      if (val instanceof OPT_MemoryOperand || 
          val instanceof OPT_StackLocationOperand) {
        int offset = ((OPT_StackLocationOperand)result).getOffset();
        byte size = ((OPT_StackLocationOperand)result).getSize();
        offset = FPOffset2SPOffset(offset) + size;
        moveESPBefore(s,offset);
        MIR_UnaryNoRes.mutate(s,IA32_PUSH,val);
      }
    } else {
      if (result instanceof OPT_MemoryOperand) {
        if (val instanceof OPT_StackLocationOperand) {
          int offset = ((OPT_StackLocationOperand)val).getOffset();
          offset = FPOffset2SPOffset(offset);
          moveESPBefore(s,offset);
          MIR_Nullary.mutate(s,IA32_POP,result);
	}
      }
    }
  }

  /**
   * Walk through the IR.  For each OPT_StackLocationOperand, replace the
   * operand with the appropriate OPT_MemoryOperand.
   */
  private void rewriteStackLocations() {
    // ESP is initially 4 bytes above where the framepointer is going to be.
    ESPOffset = getFrameFixedSize() + 4;
    OPT_Register ESP = ir.regpool.getPhysicalRegisterSet().getESP();

    boolean seenReturn = false;
    for (OPT_InstructionEnumeration e = ir.forwardInstrEnumerator(); 
	 e.hasMoreElements();) {
      OPT_Instruction s = e.next();
      
      if (s.isReturn()) {
	seenReturn = true;
	continue;
      }

      if (s.isBranch()) {
	// restore ESP to home location at end of basic block.
	moveESPBefore(s, 0);
	continue;
      }
	
      if (s.operator() == BBEND) {
	if (seenReturn) {
	  // at a return ESP will be at FrameFixedSize, 
	  seenReturn = false;
	  ESPOffset = 0;
	} else {
	  moveESPBefore(s, 0);
	}
	continue;
      }

      if (s.operator() == ADVISE_ESP) {
        ESPOffset = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
	continue;
      }

      if (s.operator() == REQUIRE_ESP) {
	// ESP is required to be at the given offset from the bottom of the frame
	moveESPBefore(s, MIR_UnaryNoRes.getVal(s).asIntConstant().value);
	continue;
      }

      if (s.operator() == YIELDPOINT_PROLOGUE ||
	  s.operator() == YIELDPOINT_BACKEDGE ||
	  s.operator() == YIELDPOINT_EPILOGUE) {
	moveESPBefore(s, 0);
	continue;
      }

      if (s.operator() == IA32_MOV) {
        rewriteMoveInstruction(s);
      }

      // pop computes the effective address of its operand after ESP
      // is incremented.  Therefore update ESPOffset before rewriting 
      // stacklocation and memory operands.
      if (s.operator() == IA32_POP) {
	ESPOffset  += 4; 
      }	

      for (OPT_OperandEnumeration ops = s.getOperands(); ops.hasMoreElements(); ) {
        OPT_Operand op = ops.next();
        if (op instanceof OPT_StackLocationOperand) {
	  OPT_StackLocationOperand sop = (OPT_StackLocationOperand)op;
          int offset = sop.getOffset();
	  if (sop.isFromTop()) {
	    offset = FPOffset2SPOffset(offset);
	  }
	  offset -= ESPOffset;
          byte size = sop.getSize();
          OPT_MemoryOperand M = 
	    OPT_MemoryOperand.BD(R(ESP),offset,
				 size, null, null); 
          s.replaceOperand(op, M);
        } else if (op instanceof OPT_MemoryOperand) {
	  OPT_MemoryOperand M = op.asMemory();
	  if ((M.base != null && M.base.register == ESP) ||
	      (M.index != null && M.index.register == ESP)) {
	    M.disp -= ESPOffset;
	  }
	}
      }

      // push computes the effective address of its operand after ESP
      // is decremented.  Therefore update ESPOffset after rewriting 
      // stacklocation and memory operands.
      if (s.operator() == IA32_PUSH) {
	ESPOffset -= 4;
      }
    }
  }

  /**
   * @param fpOffset offset in bytes from the top of the stack frame
   * @return offset in bytes from the stack pointer.
   *
   * PRECONDITION: The final frameSize is calculated before calling this
   * routine.
   */
  private int FPOffset2SPOffset(int fpOffset) {
    // Note that SP = FP - frameSize + WORDSIZE;  
    // So, FP + fpOffset = SP + frameSize - WORDSIZE
    // + fpOffset
    return frameSize + fpOffset - WORDSIZE;
  }
  /**
   * Walk over the currently available scratch registers. 
   *
   * <p>For any scratch register r which is def'ed by instruction s, 
   * spill r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which is used by instruction s, 
   * restore r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which has current contents symb, and 
   * symb is spilled to location M, and s defs M: the old value of symb is
   * dead.  Mark this.
   *
   * <p>Invalidate any scratch register assignments that are illegal in s.
   */
  void restoreScratchRegistersBefore(OPT_Instruction s) {
    for (Iterator i = scratchInUse.iterator(); i.hasNext(); ) {
      ScratchRegister scratch = (ScratchRegister)i.next();

      if (scratch.currentContents == null) continue;
      if (verboseDebug) {
        System.out.println("RESTORE: consider " + scratch);
      }
      boolean removed = false;
      boolean unloaded = false;
      if (definedIn(scratch.scratch,s) 
          || (s.isCall() && s.operator != CALL_SAVE_VOLATILE 
              && scratch.scratch.isVolatile()) 
          || (s.operator == IA32_FNINIT && scratch.scratch.isFloatingPoint())
          || (s.operator == IA32_FCLEAR && scratch.scratch.isFloatingPoint())) {
        // s defines the scratch register, so save its contents before they
        // are killed.
        if (verboseDebug) {
          System.out.println("RESTORE : unload because defined " + scratch);
        }
        unloadScratchRegisterBefore(s,scratch);

        // update mapping information
        if (verboseDebug) System.out.println("RSRB: End scratch interval " + 
                                             scratch.scratch + " " + s);
        scratchMap.endScratchInterval(scratch.scratch,s);
        OPT_Register scratchContents = scratch.currentContents;
        if (scratchContents != null) {
          if (verboseDebug) System.out.println("RSRB: End symbolic interval " + 
                                               scratch.currentContents + " " 
                                               + s);
          scratchMap.endSymbolicInterval(scratch.currentContents,s);
        } 

        i.remove();
        removed = true;
        unloaded = true;
      }

      if (usedIn(scratch.scratch,s) ||
          !isLegal(scratch.currentContents,scratch.scratch,s) ||
          (s.operator == IA32_FCLEAR && scratch.scratch.isFloatingPoint())) {
        // first spill the currents contents of the scratch register to 
        // memory 
        if (!unloaded) {
          if (verboseDebug) {
            System.out.println("RESTORE : unload because used " + scratch);
          }
          unloadScratchRegisterBefore(s,scratch);

          // update mapping information
          if (verboseDebug) System.out.println("RSRB2: End scratch interval " + 
                                               scratch.scratch + " " + s);
          scratchMap.endScratchInterval(scratch.scratch,s);
          OPT_Register scratchContents = scratch.currentContents;
          if (scratchContents != null) {
            if (verboseDebug) System.out.println("RSRB2: End symbolic interval " + 
                                                 scratch.currentContents + " " 
                                                 + s);
            scratchMap.endSymbolicInterval(scratch.currentContents,s);
          } 

        }
        // s or some future instruction uses the scratch register, 
	// so restore the correct contents.
        if (verboseDebug) {
          System.out.println("RESTORE : reload because used " + scratch);
        }
        reloadScratchRegisterBefore(s,scratch);

        if (!removed) {
          i.remove();
          removed=true;
        }
      }
    }
  }
  /**
   * Initialize some architecture-specific state needed for register
   * allocation.
   */
  void initForArch(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // We reserve the last (bottom) slot in the FPR stack as a scratch register.
    // This allows us to do one push/pop sequence in order to use the
    // top of the stack as a scratch location
    phys.getFPR(7).reserveRegister();
  }

  /**
   * Is a particular instruction a system call?
   */
  boolean isSysCall(OPT_Instruction s) {
    return s.operator == IA32_SYSCALL;
  } 
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Handle exception delivery and stack unwinding for methods 
 *  compiled by optimizing Compiler 
 *
 * @author Dave Grove
 */
final class VM_OptExceptionDeliverer extends VM_ExceptionDeliverer 
  implements VM_Constants {

  private static final boolean TRACE = false;
  
  /** 
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod compiledMethod,
			VM_Address catchBlockInstructionAddress,
			Throwable exceptionObject,
			VM_Registers registers)  {
    VM_OptCompiledMethod optMethod = (VM_OptCompiledMethod)compiledMethod;
    VM_Address fp = registers.getInnermostFramePointer();
    VM_Thread myThread = VM_Thread.getCurrentThread();
    
    if (TRACE) {
      VM.sysWrite("Frame size of ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite(" is ");
      VM.sysWrite(optMethod.getFrameFixedSize());
      VM.sysWrite("\n");
    }

    // reset sp to "empty params" state (ie same as it was after prologue)
    VM_Address sp = fp.sub(optMethod.getFrameFixedSize());
    registers.gprs[STACK_POINTER] = sp.toInt();

    // store exception object for later retrieval by catch block
    int offset = optMethod.getUnsignedExceptionOffset();
    if (offset != 0) {
      // only put the exception object in the stackframe if the catch block is expecting it.
      // (if the method hasn't allocated a stack slot for caught exceptions, then we can safely
      //  drop the exceptionObject on the floor).
      VM_Magic.setObjectAtOffset(VM_Magic.addressAsObject(fp), -offset, exceptionObject);
      if (TRACE) {
	VM.sysWrite("Storing exception object ");
	VM.sysWrite(VM_Magic.objectAsAddress(exceptionObject));
	VM.sysWrite(" at offset ");
	VM.sysWrite(offset);
	VM.sysWrite(" from framepoint ");
	VM.sysWrite(fp);
	VM.sysWrite("\n");
      }
    } 

    if (TRACE) {
      VM.sysWrite("Registers before delivering exception in ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }

    // set address at which to resume executing frame
    registers.ip = catchBlockInstructionAddress;

    if (TRACE) {
      VM.sysWrite("Set ip to ");
      VM.sysWrite(registers.ip);
      VM.sysWrite("\n");
    }

    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called

    if (VM.VerifyAssertions) VM.assert(registers.inuse == true);
    registers.inuse = false;

    // 'give back' the portion of the stack we borrowed to run 
    // exception delivery code when invoked for a hardware trap.
    // If this was a straight software trap (athrow) then setting 
    // the stacklimit should be harmless, since the stacklimit should already have exactly
    // the value we are setting it too. 
    if (!myThread.hardwareExceptionRegisters.inuse) {
      myThread.stackLimit = VM_Magic.objectAsAddress(myThread.stack).add(STACK_SIZE_GUARD);
      VM_Processor.getCurrentProcessor().activeThreadStackLimit = myThread.stackLimit;
    }

    // "branches" to catchBlockInstructionAddress
    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }
  

  /**
   * Unwind a stackframe.
   */
  void unwindStackFrame(VM_CompiledMethod compiledMethod, 
			VM_Registers registers) {
    VM_Address fp = registers.getInnermostFramePointer();
    VM_OptCompiledMethod optMethod = (VM_OptCompiledMethod)compiledMethod;
    
    if (TRACE) {
      VM.sysWrite("Registers before unwinding frame for ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }

    // restore non-volatile registers
    int frameOffset = optMethod.getUnsignedNonVolatileOffset();
    for (int i = optMethod.getFirstNonVolatileGPR(); 
	 i<NUM_NONVOLATILE_GPRS; 
	 i++, frameOffset += 4) {
      registers.gprs[NONVOLATILE_GPRS[i]] = VM_Magic.getMemoryWord(fp.sub(frameOffset));
    }
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0);
    
    registers.unwindStackFrame();

    if (TRACE) {
      VM.sysWrite("Registers after unwinding frame for ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * An instance of this class provides iteration across the references 
 * represented by a frame built by the OPT compiler.
 *
 * The architecture-specific version of the GC Map iterator.  It inherits
 * its architecture-independent code from VM_OptGenericGCMapIterator.
 * This version is for IA32
 *
 * @author Michael Hind
 */
public final class VM_OptGCMapIterator extends VM_OptGenericGCMapIterator
  implements VM_Uninterruptible {

  private static final boolean DEBUG = false;
 
  public VM_OptGCMapIterator(int[] registerLocations) {
    super(registerLocations);
  }

  /** 
   * If any non-volatile gprs were saved by the method being processed
   * then update the registerLocations array with the locations where the
   * registers were saved.  Also, check for special methods that also
   * save the volatile gprs.
   */
  void updateLocateRegisters() {

    //           HIGH MEMORY
    //
    //       +---------------+                                           |
    //  FP-> |   saved FP    |  <-- this frame's caller's frame          |
    //       +---------------+                                           |
    //       |    cmid       |  <-- this frame's compiledmethod id       |
    //       +---------------+                                           |
    //       |               |                                           |
    //       |  Spill Area   |  <-- spills and other method-specific     |
    //       |     ...       |      compiler-managed storage             |
    //       +---------------+                                           |
    //       |   Saved FP    |     only SaveVolatile Frames              |   
    //       |    State      |                                           |
    //       +---------------+                                           |
    //       |  VolGPR[0]    |                                           
    //       |     ...       |     only SaveVolatile Frames              
    //       |  VolGPR[n]    |                                           
    //       +---------------+                                           
    //       |  NVolGPR[k]   |  <-- cm.getUnsignedNonVolatileOffset()  
    //       |     ...       |   k == cm.getFirstNonVolatileGPR()      
    //       |  NVolGPR[n]   |                                           
    //       +---------------+                                           
    //
    //           LOW MEMORY
    
    int frameOffset = compiledMethod.getUnsignedNonVolatileOffset();
    if (frameOffset >= 0) {
      // get to the non vol area
      VM_Address nonVolArea = framePtr.sub(frameOffset);
    
      // update non-volatiles
      int first = compiledMethod.getFirstNonVolatileGPR();
      if (first >= 0) {
	// move to the beginning of the nonVol area
	VM_Address location = nonVolArea;
	
	for (int i = first; i < NUM_NONVOLATILE_GPRS; i++) {
	  // determine what register index corresponds to this location
	  int registerIndex = NONVOLATILE_GPRS[i];
	  registerLocations[registerIndex] = location.toInt();
          if (DEBUG) {
            VM.sysWrite("UpdateRegisterLocations: Register ");
            VM.sysWrite(registerIndex);
            VM.sysWrite(" to Location ");
            VM.sysWrite(location.toInt());
            VM.sysWrite("\n");
          }
	  location = location.sub(4);
	}
      }
      
      // update volatiles if needed
      if (compiledMethod.isSaveVolatile()) {
	// move to the beginning of the nonVol area
	VM_Address location = nonVolArea.add(4 * NUM_VOLATILE_GPRS);
	
	for (int i = 0; i < NUM_VOLATILE_GPRS; i++) {
	  // determine what register index corresponds to this location
	  int registerIndex = VOLATILE_GPRS[i];
	  registerLocations[registerIndex] = location.toInt();
          if (DEBUG) {
            VM.sysWrite("UpdateRegisterLocations: Register ");
            VM.sysWrite(registerIndex);
            VM.sysWrite(" to Location ");
            VM.sysWrite(location.toInt());
            VM.sysWrite("\n");
          }
	  location = location.sub(4);
	}
      }
    }
  }

  /** 
   *  Determine the spill location given the frame ptr and spill offset.
   *  (The location of spills varies among architectures.)
   *  @param framePtr the frame pointer
   *  @param offset  the offset for the spill 
   *  @return the resulting spill location
   */
  public VM_Address getStackLocation(VM_Address framePtr, int offset) {
    return framePtr.sub(offset);
  }

  /** 
   *  Get address of the first spill location for the given frame ptr
   *  @param the frame pointer
   *  @return the first spill location
   */
  public VM_Address getFirstSpillLoc() {
    return framePtr.sub(-VM.STACKFRAME_BODY_OFFSET);
  }

  /** 
   *  Get address of the last spill location for the given frame ptr
   *  @param the frame pointer
   *  @return the last spill location
   */
  public VM_Address getLastSpillLoc() {
    if (compiledMethod.isSaveVolatile()) {
      return framePtr.sub(compiledMethod.getUnsignedNonVolatileOffset() - 4 - SAVE_VOL_SIZE);
    } else {
      return framePtr.sub(compiledMethod.getUnsignedNonVolatileOffset() - 4);
    }
  }

  final static int VOL_SIZE = 4 * NUM_VOLATILE_GPRS;
  final static int SAVE_VOL_SIZE = VOL_SIZE + VM.FPU_STATE_SIZE;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * This interface holds constants for the Opt GC map code specific to IA32
 *
 * @author Michael Hind
 */
interface VM_OptGCMapIteratorConstants extends OPT_PhysicalRegisterConstants {
  
  // NOTE: The following two constants seem to imply that registers 
  //       that can hold references are contiguous.  This is not true,
  //       in general, however, for the GC map code we only need to make
  //       sure that all such registers are included in the range defined
  //       below these contants.

  /*
   * The index of the first nonvolatile register that may hold a reference, 
   */
  static final int FIRST_GCMAP_REG = 0;  

  /*
   * the index of last register that may hold a reference
   */
  static final int LAST_GCMAP_REG = 7;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Disassembler for the Intel instruction set
 *
 * Source code in C borrowed from C++ VisualAge, 
 * provided by Steve Turner (IBM Austin)
 * Minor change:  change output to lower case
 *
 * @author Ton Ngo 
 * @date 2/12/2001 
 */

class IntelDisassembler {

  public static byte[] intToByteArray(int[] intArray) {
    byte byteArray[] = new byte[intArray.length*4];
    for (int i=0; i<intArray.length; i++) {
      int word = intArray[i];
      int offset = i*4;
      byteArray[offset]   = (byte) ( word & 0x000000FF);
      byteArray[offset+1] = (byte) ((word & 0x0000FF00) >> 8);
      byteArray[offset+2] = (byte) ((word & 0x00FF0000) >> 16);
      byteArray[offset+3] = (byte) ((word & 0xFF000000) >> 24);
    }
    return byteArray;
  }

  /**
   * Disassemble up to the number of instruction, 
   * -if count is nonzero, disassemble up to count
   * -if count is zero, disassemble until there is no more valid 
   *  instructions in the buffer
   * 
   */
  public static native String disasm(byte[] instr, int count, int address);

  public static String disasm(byte[] instr) {
    return disasm(instr, 0, 0);
  }

  /**
   * Convenience methods:  convert from int to byte
   */
  public static String disasm(int instrArray[], int address) {
    return disasm(intToByteArray(instrArray), 0, address);
  }

  public static String disasm(int instrArray[], int count, int address) {
    return disasm(intToByteArray(instrArray), count, address);
  }


  /**
   * Compute the instruction length for a block of instruction
   * 
   *
   */
  public static native byte[] instructionLength(byte[] instr);
  public static byte[] instructionLength(int instrArray[]) {
    return instructionLength(intToByteArray(instrArray));
  }


  /**
   * Test if this is a CALL instruction
   */
  public static boolean isCallInstruction(int instrArray[]) {
    String instructionString = disasm(instrArray, 1, 0);
    if (instructionString.indexOf("CALL") == -1)
      return false;
    else 
      return true;
  }

  /**
   * Compute the target address for a call, jump, or jump conditional
   */
  public static native int getBranchTarget(byte instrArray[], int[] regs);
  // just a way to return more info from decoding the branch target
  public static native boolean isLastAddressIndirect();   

  public static int getBranchTarget(int instrArray[], int[] regs) {
    byte bArray[] = intToByteArray(instrArray);
    return getBranchTarget(bArray, regs);
  }

  /**
   * for stand alone testing 
   */
  static byte testInstructions[] = {
    (byte) 0x55, (byte) 0x89, (byte) 0xe5, (byte) 0x83, 
    (byte) 0xec, (byte) 0x08, (byte) 0xc7, (byte) 0x45, 
    (byte) 0xfc, (byte) 0x00, (byte) 0x00, (byte) 0x00, 
    (byte) 0x00, (byte) 0xc7, (byte) 0x45, (byte) 0xf8, 
    (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, 
    (byte) 0x83, (byte) 0x7d, (byte) 0xf8, (byte) 0x09, 
    (byte) 0x7e, (byte) 0x06, (byte) 0xeb, (byte) 0x14, 
    (byte) 0x8d, (byte) 0x74, (byte) 0x26, (byte) 0x00, 
    (byte) 0x8b, (byte) 0x45, (byte) 0xf8, (byte) 0x01, 
    (byte) 0x45, (byte) 0xfc, (byte) 0xff, (byte) 0x45, 
    (byte) 0xf8, (byte) 0xeb, (byte) 0xe9, (byte) 0x90, 
    (byte) 0x8d, (byte) 0x74, (byte) 0x26, (byte) 0x00, 
    (byte) 0x8b, (byte) 0x45, (byte) 0xfc, (byte) 0x50, 
    (byte) 0x68, (byte) 0x70, (byte) 0x84, (byte) 0x04, 
    (byte) 0x08, (byte) 0xe8, (byte) 0xfa, (byte) 0xfe, 
    (byte) 0xff, (byte) 0xff, (byte) 0x83, (byte) 0xc4, 
    (byte) 0x08, (byte) 0xc9, (byte) 0xc3, (byte) 0x90, 
    (byte) 0x90};

  // static byte testInstructions[] = {(byte) 0x55  };


  // for testing 
  public static void main(String args[]) {

    System.loadLibrary("IntelDisassembler");
    System.out.println("Disassemble Intel: " + testInstructions.length +
		       " bytes");
    System.out.println("return mnemonic: " + disasm(testInstructions));
    
  }


}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class compiles the prolog and epilog for all code that makes
 * the transition between Java and Native C
 * 2 cases:
 *  -from Java to C:  all user-defined native methods
 *  -C to Java:  all JNI functions in VM_JNIFunctions.java
 *
 * @author Ton Ngo
 * @author Steve Smith
 */
public class VM_JNICompiler implements VM_JNILinuxConstants, VM_BaselineConstants {

  // offsets to saved regs and addresses in java to C glue frames
  // EDI (JTOC) and EBX are nonvolatile registers in RVM
  //
  private static final int SAVED_GPRS = 5; 
  static final int EDI_SAVE_OFFSET = STACKFRAME_BODY_OFFSET;
  static final int EBX_SAVE_OFFSET = STACKFRAME_BODY_OFFSET - WORDSIZE;
  static final int EBP_SAVE_OFFSET = EBX_SAVE_OFFSET - WORDSIZE;
  static final int JNI_RETURN_ADDRESS_OFFSET = EBP_SAVE_OFFSET - WORDSIZE;
  static final int JNI_PR_OFFSET = JNI_RETURN_ADDRESS_OFFSET - WORDSIZE;

  // following used in prolog & epilog for JNIFunctions
  // offset of saved offset to preceeding java frame
  static final int SAVED_JAVA_FP_OFFSET = STACKFRAME_BODY_OFFSET;

  // following used in VM_Compiler to compute offset to first local:
  // includes 5 words:
  //   SAVED_JAVA_FP,  PR (ESI), S0 (ECX), EBX, and JTOC (EDI)
  static final int SAVED_GPRS_FOR_JNI = 5;

  /*****************************************************************
   * Handle the Java to C transition:  native methods
   *
   */

  static VM_MachineCode generateGlueCodeForNative (VM_CompiledMethod cm) {
    int compiledMethodId = cm.getId();
    VM_Method method     = cm.getMethod();
    VM_Assembler asm	 = new VM_Assembler(100);   // some size for the instruction array
    int nativeIP         = method.getNativeIP();
    // recompute some constants
    int parameterWords   = method.getParameterWords();

    // Meaning of constant offset into frame:
    // STACKFRAME_HEADER_SIZE =  12              (CHECK ??)
    // SAVED_GPRS = 4 words/registers
    // Stack frame:
    //        on entry          after prolog
    //
    //      high address	high address
    //      |          |	|          | Caller frame
    //      |          |	|          |
    // +    |arg 0     |	|arg 0     |    -> firstParameterOffset
    // +    |arg 1     |	|arg 1     |
    // +    |...       |	|...       |
    // +8   |arg n-1   |	|arg n-1   |    
    // +4   |returnAddr|	|returnAddr|
    //  0   +	       +	+saved FP  + <---- FP for glue frame
    // -4   |	       |	|methodID  |
    // -8   |	       |	|saved EDI |    -> STACKFRAME_BODY_OFFSET = -8
    // -C   |	       |	|saved EBX |
    // -10  |	       |	|saved EBP |
    // -14  |	       |	|returnAddr|  (return from OutOfLine to generated epilog)    
    // -18  |	       |	|saved PR  |
    // -1C  |	       |	|arg n-1   |  reordered args to native method
    // -20  |	       |	| ...      |  ...
    // -24  |	       |	|arg 1     |  ...
    // -28  |	       |	|arg 0     |  ...
    // -2C  |	       |	|class/obj |  required second arg to native method
    // -30  |	       |	|jniEnv    |  required first arg to native method
    // -34  |	       |	|          |    
    //      |	       |	|          |    
    //      |	       |	|          |    
    //       low address	 low address


    // TODO:  check and resize stack once on the lowest Java to C transition
    // on the stack.  Not needed if we use the thread original stack

    // Fill in frame header - similar to normal prolog
    prepareStackHeader(asm, method, compiledMethodId);

    // Process the arguments - specific to method being called
    storeParametersForLintel(asm, method);
    
    // load address of native into S0
    asm.emitMOV_Reg_Imm (S0, nativeIP);  

    // branch to outofline code in bootimage
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.invokeNativeFunctionInstructionsField.getOffset());

    // return here from VM_OutOfLineMachineCode upon return from native code

    // PR and RVM JTOC restored, T0,T1 contain return from native call

    //If the return type is reference, look up the real value in the JNIref array 

    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv    
    if (method.getReturnType().isReferenceType()) {
      asm.emitADD_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsField.getOffset());      // T0 <- address of entry (not index)
      asm.emitMOV_Reg_RegInd (T0, T0);   // get the reference
    } else if (method.getReturnType().isLongType()) {
      asm.emitPUSH_Reg(T1);    // need to use T1 in popJNIrefForEpilog and to swap order T0-T1  
    }

    // CHECK - may not do anything - leave below will remove whole frame
    // asm.emitPUSH_Reg(T0);                // push the return value onto the stack

    // pop frame in JNIRefs array (need to use
    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv    
    popJNIrefForEpilog(asm);                                
    
    // then swap order of T0 and T1 for long
    if (method.getReturnType().isLongType()) {
      asm.emitMOV_Reg_Reg(T1, T0);  
      asm.emitPOP_Reg(T0);
    }

    // CHECK EXCEPTION AND BRANCH TO ATHROW CODE OR RETURN NORMALLY

    // get pending exception from JNIEnv
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0,  S0, VM_Entrypoints.jniEnvField.getOffset());        	  // S0 <- jniEnv    
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIPendingExceptionField.getOffset());  // EBX <- JNIPendingException
    asm.emitMOV_RegDisp_Imm (S0, VM_Entrypoints.JNIPendingExceptionField.getOffset(), 0);    // clear the current pending exception

    asm.emitCMP_Reg_Imm(EBX, 0);   // check for exception pending:  JNIPendingException = non zero
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);            // Br if yes

    // if pending exception, discard the return value and current stack frame
    // then jump to athrow 
    asm.emitMOV_Reg_Reg     (T0, EBX);
    asm.emitMOV_Reg_RegDisp (T1, JTOC, VM_Entrypoints.athrowMethod.getOffset()); // acquire jump addr before restoring nonvolatiles

    asm.emitMOV_Reg_Reg     (SP, EBP);                      // discard current stack frame
    asm.emitMOV_Reg_RegDisp (JTOC, SP, EDI_SAVE_OFFSET);   // restore nonvolatile EDI/JTOC register
    asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET);    // restore nonvolatile EBX register
    asm.emitMOV_Reg_RegDisp (EBP, SP, EBP_SAVE_OFFSET);    // restore nonvolatile EBP register

    asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset());

    // don't use CALL since it will push on the stack frame the return address to here 
    asm.emitJMP_Reg(T1); // jumps to VM_Runtime.athrow

    fr.resolve(asm);  // branch to here if no exception 

    // no exception, proceed to return to caller    
    asm.emitMOV_Reg_Reg(SP, EBP);                           // discard current stack frame

    asm.emitMOV_Reg_RegDisp (JTOC, SP, EDI_SAVE_OFFSET);   // restore nonvolatile EDI/JTOC register
    asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET);    // restore nonvolatile EBX register
    asm.emitMOV_Reg_RegDisp (EBP, SP, EBP_SAVE_OFFSET);    // restore nonvolatile EBP register

    asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset());

    // return to caller 
    // pop parameters from stack (Note that parameterWords does not include "this")
    if (method.isStatic())
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE); 
    else
      asm.emitRET_Imm((parameterWords+1) << LG_WORDSIZE); 

    // return asm.makeMachineCode();
    return new VM_MachineCode(asm.getMachineCodes(), null);

  }

  /**************************************************************
   * Prepare the stack header for Java to C transition
   *         before               after
   *	   high address		high address
   *	   |          |		|          | Caller frame
   *	   |          |		|          |
   *  +    |arg 0     |		|arg 0     |    
   *  +    |arg 1     |		|arg 1     |
   *  +    |...       |		|...       |
   *  +8   |arg n-1   |		|arg n-1   |    
   *  +4   |returnAddr|		|returnAddr|
   *   0   +	      +		+saved FP  + <---- FP for glue frame
   *  -4   |	      |		|methodID  |
   *  -8   |	      |		|saved EDI |  (EDI == JTOC - for baseline methods)  
   *  -C   |	      |		|saved EBX |    
   *  -10  |	      |	        |	   |	
   *  
   *  
   *  
   */
  static void prepareStackHeader(VM_Assembler asm, VM_Method method, int compiledMethodId) {

    // set 2nd word of header = return address already pushed by CALL
    asm.emitPUSH_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset());

    // start new frame:  set FP to point to the new frame
    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              SP);

    // set first word of header: method ID
    asm.emitMOV_RegDisp_Imm (SP, STACKFRAME_METHOD_ID_OFFSET, compiledMethodId); 


    // save nonvolatile registrs: JTOC/EDI, EBX, EBP
    asm.emitMOV_RegDisp_Reg (SP, EDI_SAVE_OFFSET, JTOC); 
    asm.emitMOV_RegDisp_Reg (SP, EBX_SAVE_OFFSET, EBX);
    asm.emitMOV_RegDisp_Reg (SP, EBP_SAVE_OFFSET, EBP);
    
    asm.emitMOV_Reg_Reg     (EBP, SP); // Establish EBP as the framepointer for use in the rest of the glue frame

    // restore JTOC with the value saved in VM_Processor.jtoc for use in prolog 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());
  }

  /**************************************************************
   * Process the arguments:
   *   -insert the 2 JNI args
   *   -replace pointers
   *   -reverse the order of the args from Java to fit the C convention
   *   -
   *
   *         before               after
   *
   *	   high address		high address
   *	   |          | 	|          | Caller frame
   *	   |          |		|          | 
   *  +    |arg 0     | 	|arg 0     | 	-> firstParameterOffset
   *  +    |arg 1     |		|arg 1     | 
   *  +    |...       |		|...       | 
   *  +8   |arg n-1   | 	|arg n-1   | 	
   *  +4   |returnAddr|		|returnAddr| 
   *   0   +saved FP  + 	+saved FP  + <---- FP for glue frame
   *  -4   |methodID  |		|methodID  | 
   *  -8   |saved EDI | 	|saved EDI | 	-> STACKFRAME_BODY_OFFSET = -8
   *  -C   |saved EBX | 	|saved EBX | 	
   *  -10  |	      | 	|returnAddr|  (return from OutOfLine to generated epilog)    
   *  -14  |	      |	        |saved PR  |
   *  -18  |	      |	        |arg n-1   |  reordered args to native method (firstLocalOffset
   *  -1C  |	      |	        | ...      |  ...
   *  -20  |	      |  	|arg 1     |  ...
   *  -24  |	      |	        |arg 0     |  ...
   *  -28  |	      |	        |class/obj |  required second arg 
   *  -2C  |	      |   SP -> |jniEnv    |  required first arg  (emptyStackOffset)
   *  -30  |	      |	        |          |    
   *	   |          |  	|          | 	
   *	    low address		 low address
   */
  static void storeParametersForLintel(VM_Assembler asm, VM_Method method) {
    VM_Class klass	     = method.getDeclaringClass();
    int parameterWords       = method.getParameterWords();
    int savedRegistersSize   = SAVED_GPRS<<LG_WORDSIZE;
    int firstLocalOffset     = STACKFRAME_BODY_OFFSET - savedRegistersSize ;
    int emptyStackOffset     = firstLocalOffset - ((parameterWords+2) << LG_WORDSIZE) + WORDSIZE;
    int firstParameterOffset = STACKFRAME_BODY_OFFSET + STACKFRAME_HEADER_SIZE + (parameterWords<<LG_WORDSIZE);
    int firstActualParameter;


    VM_Type[] types = method.getParameterTypes();   // does NOT include implicit this or class ptr
    int numArguments = types.length;                // number of arguments for this method
    int numRefArguments = 1;                        // initialize with count of 1 for the JNI arg
    int numFloats = 0;                              // number of float or double arguments

    // quick count of number of references
    for (int i=0; i<numArguments; i++) {
      if (types[i].isReferenceType())
	numRefArguments++;
      if (types[i].isFloatType() || types[i].isDoubleType())
	numFloats++;
    }


    // first push the parameters passed in registers back onto the caller frame
    // to free up the registers for use
    // The number of registers holding parameter is 
    // VM_RegisterConstants.NUM_PARAMETER_GPRS
    // Their indices are in VM_RegisterConstants.VOLATILE_GPRS[]
    int gpr = 0;
    // note that firstParameterOffset does not include "this"
    int parameterOffset = firstParameterOffset;   

    // handle the "this" parameter
    if (!method.isStatic()) {
      asm.emitMOV_RegDisp_Reg(EBP, firstParameterOffset+WORDSIZE, 
                              VOLATILE_GPRS[gpr]);
      gpr++;
    }
    
    for (int i=0; i<numArguments && gpr<NUM_PARAMETER_GPRS; i++) {
      if (types[i].isDoubleType()) {
	parameterOffset -= 2*WORDSIZE;
	continue;
      } else if (types[i].isFloatType()) {
	parameterOffset -= WORDSIZE;
	continue;
      } else if (types[i].isLongType()) {
	if (gpr<NUM_PARAMETER_GPRS) {   // get the hi word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
	if (gpr<NUM_PARAMETER_GPRS) {    // get the lo word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
      } else {
	if (gpr<NUM_PARAMETER_GPRS) {   // all other types fit in one word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
      }
    }


    // bump SP to set aside room for the args + 2 additional JNI args
    asm.emitADD_Reg_Imm (SP, emptyStackOffset);                       

    // SP should now point to the bottom of the argument stack, 
    // which is arg[n-1]


    // Prepare the side stack to hold new refs
    // Leave S0 holding the jniEnv pointer
    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());

    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv

    // save PR in the jniEnv for JNI call from native
    VM_ProcessorLocalState.emitStoreProcessor(asm, S0, VM_Entrypoints.JNIEnvSavedPRField.getOffset());

    // save FP for glue frame in JNI env - used by GC when in C
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNITopJavaFPField.getOffset(), EBP);  // jniEnv.JNITopJavaFP <- FP

    //********************************************
    // Between HERE and THERE, S0 and T0 are in use
    // >>>> HERE <<<<
    startJNIrefForProlog(asm, numRefArguments);
    
    // Insert the JNI arg at the first entry:  JNI_Environment as the pointer to 
    // the JNI functions array
    // pr -> thread -> jniEnv -> function array
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIEnvAddressField.getOffset()); // ebx <- JNIEnvAddress
    asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset, EBX);                  // store as 1st arg

    // added 7/05 - check later SES
    // store current processors status word address in word after 
    // passed ->JNIFunctions (in EBX). upon return or reentry to java this
    // word is tested & it is wrong to use the processor object to find its address
    // since it may have been moved by GC while in native code.
    //
    // ASSUME T1 (EDX) is available ??? looks like PR still valid
    // COULD use JTOC since it is reloaded immediately below - 
    
    // T1<-addr or processor statusword 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T1,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    asm.emitMOV_RegDisp_Reg (EBX, WORDSIZE, T1);

    // Insert the JNI arg at the second entry: class or object as a jref index
    // first reload JTOC,  baseline compiler assumes JTOC register -> jtoc
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());
    if (method.isStatic()) {
      // For static method, push on arg stack the VM_Class object
      //    jtoc[tibOffset] -> class TIB ptr -> first TIB entry -> class object -> classForType
      klass.getClassForType();     // ensure the Java class object is created
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (EBX, JTOC, tibOffset);
      asm.emitMOV_Reg_RegInd (EBX, EBX);
      asm.emitMOV_Reg_RegDisp (EBX, EBX, VM_Entrypoints.classForTypeField.getOffset());
      firstActualParameter = 0;
    } else {
      // For nonstatic method, "this" pointer should be the first arg in the caller frame,
      // make it the 2nd arg in the glue frame
      asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset+WORDSIZE);
      firstActualParameter = 1;
    }

    // Generate the code to push this pointer in ebx on to the JNIRefs stack 
    // and use the JREF index in its place
    // Assume: S0 is the jniEnv pointer (left over from above)
    //         T0 contains the address to TOP of JNIRefs stack
    // Kill value in ebx
    // On return, ebx contains the JREF index
    pushJNIref(asm);
    asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + WORDSIZE, EBX);  // store as 2nd arg

    // VM.sysWrite("VM_JNICompiler:  processing args "); 
    // VM.sysWrite(numArguments);
    // VM.sysWrite("\n");

    // Now fill in the rest:  copy parameters from caller frame into glue frame 
    // in reverse order for C
    int i=parameterWords - 1;   
    int fpr = numFloats-1;
    for (int argIndex=numArguments-1; argIndex>=0; argIndex--) {

      // for reference, substitute with a jref index
      if (types[argIndex].isReferenceType()) {
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitCMP_Reg_Imm(EBX, 0);
	VM_ForwardReference beq = asm.forwardJcc(asm.EQ);
	pushJNIref(asm);
	beq.resolve(asm);
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	i--;
      
      // for float and double, the first NUM_PARAMETER_FPRS args have
      // been loaded in the FPU stack, need to pop them from there
      } else if (types[argIndex].isDoubleType()) {
	if (fpr < NUM_PARAMETER_FPRS) {
	  // pop this 2-word arg from the FPU stack
	  asm.emitFSTP_RegDisp_Reg_Quad(EBP, emptyStackOffset + (WORDSIZE*(2+ i - 1)), FP0);	
	} else {
	  // copy this 2-word arg from the caller frame
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i -1)), EBX);
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - ((i-1)*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i)), EBX);	  
	}
	i-=2;
	fpr--;
      } else if (types[argIndex].isFloatType()) {
	if (fpr < NUM_PARAMETER_FPRS) {
	  // pop this 1-word arg from the FPU stack
	  asm.emitFSTP_RegDisp_Reg(EBP, emptyStackOffset + (WORDSIZE*(2+ i)), FP0);
	} else {
	  // copy this 1-word arg from the caller frame
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	}
	i--;
	fpr--;
      } else if (types[argIndex].isLongType()) {
	//  copy other 2-word parameters: observe the high/low order when moving
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i - 1)), EBX);
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - ((i-1)*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i)), EBX);
	i-=2;
      } else {
	// copy other 1-word parameters
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	i--;
      }
    }

    // don't need any more since the top was bumped at the beginning
    // endJNIrefForProlog(asm);

    // >>>> THERE <<<<
    // End use of T0 and S0
  }

  /**************************************************************
   * Generate code to convert a pointer value to a JREF index
   * This includes the following steps:
   *   (1) start by calling startJNIrefForProlog()
   *   (2) for each reference, put it in ebx and call pushJNIref() 
   *       to convert; the handler will be left in ebx
   *   (3) finish by calling endJNIrefForProlog()
   *  
   *   +-------+
   *   |       |  <-JNIRefsMax (byte index of last entry)
   *   |       |
   *   |       |
   *   |       |  <-JNIRefsTop (byte index of valid top entry)
   *   |       |
   *   |       |
   *   |FPindex|  <-JNIRefsSavedFP (byte index of Java to C transition)
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |  <-JNIRefs
   *   +-------+
   *
   */

  /**
   * Start a new frame for this Java to C transition:
   * Expect: 
   *    -S0 contains a pointer to the VM_Thread.jniEnv
   * Perform these steps:
   *    -push current SavedFP index 
   *    -set SaveFP index <- current TOP
   * Leave registers ready for more push onto the jniEnv.JNIRefs array
   *    -S0 holds jniEnv so we can update jniEnv.JNIRefsTop and 
   *    -T0 holds address of top =  starting address of jniEnv.JNIRefs array + jniEnv.JNIRefsTop
   *     T0 is to be incremented before each push
   *    S0              ebx                         T0
   *  jniEnv        
   *    .         jniEnv.JNIRefs             jniEnv.JNIRefsTop
   *    .               .                    jniEnv.JNIRefsTop + 4
   *    .         jniEnv.JNIRefsSavedFP            .
   *    .               .                    jniEnv.JNIRefsTop
   *    .               .                    address(JNIRefsTop)             
   *    .
   */
  static void startJNIrefForProlog(VM_Assembler asm, int numRefsExpected) {

    // on entry, S0 contains a pointer to the VM_Thread.jniEnv
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base

    // get and check index of top for overflow
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());  // T0 <- index of top
    asm.emitADD_Reg_Imm(T0, numRefsExpected * WORDSIZE);                // increment index of top 
    asm.emitCMP_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsMaxField.getOffset());   // check against JNIRefsMax for overflow 
    // TODO:  Do something if overflow!!!

    // get and increment index of top 
    // asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopOffset);  // T0 <- index of top
    // asm.emitADD_Reg_Imm(T0, WORDSIZE);                                  // increment index of top        
    // asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopOffset, T0);  // jniEnv.JNIRefsTop <- T0
    

    asm.emitADD_RegDisp_Imm (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), WORDSIZE); // increment index of top
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());  // T0 <- index of top
    asm.emitADD_Reg_Reg(T0, EBX);                                       // T0 <- address of top (not index)

    // start new frame:  push current JNIRefsSavedFP onto stack and set it to the new top index    
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset()); // ebx <- jniEnv.JNIRefsSavedFP
    asm.emitMOV_RegInd_Reg  (T0, EBX);                                   // push (T0) <- ebx
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());   // reload T0 <- index of top
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), T0); // jniEnv.JNIRefsSavedFP <- index of top 

    // leave T0 with address pointing to the top of the frame for more push later
    asm.emitADD_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsField.getOffset());       // recompute T0 <- address of top (not index)

    // and go ahead and bump up the Top offset by the amount expected
    asm.emitADD_RegDisp_Imm(S0, VM_Entrypoints.JNIRefsTopField.getOffset(), numRefsExpected * WORDSIZE);
  }

  /**
   * Push a pointer value onto the JNIRefs array, 
   * Expect:
   *   -T0 pointing to the address of the valid top 
   *   -the pointer value in register ebx
   *   -the space in the JNIRefs array has checked for overflow 
   *   by startJNIrefForProlog()
   * Perform these steps:
   *   -increment the JNIRefsTop index in ebx by 4
   *   -push a pointer value in ebx onto the top of the JNIRefs array
   *   -put the JNIRefsTop index into the sourceReg as the replacement for the pointer
   * Note:  jniEnv.JNIRefsTop is not updated yet
   *
   */
  static void pushJNIref(VM_Assembler asm) {
    asm.emitADD_Reg_Imm (T0, WORDSIZE);                            // increment top address
    asm.emitMOV_RegInd_Reg(T0, EBX);                               // store ref at top
    asm.emitMOV_Reg_Reg (EBX, T0);                                 // replace ref in ebx with top address
    asm.emitSUB_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());   // subtract base address to get index
  }

  /**
   * Wrap up the access to the JNIRefs array
   * Expect:
   *   -T0 pointing to the address of the valid top 
   *   -S0 holding the pointer to jniEnv
   * Perform these steps:
   *   -recompute value in T0 as byte offset from jniEnv.JNIRefs base
   *   -store value in T0 back into jniEnv.JNIRefsTop
   *
   */
  // static void endJNIrefForProlog(VM_Assembler asm) {
  //   asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base
  //   asm.emitSUB_Reg_Reg     (T0, EBX);                                  // S0 <- index of top
  //   asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), T0);  // jniEnv.JNIRefsTop <- T0
  // }

  /**
   * Generate the code to pop the frame in JNIRefs array for this Java to C transition
   * Expect:
   *  -JTOC, PR registers are valid
   *  -S0 contains a pointer to the VM_Thread.jniEnv
   *  -EBX and T1 are available as scratch registers
   * Perform these steps:
   *  -jniEnv.JNIRefsTop <- jniEnv.JNIRefsSavedFP - 4
   *  -jniEnv.JNIRefsSavedFP <- (jniEnv.JNIRefs + jniEnv.JNIRefsSavedFP)
   *
   */
  static void popJNIrefForEpilog(VM_Assembler asm) {
    
    // on entry, S0 contains a pointer to the VM_Thread.jniEnv
    // set TOP to point to entry below the last frame
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset());    // ebx <- JNIRefsSavedFP
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), T1);        // JNIRefsTop <- ebx
    asm.emitSUB_RegDisp_Imm (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), WORDSIZE);  // JNIRefsTop -= 4

    // load savedFP with the index to the last frame
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base
    asm.emitMOV_Reg_RegIdx  (EBX, EBX, T1, asm.BYTE, 0);                // ebx <- (JNIRefs base + SavedFP index)
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), EBX);  // JNIRefsSavedFP <- ebx

  }
  

  /*****************************************************************
   * Handle the C to Java transition:  JNI methods in VM_JNIFunctions.java
   * NOTE:
   *   -We need PR to access Java environment, but not certain whether
   *    Linux C treats it as nonvolatile and restores it before calling, 
   *    so for now it is saved in the JNIenv and restored from there.
   *   -Unlike the powerPC scheme which has a special prolog preceding
   *    the normal Java prolog, the Intel scheme replaces the Java prolog
   *    completely with the special prolog
   *
   *            Stack on entry            Stack at end of prolog after call
   *             high memory 			   high memory
   *            |            |                   |            |
   *	EBP ->	|saved FP    | 			 |saved FP    |
   *            |  ...       |                   |  ...       |
   *            |            |                   |            |
   *		|arg n-1     | 			 |arg n-1     |
   * native    	|  ...       | 			 |  ...       |       
   * caller    	|arg 0       | 			 |arg 0       |
   *	ESP -> 	|return addr |        		 |return addr |
   *            |            |           EBP ->  |saved FP    |
   *            |            |                   |methodID    | normal MethodID for JNI function
   *            |            |                   |saved JavaFP| offset to preceeding java frame
   *            |            |                   |saved edi   |	to be used for JTOC
   *            |            |                   |  "   ebx   |	to be used for nonvolatile
   *            |            |                   |  "   ecx   |	to be used for scrach
   *            |            |                   |  "   esi   |	to be used for PR
   *            |            |                   |arg 0       | copied in reverse order
   *            |            |                   |  ...       |
   *            |            |           ESP ->  |arg n-1     |
   *            |            |                   |            | normally compiled Java code continue
   *            |            |                   |            |
   *            |            |                   |            |
   *            |            |                   |            |
   *             low memory                        low memory
   *
   */

  static void generateGlueCodeForJNIMethod(VM_Assembler asm, VM_Method method, int methodID) {
    VM_Address bootRecordAddress = VM_Magic.objectAsAddress(VM_BootRecord.the_boot_record);

    // set 2nd word of header = return address already pushed by CALL
    // NOTE: C calling convention is that EBP contains the caller's framepointer.
    //       Therefore our C to Java transition frames must follow this protocol,
    //       not the RVM protocol in which the caller's framepointer is in 
    //       pr.framePointer and EBP is a nonvolatile register.
    asm.emitPUSH_Reg(EBP);          

    // start new frame:  set FP to point to the new frame
    asm.emitMOV_Reg_Reg (EBP, SP); 

    // set first word of header: method ID
    asm.emitPUSH_Imm (methodID); 
    asm.emitSUB_Reg_Imm (SP, WORDSIZE);  // leave room for saved -> preceeding java frame, set later

    // save registers that will be used in RVM, to be restored on return to C
    asm.emitPUSH_Reg(JTOC); 
    asm.emitPUSH_Reg(EBX);         
    asm.emitPUSH_Reg(S0);         
    VM_ProcessorLocalState.emitPushProcessor(asm);
    
     // copy the arguments in reverse order
    VM_Type[] types = method.getParameterTypes();   // does NOT include implicit this or class ptr
    int numArguments = types.length;                // number of arguments for this method
    int argOffset = 2;           // add 2 to get to arg area in caller frame
    for (int i=0; i<numArguments; i++) {
      if (types[i].isLongType() || types[i].isDoubleType()) {
        // handle 2-words case:
        asm.emitMOV_Reg_RegDisp (EBX, EBP, ((argOffset+1)*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        asm.emitMOV_Reg_RegDisp (EBX, EBP, (argOffset*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        argOffset+=2;
      } else {
        // Handle 1-word case:
        // add 2 to get to arg area in caller frame
        asm.emitMOV_Reg_RegDisp (EBX, EBP, (argOffset*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        argOffset++;
      }
    }

    // START of code sequence to atomically change processor status from IN_NATIVE
    // to IN_JAVA, looping in a call to sysVirtualProcessorYield if BLOCKED_IN_NATIVE

    int retryLabel = asm.getMachineCodeIndex();     // backward branch label

    // Restore JTOC through the JNIEnv passed back from the C code as the first parameter:
    // an extra entry at the end of the JNIFunctions array contains the RVM JTOC
    //
    // NOTE - we need the JTOC here only to get the sysYield.. entry point out of the
    // bootrecord. we could alternatively also put the bootrecord address at the end
    // of JNIFunctions or put it there INSTEAD of the JTOC

    asm.emitMOV_Reg_RegDisp (EBX, EBP, (2*WORDSIZE));   // pick up arg 0 (from callers frame)
    asm.emitMOV_Reg_RegDisp (JTOC, EBX, 0);                         // JTOC<-addr of JNIFunctions[0]
    asm.emitMOV_Reg_RegDisp (JTOC, JTOC, JNIFUNCTIONS_JTOC_OFFSET); // JTOC<-JNIFunctions[saved JTOC]

    // address of current processors status word is stored at jniEnv (first arg) + 4;
    asm.emitMOV_Reg_RegDisp (S0, EBX, WORDSIZE);     // S0 <- addr of status word

    asm.emitMOV_Reg_RegInd(T0,S0);                         // T0<-contents of statusword 
    asm.emitCMP_Reg_Imm (T0, VM_Processor.IN_NATIVE);      // jmp if still IN_NATIVE
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);       // if so, skip 3 instructions

    // blocked in native, do pthread yield
    asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());  // T0<-bootrecord addr
    asm.emitCALL_RegDisp(T0, VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset());
    asm.emitJMP_Imm (retryLabel);                          // retry from beginning

    fr.resolve(asm);      // branch here if IN_NATIVE, attempt to go to IN_JAVA

    // T0 (EAX) contains "old value" (required for CMPXCNG instruction)
    // S0 contains address of status word to be swapped
    asm.emitMOV_Reg_Imm (T1, VM_Processor.IN_JAVA);  // T1<-new value (IN_JAVA)
    asm.emitCMPXCHG_RegInd_Reg(S0,T1);               // atomic compare-and-exchange
    asm.emitJCC_Cond_Imm(asm.NE,retryLabel);

    // END of code sequence to change state from IN_NATIVE to IN_JAVA

    // status is now IN_JAVA. GC can not occur while we execute on a processor
    // in this state, so it is safe to access fields of objects
    // JTOC reg has been restored to RVM JTOC

    // done saving, bump SP to reserve room for the local variables
    // SP should now be at the point normally marked as emptyStackOffset
    int numLocalVariables = method.getLocalWords() - method.getParameterWords();
    asm.emitSUB_Reg_Imm (SP, (numLocalVariables << LG_WORDSIZE));
   
    // Compute the byte offset for this thread (offset into the VM_Scheduler.threads array)
    asm.emitMOV_Reg_RegDisp (S0, JTOC, VM_Entrypoints.JNIFunctionPointersField.getOffset());   // S0 <- base addr
    asm.emitSUB_Reg_Reg (EBX, S0);                     // ebx <- offset
    asm.emitSHR_Reg_Imm (EBX, 1);                      // byte offset:  divide by 2 (=> off in threads array)
    // then get the thread and its real JNIEnv
    asm.emitMOV_Reg_RegDisp (S0, JTOC, VM_Entrypoints.threadsField.getOffset());   // S0 <- VM_Thread array
    asm.emitMOV_Reg_RegIdx  (S0, S0, EBX, asm.BYTE, 0);                 // S0 <- VM_Thread object
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.jniEnvField.getOffset());     // ebx <- JNIenv

    // now EBX -> jniEnv for thread

    // Retrieve -> preceeding "top" java FP from jniEnv and save in current 
    // frame of JNIFunction
    asm.emitMOV_Reg_RegDisp (ESI, EBX, VM_Entrypoints.JNITopJavaFPField.getOffset());  
    // asm.emitMOV_Reg_RegDisp (PR, EBX, VM_Entrypoints.JNITopJavaFPField.getOffset());  
    
    // get offset from current FP (PR <- PR - FP)
    asm.emitSUB_Reg_Reg (ESI, EBP);    

    asm.emitMOV_RegDisp_Reg (EBP, SAVED_JAVA_FP_OFFSET, ESI);                // save in hdr of current frame 

    // Restore the VM_Processor value saved on the Java to C transition
    VM_ProcessorLocalState.emitSetProcessor(asm, EBX, 
					    VM_Entrypoints.JNIEnvSavedPRField.getOffset());


    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              EBP);
    
    // Test if calling Java JNIFunction on a RVM processor or 
    // a Native processor.
    // at this point: JTOC and PR have been restored & 
    // processor status = IN_JAVA,
    // arguments for the call have been setup, space on the stack for locals
    // has been acquired.

    // load mode of current processor for testing (RVM or NATIVE)
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0,
                                              VM_Entrypoints.processorModeField.getOffset());

    asm.emitCMP_Reg_Imm (T0, VM_Processor.RVM);           // test for RVM
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);     // Br if yes

    // If here, on a native processor, it is necessary to transfer back to a
    // RVM processor before executing the Java JNI Function.

    // !!! what about saving regs, especially FPRs ??? (CHECK)

    // branch to becomeRVMThread to make the transfer.

    // If GC occurs while we are on the transfer queue of the RVM processor,
    // what will the MapIterator do, will we appear to be in the prolog of the 
    // Java JNI Function? Will there be any refs to report? any saved regs to report?

    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.becomeRVMThreadMethod.getOffset());

    // execution here is now on the RVM processor, and on a different
    // os pThread. PR now points to the new RVM processor we have
    // been transferred to.  JTOC was set when we were re-dispatched.

    // XXX Restoring regs? especially FPRs ??? (CHECK)

    fr1.resolve(asm);  // branch to here if returning on a RVM processor

    // finally proceed with the normal Java compiled code
    // skip the thread switch test for now, see VM_Compiler.genThreadSwitchTest(true)

    asm.emitNOP(); // end of prologue marker

  }

  static void generateEpilogForJNIMethod(VM_Assembler asm, VM_Method method) {

    // assume RVM PR regs still valid. potentially T1 & T0 contain return
    // values and should not be modified. we use regs saved in prolog and restored
    // before return to do whatever needs to be done.  does not assume JTOC is valid,
    // and may use it as scratch reg.

    // if returning long, switch the order of the hi/lo word in T0 and T1
    if (method.getReturnType().isLongType()) {
      asm.emitPUSH_Reg(T1);    
      asm.emitMOV_Reg_Reg(T1, T0);  
      asm.emitPOP_Reg(T0);      
    }

    // current processor status should be IN_JAVA, implying no GC and being safe
    // to reference java objects and use PR

    // S0<-addr activethread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset()); 
    asm.emitMOV_Reg_RegDisp(S0, S0, VM_Entrypoints.jniEnvField.getOffset());       // S0<-addr threads jniEnv

    // set jniEnv TopJavaFP using value saved in frame in prolog
    asm.emitMOV_Reg_RegDisp(JTOC, EBP, SAVED_JAVA_FP_OFFSET);      // JTOC<-saved TopJavaFP (offset)
    asm.emitADD_Reg_Reg(JTOC, EBP);                                // change offset from FP into address
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNITopJavaFPField.getOffset(), JTOC); // jniEnv.TopJavaFP <- JTOC

    // in case thread has migrated to different PR, reset saved PRs to current PR
    // first reset PR saved in jniEnv
    VM_ProcessorLocalState.emitStoreProcessor(asm, S0,
                                              VM_Entrypoints.JNIEnvSavedPRField.getOffset());

    // now save PR saved in preceeding JavaToNative transition frame, whose FP
    // is now in JTOC
    VM_ProcessorLocalState.emitStoreProcessor(asm, JTOC, JNI_PR_OFFSET);

    // get address of current processors statusword
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, EBX,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    // if processor changed, then statusword address, stored after the threads
    // jniEnv JNIFunction ptr needs to be changed. reuse JTOC as scratch
    asm.emitMOV_Reg_RegDisp(JTOC, S0, VM_Entrypoints.JNIEnvAddressField.getOffset());  // JTOC<-jniEnv.JNIEnvAddress
    asm.emitMOV_RegDisp_Reg (JTOC, WORDSIZE, EBX);         // [JTOC+4] <- processor statusword addr

    // change current processor status to IN_NATIVE
    asm.emitMOV_RegInd_Imm(EBX, VM_Processor.IN_NATIVE);

    // reload native/C nonvolatile regs - saved in prolog
    // what about FPRs
    VM_ProcessorLocalState.emitPopProcessor(asm);
    asm.emitPOP_Reg(S0);         
    asm.emitPOP_Reg(EBX);         
    asm.emitPOP_Reg(JTOC); 

    // NOTE: C expects the framepointer to be restored to EBP, so 
    //       the epilogue for the C to Java glue code must follow that 
    //       convention, not the RVM one!
    //       Also note that RVM treats EBP is a nonvolatile, so we don't
    //       explicitly save/restore it.
    asm.emitMOV_Reg_Reg(SP, EBP);                           // discard current stack frame
    asm.emitPOP_Reg(EBP);
    asm.emitRET();              // return to caller
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.lang.reflect.*;

/**
 *   This class implements the JNI environment, it includes:
 * -The array of JNI function pointers accessible from C
 * -Implementation of all the JNI functions
 *
 * @author Ton Ngo
 * @author Steve Smith 
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCUtil;

public class VM_JNIEnvironment implements VM_JNILinuxConstants, VM_RegisterConstants {

  private static boolean initialized = false;
  private static String[] names;

  /**
   * This is the JNI function table, the address of this array will be
   * passed to the native code
   */
  private static INSTRUCTION[][] JNIFunctions;

  /**
   * This is a table of pointers to the shared JNI function table.  All entries 
   * point to the same function table.  Each thread uses the pointer at its thread id
   * offset to allow us to determine a threads id from the pointer it is using.
   * Needed when native calls Java (JNIFunctions) and passes its JNIEnv pointer.
   * Its offset into the JNIFunctionPts array is the same as the threads offset
   * in the Scheduler.threads array.
   */
    //  private static int[] JNIFunctionPointers;
    static int[] JNIFunctionPointers;        // made public so vpStatus could be set 11/16/00 SES
                                             // maybe need set & get functions ??

  /**
   * These are thread specific information, such as:
   *  -the list of references passed to native code, for GC purpose
   *  -saved RVM system registers
   */
  VM_Address JNIEnvAddress;      // contain a pointer to the JNIFunctions array
  int savedTIreg;         // for saving thread index register on entry to native, to be restored on JNI call from native
  VM_Processor savedPRreg; // for saving processor register on entry to native, to be restored on JNI call from native
  boolean alwaysHasNativeFrame;  // true if the bottom stack frame is native, such as thread for CreateJVM or AttachCurrentThread

  int[] JNIRefs;          // references passed to native code
  int   JNIRefsTop;       // -> address of current top ref in JNIRefs array 
  int   JNIRefsMax;       // -> address of end (last entry) of JNIRefs array
  int   JNIRefsSavedFP;   // -> previous frame boundary in JNIRefs array
  public VM_Address JNITopJavaFP;     // -> Top java frame when in C frames on top of the stack

  Throwable pendingException = null;

  // Saved context for thread attached to external pthread.  This context is
  // saved by the JNIService thread and points to the point in JNIStartUp thread
  // where it yields to the queue in the native VM_Processor.
  // When DetachCurrentThread is called, the JNIService thread restores this context 
  // to allow the thread to run VM_Thread.terminate on its original stack.
  VM_Registers savedContextForTermination;

  // temporarily use a fixed size array for JNI refs, later grow as needed
  static final int JNIREFS_ARRAY_LENGTH = 100;

  public static void init() {

    // allocate the first dimension of the function array in the boot image so that
    // we have an address pointing to it.  This is necessary for thread creation
    // since the VM_JNIEnvironment object will contain a field pointing to this array

    // An extra entry is allocated, to hold the RVM JTOC 07/01 SES

    // JNIFunctions = new INSTRUCTION[FUNCTIONCOUNT][];
    // Why is INSTRUCTION not working?  getting filled with null
    JNIFunctions = new byte[FUNCTIONCOUNT+1][];

    // First word is a pointer to the JNIFunction table
    // Second word is address of current processors vpStatus word
    // (JTOC is now stored at end of shared JNIFunctions array)
    JNIFunctionPointers = new int[VM_Scheduler.MAX_THREADS * 2];
  }

  /**
   *  Initialize the array of JNI functions
   *  To be called from VM_DynamicLibrary.java when a library is loaded,
   *  expecting native calls to be made
   *
   */
  public static void boot() {

    if (initialized)
      return;

    // fill an array of JNI names
    setNames();

    // fill in the IP entries for each AIX linkage triplet
    try {
      VM_Class cls = VM_Class.forName("VM_JNIFunctions");
      VM_Method[] mths = cls.getDeclaredMethods();
      for (int i=0; i<mths.length; i++) {
	String methodName = mths[i].getName().toString();
	int jniIndex = indexOf(methodName);
	if (jniIndex!=-1) {
	  JNIFunctions[jniIndex] = mths[i].getCurrentCompiledMethod().getInstructions();
	  // VM.sysWrite("   " + methodName + "=" + VM.intAsHexString(JNIFunctions[jniIndex]));
	} 
      }

    } catch (VM_ResolutionException e) {
      throw new InternalError("VM_JNIEnvironment fails to initialize, has the class been renamed\n");
    }

    // store RVM JTOC address in last (extra) entry in JNIFunctions array
    // to be restored when native C invokes JNI functions implemented in java
    //
    // following causes exception in checkstore, so forced to setMemoryWord instead
    // JNIFunctions[FUNCTIONCOUNT+1] = VM_Magic.addressAsByteArray(VM_Magic.getTocPointer());
    VM_Magic.setMemoryAddress(VM_Magic.objectAsAddress(JNIFunctions).add(JNIFUNCTIONS_JTOC_OFFSET),
			      VM_Magic.getTocPointer());

    initialized = true;
  }

  // Instance:  create a thread specific JNI environment.  threadSlot = creating threads
  // thread id == index of its entry in Scheduler.threads array
  //
  public VM_JNIEnvironment (int threadSlot) {

    JNIFunctionPointers[threadSlot * 2] = VM_Magic.objectAsAddress(JNIFunctions).toInt();
    JNIFunctionPointers[(threadSlot * 2)+1] = 0;  // later contains addr of processor vpStatus word
    JNIEnvAddress = VM_Magic.objectAsAddress(JNIFunctionPointers).add(threadSlot*8);
    JNIRefs = new int[JNIREFS_ARRAY_LENGTH];
    JNIRefs[0] = 0;                       // 0 entry for bottom of stack
    JNIRefsTop = 0;
    JNIRefsSavedFP = 0;
    JNIRefsMax = (JNIRefs.length - 1) * 4;   // byte offset to last entry

    // initially TOP and SavedFP -> entry 0 containing 0

    alwaysHasNativeFrame = false;
  }

  // push a reference onto thread local JNIRefs stack.  To be used by JNI
  // Functions when returning a reference back to JNI native C code
  // Taken:    Object to put on stack
  // Returned: offset of entry in JNIRefs stack
  // 
  public int pushJNIRef( Object ref ) {
    if (ref == null) return 0;
    if (VM.VerifyAssertions) VM.assert( VM_GCUtil.validRef( VM_Magic.objectAsAddress(ref) ) );
    JNIRefsTop += 4;
    if (JNIRefsTop >> 2 >= JNIRefs.length) {
	int[] newrefs = new int[ JNIRefs.length * 2 ];
	for(int i = 0; i < JNIRefs.length; i++) newrefs[i] = JNIRefs[i];
	JNIRefs = newrefs;
    }
    JNIRefs[ JNIRefsTop >> 2 ] = VM_Magic.objectAsAddress(ref).toInt();
    return JNIRefsTop;
  }

  // get a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  // Returned: reference at that offset
  public Object getJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP, ");
      VM.sysWrite(offset); 
      VM.sysWrite("(top is ");
      VM.sysWrite(JNIRefsTop);
      VM.sysWrite(")\n");
      return null;
    }
    if (offset < 0)
	return VM_JNIGlobalRefTable.ref( offset );
    else
	return VM_Magic.addressAsObject( VM_Address.fromInt(JNIRefs[ offset>>2 ]) );
    
  }

  // remove a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  public void deleteJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP, ");
      VM.sysWrite(offset); 
      VM.sysWrite("(top is ");
      VM.sysWrite(JNIRefsTop);
      VM.sysWrite(")\n");
    }
    
    JNIRefs[ offset>>2 ] = 0;

    if (offset == JNIRefsTop) JNIRefsTop -= 4;
  }

  // record an exception as pending so that it will be delivered on the return
  // to the Java caller;  clear the exception by recording null
  // Taken:  an exception or error
  // Returned:  nothing
  //
  public void recordException(Throwable e) {
    // don't overwrite the first exception except to clear it
    if (pendingException==null || e==null)
      pendingException = e;
  }

  // return the pending exception
  // Taken:  nothing
  // Returned:  an exception or error
  //
  public Throwable getException() {
    return pendingException;
  }

  //
  // get the address of the JNIFunctions array, which should be in the JTOC
  // Taken:    nothing 
  // Returned: the address of the JNIFunctions array 
  // 
  public VM_Address getJNIenvAddress() {
    return JNIEnvAddress;
  }

  public INSTRUCTION[] getInstructions(int id) {    
    return JNIFunctions[id];
  }

  //
  // get the JNI index for a function name
  // Taken:    a JNI function name
  // Returned: the index for this function, -1 if not found
  //
  private static int indexOf(String functionName) {
    for (int i=0; i<FUNCTIONCOUNT; i++) {
      if (names[i].equals(functionName))
	return i;
    }
    return -1;
  }

  private static String[] setNames() {
    names = new String[FUNCTIONCOUNT];
    names[0]                             = new String("undefined");
    names[RESERVED0]                     = new String("reserved0")                     ;	  
    names[RESERVED1]                     = new String("reserved1")                     ;	  
    names[RESERVED2]                     = new String("reserved2")                     ;	  
    names[RESERVED3]                     = new String("reserved3")                     ;	  
    names[GETVERSION]                    = new String("GetVersion")                    ;	  
    names[DEFINECLASS]                   = new String("DefineClass")                   ;	  
    names[FINDCLASS]                     = new String("FindClass")                     ;	  
    names[FROMREFLECTEDMETHOD]         	 = new String("FromReflectedMethod"); //  JDK1.2, #7      
    names[FROMREFLECTEDFIELD]          	 = new String("FromReflectedField");  //  JDK1.2, #8      
    names[TOREFLECTEDMETHOD]           	 = new String("ToReflectedMethod");   //  JDK1.2, #9      
    names[GETSUPERCLASS]                 = new String("GetSuperclass")                 ;	  
    names[ISASSIGNABLEFROM]              = new String("IsAssignableFrom")              ;	  
    names[TOREFLECTEDFIELD]            	 = new String("ToReflectedField");    //  JDK1.2, #12      
    names[THROW]                         = new String("Throw")                         ;	  
    names[THROWNEW]                      = new String("ThrowNew")                      ;	  
    names[EXCEPTIONOCCURRED]             = new String("ExceptionOccurred")             ;	  
    names[EXCEPTIONDESCRIBE]             = new String("ExceptionDescribe")             ;	  
    names[EXCEPTIONCLEAR]                = new String("ExceptionClear")                ;	  
    names[FATALERROR]                    = new String("FatalError")                    ;	  
    names[PUSHLOCALFRAME]              	 = new String("PushLocalFrame");      //  JDK1.2, #19      
    names[POPLOCALFRAME]               	 = new String("PopLocalFrame");       //  JDK1.2, #20      
    names[NEWGLOBALREF]                  = new String("NewGlobalRef")                  ;	  
    names[DELETEGLOBALREF]               = new String("DeleteGlobalRef")               ;	  
    names[DELETELOCALREF]                = new String("DeleteLocalRef")                ;	  
    names[ISSAMEOBJECT]                  = new String("IsSameObject")                  ;	  
    names[NEWLOCALREF]                 	 = new String("NewLocalRef");         //  JDK1.2, #25      
    names[ENSURELOCALCAPACITY]         	 = new String("EnsureLocalCapacity"); //  JDK1.2, #26   
    names[ALLOCOBJECT]                   = new String("AllocObject")                   ;	  
    names[NEWOBJECT]                     = new String("NewObject")                     ;	  
    names[NEWOBJECTV]                    = new String("NewObjectV")                    ;	  
    names[NEWOBJECTA]                    = new String("NewObjectA")                    ;	  
    names[GETOBJECTCLASS]                = new String("GetObjectClass")                ;	  
    names[ISINSTANCEOF]                  = new String("IsInstanceOf")                  ;	  
    names[GETMETHODID]                   = new String("GetMethodID")                   ;	  
    names[CALLOBJECTMETHOD]              = new String("CallObjectMethod")              ;	  
    names[CALLOBJECTMETHODV]             = new String("CallObjectMethodV")             ;	  
    names[CALLOBJECTMETHODA]             = new String("CallObjectMethodA")             ;	  
    names[CALLBOOLEANMETHOD]             = new String("CallBooleanMethod")             ;	  
    names[CALLBOOLEANMETHODV]            = new String("CallBooleanMethodV")            ;	  
    names[CALLBOOLEANMETHODA]            = new String("CallBooleanMethodA")            ;	  
    names[CALLBYTEMETHOD]                = new String("CallByteMethod")                ;	  
    names[CALLBYTEMETHODV]               = new String("CallByteMethodV")               ;	  
    names[CALLBYTEMETHODA]               = new String("CallByteMethodA")               ;	  
    names[CALLCHARMETHOD]                = new String("CallCharMethod")                ;	  
    names[CALLCHARMETHODV]               = new String("CallCharMethodV")               ;	  
    names[CALLCHARMETHODA]               = new String("CallCharMethodA")               ;	  
    names[CALLSHORTMETHOD]               = new String("CallShortMethod")               ;	  
    names[CALLSHORTMETHODV]              = new String("CallShortMethodV")              ;	  
    names[CALLSHORTMETHODA]              = new String("CallShortMethodA")              ;	  
    names[CALLINTMETHOD]                 = new String("CallIntMethod")                 ;	  
    names[CALLINTMETHODV]                = new String("CallIntMethodV")                ;	  
    names[CALLINTMETHODA]                = new String("CallIntMethodA")                ;	  
    names[CALLLONGMETHOD]                = new String("CallLongMethod")                ;	  
    names[CALLLONGMETHODV]               = new String("CallLongMethodV")               ;	  
    names[CALLLONGMETHODA]               = new String("CallLongMethodA")               ;	  
    names[CALLFLOATMETHOD]               = new String("CallFloatMethod")               ;	  
    names[CALLFLOATMETHODV]              = new String("CallFloatMethodV")              ;	  
    names[CALLFLOATMETHODA]              = new String("CallFloatMethodA")              ;	  
    names[CALLDOUBLEMETHOD]              = new String("CallDoubleMethod")              ;	  
    names[CALLDOUBLEMETHODV]             = new String("CallDoubleMethodV")             ;	  
    names[CALLDOUBLEMETHODA]             = new String("CallDoubleMethodA")             ;	  
    names[CALLVOIDMETHOD]                = new String("CallVoidMethod")                ;	  
    names[CALLVOIDMETHODV]               = new String("CallVoidMethodV")               ;	  
    names[CALLVOIDMETHODA]               = new String("CallVoidMethodA")               ;	  
    names[CALLNONVIRTUALOBJECTMETHOD]    = new String("CallNonvirtualObjectMethod")    ;	  
    names[CALLNONVIRTUALOBJECTMETHODV]   = new String("CallNonvirtualObjectMethodV")   ;	  
    names[CALLNONVIRTUALOBJECTMETHODA]   = new String("CallNonvirtualObjectMethodA")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHOD]   = new String("CallNonvirtualBooleanMethod")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHODV]  = new String("CallNonvirtualBooleanMethodV")  ;	  
    names[CALLNONVIRTUALBOOLEANMETHODA]  = new String("CallNonvirtualBooleanMethodA")  ;	  
    names[CALLNONVIRTUALBYTEMETHOD]      = new String("CallNonvirtualByteMethod")      ;	  
    names[CALLNONVIRTUALBYTEMETHODV]     = new String("CallNonvirtualByteMethodV")     ;	  
    names[CALLNONVIRTUALBYTEMETHODA]     = new String("CallNonvirtualByteMethodA")     ;	  
    names[CALLNONVIRTUALCHARMETHOD]      = new String("CallNonvirtualCharMethod")      ;	  
    names[CALLNONVIRTUALCHARMETHODV]     = new String("CallNonvirtualCharMethodV")     ;	  
    names[CALLNONVIRTUALCHARMETHODA]     = new String("CallNonvirtualCharMethodA")     ;	  
    names[CALLNONVIRTUALSHORTMETHOD]     = new String("CallNonvirtualShortMethod")     ;	  
    names[CALLNONVIRTUALSHORTMETHODV]    = new String("CallNonvirtualShortMethodV")    ;	  
    names[CALLNONVIRTUALSHORTMETHODA]    = new String("CallNonvirtualShortMethodA")    ;	  
    names[CALLNONVIRTUALINTMETHOD]       = new String("CallNonvirtualIntMethod")       ;	  
    names[CALLNONVIRTUALINTMETHODV]      = new String("CallNonvirtualIntMethodV")      ;	  
    names[CALLNONVIRTUALINTMETHODA]      = new String("CallNonvirtualIntMethodA")      ;	  
    names[CALLNONVIRTUALLONGMETHOD]      = new String("CallNonvirtualLongMethod")      ;	  
    names[CALLNONVIRTUALLONGMETHODV]     = new String("CallNonvirtualLongMethodV")     ;	  
    names[CALLNONVIRTUALLONGMETHODA]     = new String("CallNonvirtualLongMethodA")     ;	  
    names[CALLNONVIRTUALFLOATMETHOD]     = new String("CallNonvirtualFloatMethod")     ;	  
    names[CALLNONVIRTUALFLOATMETHODV]    = new String("CallNonvirtualFloatMethodV")    ;	  
    names[CALLNONVIRTUALFLOATMETHODA]    = new String("CallNonvirtualFloatMethodA")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHOD]    = new String("CallNonvirtualDoubleMethod")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHODV]   = new String("CallNonvirtualDoubleMethodV")   ;	  
    names[CALLNONVIRTUALDOUBLEMETHODA]   = new String("CallNonvirtualDoubleMethodA")   ;	  
    names[CALLNONVIRTUALVOIDMETHOD]      = new String("CallNonvirtualVoidMethod")      ;	  
    names[CALLNONVIRTUALVOIDMETHODV]     = new String("CallNonvirtualVoidMethodV")     ;	  
    names[CALLNONVIRTUALVOIDMETHODA]     = new String("CallNonvirtualVoidMethodA")     ;	  
    names[GETFIELDID]                    = new String("GetFieldID")                    ;	  
    names[GETOBJECTFIELD]                = new String("GetObjectField")                ;	  
    names[GETBOOLEANFIELD]               = new String("GetBooleanField")               ;	  
    names[GETBYTEFIELD]                  = new String("GetByteField")                  ;	  
    names[GETCHARFIELD]                  = new String("GetCharField")                  ;	  
    names[GETSHORTFIELD]                 = new String("GetShortField")                 ;	  
    names[GETINTFIELD]                   = new String("GetIntField")                   ;	  
    names[GETLONGFIELD]                  = new String("GetLongField")                  ;	  
    names[GETFLOATFIELD]                 = new String("GetFloatField")                 ;	  
    names[GETDOUBLEFIELD]                = new String("GetDoubleField")                ;	  
    names[SETOBJECTFIELD]                = new String("SetObjectField")                ;	  
    names[SETBOOLEANFIELD]               = new String("SetBooleanField")               ;	  
    names[SETBYTEFIELD]                  = new String("SetByteField")                  ;	  
    names[SETCHARFIELD]                  = new String("SetCharField")                  ;	  
    names[SETSHORTFIELD]                 = new String("SetShortField")                 ;	  
    names[SETINTFIELD]                   = new String("SetIntField")                   ;	  
    names[SETLONGFIELD]                  = new String("SetLongField")                  ;	  
    names[SETFLOATFIELD]                 = new String("SetFloatField")                 ;	  
    names[SETDOUBLEFIELD]                = new String("SetDoubleField")                ;	  
    names[GETSTATICMETHODID]             = new String("GetStaticMethodID")             ;	  
    names[CALLSTATICOBJECTMETHOD]        = new String("CallStaticObjectMethod")        ;	  
    names[CALLSTATICOBJECTMETHODV]       = new String("CallStaticObjectMethodV")       ;	  
    names[CALLSTATICOBJECTMETHODA]       = new String("CallStaticObjectMethodA")       ;	  
    names[CALLSTATICBOOLEANMETHOD]       = new String("CallStaticBooleanMethod")       ;	  
    names[CALLSTATICBOOLEANMETHODV]      = new String("CallStaticBooleanMethodV")      ;	  
    names[CALLSTATICBOOLEANMETHODA]      = new String("CallStaticBooleanMethodA")      ;	  
    names[CALLSTATICBYTEMETHOD]          = new String("CallStaticByteMethod")          ;	  
    names[CALLSTATICBYTEMETHODV]         = new String("CallStaticByteMethodV")         ;	  
    names[CALLSTATICBYTEMETHODA]         = new String("CallStaticByteMethodA")         ;	  
    names[CALLSTATICCHARMETHOD]          = new String("CallStaticCharMethod")          ;	  
    names[CALLSTATICCHARMETHODV]         = new String("CallStaticCharMethodV")         ;	  
    names[CALLSTATICCHARMETHODA]         = new String("CallStaticCharMethodA")         ;	  
    names[CALLSTATICSHORTMETHOD]         = new String("CallStaticShortMethod")         ;	  
    names[CALLSTATICSHORTMETHODV]        = new String("CallStaticShortMethodV")        ;	  
    names[CALLSTATICSHORTMETHODA]        = new String("CallStaticShortMethodA")        ;	  
    names[CALLSTATICINTMETHOD]           = new String("CallStaticIntMethod")           ;	  
    names[CALLSTATICINTMETHODV]          = new String("CallStaticIntMethodV")          ;	  
    names[CALLSTATICINTMETHODA]          = new String("CallStaticIntMethodA")          ;	  
    names[CALLSTATICLONGMETHOD]          = new String("CallStaticLongMethod")          ;	  
    names[CALLSTATICLONGMETHODV]         = new String("CallStaticLongMethodV")         ;	  
    names[CALLSTATICLONGMETHODA]         = new String("CallStaticLongMethodA")         ;	  
    names[CALLSTATICFLOATMETHOD]         = new String("CallStaticFloatMethod")         ;	  
    names[CALLSTATICFLOATMETHODV]        = new String("CallStaticFloatMethodV")        ;	  
    names[CALLSTATICFLOATMETHODA]        = new String("CallStaticFloatMethodA")        ;	  
    names[CALLSTATICDOUBLEMETHOD]        = new String("CallStaticDoubleMethod")        ;	  
    names[CALLSTATICDOUBLEMETHODV]       = new String("CallStaticDoubleMethodV")       ;	  
    names[CALLSTATICDOUBLEMETHODA]       = new String("CallStaticDoubleMethodA")       ;	  
    names[CALLSTATICVOIDMETHOD]          = new String("CallStaticVoidMethod")          ;	  
    names[CALLSTATICVOIDMETHODV]         = new String("CallStaticVoidMethodV")         ;	  
    names[CALLSTATICVOIDMETHODA]         = new String("CallStaticVoidMethodA")         ;	  
    names[GETSTATICFIELDID]              = new String("GetStaticFieldID")              ;	  
    names[GETSTATICOBJECTFIELD]          = new String("GetStaticObjectField")          ;	  
    names[GETSTATICBOOLEANFIELD]         = new String("GetStaticBooleanField")         ;	  
    names[GETSTATICBYTEFIELD]            = new String("GetStaticByteField")            ;	  
    names[GETSTATICCHARFIELD]            = new String("GetStaticCharField")            ;	  
    names[GETSTATICSHORTFIELD]           = new String("GetStaticShortField")           ;	  
    names[GETSTATICINTFIELD]             = new String("GetStaticIntField")             ;	  
    names[GETSTATICLONGFIELD]            = new String("GetStaticLongField")            ;	  
    names[GETSTATICFLOATFIELD]           = new String("GetStaticFloatField")           ;	  
    names[GETSTATICDOUBLEFIELD]          = new String("GetStaticDoubleField")          ;	  
    names[SETSTATICOBJECTFIELD]          = new String("SetStaticObjectField")          ;	  
    names[SETSTATICBOOLEANFIELD]         = new String("SetStaticBooleanField")         ;	  
    names[SETSTATICBYTEFIELD]            = new String("SetStaticByteField")            ;	  
    names[SETSTATICCHARFIELD]            = new String("SetStaticCharField")            ;	  
    names[SETSTATICSHORTFIELD]           = new String("SetStaticShortField")           ;	  
    names[SETSTATICINTFIELD]             = new String("SetStaticIntField")             ;	  
    names[SETSTATICLONGFIELD]            = new String("SetStaticLongField")            ;	  
    names[SETSTATICFLOATFIELD]           = new String("SetStaticFloatField")           ;	  
    names[SETSTATICDOUBLEFIELD]          = new String("SetStaticDoubleField")          ;	  
    names[NEWSTRING]                     = new String("NewString")                     ;	  
    names[GETSTRINGLENGTH]               = new String("GetStringLength")               ;	  
    names[GETSTRINGCHARS]                = new String("GetStringChars")                ;	  
    names[RELEASESTRINGCHARS]            = new String("ReleaseStringChars")            ;	  
    names[NEWSTRINGUTF]                  = new String("NewStringUTF")                  ;	  
    names[GETSTRINGUTFLENGTH]            = new String("GetStringUTFLength")            ;	  
    names[GETSTRINGUTFCHARS]             = new String("GetStringUTFChars")             ;	  
    names[RELEASESTRINGUTFCHARS]         = new String("ReleaseStringUTFChars")         ;	  
    names[GETARRAYLENGTH]                = new String("GetArrayLength")                ;	  
    names[NEWOBJECTARRAY]                = new String("NewObjectArray")                ;	  
    names[GETOBJECTARRAYELEMENT]         = new String("GetObjectArrayElement")         ;	  
    names[SETOBJECTARRAYELEMENT]         = new String("SetObjectArrayElement")         ;	  
    names[NEWBOOLEANARRAY]               = new String("NewBooleanArray")               ;	  
    names[NEWBYTEARRAY]                  = new String("NewByteArray")                  ;	  
    names[NEWCHARARRAY]                  = new String("NewCharArray")                  ;	  
    names[NEWSHORTARRAY]                 = new String("NewShortArray")                 ;	  
    names[NEWINTARRAY]                   = new String("NewIntArray")                   ;	  
    names[NEWLONGARRAY]                  = new String("NewLongArray")                  ;	  
    names[NEWFLOATARRAY]                 = new String("NewFloatArray")                 ;	  
    names[NEWDOUBLEARRAY]                = new String("NewDoubleArray")                ;	  
    names[GETBOOLEANARRAYELEMENTS]       = new String("GetBooleanArrayElements")       ;	  
    names[GETBYTEARRAYELEMENTS]          = new String("GetByteArrayElements")          ;	  
    names[GETCHARARRAYELEMENTS]          = new String("GetCharArrayElements")          ;	  
    names[GETSHORTARRAYELEMENTS]         = new String("GetShortArrayElements")         ;	  
    names[GETINTARRAYELEMENTS]           = new String("GetIntArrayElements")           ;	  
    names[GETLONGARRAYELEMENTS]          = new String("GetLongArrayElements")          ;	  
    names[GETFLOATARRAYELEMENTS]         = new String("GetFloatArrayElements")         ;	  
    names[GETDOUBLEARRAYELEMENTS]        = new String("GetDoubleArrayElements")        ;	  
    names[RELEASEBOOLEANARRAYELEMENTS]   = new String("ReleaseBooleanArrayElements")   ;	  
    names[RELEASEBYTEARRAYELEMENTS]      = new String("ReleaseByteArrayElements")      ;	  
    names[RELEASECHARARRAYELEMENTS]      = new String("ReleaseCharArrayElements")      ;	  
    names[RELEASESHORTARRAYELEMENTS]     = new String("ReleaseShortArrayElements")     ;	  
    names[RELEASEINTARRAYELEMENTS]       = new String("ReleaseIntArrayElements")       ;	  
    names[RELEASELONGARRAYELEMENTS]      = new String("ReleaseLongArrayElements")      ;	  
    names[RELEASEFLOATARRAYELEMENTS]     = new String("ReleaseFloatArrayElements")     ;	  
    names[RELEASEDOUBLEARRAYELEMENTS]    = new String("ReleaseDoubleArrayElements")    ;	  
    names[GETBOOLEANARRAYREGION]         = new String("GetBooleanArrayRegion")         ;	  
    names[GETBYTEARRAYREGION]            = new String("GetByteArrayRegion")            ;	  
    names[GETCHARARRAYREGION]            = new String("GetCharArrayRegion")            ;	  
    names[GETSHORTARRAYREGION]           = new String("GetShortArrayRegion")           ;	  
    names[GETINTARRAYREGION]             = new String("GetIntArrayRegion")             ;	  
    names[GETLONGARRAYREGION]            = new String("GetLongArrayRegion")            ;	  
    names[GETFLOATARRAYREGION]           = new String("GetFloatArrayRegion")           ;	  
    names[GETDOUBLEARRAYREGION]          = new String("GetDoubleArrayRegion")          ;	  
    names[SETBOOLEANARRAYREGION]         = new String("SetBooleanArrayRegion")         ;	  
    names[SETBYTEARRAYREGION]            = new String("SetByteArrayRegion")            ;	  
    names[SETCHARARRAYREGION]            = new String("SetCharArrayRegion")            ;	  
    names[SETSHORTARRAYREGION]           = new String("SetShortArrayRegion")           ;	  
    names[SETINTARRAYREGION]             = new String("SetIntArrayRegion")             ;	  
    names[SETLONGARRAYREGION]            = new String("SetLongArrayRegion")            ;	  
    names[SETFLOATARRAYREGION]           = new String("SetFloatArrayRegion")           ;	  
    names[SETDOUBLEARRAYREGION]          = new String("SetDoubleArrayRegion")          ;	  
    names[REGISTERNATIVES]               = new String("RegisterNatives")               ;	  
    names[UNREGISTERNATIVES]             = new String("UnregisterNatives")             ;	  
    names[MONITORENTER]                  = new String("MonitorEnter")                  ;	  
    names[MONITOREXIT]                   = new String("MonitorExit")                   ;	  
    names[GETJAVAVM]                     = new String("GetJavaVM")                     ;	  
    names[GETSTRINGREGION]             	 = new String("GetStringRegion");           // JDK 1.2, #220
    names[GETSTRINGUTFREGION]         	 = new String("GetStringUTFRegion");        // JDK 1.2, #221
    names[GETPRIMITIVEARRAYCRITICAL]   	 = new String("GetPrimitiveArrayCritical"); // JDK 1.2, #222
    names[RELEASEPRIMITIVEARRAYCRITICAL] = new String("ReleasePrimitiveArrayCritical"); // JDK 1.2, #223
    names[GETSTRINGCRITICAL]           	 = new String("GetStringCritical");         // JDK 1.2, # 224
    names[RELEASESTRINGCRITICAL]       	 = new String("ReleaseStringCritical");     // JDK 1.2, #225
    names[NEWWEAKGLOBALREF]            	 = new String("NewWeakGlobalRef");    	    // JDK 1.2, #226
    names[DELETEWEAKGLOBALREF]         	 = new String("DeleteWeakGlobalRef"); 	    // JDK 1.2, #227
    names[EXCEPTIONCHECK]              	 = new String("ExceptionCheck");      	    // JDK 1.2, #228

    return names;

  }

  /*****************************************************************************
   * Utility function called from VM_JNIFunction
   * (cannot be placed in VM_JNIFunction because methods there are specially compiled
   * to be called from native)
   *****************************************************************************/


  /**
   * Get a VM_Field of an object given the index for this field
   * @param obj an Object
   * @param fieldIndex an index into the VM_Field array that describes the fields of this object
   * @return the VM_Field pointed to by the index, or null if the index or the object is invalid
   *
   */
  public static VM_Field getFieldAtIndex (Object obj, int fieldIndex) {
    // VM.sysWrite("GetObjectField: field at index " + fieldIndex + "\n");

    VM_Type objType = VM_Magic.getObjectType(obj);
    if (objType.isClassType()) {
      VM_Field[] fields = objType.asClass().getInstanceFields();
      if (fieldIndex>=fields.length) {
	return null;                                      // invalid field index
      } else {
	return fields[fieldIndex];
      } 
    } else {
      // object is not a class type, probably an array or an invalid reference
      return null;
    }
  }

  /**
   * Common code shared by the JNI functions NewObjectA, NewObjectV, NewObject
   * (object creation)
   * @param methodID the method ID for a constructor
   * @return a new object created by the specified constructor
   */
  public static Object invokeInitializer(Class cls, int methodID, VM_Address argAddress, 
					 boolean isJvalue, boolean isDotDotStyle) 
    throws Exception  {

    // get the parameter list as Java class
    VM_Method mth = VM_MethodDictionary.getValue(methodID);
    VM_Type[] argTypes = mth.getParameterTypes();
    Class[]   argClasses = new Class[argTypes.length];
    for (int i=0; i<argClasses.length; i++) {
      argClasses[i] = argTypes[i].getClassForType();
    }

    Constructor constMethod = cls.getConstructor(argClasses);
    if (constMethod==null)
      throw new Exception("Constructor not found");


    // Package the parameters for the constructor
    VM_Address varargAddress;
    if (isDotDotStyle) 
      // flag is false because this JNI function has 3 args before the var args
      varargAddress = getVarArgAddress(false);    
    else
      varargAddress = argAddress;

    Object argObjs[];
    if (isJvalue)
      argObjs = packageParameterFromJValue(mth, argAddress);
    else
      argObjs = packageParameterFromVarArg(mth, varargAddress);

    // construct the new object
    Object newobj = constMethod.newInstance(argObjs);
    
    return newobj;

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>Method
   * (static method invocation)
   * @param methodID the method ID
   * @param expectReturnType the return type of the method to be invoked
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(int methodID, VM_Type expectReturnType)
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();	// expect a certain stack frame structure

    VM_Address varargAddress = getVarArgAddress(false);    
    return packageAndInvoke(null, methodID, varargAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>Method
   * (virtual method invocation)
   * @param obj the object instance 
   * @param methodID the method ID
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args  true if the calling JNI Function takes 4 args before the vararg
   *                   false if the calling JNI Function takes 3 args before the vararg
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(Object obj, int methodID, 
					      VM_Type expectReturnType, boolean skip4Args)
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();	// expect a certain stack frame structure

    VM_Address varargAddress = getVarArgAddress(skip4Args);    
    return packageAndInvoke(obj, methodID, varargAddress, expectReturnType, skip4Args, true);

  }

  /**
   * This method supports var args passed from C
   *
   * In the Linux Intel C convention, the caller places the args immediately above the
   * saved return address, starting with the first arg
   *
   *
   *
   *
   * For the JNI functions that takes var args, their prolog code will save the
   * var arg in the glue frame because the values in the register may be lost by 
   * subsequent calls.
   *
   * This method copies the var arg values that were saved earlier in glue frame into
   * the spill area of the original caller, thereby doing the work that the callee
   * normally performs in the AIX C convention.
   *
   * NOTE: This method contains internal stack pointer.
   * For now we assume that the stack will not be relocatable while native code is running
   * because native code can hold an address into the stack, so this code is OK,
   * but this is an issue to be resolved later
   *
   * NOTE:  this method assumes that it is immediately above the 
   * invokeWithDotDotVarArg frame, the JNI frame, the glue frame and 
   * the C caller frame in the respective order.  
   * Therefore, this method will not work if called from anywhere else
   *
   *  low address
   *
   *   |  fp  | <- VM_JNIEnvironment.getVarArgAddress
   *   | mid  |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- VM_JNIEnvironment.invokeWithDotDotVarArg frame
   *   | mid  |
   *   | ...  |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- JNI method frame
   *   | mid  |
   *   | ...  |
   *   | arg 0|    args copied by JNI prolog (3 for static, nonvirtual, 
   *   | arg 1|    or 4 for virtual)
   *   | arg 2|
   *   |      |
   *   |      |
   *   |------|
   *   | fp   | <- Native C caller frame
   *   |return|
   *   | arg 0|    
   *   | arg 1|    
   *   | arg 2|
   *   | arg 3|
   *   | arg 4|
   *   | arg 5|
   *   | arg 6|
   *   | arg 7|
   *   | arg 8|    
   *   | arg 9|
   *   |      |
   *   |      |
   *   |      |
   *
   *
   *   high address
   *
   *
   * @param skip4Args if true, the calling JNI function has 4 args before the vararg
   *                  if false, the calling JNI function has 3 args before the vararg
   * @return the starting address of the vararg in the caller stack frame
   */
  private static VM_Address getVarArgAddress(boolean skip4Args) {
    
    VM_Address fp = VM_Magic.getFramePointer();
    fp = VM_Magic.getMemoryAddress(fp);
    fp = VM_Magic.getMemoryAddress(fp);
    return (fp.add(2*4 + (skip4Args ? 4*4 : 3*4)));

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodV
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {

    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>MethodV
   * @param obj the object instance 
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {

    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, true);

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodA
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {
    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, false);
  }

  /**
   * Common code shared by the JNI functions Call<type>MethodA
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {

    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, false);

  }

  /**
   * Common code shared by invokeWithJValue, invokeWithVarArg and invokeWithDotDotVarArg
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args This flag is received from the JNI function and passed directly to 
   *                     VM_Reflection.invoke().  
   *                     It is true if the actual method is to be invoked, which could be
   *                     from the superclass.
   *                     It is false if the method from the real class of the object 
   *                     is to be invoked, which may not be the actual method specified by methodID
   * @param isVarArg  This flag describes whether the array of parameters is in var arg format or
   *                  jvalue format
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object packageAndInvoke(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args, 
					boolean isVarArg) 
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();   // expect a certain stack frame structure

    VM_Method targetMethod;
    int returnValue;

    // VM.sysWrite("JNI CallXXXMethod:  method ID " + methodID + " with args at " + 
    // 		   VM.intAsHexString(argAddress) + "\n");
    
    targetMethod = VM_MethodDictionary.getValue(methodID);
    VM_Type returnType = targetMethod.getReturnType();

    // VM.sysWrite("JNI CallXXXMethod:  " + targetMethod.getDeclaringClass().toString() +
    //		"." + targetMethod.getName().toString() + "\n");

    if (expectReturnType==null) {   // for reference return type 
      if (!returnType.isReferenceType())
	throw new Exception("Wrong return type for method: expect reference type instead of " + returnType);      
    } 
    else {    // for primitive return type
      if (returnType!=expectReturnType) 
	throw new Exception("Wrong return type for method: expect " + expectReturnType + 
			    " instead of " + returnType);
    }  

    // Repackage the arguments into an array of objects based on the signature of this method
    Object[] argObjectArray;
    if (isVarArg) {
      argObjectArray = packageParameterFromVarArg(targetMethod, argAddress);
    } else {
      argObjectArray = packageParameterFromJValue(targetMethod, argAddress);
    }

    // now invoke the method
    Object returnObj = VM_Reflection.invoke(targetMethod, obj, argObjectArray, skip4Args);
    
    return returnObj;

  }

  /**
   * Repackage the arguments passed as a variable argument list into an array of Object,
   * used by the JNI functions CallStatic<type>MethodV
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromVarArg(VM_Method targetMethod, VM_Address argAddress) {
    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromVarArg: packaging " + argCount + " arguments\n");

    VM_Address addr = argAddress;
    for (int i=0; i<argCount; i++) {

      int loword = VM_Magic.getMemoryWord(addr);
      int hiword;

      // VM.sysWrite("JNI packageParameterFromVarArg:  arg " + i + " = " + loword + 
      // " or " + VM.intAsHexString(loword) + "\n");

      addr = addr.add(4);

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	// NOTE:  in VarArg convention, C compiler will expand a float to a double that occupy 2 words
	// so we have to extract it as a double and convert it back to a float
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);                       
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapFloat((float) (Double.longBitsToDouble(doubleBits)));
	
      } else if (argTypes[i].isDoubleType()) {
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte		
	argObjectArray[i] = VM_Reflection.wrapBoolean(loword);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) loword);

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) loword);

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) loword);

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(loword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(loword);

      } else {
	return null;
      }

    }

    return argObjectArray;
    

  }

  /**
   * Repackage the arguments passed as an array of jvalue into an array of Object,
   * used by the JNI functions CallStatic<type>MethodA
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromJValue(VM_Method targetMethod, VM_Address argAddress) {

    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromJValue: packaging " + argCount + " arguments\n");

    VM_Address addr = argAddress;
    for (int i=0; i<argCount; i++, addr = addr.add(8)) {

      int loword = VM_Magic.getMemoryWord(addr);
      int hiword;

      // VM.sysWrite("JNI packageParameterFromJValue:  arg " + i + " = " + loword + 
      //  " or " + VM.intAsHexString(loword) + ", at address " + 
      //	  VM.intAsHexString(addr) + "\n");

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	argObjectArray[i] = VM_Reflection.wrapFloat(Float.intBitsToFloat(loword));

      } else if (argTypes[i].isDoubleType()) {
	hiword = VM_Magic.getMemoryWord(addr.add(4));
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	hiword = VM_Magic.getMemoryWord(addr.add(4));
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte	
	argObjectArray[i] = VM_Reflection.wrapBoolean(loword & 0x000000FF);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) (loword & 0x000000FF));

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) (loword & 0x0000FFFF));

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) (loword & 0x0000FFFF));

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(loword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(loword);

      } else {
	return null;
      }

    }

    return argObjectArray;


  }

  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java byte[] with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java byte[]
   */
  static byte[] createByteArrayFromC(VM_Address stringAddress) {
    int word;
    int length = 0;
    VM_Address addr = stringAddress;

    // scan the memory for the null termination of the string
    while (true) {
      word = VM_Magic.getMemoryWord(addr);
      int byte3 = ((word >> 24) & 0xFF);
      int byte2 = ((word >> 16) & 0xFF);
      int byte1 = ((word >> 8) & 0xFF);
      int byte0 = (word & 0xFF);
      if (byte0==0)
	break;
      length++;
      if (byte1==0) 
	break;
      length++;
      if (byte2==0)
	break;
      length++;
      if (byte3==0)
	break;
      length++;
      addr = addr.add(4);
    }

   byte[] contents = new byte[length];
   VM_Memory.memcopy(VM_Magic.objectAsAddress(contents), stringAddress, length);
   
   return contents;
  }

  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java String with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java String
   */
  static String createStringFromC(VM_Address stringAddress) {

    byte[] contents = createByteArrayFromC( stringAddress );
    return new String(contents);

  }

  public void dumpJniRefsStack () {
    int jniRefOffset = JNIRefsTop;
    VM.sysWrite("\n* * dump of JNIEnvironment JniRefs Stack * *\n");
    VM.sysWrite("* JNIRefs = ");
    VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs));
    VM.sysWrite(" * JNIRefsTop = ");
    VM.sysWrite(JNIRefsTop,false);
    VM.sysWrite(" * JNIRefsSavedFP = ");
    VM.sysWrite(JNIRefsSavedFP,false);
    VM.sysWrite(".\n*\n");
    while ( jniRefOffset >= 0 ) {
      VM.sysWrite(jniRefOffset,false);
      VM.sysWrite(" ");
      VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs).add(jniRefOffset));
      VM.sysWrite(" ");
      VM_GCUtil.dumpRef(VM_Address.fromInt(JNIRefs[ jniRefOffset >> 2 ]));
      jniRefOffset -= 4;
    }
    VM.sysWrite("\n* * end of dump * *\n");
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frames inserted at the transition from Java to
 * JNI Native C.  It will report JREFs associated with the executing
 * C frames which are in the "JREFs stack" attached to the executing
 * Threads JNIEnvironment.  It will update register location addresses
 * for the non-volatile registers to point to the registers saved
 * in the transition frame.
 *
 * @see VM_JNICompiler
 * @author Steve Smith
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_JNIGCMapIterator extends MM.VM_GCMapIterator 
    implements VM_BaselineConstants, VM_Uninterruptible {

  // Java to Native C transition frame...(see VM_JNICompiler)
  //
  //  0   	+ saved FP   + <---- FP for Jave to Native C glue frame
  // -4   	| methodID   |
  // -8   	| saved EDI  |  non-volatile GPR (JTOC for baseline callers or ? for opt callers)
  // -C  	| saved EBX  |  non-volatile GPR  
  // -10  	| saved EBP  |  non-volatile GPR  
  // -14        | returnAddr |  (for return from OutOfLineMachineCode)
  // -18        | saved PR   |  
  // -1C	| arg n-1    |  reordered arguments to native method
  // -20	|  ...       |  ...
  // -24	| arg 1      |  ...
  // -28  	| arg 0      |  ...
  // -2C  	| class/obj  |  required 2nd argument to all native methods
  // -30  	| jniEnv     |  required 1st argument to all native methods
  // -34   	| returnAddr |  return address pushed by call to native method  
  //    	+ saved FP   +  <---- FP for called native method  

  // additional instance fields added by this subclass of VM_GCMapIterator
  int[]         jniRefs;
  int           jniNextRef;
  int           jniFramePtr;
  VM_Address    jniSavedProcessorRegAddr;     // -> saved PR reg
  VM_Address    jniSavedReturnAddr;           // -> return addr in generated transition prolog
  
  public VM_JNIGCMapIterator(int[] registerLocations) {
    this.registerLocations = registerLocations;
  }
  
  // Override newStackWalk() in parent class VM_GCMapIterator to
  // initialize iterator for scan of JNI JREFs stack of refs
  // Taken:    thread
  // Returned: nothing
  //
  public void newStackWalk(VM_Thread thread) {
    super.newStackWalk(thread);   // sets this.thread, inits registerLocations[]
    VM_JNIEnvironment env = this.thread.getJNIEnv();
    // the "primordial" thread, created by JDK in the bootimage, does not have
    // a JniEnv object, all threads created by the VM will.
    if (env != null) {
      this.jniRefs = env.JNIRefs;
      this.jniNextRef = env.JNIRefsTop;
      this.jniFramePtr = env.JNIRefsSavedFP;  
      this.jniSavedProcessorRegAddr = VM_Address.zero();  
                                    // necessary so getNextRefAddr() can be used to report
                                    // jniRefs in a "frame", without calling setup. 
    } 
  }
     
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address framePtr) {
    this.framePtr = framePtr;

    // processor reg (PR) was saved at JNI_PR_OFFSET, and will be used to
    // set processor reg upon return to java.  it must be reported during
    // GC so it will be relocated, if necessary.
    //
    jniSavedProcessorRegAddr = framePtr.add(VM_JNICompiler.JNI_PR_OFFSET);

    // return address into generated prolog must be relocated if the code object
    // for that prolog/epilog is moved by GC
    jniSavedReturnAddr       = framePtr.add(VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

  } //- implements VM_GCMapIterator
   
  // return (address of) next ref in the current "frame" on the
  // threads JNIEnvironment stack of refs	  
  // When at the end of the current frame, update register locations to point
  // to the non-volatile registers saved in the JNI transition frame.
  //
  public VM_Address getNextReferenceAddress() {
    int nextFP;
    VM_Address ref_address;

    // first report jni refs in the current frame in the jniRef side stack
    // until all in the frame are reported
    //
    if ( jniNextRef > jniFramePtr ) {
      ref_address = VM_Magic.objectAsAddress(jniRefs).add(jniNextRef);
      jniNextRef = jniNextRef - 4;
      return ref_address;
    }

    // report location of saved processor reg in the Java to C frame
    if ( !jniSavedProcessorRegAddr.isZero() ) {
      ref_address = jniSavedProcessorRegAddr;
      jniSavedProcessorRegAddr = VM_Address.zero();
      return ref_address;
    }

    // no more refs to report, before returning 0, setup for processing
    // the next jni frame, if any

    // jniNextRef -> savedFramePtr for another "frame" of refs for another
    // sequence of Native C frames lower in the stack, or to 0 if this is the
    // last jni frame in the JNIRefs stack.  If more frames, initialize for a
    // later scan of those refs.
    //
    if ( jniFramePtr > 0 ) {
      jniFramePtr = jniRefs[jniFramePtr >> 2];
      jniNextRef = jniNextRef - 4;
    }

    // set register locations for non-volatiles to point to registers saved in
    // the JNI transition frame at a fixed negative offset from the callers FP.
    // the save non-volatiles are EBX, EBP,  and EDI (JTOC)
    //
    registerLocations[JTOC] = framePtr.add(VM_JNICompiler.EDI_SAVE_OFFSET).toInt();
    registerLocations[EBX]  = framePtr.add(VM_JNICompiler.EBX_SAVE_OFFSET).toInt();
    registerLocations[EBP]  = framePtr.add(VM_JNICompiler.EBP_SAVE_OFFSET).toInt();

    return VM_Address.zero();  // no more refs to report
  } //- implements VM_GCMapIterator
  
  public VM_Address getNextReturnAddressAddress() {
    VM_Address ref_address;
    if ( !jniSavedReturnAddr.isZero() ) {
      ref_address = jniSavedReturnAddr;
      jniSavedReturnAddr = VM_Address.zero();
      return ref_address;
    }
    return VM_Address.zero();
  } //- implements VM_GCMapIterator
  
  public void reset() {
  } //- implements VM_GCMapIterator
  
  public void cleanupPointers() {
  } //- implements VM_GCMapIterator
  
  public int getType() {
    return VM_CompiledMethod.JNI;
  } //- implements VM_GCMapIterator
   
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Constants for JNI support
 *
 * @author Ton Ngo
 * @author Steve Smith
 */
interface VM_JNILinuxConstants extends VM_JNIConstants {
  // byte offset of saved jtoc at end of JNIFunctions array
  static final int JNIFUNCTIONS_JTOC_OFFSET = FUNCTIONCOUNT * 4;

  // index of IP in the AIX linkage triplet
  // static final int IP = 0;                    

  // index of TOC in the AIX linage triplet
  // static final int TOC = 1;                   
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine specific helper functions for dynamic linking.
 * 
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
class VM_DynamicLinkerHelper implements VM_Constants, VM_Uninterruptible {

  /**
   * Reach up two stack frames into a frame that is compiled
   * with the DynamicBridge register protocol and grap 
   * the receiver object of the invoke (ie the first param).
   * NOTE: assumes that caller has disabled GC.
   */
  static Object getReceiverObject() throws VM_PragmaNoInline {

    VM_Address callingFrame = VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());
    callingFrame = VM_Magic.getCallerFramePointer(callingFrame);
    VM_Address location = VM_Address.zero();
    if (0 < NUM_PARAMETER_GPRS) {
      location = VM_Magic.getMemoryAddress(callingFrame.add(VM_BaselineConstants.STACKFRAME_FIRST_PARAMETER_OFFSET));

    } else {
      VM.sysFail("VM_DynamicLinerHelper: assumes at least one param passed in registers");
    }
    return VM_Magic.addressAsObject(location);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$ 

/**
 * An interface conflict resolution stub uses a hidden parameter to
 * distinguish among multiple interface methods of a class that map to
 * the same slot in the class's IMT. </p>
 * 
 * <p><STRONG>Assumption:</STRONG>
 * Register EAX contains the "this" parameter of the
 * method being called invoked.
 *
 * <p><STRONG>Assumption:</STRONG>
 * Register ECX is available as a scratch register (we need one!)
 * 
 * @author Bowen Alpern
 * @author Dave Grove
 */
class VM_InterfaceMethodConflictResolver implements VM_Constants {

  // Create a conflict resolution stub for the set of interface method signatures l.
  // 
  static INSTRUCTION[] createStub(int[] sigIds, VM_Method[] targets) {
    int numEntries = sigIds.length;
    // (1) Create an assembler.
    VM_Assembler asm = new VM_Assembler(numEntries); 
    
    // (2) signatures must be in ascending order (to build binary search tree).
    if (VM.VerifyAssertions) {
      for (int i=1; i<sigIds.length; i++) {
	VM.assert(sigIds[i-1] < sigIds[i]);
      }
    }

    // (3) Assign synthetic bytecode numbers to each switch such that we'll generate them
    // in ascending order.  This lets us use the general forward branching mechanisms
    // of the VM_Assembler.
    int[] bcIndices = new int[numEntries];
    assignBytecodeIndices(0, bcIndices, 0, numEntries -1);
    
    // (4) Generate the stub.
    insertStubPrologue(asm);
    insertStubCase(asm, sigIds, targets, bcIndices, 0, numEntries-1);
    
    return asm.getMachineCodes();
  }


  // Assign ascending bytecode indices to each case (in the order they will be generated)
  private static int assignBytecodeIndices(int bcIndex, int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    bcIndices[middle] = bcIndex++;
    if (low == middle && middle == high) {
      return bcIndex;
    } else {
      // Recurse.
      if (low < middle) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, middle+1, high);
      }
      return bcIndex;
    }
  }

  // Make a stub prologue: get TIB into ECX
  // factor out to reduce code space in each call.
  //
  private static void insertStubPrologue (VM_Assembler asm) {
    VM_ObjectModel.baselineEmitLoadTIB(asm,ECX,EAX);
  }

  // Generate a subtree covering from low to high inclusive.
  private static void insertStubCase(VM_Assembler asm,  
				     int[] sigIds, VM_Method[] targets,
				     int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    asm.resolveForwardReferences(bcIndices[middle]);
    if (low == middle && middle == high) {
      // a leaf case; can simply invoke the method directly.
      VM_Method target = targets[middle];
      if (target.isStatic()) { // an error case...
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX, VM_Entrypoints.jtocField.getOffset());
      }
      asm.emitJMP_RegDisp(ECX, target.getOffset());
    } else {
      int disp = VM_Entrypoints.hiddenSignatureIdField.getOffset();
      VM_ProcessorLocalState.emitCompareFieldWithImm(asm, disp, sigIds[middle]);
      if (low < middle) {
	asm.emitJCC_Cond_Label(asm.LT, bcIndices[(low+middle-1)/2]);
      }
      if (middle < high) {
	asm.emitJCC_Cond_Label(asm.GT, bcIndices[(middle+1+high)/2]);
      }
      // invoke the method for middle.
      VM_Method target = targets[middle];
      if (target.isStatic()) { // an error case...
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX, VM_Entrypoints.jtocField.getOffset());
      }
      asm.emitJMP_RegDisp(ECX, target.getOffset());
      // Recurse.
      if (low < middle) {
	insertStubCase(asm, sigIds, targets, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	insertStubCase(asm, sigIds, targets, bcIndices, middle+1, high);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generate a "trampoline" that jumps to the shared lazy compilation stub.
 * We do this to enable the optimizing compiler to use ptr equality of
 * target instructions to imply logical (source) equality of target methods.
 * This is used to perform guarded inlining using the "method test."
 * Without per-method lazy compilation trampolines, ptr equality of target
 * instructions does not imply source equality, since both targets may in fact
 * be the globally shared lazy compilation stub.
 * 
 * @author Dave Grove
 */
class VM_LazyCompilationTrampolineGenerator implements VM_BaselineConstants {

  /** Generate a new lazy compilation trampoline. */
  static INSTRUCTION[] getTrampoline (){
    VM_Assembler asm = new VM_Assembler(0); 
    // get JTOC into ECX
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX,
                                              VM_Entrypoints.jtocField.getOffset());
    // jmp to real lazy mathod invoker
    asm.emitJMP_RegDisp(ECX, VM_Entrypoints.lazyMethodInvokerMethod.getOffset()); 
    return asm.getMachineCodes();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine dependent portion of Reflective method invoker.
 *
 * @author Maria Butrico
 */
public class VM_MachineReflection implements VM_Constants {
  //-----------//
  // interface //
  //-----------//
   

   //----------------//
   // implementation //
   //----------------//
   
   // Determine number/type of registers and parameters required to
   // call specified method.
   //  Unlike the PowerPC code we count all the parameters, not just the
   // ones that spill.  This allow us to make enough space on the stack
   // following the calling convention.
   //
  static int 
    countParameters(VM_Method method) {
    int GPRs   = 0;
    int FPRs   = 0;
    int parameters = 0;	// parameters size in 32-bits quant.

    int gp = NUM_PARAMETER_GPRS; // 0, 1, 2
    int fp = NUM_PARAMETER_FPRS; // 0-8

    if (!method.isStatic()) {
      if (gp > 0) {GPRs++; gp--;}
      parameters++;
    }

    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
	if (gp > 0) {
	  GPRs++; gp--;
	  if (gp > 0) {GPRs++; gp--;}
	}
	parameters+=2; 
      } else if (t.isFloatType()) {
	if (fp > 0) {FPRs++; fp--;}
	parameters++;
      } else if (t.isDoubleType()) {
	if (fp > 0) {FPRs++; fp--;}
	parameters+=2;
      } else { // t is object, int, short, char, byte, or boolean
	if (gp > 0) {GPRs++; gp--;}
	parameters++;
      }
    }

    // hack to return triple
    return (parameters<<(REFLECTION_FPRS_BITS+REFLECTION_GPRS_BITS)) |
      (FPRs<<REFLECTION_GPRS_BITS) | GPRs;
  }


  // Collect parameters into arrays of registers/spills, as required to
  // call specified method.
  static void 
    packageParameters(VM_Method method, Object thisArg, Object[] otherArgs,
		      int[] GPRs, double[] FPRs, int[] Parameters) {
    int GPR	   	= 0;
    int FPR		= FPRs.length;
    int parameter	= 0;


    int gp = NUM_PARAMETER_GPRS; // 0, 1, 2
    int fp = NUM_PARAMETER_FPRS; // 0-8

    if (!method.isStatic()) {
      if (gp > 0) {
	gp--;
	GPRs[GPR++] = VM_Reflection.unwrapObject(thisArg);
      }
      Parameters[parameter++] = VM_Reflection.unwrapObject(thisArg);
    }

    VM_Type [] types = method.getParameterTypes();

    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];

      if (t.isLongType()) {
	long l = VM_Reflection.unwrapLong(otherArgs[i]);
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int)(l>>>32);
	  if (gp > 0) {
	    gp--;
	    GPRs[GPR++] = (int)(l);
	  }
	}
	Parameters[parameter++] = (int)(l>>>32);
	Parameters[parameter++] = (int)l;

      } else if (t.isFloatType()) {
	if (fp > 0) {
	  fp--;
	  FPRs[--FPR] = VM_Reflection.unwrapFloat(otherArgs[i]);
	}
	float f = VM_Reflection.unwrapFloat(otherArgs[i]);
	Parameters[parameter++] = Float.floatToIntBits(f);

      } else if (t.isDoubleType()) {
	if (fp > 0) {
	  fp--;
	  FPRs[--FPR] = VM_Reflection.unwrapDouble(otherArgs[i]);
	}
	double d = VM_Reflection.unwrapDouble(otherArgs[i]);
	long l = Double.doubleToLongBits(d);
	Parameters[parameter++] = (int)(l>>>32);
	Parameters[parameter++] = (int)l;

      } else if (t.isBooleanType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);

      } else if (t.isByteType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapByte(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapByte(otherArgs[i]);

      } else if (t.isCharType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapChar(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapChar(otherArgs[i]);

      } else if (t.isShortType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapShort(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapShort(otherArgs[i]);

      } else if (t.isIntType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapInt(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapInt(otherArgs[i]);

      } else if (!t.isPrimitiveType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapObject(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapObject(otherArgs[i]);

      } else  {
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
      }
    }
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A place to put hand written machine code typically invoked by VM_Magic 
 * methods.
 *
 * <p>Hand coding of small inline instruction sequences is typically handled by 
 * each compiler's implementation of VM_Magic methods.  
 * A few VM_Magic methods are so complex that their implementations require 
 * many instructions.  But our compilers do not inline 
 * arbitrary amounts of machine code. We therefore write such code blocks 
 * here, out of line.
 * 
 * <p>These code blocks can be shared by all compilers. They can be branched to
 * via a jtoc offset (obtained from VM_Entrypoints.XXXInstructionsField).
 * 
 * <p> 17 Mar 1999 Derek Lieber (adapted from powerPC version in 2000 
 * by somebody)
 * 
 * <p> 15 Jun 2001 Dave Grove and Bowen Alpern (Derek believed that compilers 
 * could inline these methods if they wanted.  We do not believe this would 
 * be very easy since they return assuming the return address is on the stack.)
 *
 * @author Maria Butrico
 */
class VM_OutOfLineMachineCode implements VM_BaselineConstants {
  //-----------//
  // interface //
  //-----------//
   
  static void init() {
    reflectiveMethodInvokerInstructions        = generateReflectiveMethodInvokerInstructions();
    saveThreadStateInstructions                = generateSaveThreadStateInstructions();
    threadSwitchInstructions                   = generateThreadSwitchInstructions();
    restoreHardwareExceptionStateInstructions  = generateRestoreHardwareExceptionStateInstructions();
    invokeNativeFunctionInstructions           = generateInvokeNativeFunctionInstructions();
  }

  //----------------//
  // implementation //
  //----------------//

  private static INSTRUCTION[] reflectiveMethodInvokerInstructions;
  private static INSTRUCTION[] saveThreadStateInstructions;
  private static INSTRUCTION[] threadSwitchInstructions;
  private static INSTRUCTION[] restoreHardwareExceptionStateInstructions;
  private static INSTRUCTION[] invokeNativeFunctionInstructions;
   
  private static final int PARAMS_FP_OFFSET	= WORDSIZE * 2;
  private static final int FPRS_FP_OFFSET	= WORDSIZE * 3;
  private static final int GPRS_FP_OFFSET	= WORDSIZE * 4;
  private static final int CODE_FP_OFFSET	= WORDSIZE * 5;


  /**
   * Machine code for reflective method invocation.
   *
   * VM compiled with NUM_PARAMETERS_GPRS == 0
   *   Registers taken at runtime:
   *     none
   *   Stack taken at runtime:
   *     hi-mem
   *         address of method entrypoint to be called
   *         address of gpr registers to be loaded
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * VM compiled with NUM_PARAMETERS_GPRS == 1
   *   T0 == address of method entrypoint to be called
   *   Stack taken at runtime:
   *     hi-mem
   *         space ???
   *         address of gpr registers to be loaded
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * VM compiled with NUM_PARAMETERS_GPRS == 2
   *   T0 == address of method entrypoint to be called
   *   T1 == address of gpr registers to be loaded
   *   Stack taken at runtime:
   *     hi-mem
   *         space ???
   *         space ???
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * Registers returned at runtime:
   *   standard return value conventions used
   *
   * Side effects at runtime:
   *   artificial stackframe created and destroyed
   *   volatile, and scratch registers destroyed
   *
  */
  private static INSTRUCTION[] generateReflectiveMethodInvokerInstructions() {
    VM_Assembler asm = new VM_Assembler(100);

    /* write at most 2 parameters from registers in the stack.  This is
     * logically equivalent to ParamaterRegisterUnload in the compiler
     */
    int gprs;
    int fpOffset = VM_Entrypoints.framePointerField.getOffset();
    byte T = T0;
    gprs = NUM_PARAMETER_GPRS;
    int offset = 4 << LG_WORDSIZE;		// we have exactly 4 paramaters
    if (gprs > 0) {
      gprs--;
      asm.emitMOV_RegDisp_Reg(SP, offset, T); 
      T = T1;
      offset -= WORDSIZE;
    }

    if (gprs > 0)
      asm.emitMOV_RegDisp_Reg(SP, offset, T); 
    /* available registers S0, T0, T1 */


    /* push a new frame */
    asm.emitPUSH_RegDisp(PR, fpOffset); // link this frame with next
    VM_ProcessorLocalState.emitMoveRegToField(asm, fpOffset, SP); // establish base of new frame
    asm.emitPUSH_Imm    (INVISIBLE_METHOD_ID);
    asm.emitADD_Reg_Imm (SP, STACKFRAME_BODY_OFFSET);
    
    /* write parameters on stack 
     * move data from memory addressed by Paramaters array, the fourth
     * parameter to this, into the stack.
     * SP target address
     * S0 source address
     * T1 length
     * T0 scratch
     */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, PARAMS_FP_OFFSET);// S0 <- Parameters
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- Parameters.length()
    asm.emitCMP_Reg_Imm     (T1, 0);			// length == 0 ?

    int parameterLoopLabel = asm.getMachineCodeIndex();
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);	// done? --> branch to end
    asm.emitMOV_Reg_RegInd (T0, S0);			// T0 <- Paramaters[i]
    asm.emitPUSH_Reg (T0);				// mem[j++] <- Parameters[i]
    asm.emitADD_Reg_Imm (S0, WORDSIZE);			// i++
    asm.emitADD_Reg_Imm (T1, -1);			// length--
    asm.emitJMP_Imm (parameterLoopLabel);

    fr1.resolve(asm);					// end of the loop
    
    /* write fprs onto fprs registers */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, FPRS_FP_OFFSET);	// S0 <- FPRs
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- FPRs.length()
    asm.emitSHL_Reg_Imm (T1, LG_WORDSIZE + 1 );		// length in bytes
    asm.emitADD_Reg_Reg (S0, T1);			// S0 <- last FPR + 8
    asm.emitCMP_Reg_Imm (T1, 0);			// length == 0 ?

    int fprsLoopLabel = asm.getMachineCodeIndex();
    VM_ForwardReference fr2 = asm.forwardJcc(asm.EQ);	// done? --> branch to end
    asm.emitSUB_Reg_Imm ( S0, 2 * WORDSIZE);		// i--
    asm.emitFLD_Reg_RegInd_Quad (FP0, S0);		// frp[fpr_sp++] <-FPRs[i]
    asm.emitSUB_Reg_Imm (T1, 2* WORDSIZE);		// length--
    asm.emitJMP_Imm (fprsLoopLabel);

    fr2.resolve(asm);					// end of the loop


    /* write gprs: S0 = Base address of GPRs[], T1 = GPRs.length */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, GPRS_FP_OFFSET);	// S0 <- GPRs
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- GPRs.length()
    asm.emitCMP_Reg_Imm (T1, 0);			// length == 0 ?
    VM_ForwardReference fr3 = asm.forwardJcc(asm.EQ);	// result 0 --> branch to end
    asm.emitMOV_Reg_RegInd (T0, S0);			// T0 <- GPRs[0]
    asm.emitADD_Reg_Imm (S0, WORDSIZE);			// S0 += WORDSIZE
    asm.emitADD_Reg_Imm (T1, -1);			// T1--
    VM_ForwardReference fr4 = asm.forwardJcc(asm.EQ);	// result 0 --> branch to end
    asm.emitMOV_Reg_RegInd (T1, S0);			// T1 <- GPRs[1]
    fr3.resolve(asm);
    fr4.resolve(asm);

    /* branch to method.  On a good day we might even be back */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, CODE_FP_OFFSET);	// S0 <- code
    asm.emitCALL_Reg (S0);				// go there
    // T0/T1 have returned value

    /* and get out */
    // NOTE: RVM callee has popped the params, so we can simply
    //       add back in the initial SP to FP delta to get SP to be a framepointer again!
    asm.emitADD_Reg_Imm (SP, -STACKFRAME_BODY_OFFSET + 4); 
    asm.emitPOP_RegDisp (PR, fpOffset);

    asm.emitRET_Imm(4 << LG_WORDSIZE);			// again, exactly 4 parameters

    return asm.getMachineCodes();
  }


  /**
   * Machine code to implement "VM_Magic.saveThreadState()".
   * 
   *  Registers taken at runtime:
   *    T0 == address of VM_Registers object
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    S0, T1 destroyed
   *    Thread state stored into VM_Registers object 
   */
  private static INSTRUCTION[] generateSaveThreadStateInstructions() {
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0); // assuming no NV FPRs (otherwise would have to save them here)
    VM_Assembler asm = new VM_Assembler(0);
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    asm.emitMOV_Reg_RegDisp(S0, PR, VM_Entrypoints.framePointerField.getOffset()); 
    asm.emitMOV_RegDisp_Reg(T0, fpOffset, S0);        // registers.fp := pr.framePointer
    asm.emitPOP_Reg        (T1);                      // T1 := return address 
    asm.emitADD_Reg_Imm    (SP, 4);                   // throw away space for registers parameter (in T0)
    asm.emitMOV_Reg_RegDisp(S0, T0, gprsOffset);      // S0 := registers.gprs[]
    asm.emitMOV_RegDisp_Reg(S0, SP<<LG_WORDSIZE, SP); // registers.gprs[#SP] := SP
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_RegDisp_Reg(S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE, NONVOLATILE_GPRS[i]); // registers.gprs[i] := i'th register
    }
    asm.emitJMP_Reg        (T1);                      // return to return address
    return asm.getMachineCodes();
  }
      

  /**
   * Machine code to implement "VM_Magic.threadSwitch()".
   * 
   *  Parameters taken at runtime:
   *    T0 == address of VM_Thread object for the current thread
   *    T1 == address of VM_Registers object for the new thread
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    sets current Thread's beingDispatched field to false
   *    saves current Thread's nonvolatile hardware state in its VM_Registers object
   *    restores new thread's VM_Registers nonvolatile hardware state.
   *    execution resumes at address specificed by restored thread's VM_Registers ip field
   */
  private static INSTRUCTION[] generateThreadSwitchInstructions() {
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0); // assuming no NV FPRs (otherwise would have to save them here)
    VM_Assembler asm = new VM_Assembler(0);
    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    int regsOffset = VM_Entrypoints.threadContextRegistersField.getOffset();

    // (1) Save hardware state of thread we are switching off of.
    asm.emitMOV_Reg_RegDisp  (S0, T0, regsOffset);      // S0 = T0.contextRegisters
    asm.emitPOP_RegDisp      (S0, ipOffset);            // T0.contextRegisters.ip = returnAddress
    asm.emitPUSH_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset()); // push PR.framePointer
    asm.emitPOP_RegDisp      (S0, fpOffset);            // T0.contextRegisters.fp = pushed framepointer
    asm.emitADD_Reg_Imm      (SP, 8);                   // discard 2 words of parameters (T0, T1)
    asm.emitMOV_Reg_RegDisp  (S0, S0, gprsOffset);      // S0 = T0.contextRegisters.gprs;
    asm.emitMOV_RegDisp_Reg  (S0, SP<<LG_WORDSIZE, SP); // T0.contextRegisters.gprs[#SP] := SP
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_RegDisp_Reg(S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE, NONVOLATILE_GPRS[i]); // T0.contextRegisters.gprs[i] := i'th register
    }

    // (2) Set currentThread.beingDispatched to false
    asm.emitMOV_RegDisp_Imm(T0, VM_Entrypoints.beingDispatchedField.getOffset(), 0); // previous thread's stack is nolonger in use, so it can now be dispatched on any virtual processor 
    
    // (3) Restore hardware state of thread we are switching to.
    asm.emitMOV_Reg_RegDisp(S0, T1, fpOffset);        // S0 := restoreRegs.fp
    VM_ProcessorLocalState.emitMoveRegToField(asm, VM_Entrypoints.framePointerField.getOffset(), S0); // PR.framePointer = restoreRegs.fp
    asm.emitMOV_Reg_RegDisp(S0, T1, gprsOffset);      // S0 := restoreRegs.gprs[]
    asm.emitMOV_Reg_RegDisp(SP, S0, SP<<LG_WORDSIZE); // SP := restoreRegs.gprs[#SP]
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_Reg_RegDisp(NONVOLATILE_GPRS[i], S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE); // i'th register := restoreRegs.gprs[i]
    }
    asm.emitJMP_RegDisp    (T1, ipOffset);            // return to (save) return address
    return asm.getMachineCodes();
  }
      

  /**
   * Machine code to implement "VM_Magic.restoreHardwareExceptionState()".
   * 
   *  Registers taken at runtime:
   *    T0 == address of VM_Registers object
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    all registers are restored except PROCESSOR_REGISTER and EFLAGS;
   *    execution resumes at "registers.ip"
   */
  private static INSTRUCTION[] generateRestoreHardwareExceptionStateInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();

    // Set PR.framePointer to be registers.fp
    asm.emitMOV_Reg_RegDisp(S0, T0, fpOffset); 
    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              S0);

    // Restore SP
    asm.emitMOV_Reg_RegDisp (S0, T0, gprsOffset);
    asm.emitMOV_Reg_RegDisp (SP, S0, SP<<LG_WORDSIZE);
    
    // Push registers.ip to stack (now that SP has been restored)
    asm.emitPUSH_RegDisp(T0, ipOffset);

    // Restore the GPRs except for S0, PR, and SP 
    // (restored above and then modified by pushing registers.ip!)
    for (byte i= 0; i < NUM_GPRS; i++) {
      if (i != S0 && i != ESI && i != SP) {
	asm.emitMOV_Reg_RegDisp(i, S0, i<<LG_WORDSIZE);
      }
    }
    
    // Restore S0
    asm.emitMOV_Reg_RegDisp(S0, S0, S0<<LG_WORDSIZE);

    // Return to registers.ip (popping stack)
    asm.emitRET();
    return asm.getMachineCodes();
  }

  // Out of line prolog/epilog called from generated prologues for user
  // written native methods (see VM_JNICompiler).  Completes the call
  // into native code from java and handles the return from native back
  // to java.
  //
  // on entry assume:
  //   TOC = TOC for native call
  //   S0  = address of native function to branch to
  //
  private static INSTRUCTION[]
  generateInvokeNativeFunctionInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    // save PR in glue frame - to be relocated by GC
    VM_ProcessorLocalState.emitStoreProcessor(asm, EBP, 
                                              VM_JNICompiler.JNI_PR_OFFSET);

    // save callers ret addr in glue frame
    asm.emitPOP_RegDisp (EBP, VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

    // change processor status to IN_NATIVE
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0, 
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    asm.emitMOV_RegInd_Imm(T0, VM_Processor.IN_NATIVE);

    // make the call...
    asm.emitCALL_Reg(S0);

    // return from native code here...
    // T0 contains single word return value from native
    // T1 ...

    // push return values on stack
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);

    int retryLabel = asm.getMachineCodeIndex();     // backward branch label

    // reload PR ref from glue frame
    VM_ProcessorLocalState.emitLoadProcessor(asm, EBP,
                                             VM_JNICompiler.JNI_PR_OFFSET);

    // reload JTOC from processor NOTE: JTOC saved in glue frame may not be
    // the RVM JTOC 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());

    // S0<-addr of statusword
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());

    asm.emitMOV_Reg_RegInd(T0,S0);                         // T0<-contents of statusword 
    asm.emitCMP_Reg_Imm (T0, VM_Processor.IN_NATIVE);      // jmp if still IN_NATIVE
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);       // if so, skip 3 instructions

    // blocked in native, do pthread yield
    asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());  // T0<-bootrecord addr
    asm.emitCALL_RegDisp(T0, VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset());
    asm.emitJMP_Imm (retryLabel);                          // retry from beginning

    fr.resolve(asm);      // branch here if IN_NATIVE, attempt to go to IN_JAVA

    // T0 (EAX) contains "old value" (required for CMPXCNG instruction)
    // S0 contains address of status word to be swapped
    asm.emitMOV_Reg_Imm (T1, VM_Processor.IN_JAVA);  // T1<-new value (IN_JAVA)
    asm.emitCMPXCHG_RegInd_Reg(S0,T1);               // atomic compare-and-exchange
    asm.emitJCC_Cond_Imm(asm.NE,retryLabel);
									
    // status is now IN_JAVA. GC can not occur while we execute on a processor
    // in this state, so it is safe to access fields of objects

    // Test if returning to Java on a RVM processor or a Native processor.

    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0,
                                              VM_Entrypoints.processorModeField.getOffset());
    asm.emitCMP_Reg_Imm (T0, VM_Processor.RVM);           // test for RVM
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);     // Br if yes

    // If here, on a native processor, it is necessary to transfer back to a
    // RVM processor before returning to the Java calling method.

    // !!! volatile FPRs will be lost during the yield to the transfer
    // queue of the RVM processor, and later redispatch on that processor.
    // ADD A SAVE OF FPR return reg (top on FPR stack?) before trnsferring
    // branch to becomeRVMThread to make the transfer

    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.becomeRVMThreadMethod.getOffset());

    // execution here is now on the RVM processor, and on a different
    // os pThread. non-volatile GRPs & FPRs have been saved and restored
    // during the transfer. PR now points to the RVM processor we have
    // been transferred to.

    // XXX Restore the saved FPR return reg, before returning

    fr1.resolve(asm);  // branch to here if returning on a RVM processor

    // pop return values off stack into expected regs before returning to caller
    asm.emitPOP_Reg(T1);
    asm.emitPOP_Reg(T0);

    // push callers return address onto stack, prevoiusly saved in glue frame
    asm.emitPUSH_RegDisp (EBP, VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

    asm.emitRET();

    return asm.getMachineCodes();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

//-#if RVM_WITH_OPT_COMPILER
import instructionFormats.*;
//-#endif 

/**
 * This class provides a layer of abstraction that the rest of the VM must
 * use in order to access the current <code>VM_Processor</code> object.
 *
 * @see VM_Processor
 *
 * @author Stephen Fink
 */
public final class VM_ProcessorLocalState 
//-#if RVM_WITH_OPT_COMPILER
extends OPT_IRTools
//-#endif 
{
  
  static byte PROCESSOR_REGISTER = VM_RegisterConstants.ESI;

  /**
   * The C bootstrap program has placed a pointer to the initial
   * VM_Processor in ESI.  
   */
  static void boot() {
    // do nothing - everything is already set up.
  }


  /**
   * Return the current VM_Processor object
   */
  public static VM_Processor getCurrentProcessor() throws VM_PragmaUninterruptible {
    return VM_Magic.getESIAsProcessor();
  }

  /**
   * Set the current VM_Processor object
   */
  public static void setCurrentProcessor(VM_Processor p) throws VM_PragmaUninterruptible {
    VM_Magic.setESIAsProcessor(p);
  }

  /**
   * Emit an instruction sequence to move the value of a register into a field 
   * in the current processor offset 
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param reg number of the register supplying the new value
   */
  static void emitMoveRegToField(VM_Assembler asm, int offset, byte reg) {
    asm.emitMOV_RegDisp_Reg(PROCESSOR_REGISTER,offset,reg);
  }

  /**
   * Emit an instruction sequence to move an immediate value into a field 
   * in the current processor offset 
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param imm immediate value
   */
  static void emitMoveImmToField(VM_Assembler asm, int offset, int imm) {
    asm.emitMOV_RegDisp_Imm(PROCESSOR_REGISTER,offset,imm);
  }

  /**
   * Emit an instruction sequence to move the value of a field in the 
   * current processor offset to a register
   *
   * @param asm assembler object
   * @param dest number of destination register
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitMoveFieldToReg(VM_Assembler asm, byte dest, int offset) {
    asm.emitMOV_Reg_RegDisp(dest,PROCESSOR_REGISTER,offset);
  }

  /**
   * Emit an instruction sequence to compare the value of a field in the 
   * current processor offset with an immediate value
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param imm immediate value to compare with
   */
  static void emitCompareFieldWithImm(VM_Assembler asm, int offset, int imm) {
    asm.emitCMP_RegDisp_Imm(PROCESSOR_REGISTER,offset,imm);
  }
  /**
   * Emit an instruction sequence to decrement the value of a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitDecrementField(VM_Assembler asm, int offset) {
    asm.emitDEC_RegDisp(PROCESSOR_REGISTER,offset);
  }
  /**
   * Emit an instruction sequence to PUSH the value of a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitPushField(VM_Assembler asm, int offset) {
    asm.emitPUSH_RegDisp(PROCESSOR_REGISTER,offset);
  }
  /**
   * Emit an instruction sequence to POP a value into a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitPopField(VM_Assembler asm, int offset) {
    asm.emitPOP_RegDisp(PROCESSOR_REGISTER,offset);
  }

  /**
   * Emit an instruction sequence to set the current VM_Processor 
   * to be the value at [base] + offset
   *
   * <P>TODO: this method is used only by the JNI compiler.  Consider
   * rewriting the JNI compiler to allow us to deprecate this method.
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitSetProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_Reg_RegDisp(PROCESSOR_REGISTER, base, offset);
  }

  /**
   * Emit an instruction sequence to PUSH a pointer to the current VM_Processor
   * object on the stack.
   *
   * @param asm assembler object
   */
  static void emitPushProcessor(VM_Assembler asm) {
    asm.emitPUSH_Reg(PROCESSOR_REGISTER);
  }

  /**
   * Emit an instruction sequence to POP a value on the stack, and set the
   * current processor reference to be this value.
   *
   * @param asm assembler object
   */
  static void emitPopProcessor(VM_Assembler asm) {
    asm.emitPOP_Reg(PROCESSOR_REGISTER);
  }

  /**
   * Emit an instruction sequence to store a pointer to the current VM_Processor
   * object at a location defined by [base]+offset
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitStoreProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_RegDisp_Reg(base,offset,PROCESSOR_REGISTER);
  }
  /**
   * Emit an instruction sequence to load current VM_Processor
   * object from a location defined by [base]+offset
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitLoadProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_Reg_RegDisp(PROCESSOR_REGISTER,base,offset);
  }

  //-#if RVM_WITH_OPT_COMPILER
  /**
   * Insert code during BURS to load a pointer to the current processor
   * into a symbolic register, and return the resultant operand
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_BURS burs) {
    OPT_RegisterOperand result =
      burs.ir.regpool.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
    OPT_Register ESI = burs.ir.regpool.getPhysicalRegisterSet().getESI();

    burs.append(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }

  /**
   * Insert code before instruction s to load a pointer to the current 
   * processor into a symbolic register, and return the resultant operand
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s) {
    OPT_RegisterOperand result = ir.regpool.makeTemp
                                 (OPT_ClassLoaderProxy.VM_ProcessorType);
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    s.insertBefore(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }
  /**
   * Insert code before instruction s to load a pointer to the current 
   * processor into a particular register operand.
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s,
                                                       OPT_RegisterOperand rop)
  {
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    OPT_RegisterOperand result = rop.copyRO();
    s.insertBefore(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }
  /**
   * Insert code after instruction s to set the current 
   * processor to be the value of a particular register operand.
   */
  static OPT_RegisterOperand appendSetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s,
                                                       OPT_RegisterOperand rop)
  {
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    s.insertBefore(MIR_Move.create(IA32_MOV,R(ESI),rop.copyRO()));
    return rop;
  }
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frame  built by the Baseline compiler
 * An Instance of this class will iterate through a particular 
 * reference map of a method returning the offsets of any refereces
 * that are part of the input parameters, local variables, and 
 * java stack for the stack frame.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_BaselineGCMapIterator extends VM_GCMapIterator 
  implements VM_BaselineConstants,
	     VM_Uninterruptible  {

  // Iterator state for mapping any stackframe.
  //
  private   int              mapOffset; // current offset in current map
  private   int              mapId;     // id of current map out of all maps
  private   VM_ReferenceMaps maps;      // set of maps for this method

  // Additional iterator state for mapping dynamic bridge stackframes.
  //
  private VM_DynamicLink dynamicLink;                    // place to keep info returned by VM_CompiledMethod.getDynamicLink
  private VM_Method      bridgeTarget;                   // method to be invoked via dynamic bridge (null: current frame is not a dynamic bridge)
  private VM_Method      currentMethod;                  // method for the frame
  private VM_Type[]      bridgeParameterTypes;           // parameter types passed by that method
  private boolean        bridgeParameterMappingRequired; // have all bridge parameters been mapped yet?
  private boolean        bridgeRegistersLocationUpdated; // have the register location been updated
  private int            bridgeParameterInitialIndex;    // first parameter to be mapped (-1 == "this")
  private int            bridgeParameterIndex;           // current parameter being mapped (-1 == "this")
  private int            bridgeRegisterIndex;            // gpr register it lives in
  private VM_Address     bridgeRegisterLocation;         // memory address at which that register was saved


  //
  // Remember the location array for registers. This array needs to be updated
  // with the location of any saved registers.
  // This information is not used by this iterator but must be updated for the
  // other types of iterators (ones for the quick and opt compiler built frames)
  // The locations are kept as addresses within the stack.
  //
  public VM_BaselineGCMapIterator(int registerLocations[]) {
    this.registerLocations = registerLocations; // (in superclass)
    dynamicLink  = new VM_DynamicLink();
  }

  //
  // Set the iterator to scan the map at the machine instruction offset provided.
  // The iterator is positioned to the beginning of the map
  //
  //   method - identifies the method and class
  //   instruction offset - identifies the map to be scanned.
  //   fp  - identifies a specific occurrance of this method and
  //         allows for processing instance specific information
  //         i.e JSR return address values
  //
  //  NOTE: An iterator may be reused to scan a different method and map.
  //
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address fp) {
    currentMethod = compiledMethod.getMethod();

    // setup superclass
    //
    framePtr = fp;
      
    // setup stackframe mapping
    //
    maps      = ((VM_BaselineCompiledMethod)compiledMethod).referenceMaps;
    mapId     = maps.locateGCPoint(instructionOffset, currentMethod);
    mapOffset = 0;
    if (mapId < 0) {
      // lock the jsr lock to serialize jsr processing
      VM_ReferenceMaps.jsrLock.lock();
      maps.setupJSRSubroutineMap( framePtr, mapId, compiledMethod);
    }
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator setupIterator mapId = ");
      VM.sysWrite(mapId);
      VM.sysWrite(" for ");
      VM.sysWrite(currentMethod);
      VM.sysWrite(".\n");
    }
      
    // setup dynamic bridge mapping
    //
    bridgeTarget                   = null;
    bridgeParameterTypes           = null;
    bridgeParameterMappingRequired = false;
    bridgeRegistersLocationUpdated = false;
    bridgeParameterIndex           = 0;
    bridgeRegisterIndex            = 0;
    bridgeRegisterLocation         = VM_Address.zero();

    if (currentMethod.getDeclaringClass().isDynamicBridge()) {
      fp                       = VM_Magic.getCallerFramePointer(fp);
      VM_Address        ip                       = VM_Magic.getNextInstructionAddress(fp);
      int               callingCompiledMethodId  = VM_Magic.getCompiledMethodID(fp);
      VM_CompiledMethod callingCompiledMethod    = VM_CompiledMethods.getCompiledMethod(callingCompiledMethodId);
      int               callingInstructionOffset = ip.diff(VM_Magic.objectAsAddress(callingCompiledMethod.getInstructions()));

      callingCompiledMethod.getDynamicLink(dynamicLink, callingInstructionOffset);
      bridgeTarget                = dynamicLink.methodRef();
      bridgeParameterInitialIndex = dynamicLink.isInvokedWithImplicitThisParameter() ? -1 : 0;
      bridgeParameterTypes        = bridgeTarget.getParameterTypes();
    }
        
    reset();
  }
  
  // Reset iteration to initial state.
  // This allows a map to be scanned multiple times
  //
  public void reset() {

    mapOffset = 0;

    if (bridgeTarget != null) {
      // point to first saved gpr
      bridgeParameterMappingRequired = true;
      bridgeParameterIndex   = bridgeParameterInitialIndex;
      bridgeRegisterIndex    = FIRST_VOLATILE_GPR;
      bridgeRegisterLocation = VM_Address.fromInt(VM_Magic.getMemoryWord(framePtr));
      bridgeRegisterLocation = bridgeRegisterLocation.sub(8 * (LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) +
							  4 * (LAST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR + 1));
    }
  }

  // Get location of next reference.
  // A zero return indicates that no more references exist.
  //
  public VM_Address getNextReferenceAddress() {

    if (mapId < 0)
      mapOffset = maps.getNextJSRRef(mapOffset);
    else
      mapOffset = maps.getNextRef(mapOffset, mapId);
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
      if (mapId < 0) 
	VM.sysWrite("Offset is a JSR return address ie internal pointer.\n");
    }

    if (mapOffset != 0) {
      return (framePtr.add(mapOffset));

    } else if (bridgeParameterMappingRequired) {

      if (VM.TraceStkMaps) {
	VM.sysWrite("getNextReferenceAddress: bridgeTarget="); VM.sysWrite(bridgeTarget); VM.sysWrite("\n");
      }
      if (!bridgeRegistersLocationUpdated) {
	// point registerLocations[] to our callers stackframe
	//
	VM_Address location = framePtr.add(VM_Compiler.getFrameSize(currentMethod));
	location = location.sub((LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) * 8); 
	// skip non-volatile and volatile fprs
	for (int i = LAST_NONVOLATILE_GPR; i >= FIRST_VOLATILE_GPR; --i) {
	  location = location.sub(4);
	  registerLocations[i] = location.toInt();
	}

	bridgeRegistersLocationUpdated = true;
      }

      // handle implicit "this" parameter, if any
      //
      if (bridgeParameterIndex == -1) {
	bridgeParameterIndex   += 1;
	bridgeRegisterIndex    += 1;
	bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	return bridgeRegisterLocation.sub(4);
      }
         
      // now the remaining parameters
      //
      while (true) {
	if (bridgeParameterIndex == bridgeParameterTypes.length || bridgeRegisterIndex > LAST_VOLATILE_GPR) {
	  bridgeParameterMappingRequired = false;
	  break;
	}
	VM_Type bridgeParameterType = bridgeParameterTypes[bridgeParameterIndex++];
	if (bridgeParameterType.isReferenceType()) {
	  bridgeRegisterIndex    += 1;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	  return bridgeRegisterLocation.sub(4);
	} else if (bridgeParameterType.isLongType()) {
	  bridgeRegisterIndex    += 2;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(8);
	} else if (bridgeParameterType.isDoubleType() || bridgeParameterType.isFloatType()) {
	  // no gpr's used
	} else {
	  // boolean, byte, char, short, int
	  bridgeRegisterIndex    += 1;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	}
      }
    }
      
    return VM_Address.zero();
  }

  //
  // Gets the location of the next return address
  // after the current position.
  //  a zero return indicates that no more references exist
  //
  public VM_Address getNextReturnAddressAddress() {

    if (mapId >= 0) {
      if (VM.TraceStkMaps) {
	VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset mapId = ");
	VM.sysWrite(mapId);
	VM.sysWrite(".\n");
      }
      return VM_Address.zero();
    }
    mapOffset = maps.getNextJSRReturnAddr(mapOffset);
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
    }
    return (mapOffset == 0) ? VM_Address.zero() : framePtr.add(mapOffset);
  }

  // cleanup pointers - used with method maps to release data structures
  //    early ... they may be in temporary storage ie storage only used
  //    during garbage collection
  //
  public void cleanupPointers() {
    maps.cleanupPointers();
    maps = null;
    if (mapId < 0) 
      VM_ReferenceMaps.jsrLock.unlock();
    bridgeTarget         = null;
    bridgeParameterTypes = null;
  }
       
  public int getType() {
    return VM_CompiledMethod.BASELINE;
  }

  // For debugging (used with checkRefMap)
  //
  public int getStackDepth() {
    return maps.getStackDepth(mapId);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine code generators:
 *
 * Corresponding to a PowerPC assembler instruction of the form
 *    xx A,B,C
 * there will be a method
 *    void emitXX (int A, int B, int C).
 * 
 * The emitXX method appends this instruction to an VM_MachineCode object.
 * The name of a method for generating assembler instruction with the record
 * bit set (say xx.) will be end in a lower-case r (emitXXr).
 * 
 * mIP will be incremented to point to the next machine instruction.
 * 
 * Machine code generators:
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 * @author Derek Lieber
 * @modified Dave Grove
 */
final class VM_Assembler implements VM_BaselineConstants,
				    VM_AssemblerConstants {

  private VM_MachineCode mc;
  private int mIP; // current machine code instruction
  private boolean shouldPrint;

  VM_Assembler (int length) {
    this(length, false);
  }

  VM_Assembler (int length, boolean sp) {
    mc = new VM_MachineCode();
    mIP = 0;
    shouldPrint = sp;
  }

  /* assembler stuff:

     labels and comments can be added to the an assembler instruction by
     calling the label and comment methods with the appropriate Strings
     BEFORE calling the emit method that generates the instruction.

  */

  final static boolean fits (int val, int bits) {
    val = val >> bits-1;
    return (val == 0 || val == -1);
  }

  final static String hex (int i) {
    if (i == 0) return "0x0";
    String s = VM.intAsHexString(i).substring(2);
    while (s.substring(0,1).equals("0")) 
      s = s.substring(1);
    return "0x" + s;
  }

  private final static String signedHex (int i) {
    if (i > 0) return hex(i);
    if (i < 0) return "-" + hex(-i);
    return "0x0";
  }

  private final static String left (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(0,w);
    for (int i=n; i<w; i++) {
      s = s + " ";
    }
    return s; 
  }

  private final static String right (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(n-w);
    for (int i=n; i<w; i++) {
      s = " " + s;
    } 
    return s; 
  }

  void noteBytecode (int i, String bcode) {
    if (!VM.TraceAssembler) return;
    VM.sysWrite("[" + i + "] " + bcode + "\n");
    if (!VM.TraceAssembler) return;
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, String args) {
    String instr = right(hex(inum<<2),6) + "| " + right(hex((int)mi),8);
    instr += " " + left(opcode,6) + left(args,20);
    System.out.println(instr);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode) {
    String args = "";
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT) {
    String args = right(""+RT,2);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, String s) {
    String args = right(""+RT,2) + right(s,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA) {
    String args = right(""+RT,2) + right(""+RA,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, String D, int RA) {
    String args = right(""+RT,2) + right(D,7) + right("("+RA,3) + ")";
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, int RB) {
    String args = right(""+RT,2) + right(" "+RA,3) + ", " + right(""+RB,2);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, String V){
    String args = right(""+RT,2) + right(" "+RA,3) + ", " + left(V,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, int RC, int RB) {
    String args = right(""+RT,2) + right(""+RA,7) 
                                 + ", " + right(""+RC,2) 
                                 + ", " + right(""+RB,2);
    asm (inum, mi, opcode, args);
  }

  /* Handling backward branch references */

  int getMachineCodeIndex () {
    return mIP;
  }

  /* Handling forward branch references */

  VM_ForwardReference forwardRefs = null;

  /* call before emiting code for the branch */
  final void reserveForwardBranch (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.UnconditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the branch */
  final void reserveForwardConditionalBranch (int where) {
    emitNOP();
    VM_ForwardReference fr = new VM_ForwardReference.ConditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the branch */
  final void reserveShortForwardConditionalBranch (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.ConditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting data for the case branch */
  final void reserveForwardCase (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.SwitchCase(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the target */
  final void resolveForwardReferences (int label) {
    if (forwardRefs == null) return; 
    forwardRefs = VM_ForwardReference.resolveMatching(this, forwardRefs, label);
  }

  final void patchUnconditionalBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" <- " + hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if (VM.VerifyAssertions) VM.assert((delta>>>23) == 0); // delta (positive) fits in 24 bits
    instr |= (delta<<2);
    mc.putInstruction(sourceMachinecodeIndex, instr);
  }
  
  final void patchConditionalBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" << " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if ((delta>>>13) == 0) { // delta (positive) fits in 14 bits
      instr |= (delta<<2);
      mc.putInstruction(sourceMachinecodeIndex, instr);
    } else {
      if (VM.VerifyAssertions) VM.assert((delta>>>23) == 0); // delta (positive) fits in 24 bits
      instr ^= 0x01000008; // make skip instruction with opposite sense
      mc.putInstruction(sourceMachinecodeIndex-1, instr); // skip unconditional branch to target
      mc.putInstruction(sourceMachinecodeIndex,  Btemplate | (delta&0xFFFFFF)<<2);
    }
  }

  final void patchShortBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" << " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if ((delta>>>13) == 0) { // delta (positive) fits in 14 bits
      instr |= (delta<<2);
      mc.putInstruction(sourceMachinecodeIndex, instr);
    } else {
      throw new InternalError("Long offset doesn't fit in short branch\n");
    }
  }

  final void patchSwitchCase(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" <+ " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = (mIP - sourceMachinecodeIndex) << 2;
    // correction is number of bytes of source off switch base
    int         correction = (int)mc.getInstruction(sourceMachinecodeIndex);
    INSTRUCTION offset = (INSTRUCTION)(delta+correction);
    mc.putInstruction(sourceMachinecodeIndex, offset);
  }


  /* machine instructions */

  static final int Atemplate = 31<<26 | 10<<1;

  final void emitA (int RT, int RA, int RB) {
    INSTRUCTION mi = Atemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "a", RT,  RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int AEtemplate = 31<<26 | 138<<1;

  final void emitAE (int RT, int RA, int RB) {
    INSTRUCTION mi = AEtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "ae", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int AIrtemplate = 13<<26;

  final void emitAIr (int RT, int RA, int SI) {
    if (VM.VerifyAssertions) VM.assert(fits(SI, 16));
    INSTRUCTION mi = AIrtemplate | RT<<21 | RA<<16 | (SI & 0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "ai.", RT, RA, signedHex(SI));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ANDtemplate = 31<<26 | 28<<1;

  final void emitAND (int RA, int RS, int RB) {
    INSTRUCTION mi = ANDtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "and", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ANDItemplate = 28<<26;

  final void emitANDI (int RA, int RS, int U) {
    if (VM.VerifyAssertions) VM.assert((U>>>16) == 0);
    INSTRUCTION mi = ANDItemplate | RS<<21 | RA<<16 | U;
    if (VM.TraceAssembler)
      asm(mIP, mi, "andi.", RA, RS, hex(U));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int Btemplate = 18<<26;

  private final void _emitB (int relative_address) {
    if (VM.VerifyAssertions) VM.assert(fits(relative_address,24));
    INSTRUCTION mi = Btemplate | (relative_address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "b", signedHex(relative_address<<2));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitB (int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitB(relative_address);
  }

  final void emitB (int relative_address) {
    relative_address -= mIP;
    if (VM.VerifyAssertions) VM.assert(relative_address < 0);
    _emitB(relative_address);
  }

  final VM_ForwardReference emitForwardB() {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitB(0);
    return fr;
  }

  static final int BLAtemplate = 18<<26 | 3;

  final void emitBLA (int address) {
    if (VM.VerifyAssertions) VM.assert(fits(address,24));
    INSTRUCTION mi = BLAtemplate | (address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bla", hex(address));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BLtemplate = 18<<26 | 1;

  private final void _emitBL (int relative_address) {
    if (VM.VerifyAssertions) VM.assert(fits(relative_address,24));
    INSTRUCTION mi = BLtemplate | (relative_address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bl", signedHex(relative_address<<2));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitBL (int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBL(relative_address);
  }


  final VM_ForwardReference emitForwardBL() {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitBL(0);
    return fr;
  }

  static final int BCtemplate = 16<<26;

  public static final int flipCode(int cc) {
    switch(cc) {
    case LT: return GE;
    case GT: return LE;
    case EQ: return NE;
    case LE: return GT;
    case GE: return LT;
    case NE: return EQ;
    }
    if (VM.VerifyAssertions) VM.assert(false);
    return -1;
  }

  private final void _emitBC (int cc, int relative_address) {
    if (fits(relative_address, 14)) {
      INSTRUCTION mi = BCtemplate | cc | (relative_address&0x3FFF)<<2;
      if (VM.TraceAssembler) {
	switch(cc) {
	case LT: asm(mIP, mi, "blt", signedHex(relative_address<<2)); break;
	case GT: asm(mIP, mi, "bgt", signedHex(relative_address<<2)); break;
	case EQ: asm(mIP, mi, "beq", signedHex(relative_address<<2)); break;
	case LE: asm(mIP, mi, "ble", signedHex(relative_address<<2)); break;
	case GE: asm(mIP, mi, "bge", signedHex(relative_address<<2)); break;
	case NE: asm(mIP, mi, "bne", signedHex(relative_address<<2)); break;
	}
      }
      mIP++;
      mc.addInstruction(mi);
    } else {
      _emitBC(flipCode(cc), 2);
      _emitB(relative_address-1);
    }
  }

  final void emitBC (int cc, int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardConditionalBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBC(cc, relative_address);
  }

  final void emitShortBC (int cc, int relative_address, int label) {
    if (relative_address == 0) {
      reserveShortForwardConditionalBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBC(cc, relative_address);
  }

  final void emitBC (int cc, int relative_address) {
    relative_address -= mIP;
    if (VM.VerifyAssertions) VM.assert(relative_address < 0);
    _emitBC(cc, relative_address);
  }

  final VM_ForwardReference emitForwardBC(int cc) {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitBC(cc, 0);
    return fr;
  }

  // delta i: difference between address of case i and of delta 0
  final void emitSwitchCase(int i, int relative_address, int bTarget) {
    int data = i << 2;
    if (relative_address == 0) {
      reserveForwardCase(bTarget);
    } else {
      data += ((relative_address - mIP) << 2);
    }
    if (VM.TraceAssembler) asm(mIP, data, "DATA", "" + data);
    mIP++;
    mc.addInstruction(data);
  }

  static final int BLRtemplate = 19<<26 | 0x14<<21 | 16<<1;

  final void emitBLR () {
    INSTRUCTION mi = BLRtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "blr");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BLRLtemplate = 19<<26 | 0x14<<21 | 16<<1 | 1;

  final void emitBLRL () {
    INSTRUCTION mi = BLRLtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "blrl");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BCTRtemplate = 19<<26 | 0x14<<21 | 528<<1;

  final void emitBCTR () {
    INSTRUCTION mi = BCTRtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bctr");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BCTRLtemplate = 19<<26 | 0x14<<21 | 528<<1 | 1;

  final void emitBCTRL () {
    INSTRUCTION mi = BCTRLtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bctrl");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CALtemplate = 14<<26;

  final void emitCAL (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = CALtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cal", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CAUtemplate = 15<<26;

  final void emitCAU (int RT, int RA, int UI) {
    if (VM.VerifyAssertions) VM.assert(UI == (UI&0xFFFF));
    INSTRUCTION mi = CAUtemplate | RT<<21 | RA<<16 | UI;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cau", RT, RA, hex(UI));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCAU (int RT, int UI) {
    if (VM.VerifyAssertions) VM.assert(UI == (UI&0xFFFF));
    INSTRUCTION mi = CAUtemplate | RT<<21 | UI;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cau", RT, 0, hex(UI));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPtemplate = 31<<26;
  final void emitCMP (int BF, int RA, int RB) {
    INSTRUCTION mi = CMPtemplate | BF<<23 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmp", BF, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMP (int RA, int RB) {
    INSTRUCTION mi = CMPtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmp", 0, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPItemplate = 11<<26;

  final void emitCMPI (int BF, int RA, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = CMPItemplate | BF<<23 | RA<<16 | (V&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpi", BF, RA, signedHex(V));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMPI (int RA, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = CMPItemplate | RA<<16 | (V&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpi", 0, RA, signedHex(V));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPLtemplate = 31<<26 | 32<<1;

  final void emitCMPL (int BF, int RA, int RB) {
    INSTRUCTION mi = CMPLtemplate | BF<<23 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpl", BF, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMPL (int RA, int RB) {
    INSTRUCTION mi = CMPLtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpl", 0, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRANDtemplate = 19<<26 | 257<<1;

  final void emitCRAND (int BT, int BA, int BB) {
    INSTRUCTION mi = CRANDtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crand", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRANDCtemplate = 19<<26 | 129<<1;

  final void emitCRANDC (int BT, int BA, int BB) {
    INSTRUCTION mi = CRANDCtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crandc", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRORtemplate = 19<<26 | 449<<1;

  final void emitCROR (int BT, int BA, int BB) {
    INSTRUCTION mi = CRORtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cror", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRORCtemplate = 19<<26 | 417<<1;

  final void emitCRORC (int BT, int BA, int BB) {
    INSTRUCTION mi = CRORCtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crorc", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FAtemplate = 63<<26 | 21<<1;

  final void emitFA (int FRT, int FRA,int FRB) {
    INSTRUCTION mi = FAtemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fa", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FAstemplate = 59<<26 | 21<<1; // single-percision add

  final void emitFAs (int FRT, int FRA,int FRB) {
    INSTRUCTION mi = FAstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "faS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FABStemplate = 63<<26 | 264<<1;

  final void emitFABS (int FRT, int FRB) {
    INSTRUCTION mi = FABStemplate | FRT<<21 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fabs", FRT, FRB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FCMPUtemplate = 63<<26;

  final void emitFCMPU (int FRA,int FRB) {
    INSTRUCTION mi = FCMPUtemplate | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fcmpu", FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FDtemplate = 63<<26 | 18<<1;

  final void emitFD (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FDtemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fd", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FDstemplate = 59<<26 | 18<<1; // single-precision divide

  final void emitFDs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FDstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fdS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMtemplate = 63<<26 | 25<<1;

  final void emitFM (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FMtemplate | FRT<<21 | FRA<<16 | FRB<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fm", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMstemplate = 59<<26 | 25<<1; // single-precision fm

  final void emitFMs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FMstemplate | FRT<<21 | FRA<<16 | FRB<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fmS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMAtemplate = 63<<26 | 29<<1;

  final void emitFMA (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FMAtemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fma", FRT, FRA, FRC, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FNMStemplate = 63<<26 | 30<<1;

  final void emitFNMS (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FNMStemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fnms", FRT, FRA, FRC, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FNEGtemplate = 63<<26 | 40<<1;

  final void emitFNEG (int FRT, int FRB) {
    INSTRUCTION mi = FNEGtemplate | FRT<<21 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fneg", FRT, FRB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FStemplate = 63<<26 | 20<<1;

  final void emitFS (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FStemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fs", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FSstemplate = 59<<26 | 20<<1;

  final void emitFSs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FSstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "FSs", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FSELtemplate = 63<<26 | 23<<1;

  final void emitFSEL (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FSELtemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fsel", FRT, FRA, FRB, FRC );
    mIP++;
    mc.addInstruction(mi);
  }

  // LOAD/ STORE MULTIPLE

  // TODO!! verify that D is sign extended 
  // (the Assembler Language Reference seems ambiguous) 
  //
  final void emitLM(int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = (46<<26)  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lm", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  // TODO!! verify that D is sign extended 
  // (the Assembler Language Reference seems ambiguous) 
  //
  final void emitSTM(int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = (47<<26)  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lm", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }


  static final int Ltemplate = 32<<26;

  final void emitL (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = Ltemplate  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "l", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LBZtemplate = 34<<26;

  final void emitLBZ (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LBZtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lbz", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LBZXtemplate = 31<<26 | 87<<1;

  final void emitLBZX (int RT, int RA, int RB) {
    INSTRUCTION mi = LBZXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lbzx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHAtemplate = 42<<26;

  final void emitLHA (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LHAtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lha", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHZtemplate = 40<<26;

  final void emitLHZ (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LHZtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhz", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDtemplate = 50<<26;

  final void emitLFD (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFDtemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfd", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDUtemplate = 51<<26;

  final void emitLFDU (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFDUtemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfdu", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDXtemplate = 31<<26 | 599<<1;

  final void emitLFDX (int FRT, int RA, int RB) {
    INSTRUCTION mi = LFDXtemplate | FRT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfdx", FRT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFStemplate = 48<<26;

  final void emitLFS (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFStemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfs", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHAXtemplate = 31<<26 | 343<<1;

  final void emitLHAX (int RT, int RA, int RB) {
    INSTRUCTION mi = LHAXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhax", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHZXtemplate = 31<<26 | 279<<1;

  final void emitLHZX (int RT, int RA, int RB) {
    INSTRUCTION mi = LHZXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhzx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitLIL (int RT, int D) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = CALtemplate | RT<<21 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lil", RT, signedHex(D));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LUtemplate = 33<<26;

  final void emitLU (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LUtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lu", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LXtemplate = 31<<26 | 23<<1;

  final void emitLX (int RT, int RA, int RB) {
    INSTRUCTION mi = LXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LUXtemplate = 31<<26 | 55<<1;

  final void emitLUX (int RT, int RA, int RB) {
    INSTRUCTION mi = LUXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lux", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  
  static final int LWARXtemplate = 31<<26 | 20<<1;

  final void emitLWARX (int RT, int RA, int RB) {
    INSTRUCTION mi = LWARXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lwarx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFLRtemplate = 31<<26 | 0x08<<16 | 339<<1;

  final void emitMFLR (int RT) {
    INSTRUCTION mi = MFLRtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mflr", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFCRtemplate = 31<<26 | 19<<1;

  final void emitMFCR (int RT) {
    INSTRUCTION mi = MFCRtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mfcr", RT);
    mIP++;
    mc.addInstruction(mi);
  }
  
  static final int MFSPRtemplate = 31<<26 | 339<<1;

  final void emitMFSPR (int RT, int SPR) {
    INSTRUCTION mi = MFSPRtemplate | RT<<21 | SPR<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mfspr", RT, SPR);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MTLRtemplate = 31<<26 | 0x08<<16 | 467<<1;

  final void emitMTLR (int RS) {
    INSTRUCTION mi = MTLRtemplate | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtlr", RS);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MTCRFtemplate = 31<<26 | 144<<1;

  final void emitMTCRF (int mask, int RS) {
    INSTRUCTION mi = MTCRFtemplate | mask<<12 | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtcrf", mask, RS);
    mIP++;
    mc.addInstruction(mi);
  }


  static final int MTCTRtemplate = 31<<26 | 0x09<<16 | 467<<1;

  final void emitMTCTR (int RS) {
    INSTRUCTION mi = MTCTRtemplate | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtctr", RS);
    mIP++;
    mc.addInstruction(mi);
  }
 
  static final int MULHWUtemplate = 31<<26 | 11<<1;

  final void emitMULHWU (int RT, int RA, int RB) {
    INSTRUCTION mi = MULHWUtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mulhwu", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DIVtemplate = 31<<26 | 491<<1;

  final void emitDIV (int RT, int RA, int RB) {
    INSTRUCTION mi = DIVtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "div", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MULStemplate = 31<<26 | 235<<1;

  final void emitMULS (int RT, int RA, int RB) {
    INSTRUCTION mi = MULStemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "muls", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int NEGtemplate = 31<<26 | 104<<1;

  final void emitNEG (int RT, int RA) {
    INSTRUCTION mi = NEGtemplate | RT<<21 | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "neg", RT, RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ORtemplate = 31<<26 | 444<<1;

  final void emitOR (int RA, int RS, int RB) {
    INSTRUCTION mi = ORtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "or", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int RLWINM_template = 21<<26;

  final void emitRLWINM (int RA, int RS, int SH, int MB, int ME) {
    INSTRUCTION mi = RLWINM_template | RS<<21 | RA<<16 | SH<<11 | MB<<6 | ME<<1;
    /*
    if (VM.TraceAssembler)
      asm(mIP, mi, "rlwinm", RA, RS, SH, MB, ME);
    */
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFrtemplate = 31<<26 | 8<<1 | 1;

  final void emitSFr (int RT, int RA, int RB) {
    INSTRUCTION mi = SFrtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sf.", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFtemplate = 31<<26 | 8<<1;

  final void emitSF (int RT, int RA, int RB) {
    INSTRUCTION mi = SFtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sf", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFItemplate = 8<<26;

  final void emitSFI (int RA, int RS, int S) {
    if (VM.VerifyAssertions) VM.assert(fits(S,16));
    INSTRUCTION mi = SFItemplate | RS<<21 | RA<<16 | S;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfi", RA, RS, signedHex(S));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFErtemplate = 31<<26 | 136<<1 | 1;

  final void emitSFEr (int RT, int RA, int RB) {
    INSTRUCTION mi = SFErtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfe.", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFEtemplate = 31<<26 | 136<<1;

  final void emitSFE (int RT, int RA, int RB) {
    INSTRUCTION mi = SFEtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfe", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFZEtemplate = 31<<26 | 200<<1;

  final void emitSFZE (int RT, int RA) {
    INSTRUCTION mi = SFZEtemplate | RT<<21 | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfze", RT, RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SLtemplate = 31<<26 | 24<<1;

  final void emitSL (int RA, int RS, int RB) {
    INSTRUCTION mi = SLtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sl", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SLItemplate = 21<<26;

  final void emitSLI (int RA, int RS, int N) {
    INSTRUCTION mi = SLItemplate | RS<<21 | RA<<16 | N<<11 | (31-N)<<1;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sli", RA, RS, signedHex(N));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRtemplate = 31<<26 | 536<<1;

  final void emitSR (int RA, int RS, int RB) {
    INSTRUCTION mi = SRtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sr", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAtemplate = 31<<26 | 792<<1;

  final void emitSRA (int RA, int RS, int RB) {
    INSTRUCTION mi = SRAtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sra", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAItemplate = 31<<26 | 824<<1;

  final void emitSRAI (int RA, int RS, int SH) {
    INSTRUCTION mi = SRAItemplate | RS<<21 | RA<<16 | SH<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "srai", RA, RS, signedHex(SH));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAIrtemplate = 31<<26 | 824<<1 | 1;

  final void emitSRAIr (int RA, int RS, int SH) {
    INSTRUCTION mi = SRAIrtemplate | RS<<21 | RA<<16 | SH<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "srai.", RA, RS, signedHex(SH));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STtemplate = 36<<26;

  final void emitST (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "st", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STBtemplate = 38<<26;

  final void emitSTB (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STBtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stb", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STBXtemplate = 31<<26 | 215<<1;

  final void emitSTBX (int RS, int RA, int RB) {
    INSTRUCTION mi = STBXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stbx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STHXtemplate = 31<<26 | 407<<1;

  final void emitSTHX (int RS, int RA, int RB) {
    INSTRUCTION mi = STHXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sthx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STXtemplate = 31<<26 | 151<<1;

  final void emitSTX (int RS, int RA, int RB) {
    INSTRUCTION mi = STXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDtemplate = 54<<26;

  final void emitSTFD (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFDtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfd", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDUtemplate = 55<<26;

  final void emitSTFDU (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFDUtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfdu", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDXtemplate = 31<<26 | 727<<1;

  final void emitSTFDX (int FRS, int RA, int RB) {
    INSTRUCTION mi = STFDXtemplate | FRS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfdx", FRS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFStemplate = 52<<26;

  final void emitSTFS (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFStemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfs", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFSUtemplate = 53<<26;

  final void emitSTFSU (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFSUtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfsu", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STUtemplate = 37<<26;

  final void emitSTU (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STUtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stu", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STUXtemplate = 31<<26 | 183<<1;

  final void emitSTUX (int RS, int RA, int RB) {
    INSTRUCTION mi = STUXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stux", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STWCXrtemplate = 31<<26 | 150<<1 | 1;

  final void emitSTWCXr (int RS, int RA, int RB) {
    INSTRUCTION mi = STWCXrtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stwcx.", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int Ttemplate = 31<<26 | 4<<1;

  static final int TItemplate = 3<<26;

  final void emitTI (int TO, int RA, int SI) {
    INSTRUCTION mi = TItemplate | TO<<21 | RA<<16 | SI&0xFFFF;
    if (VM.TraceAssembler)
      asm(mIP, mi, "ti", TO, RA, signedHex(SI));
    mIP++;
    mc.addInstruction(mi);
  }
  
  static final int TLEtemplate = 31<<26 | 0x14<<21 | 4<<1;

  final void emitTLE (int RA, int RB) {
    INSTRUCTION mi = TLEtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tle", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TLTtemplate = 31<<26 | 0x10<<21 | 4<<1;

  final void emitTLT (int RA, int RB) {
    INSTRUCTION mi = TLTtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tlt", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TLLEtemplate = 31<<26 | 0x6<<21 | 4<<1;

  final void emitTLLE (int RA, int RB) {
    INSTRUCTION mi = TLLEtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tlle", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TEQItemplate = 3<<26 | 0x4<<21;

  final void emitTEQ0 (int RA) {
    INSTRUCTION mi = TEQItemplate | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "teqi", RA, "0");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TWItemplate = 3<<26 | 0x3EC<<16;	// RA == 12

  final void emitTWI (int imm) {
    INSTRUCTION mi = TWItemplate | imm;
    if (VM.TraceAssembler)
      asm(mIP, mi, "twi", imm);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int XORtemplate = 31<<26 | 316<<1;

  final void emitXOR (int RA, int RS, int RB) {
    INSTRUCTION mi = XORtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "xor", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int XORItemplate = 26<<26;

  final void emitXORI (int RA, int RS, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = XORItemplate |  RS<<21 | RA<<16  | V&0xFFFF;
    if (VM.TraceAssembler)
      asm(mIP, mi, "xori", RA, RS, V);
    mIP++;
    mc.addInstruction(mi);
  }

  /* macro instructions */

  static final int NOPtemplate = 19<<26 | 449<<1;

  final void emitNOP () {
    INSTRUCTION mi = NOPtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "nop");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SENTINALtemplate = 19<<26 | 0x1F<<21 | 0x1F<<16 | 0x1F<<11 | 449<<1;

  final void emitSENTINAL () {
    INSTRUCTION mi = SENTINALtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "end", "prologue");
    mIP++;
    mc.addInstruction(mi);
  }

  // branch conditional -- don't thread switch
  static final int BNTStemplate = BCtemplate | GE | THREAD_SWITCH_BIT<<16;
  final VM_ForwardReference emitBNTS () {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    INSTRUCTION mi = BNTStemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bge", THREAD_SWITCH_BIT, signedHex(0));
    mIP++;
    mc.addInstruction(mi);
    return fr;
  }

  final void emitLoffset(int RT, int RA, int offset) {
    if (fits(offset, 16)) {
      emitL  (RT, offset, RA);
    } else if ((offset & 0x8000) == 0) {
      emitCAU(RT, RA, offset>>16);
      emitL  (RT, offset&0xFFFF, RT);
    } else {
      emitCAU(RT, RA, (offset>>16)+1);
      emitL  (RT, offset|0xFFFF0000, RT);
    }
  }
    

  final void emitLtoc (int RT, int offset) {
    emitLoffset(RT, JTOC, offset);
  }

  final void emitSTtoc (int RT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitST(RT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU(Rz, JTOC, offset>>16);
      emitST (RT, offset&0xFFFF, Rz);
    } else {
      emitCAU(Rz, JTOC, (offset>>16)+1);
      emitST (RT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitCALtoc (int RT, int offset) {
    if (fits(offset, 16)) {
      emitCAL(RT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU(RT, JTOC, offset>>16);
      emitCAL(RT, offset&0xFFFF, RT);
    } else {
      emitCAU(RT, JTOC, (offset>>16)+1);
      emitCAL(RT, offset|0xFFFF0000, RT);
    }
  }

  final void emitLFDtoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitLFD(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU( Rz, JTOC, offset>>16);
      emitLFD(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU( Rz, JTOC, (offset>>16)+1);
      emitLFD(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitSTFDtoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitSTFD(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU ( Rz, JTOC, offset>>16);
      emitSTFD(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU ( Rz, JTOC, (offset>>16)+1);
      emitSTFD(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitLFStoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitLFS(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU( Rz, JTOC, offset>>16);
      emitLFS(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU( Rz, JTOC, (offset>>16)+1);
      emitLFS(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitSTFStoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitSTFS(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU ( Rz, JTOC, offset>>16);
      emitSTFS(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU ( Rz, JTOC, (offset>>16)+1);
      emitSTFS(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitLVAL (int RT, int val) {
    if (fits(val, 16)) { 
      emitLIL(RT, val);
    } else if ((val&0x8000) == 0) {
      emitLIL(RT, val&0xFFFF);
      emitCAU(RT, RT,  val>>>16);
    } else {// top half of RT is 0xFFFF
      emitLIL(RT, val|0xFFFF0000);
      emitCAU(RT, RT, (val>>>16)+1);
    }
  }

  // Convert generated machine code into final form.
  //
  VM_MachineCode finalizeMachineCode (int[] bytecodeMap) {
    mc.setBytecodeMap(bytecodeMap);
    return makeMachineCode();
  }

  VM_MachineCode makeMachineCode () {
    mc.finish();
    if (shouldPrint) {
      INSTRUCTION[] instructions = mc.getInstructions();
      boolean saved = VM_BaselineCompiler.options.PRINT_MACHINECODE;
      try {
	VM_BaselineCompiler.options.PRINT_MACHINECODE = false;
	for (int i = 0; i < instructions.length; i++) {
	  VM.sysWrite(VM_Services.getHexString(i << LG_INSTRUCTION_WIDTH, true));
	  VM.sysWrite(" : ");
	  VM.sysWrite(VM_Services.getHexString(instructions[i], false));
	  VM.sysWrite("  ");
	  VM.sysWrite(PPC_Disassembler.disasm(instructions[i], i << LG_INSTRUCTION_WIDTH));
	  VM.sysWrite("\n");
	}
      } finally {
	VM_BaselineCompiler.options.PRINT_MACHINECODE = saved;
      }
    }
    return mc;
  }

  /**
   * Append an array of INSTRUCTION to the current machine code
   */
  void appendInstructions (INSTRUCTION[] instructionSegment) {
    for (int i=0; i<instructionSegment.length; i++) {
      mIP++;
      mc.addInstruction(instructionSegment[i]);
    }
  }

  // new PowerPC instuctions

  static final int SYNCtemplate = 31<<26 | 598<<1;
  
  final void emitSYNC () {
    INSTRUCTION mi = SYNCtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sync");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ICBItemplate = 31<<26 | 982<<1;
  
  final void emitICBI (int RA, int RB) {
    INSTRUCTION mi = ICBItemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "icbi", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ISYNCtemplate = 19<<26 | 150<<1;
  
  final void emitISYNC () {
    INSTRUCTION mi = ISYNCtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "isync");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DCBFtemplate = 31<<26 | 86<<1;
  
  final void emitDCBF (int RA, int RB) {
    INSTRUCTION mi = DCBFtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "dcbf", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DCBSTtemplate = 31<<26 | 54<<1;
  
  final void emitDCBST (int RA, int RB) {
    INSTRUCTION mi = DCBSTtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "dcbst", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFTBtemplate = 31<<26 | 392<<11 | 371<<1;
  
  final void emitMFTB (int RT) {
    INSTRUCTION mi = MFTBtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mftb", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFTBUtemplate = 31<<26 | 424<<11 | 371<<1;
  
  final void emitMFTBU (int RT) {
    INSTRUCTION mi = MFTBUtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mftbu", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FCTIZtemplate = 63<<26 | 15<<1;
  
  final void emitFCTIZ (int RA, int RB) {
    INSTRUCTION mi = FCTIZtemplate | RA<<21 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fctiz", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  // -----------------------------------------------------------//
  // The following section contains assembler "macros" used by: //
  //    VM_Compiler                                             //
  //    VM_MagicCompiler                                        //
  //    VM_Barriers                                             //
  // -----------------------------------------------------------//
  
  // Emit baseline stack overflow instruction sequence.
  // Before:   FP is current (calling) frame
  //           PR is the current VM_Processor, which contains a pointer to the active thread.
  // After:    R0, S0 destroyed
  //
  void emitStackOverflowCheck (int frameSize) {
    emitL   ( 0,  VM_Entrypoints.activeThreadStackLimitField.getOffset(), PROCESSOR_REGISTER);   // R0 := &stack guard page
    emitCAL (S0, -frameSize, FP);                        // S0 := &new frame
    emitTLT (S0,  0);                                    // trap if new frame below guard page
  }

  // Emit baseline stack overflow instruction sequence for native method prolog.
  // For the lowest Java to C transition frame in the stack, check that there is space of
  // STACK_SIZE_NATIVE words available on the stack;  enlarge stack if necessary.
  // For subsequent Java to C transition frames, check for the requested size and don't resize
  // the stack if overflow
  // Before:   FP is current (calling) frame
  //           PR is the current VM_Processor, which contains a pointer to the active thread.
  // After:    R0, S0 destroyed
  //
  void emitNativeStackOverflowCheck (int frameSize) {
    emitL    (S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // S0 := thread pointer
    emitL    (S0, VM_Entrypoints.jniEnvField.getOffset(), S0);      // S0 := thread.jniEnv
    emitL    ( 0, VM_Entrypoints.JNIRefsTopField.getOffset(),S0);   // R0 := thread.jniEnv.JNIRefsTop
    emitL    (S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // S0 := thread pointer
    emitCMPI ( 0, 0);                                 	 // check if S0 == 0 -> first native frame on stack
    VM_ForwardReference fr1 = emitForwardBC(EQ);
    // check for enough space for requested frame size
    emitL   ( 0,  VM_Entrypoints.stackLimitField.getOffset(), S0);  // R0 := &stack guard page
    emitCAL (S0, -frameSize, FP);                        // S0 := &new frame pointer
    emitTLT (S0,  0);                                    // trap if new frame below guard page
    VM_ForwardReference fr2 = emitForwardB();

    // check for enough space for STACK_SIZE_JNINATIVE 
    fr1.resolve(this);
    emitL   ( 0,  VM_Entrypoints.stackLimitField.getOffset(), S0);  // R0 := &stack guard page
    emitLIL(S0, 1);
    emitSLI(S0, S0, STACK_LOG_JNINATIVE);
    emitSF (S0, S0, FP);             // S0 := &new frame pointer

    emitCMP(0, S0);
    VM_ForwardReference fr3 = emitForwardBC(LE);
    emitTWI ( 1 );                                    // trap if new frame pointer below guard page
    fr2.resolve(this);
    fr3.resolve(this);
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  static final int CALL_INSTRUCTIONS = 3; // number of instructions generated by emitCall()
  void emitCall (int spSaveAreaOffset) {
    emitST(SP, spSaveAreaOffset, FP); // save SP
    emitBLRL  ();
    emitL (SP, spSaveAreaOffset, FP); // restore SP
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           "hidden" parameter (e.g. for fast invokeinterface collision resolution
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitCallWithHiddenParameter (int spSaveAreaOffset, int hiddenParameter) {
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(SP, hiddenParameter);      // pass "hidden" parameter in SP scratch  register
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }

  //-#if RVM_WITH_SPECIALIZATION

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           call site number for specialization
  //
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitSpecializationCall (int spSaveAreaOffset, VM_Method m, int bIP) {
    int callSiteNumber = 0;
    if (VM_SpecializationSentry.isValid()) {
      callSiteNumber = VM_SpecializationCallSites.getCallSiteNumber(null, m, bIP);
    }
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(0, callSiteNumber<<2);      // pass call site in reg. 0
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           call site number for specialization
  //
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitSpecializationCallWithHiddenParameter(int spSaveAreaOffset, 
						 int hiddenParameter,
						 VM_Method m,
						 int bIP) {
    int callSiteNumber = 0;
    if (VM_SpecializationSentry.isValid()) {
      callSiteNumber = VM_SpecializationCallSites.getCallSiteNumber(null, m, bIP);
    }
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(SP, hiddenParameter);    // pass "hidden" parameter in reg. SP 
    emitLVAL(0, callSiteNumber<<2);      // pass call site in reg. 0
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Event used by the Adaptive Inlining Organizer
 * to notify the controller that a call arc 
 * originating in a hot method has become hot
 * and therefore recompilation of the method should
 * be considered to enable additional profile-directed inlining.
 *
 * @author Dave Grove 
 * @author Matthew Arnold
 */
public final class VM_AINewHotEdgeEvent extends VM_HotMethodEvent 
  implements VM_ControllerInputEvent {

  /**
   * Estimate of the expected benefit if the method is 
   * recompiled AT THE SAME OPT LEVEL with the newly
   * enabled profile-directed inlining.
   * <p>
   * TODO: Think about reasonable ways to encode the expected 
   * boost factor for recompiling at higher opt levels.
   * In the short run, this is academic, since we only plan to
   * create an instance of this event for methods already compiled
   * at max opt level, but it may be required later.
   * <p>
   * NB: Boost factor is a value >= 1.0!
   * (1.0 means no boost, 1.1 means a 10% improvement, etc).
   */
  private double boostFactor;
  public final double getBoostFactor() { return boostFactor; }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   * @param _boostFactor improvement expected by applying FDO
   */
  VM_AINewHotEdgeEvent(VM_CompiledMethod _cm, double _numSamples, double _boostFactor) {
    super(_cm, _numSamples);
    if (VM.VerifyAssertions) VM.assert(_boostFactor >= 1.0);
    boostFactor = _boostFactor;
  }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   * @param _boostFactor improvement expected by applying FDO
   */
  VM_AINewHotEdgeEvent(VM_CompiledMethod _cm, int _numSamples, double _boostFactor) {
    this(_cm, (double)_numSamples, _boostFactor);
  }


  public final String toString() {
    return "NewHotEdgeEvent: "+super.toString()+
      ", boost factor = "+getBoostFactor();
  }


  /**
   * Called when the controller is ready to process this event.
   * Simply passes itself to the recompilation strategy.
   */
  public void process() {
    VM_CompiledMethod cmpMethod = getCompiledMethod();
    VM_Controller.recompilationStrategy.considerHotCallEdge(cmpMethod,this);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;
import java.io.*;

/**
 *  VM_AdaptiveInlining.java
 *
 *  Collection of static methods to assist with adaptive inlining.
 *
 *  @author Stephen Fink
 *  @modified Michael Hind
 *  @modified Peter F. Sweeney
 *  @modified Matthew Arnold
 *  @modified Dave Grove
 */
class VM_AdaptiveInlining {
  final private static boolean DEBUG = false;

  /** Interface */

  /**
   * Set parameters.
   * Must be called after parsing command-line.
   */
  static void boot(VM_AOSOptions options) {
    setHotEdgeThreshold(options.INITIAL_AI_THRESHOLD);

    // create and register the dcg as a decayable object
    dcg = new VM_PartialCallGraph(); 
    VM_RuntimeMeasurements.registerDecayableObject(dcg);

    plan = new OPT_ContextFreeInlinePlan();
    if (options.USE_OFFLINE_INLINE_PLAN) {
      try {
	//  Read the plan from disk
	String fn = options.OFFLINE_INLINE_PLAN_NAME;
	LineNumberReader in = new LineNumberReader(new FileReader(fn));
	VM_AdaptiveInlining.plan.readObject(in, VM_SystemClassLoader.getVMClassLoader());
      } catch (Exception e) {
	e.printStackTrace();
	throw new OPT_OptimizingCompilerException("Inline",
					        "Error creating offline plan");
      }
      // now make an inlining oracle
      VM_RuntimeCompiler.offlineInlineOracle = 
	new OPT_AdaptiveInlineOracle(plan);
    }
  }

  /** 
   * Get the inlining oracle for a method
   */
  static OPT_AdaptiveInlineOracle getInlineOracle(VM_Method m) {
    OPT_AdaptiveInlineOracle oracle = null;
    synchronized(plan) {
      if (VM_Controller.options.ADAPTIVE_INLINING) {
	oracle = new OPT_AdaptiveInlineOracle(plan);
      }
    }
    return oracle;
  }

  /**
   * Return a pointer to the dynamic call graph
   */
  static VM_PartialCallGraph getPartialCallGraph() { return dcg; }

  /**
   * Recompute the set of "hot" edges used for adaptive inlining.
   */
  private static int recomputeHotEdgesTrips = 0;
  public static Vector recomputeHotEdges() {
    if(DEBUG) {
      VM.sysWrite(" VM_AdaptiveInlining.recomputeHotEdges() hotEdgeThreshold: "
		  + hotEdgeThreshold +" "+(++recomputeHotEdgesTrips)+"\n");
    }
    Vector vectorOfTriples = new Vector();
      
    /** Compute the set of hot edges */
    for (java.util.Iterator i = dcg.getEdges(); i.hasNext(); ) {
      VM_CallSiteTriple triple = (VM_CallSiteTriple)i.next();
      if(DEBUG)VM.sysWrite(" :"+ triple +"\n");
	 
      double weight = triple.getWeight()/nYieldPointsTaken;
      if (weight >= hotEdgeThreshold) { 
	VM_Method caller = triple.getCaller();
	VM_Method callee = triple.getCallee();
	int bcIndex = triple.getBytecodeIndex();
	plan.addRule(caller,bcIndex,callee);
	vectorOfTriples.addElement(triple);
      }
    }

    if(DEBUG){ VM.sysWrite("\nEdges found:\n");
    for (int i=0; i<vectorOfTriples.size(); i++) {
      VM_CallSiteTriple triple = 
	(VM_CallSiteTriple)vectorOfTriples.elementAt(i);
      VM.sysWrite((i+1)+": "+triple.toString()+"\n");
    }  }

    if(DEBUG)VM.sysWrite(" VM_AdaptiveInlining.recomputeHotEdges() exit\n");

    // now adjust the AI Threshold
    double newThreshold = Math.max(hotEdgeThreshold/2.0,
				   VM_Controller.options.FINAL_AI_THRESHOLD);
    setHotEdgeThreshold(newThreshold);

    return vectorOfTriples;
  }


  /**
   * Hook to allow the AdaptiveInliningOracle to record that the opt compiler
   * was aware that a call edge was hot, but still refused to inline it.
   */
  static void recordRefusalToInlineHotEdge(int cmid, 
					   VM_Method caller, 
					   int bcX, 
					   VM_Method callee) {
    VM_CallSiteTriple edge = new VM_CallSiteTriple(caller, bcX, callee);
    Integer key = new Integer(cmid);
    NonInlinedElement oldEdges = (NonInlinedElement)nonInlinedEdges.get(key);
    NonInlinedElement p = oldEdges;
    while (p != null) {
      if (p.cmid == cmid && 
	  p.edge.getCaller() == edge.getCaller() &&
	  p.edge.getCallee() == edge.getCallee() &&
	  p.edge.getBytecodeIndex() == edge.getBytecodeIndex()) {
	return;
      }
      p = p.next;
    }
    if (DEBUG) 
      VM.sysWrite("Recording that "+edge+" was not inlined into "+cmid+"\n");
    nonInlinedEdges.put(key, new NonInlinedElement(cmid, edge, oldEdges));
  }


  /**
   * Allow AI Organizer to check to see if the compiler previously refused to inline a hot edge.
   */
  static boolean knownNonInlinedEdge(int cmid, VM_CallSiteTriple edge) {
    Integer key = new Integer(cmid);
    NonInlinedElement oldEdges = (NonInlinedElement)nonInlinedEdges.get(key);
    NonInlinedElement p = oldEdges;
    while (p != null) {
      if (p.cmid == cmid && 
	  p.edge.getCaller() == edge.getCaller() &&
	  p.edge.getCallee() == edge.getCallee() &&
	  p.edge.getBytecodeIndex() == edge.getBytecodeIndex()) {
	if (DEBUG) VM.sysWrite ("Squashing "+edge+" into "+cmid);
	return true;
      }
      p = p.next;
    }
    return false;
  }

  /**
   * Called when a compiled version becomes obsolete 
   * (to clear the now irrelevant data)
   */
  static void clearNonInlinedEdges(int cmid) {
    Integer key = new Integer(cmid);
    nonInlinedEdges.remove(key);
  }
    
  // Mapping from Integer(CMID) to nonInlinedElement
  private static java.util.HashMap nonInlinedEdges = new java.util.HashMap();
  static class NonInlinedElement {
    int cmid;
    VM_CallSiteTriple edge;
    NonInlinedElement next;
    NonInlinedElement(int c, VM_CallSiteTriple e, NonInlinedElement n) {
      cmid = c;
      edge = e;
      next = n;
    }
  }


  /**
   * Set the threshold for hot edges.
   * If threshold is x, then an edge must accout for at least x% of
   * the total number of dynamic calls to be "hot"
   */
  static void setHotEdgeThreshold(double x) {
    hotEdgeThreshold = x;
  }

  /**
   */
  static void report() {
    VM.sysWrite("Adaptive inlining context-free plan:\n");
    synchronized(plan) {
      VM.sysWrite(plan.toString() + "\n");
    }
  }

  /**
   * record that more yield points have been taken
   * @param n
   */
  static void incrementNumYieldPoints(double n) {
    synchronized(plan) {
      nYieldPointsTaken += n; 
    }
  }
  /**
   * decay the number of yield points taken
   */
  static void decay() {
    synchronized(plan) {
      nYieldPointsTaken /= VM_Controller.options.DECAY_RATE; 
    }
  }
  /**
   */
  static double getNumYieldPoints() { return nYieldPointsTaken; }

  /** Implementation */
  static VM_PartialCallGraph dcg;
  static OPT_ContextFreeInlinePlan plan;
  static double hotEdgeThreshold = 1.0;
  // (decayed) number of yield points taken: maintained by AI organizer
  static double nYieldPointsTaken = 0.0; 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class encapsulates the analytic model used by the controller
 * to guide multi-level recompilation decisions.  An early version of
 * this model is described in the OOPSLA'2000 paper, but we've made
 * some improvements since then...
 *
 * @see VM_MultiLevelAdaptiveModel
 *
 * @author Mike Hind
 * @author Dave Grove
 * @author Peter Sweeney
 * @author Stephen Fink
 * @author Matthew Arnold 
 */
abstract class VM_AnalyticModel extends VM_RecompilationStrategy {

  //---- Interface ------
  // Code that inherits from VM_AnalyticModel must define the
  // following behavior

  /**
   * Initialize the set of "optimization choices" that the
   * cost-benefit model will consider when using will consider when
   * using adaptive compilation.
   */
  abstract void populateRecompilationChoices(); 

  /**
   * Compute the set of optimization choices that should be
   * considered by the cost-benefit model, given the previous compiler.  
   *
   * @param prevCompiler The compiler compiler that was used to 
   *                     comile cmpMethod
   * @param cmpMethod The compiled method being considered
   */
  abstract VM_RecompilationChoice[] 
    getViableRecompilationChoices(int prevCompiler, 
				  VM_CompiledMethod cmpMethod);


  // ----------------------------------------------------- 
  // Below code that is (currently) common to all recompilation
  // strategies that use the analytic model.

  /**
   * Initialize the analytic model:
   *
   *  NOTE: The call to super.init() uses the command line options to
   *  set up the optimization plans, so this must be run after the
   *  command line options are available.  
   */
  void init() {

    // Do the common initialization first
    super.init();

    // setup the recompilation choices that are available to the
    // analytic model
    populateRecompilationChoices();
  }

    
  /**
   * This method is the main decision making loop for all
   * recompilation strategies that use the analytic model.  
   * <p>
   * Given a HotMethodRecompilationEvent, this code will determine 
   * IF the method should be recompiled, and if so, HOW to perform 
   * the recompilation, i.e., what compilation plan should be used.
   * The method returns a controller plan, which contains the compilation
   * plan and other goodies.
   *
   * @param cmpMethod the compiled method of interest
   * @param hme       the VM_HotMethodRecompilationEvent
   * @return the controller plan to be used or NULL, if no 
   *                   compilation is to be performed.  */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {
    // Compiler used for the previous compilation
    int prevCompiler = getPreviousCompiler(cmpMethod); 
    if (prevCompiler == -1) {
      return null; // Not a method that we can recompile (trap, JNI).
    }
    
    VM_ControllerPlan plan = VM_ControllerMemory.findMatchingPlan(cmpMethod);
    double prevCompileTime = 
      getPreviousCompilationTime(hme, plan, prevCompiler);
    if (prevCompileTime < 0.0) {
      // For one of a number of reasons, we've decided that this method is
      // not a candidate for recompilation at this time.
      return null;
    }
    
    // Now we know the compiler that generated the method (prevCompiler).
    // the compile time it took to generate it (prevCompileTime), and that 
    // the method is a potential candidate for additional recompilation. 
    // So, next decide what, if anything, should be done now.  
    // We consider doing nothing (ie leaving the method at the current 
    // opt level, which incurs no  compilation cost), and recompiling the 
    // method at each greater compilation level.
    
    double futureTimeForMethod = futureTimeForMethod(hme);
    
    // initialize bestAction as doing nothing, which means we'll 
    // spend just as much time in the method in the future as we have so far.
    VM_RecompilationChoice bestActionChoice = null;
    double bestActionTime = futureTimeForMethod;
    
    if (VM.LogAOSEvents) { 
      VM_AOSLogging.recordControllerEstimateCostDoNothing
	(cmpMethod.getMethod(),
	 VM_CompilerDNA.getOptLevel(prevCompiler),
	 bestActionTime);
    }
    
    // Get a vector of optimization choices to consider
    VM_RecompilationChoice[] recompilationChoices =
      getViableRecompilationChoices(prevCompiler,cmpMethod);
    
    // Consider all choices in the vector of possibilities
    for (int i=0; i< recompilationChoices.length; i++) {
      VM_RecompilationChoice choice = recompilationChoices[i];

      // Get the cost and benefit of this choice
      double cost = choice.getCost(prevCompiler, prevCompileTime);
      double futureExecutionTime = 
	choice.getFutureExecutionTime(prevCompiler,futureTimeForMethod);
      
      double curActionTime = cost + futureExecutionTime;
      
      if (VM.LogAOSEvents) { 
	VM_AOSLogging.recordControllerEstimateCostOpt
	  (cmpMethod.getMethod(),
	   choice.toString(),
	   curActionTime);
      }
      
      if (curActionTime < bestActionTime) {
	bestActionTime = curActionTime;
	bestActionChoice = choice;
      }
    }
    
    // if the best action is the previous than we don't need to recompile
    if (bestActionChoice == null) {	
      plan = null;
    } else {
      plan = bestActionChoice.makeControllerPlan(cmpMethod, prevCompiler,
						 futureTimeForMethod,
						 bestActionTime);
    }
    return plan;
  }



  /**
   * This function defines how the analytic model handles a
   * VM_AINewHotEdgeEvent.  The basic idea is to use the model to
   * evaluate whether it would be better to do nothing or to recompile
   * at the same opt level, assuming there would be some "boost" after
   * performing inlining.  
   */
  void considerHotCallEdge(VM_CompiledMethod cmpMethod, 
			   VM_AINewHotEdgeEvent event) {

    // Compiler used for the previous compilation
    int prevCompiler = getPreviousCompiler(cmpMethod); 
    if (prevCompiler == -1) {
      return; // Not a method we can recompile (trap, JNI).
    }

    VM_ControllerPlan plan = VM_ControllerMemory.findMatchingPlan(cmpMethod);
    double prevCompileTime = 
      getPreviousCompilationTime(event, plan, prevCompiler);
    if (prevCompileTime < 0.0) {
      // For one of a number of reasons, we've decided that this method is
      // not a candidate for recompilation at this time.
      return;
    }

    // Use the model to caclulate expected cost of (1) doing nothing
    // and (2) recompiling at the same opt level with the FDO boost
    double futureTimeForMethod = futureTimeForMethod(event);
    double futureTimeForFDOMethod = 
      prevCompileTime + (futureTimeForMethod/event.getBoostFactor());
    
    if (VM.LogAOSEvents) { 
      int prevOptLevel = VM_CompilerDNA.getOptLevel(prevCompiler);
      VM_AOSLogging.recordControllerEstimateCostDoNothing(cmpMethod.getMethod(),
							  prevOptLevel,
							  futureTimeForMethod);
      VM_AOSLogging.recordControllerEstimateCostOpt(cmpMethod.getMethod(),
						    "O"+prevOptLevel+"AI",
						    futureTimeForFDOMethod);
    }

    if (futureTimeForFDOMethod < futureTimeForMethod) {
      // Profitable to recompile with FDO, so do it.
      int optLevel = VM_CompilerDNA.getOptLevel(prevCompiler);
      double priority = futureTimeForMethod - futureTimeForFDOMethod;
      plan = createControllerPlan(cmpMethod.getMethod(), 
				  optLevel, null, 
				  cmpMethod.getId(), 
				  event.getBoostFactor(),
				  priority);
      
      plan.execute();
    }
  }


  /**
   * How much time do we expect to spend in the method in the future if
   * we take no recompilation action?
   * The key assumption is that we'll spend just as much time 
   * executing in the the method in the future as we have done so far
   * in the past.
   * 
   * @param hme The VM_HotMethodEvent in question
   * @return estimate of future execution time to be spent in this method
   */
   double futureTimeForMethod(VM_HotMethodEvent hme) {
    VM_AOSOptions opts = VM_Controller.options;
    double numSamples = hme.getNumSamples();
    double timePerSample;
    if (!VM.UseEpilogueYieldPoints) { 
      // NOTE: we take two samples per timer interrupt, so we have to
      // adjust here (otherwise we'd give the method twice as much time
      // as it actually deserves).
      timePerSample = (opts.SAMPLE_FREQ_MILLIS / 2.0) * (1.0 - opts.DARK_MATTER);
    } else {
      // If we use epilogue yield points, we only have 1 sample per interrupt
      //  prologue => calling method
      //  backedge/epilogue => current method
      timePerSample = opts.SAMPLE_FREQ_MILLIS * (1.0 - opts.DARK_MATTER);
    }

    double timeInMethodSoFar = numSamples * timePerSample;
    return timeInMethodSoFar;
  }


}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * This class contains top level adaptive compilation subsystem functions.
 *
 * @author Michael Hind
 * @author Dave Grove
 * @author Stephen Fink
 */
class VM_Controller implements VM_Callbacks.ExitMonitor,
			       VM_Callbacks.AppRunCompleteMonitor {

  /**
   * Signals when the options and (optional) logging mechanism are enabled
   */
  public static boolean enabled = false;

  /**
   * Controller subsystem control options
   */
  public static VM_AOSOptions options = null;
  
  /**
   * Deferred command line arguments for the opt compiler
   */
  private static String[] optCompilerOptions = new String[0];
  /**
   * Add a deferred command line argument
   */
  public static void addOptCompilerOption(String arg) {
    String[] tmp = new String[optCompilerOptions.length+1];
    for (int i=0; i< optCompilerOptions.length; i++) {
      tmp[i] = optCompilerOptions[i];
    }
    tmp[optCompilerOptions.length] = arg;
    optCompilerOptions = tmp;
  }
  /**
   * Get the deferred command line arguments
   */
  public static String[] getOptCompilerOptions() {return optCompilerOptions;}

  /**
   * The controller thread, it makes all the decisions
   * (the thread sets this field when it is created.)
   */
  public static VM_ControllerThread controllerThread = null;

  /**
   * Thread that will perform opt-compilations as directed by the controller
   * (the thread sets this field when it is created.)
   */
  public static VM_CompilationThread compilationThread = null;

  /**
   * Threads that will organize profile data as directed by the controller
   */
  public static Vector organizers = new Vector();


  /**
   * A blocking priority queue where organizers place events to 
   * be processed by the controller
   * (an input to the controller thread)
   */
  public static VM_BlockingPriorityQueue controllerInputQueue;

  /**
   * A blocking priority queue where the controller will place methods 
   * to be opt compiled
   * (an output of the controller thread)
   */
  public static VM_BlockingPriorityQueue compilationQueue;

  /**
   * The strategy used to make recompilation decisions
   */
  public static VM_RecompilationStrategy recompilationStrategy;

  /**
   *  a controller virtual clock, ticked every thread switch.
   */
  public static int controllerClock = 0;

  /**
   * The main hot method raw data object.
   */
  public static VM_MethodCountData methodSamples;

  /**
   * Initialize the controller subsystem (called from VM.boot)
   * This method is called AFTER the command line options are processed.
   */
  private static boolean booted = false;
  public static void boot() {
    // check to see if the AOS options were created during the process
    // of check command line args, if not we'll create a default one
    if (options == null) {
      options = new VM_AOSOptions();
    }
    
    // Signal that the options and (optional) logging mechanism are set
    // VM_RuntimeCompiler checks this flag
    enabled = true;

    // Initialize the controller input queue
    controllerInputQueue = 
      new VM_BlockingPriorityQueue(options.CONTROLLER_INPUT_QUEUE_SIZE,
				   new VM_BlockingPriorityQueue.CallBack() {
				       void aboutToWait() { controllerThread.aboutToWait(); }
				       void doneWaiting() { controllerThread.doneWaiting(); }
				     });

    compilationQueue = 
      new VM_BlockingPriorityQueue(options.COMPILATION_QUEUE_SIZE);

    // Create the analytic model used to make cost/benefit decisions.
    if (options.ADAPTIVE_RECOMPILATION) {
      // Multi-level adaptive, ala OOPSLA 2000
      recompilationStrategy = new VM_MultiLevelAdaptiveModel();
    } 
    else {
      // SLA, ala OOPSLA 2000
      recompilationStrategy = new VM_SingleLevelAdaptive();
    }

    // boot the runtime measurement systems
    VM_RuntimeMeasurements.boot();

    // Initialize subsystems, if being used
    VM_AdaptiveInlining.boot(options);
    
    // boot any instrumentation options
    VM_Instrumentation.boot(options);

    // boot the aos database
    VM_AOSDatabase.boot(options);

    createControllerThread();

    VM_Callbacks.addExitMonitor(new VM_Controller());
    VM_Callbacks.addAppRunCompleteMonitor(new VM_Controller());

    booted=true;
  }

  /**
   * To be called when the VM is about to exit.
   * @param value the exit value
   */
  public void notifyExit(int value) {
    report();
  }

  /**
   * To be called when the application completes one of its run
   */
  public void notifyAppRunComplete(int i) {
    if (VM.LogAOSEvents) VM_AOSLogging.appRunComplete();
  }

  // Create the ControllerThread
  static void createControllerThread() {
    Object sentinel = new Object();
    VM_ControllerThread tt = new VM_ControllerThread(sentinel);
    tt.makeDaemon(true);
    tt.start();
    // wait until controller threads are up and running.
    try {
      synchronized(sentinel) {
	sentinel.wait();
      }
    } catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to start up controller subsystem");
    }
  }


  /**
   * Process any command line arguments passed to the controller subsystem.
   * <p>
   * This method has the responsibility of creating the options object
   * if it does not already exist
   * <p>
   * NOTE: All command line argument processing should be handled via
   * the automatically generated code in VM_AOSOptions.java.  
   * Don't even think of adding handwritten stuff here! --dave
   *
   * @param arg the command line argument to be processed
   */
  public static void processCommandLineArg(String arg) {
    if (options == null) {
      options = new VM_AOSOptions();
    }
    
    if (!options.processAsOption("-X:aos", arg)) {
      VM.sysWrite("vm: illegal adaptive configuration directive \""+arg+"\" specified as -X:aos:"+arg+"\n");
      VM.sysExit(-1);
    }
  }

  /**
   * This method is called when the VM is exiting to provide a hook to allow
   * the adpative optimization subsystem to generate a summary report.
   * It can also be called directly from driver programs to allow
   * reporting on a single run of a benchmark that the driver program
   * is executing in a loop (in which case the adaptive system isn't actually
   * exiting.....so some of the log messages may get a little wierd).
   */
  public static void report() {
    if (!booted) return;
    VM_ControllerThread.report();
    VM_RuntimeMeasurements.report();

    for (Enumeration e = organizers.elements(); e.hasMoreElements(); ) {
      VM_Organizer organizer = (VM_Organizer)e.nextElement();
      organizer.report();
    }
    if (options.DUMP_AI_DECISIONS) {
      VM_AdaptiveInlining.report();
    }

    if (options.REPORT_STATIC_PROGRAM_STATS) {
      VM_OptStaticProgramStats.report();
    }

    if (options.FINAL_REPORT_LEVEL >= 2) {
      VM_EdgeCounts.dumpCounts();
    }

    if (VM.LogAOSEvents) VM_AOSLogging.systemExiting();
  }


  /**
   * Stop all AOS threads and exit the adaptive system.
   * Can be used to assess code quality in a steady state by
   * allowing the adaptive system to run "for a while" and then
   * stoppping it
   */
  public static void stop() {
    if (!booted) return;
    VM.sysWrite("\nAOS: Killing all adaptive system threads\n");
    for (Enumeration e = organizers.elements(); e.hasMoreElements(); ) {
      VM_Organizer organizer = (VM_Organizer)e.nextElement();
      organizer.kill(new ThreadDeath());
    }
    compilationThread.kill(new ThreadDeath());
    controllerThread.kill(new ThreadDeath());
    VM_RuntimeMeasurements.stop();
    report();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Abstract parent class for events from organizers to the controller. 
 *
 * @author Stephen Fink 
 */
interface VM_ControllerInputEvent {

   /** 
    * This method is called by the controller upon dequeuing this
    * event from the controller input queue
    */
   void process();
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Hashtable;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Enumeration;
import java.util.LinkedList;
import java.io.PrintStream;

/**
 *  This class records decisions taken by the controller.  It will remember
 *  controller plans, which contain compilation plans and other goodies,
 *  and allows searching for previous decisions
 *
 *  @author Michael Hind
 *  @author Stephen Fink
 */
final class VM_ControllerMemory implements VM_Constants {

  /**
   *  This is a hashtable of controller plans indexed on the method ID.  
   *  Each method ID can have a list of such plans associated with.
   */
  private static Hashtable table;

  /**
   * Number of times controller is awoken and did nothing.
   */
  private static int didNothing = 0;

  /**
   * Number of times controller is awoken
   */
  private static int awoken     = 0;

  // counters for chosen opt levels
  private static int numMethodsConsidered               = 0;
  private static int numMethodsScheduledForRecomp       = 0;
  private static int numBase                            = 0;
  private static int numOpt0                            = 0;
  private static int numOpt1                            = 0;
  private static int numOpt2                            = 0;
  private static int numOpt3                            = 0;
  private static int numOpt4                            = 0;

  static int getNumAwoken()                    { return awoken; }
  static int getNumDidNothing()                { return didNothing; }
  static int getNumMethodsConsidered()         { return numMethodsConsidered; }
  static int getNumMethodsScheduledForRecomp() 
    { return numMethodsScheduledForRecomp; }
  static int getNumBase()                       { return numBase; }
  static int getNumOpt0()                       { return numOpt0; }
  static int getNumOpt1()                       { return numOpt1; }
  static int getNumOpt2()                       { return numOpt2; }
  static int getNumOpt3()                       { return numOpt3; }
  static int getNumOpt4()                       { return numOpt4; }

  static void incrementNumAwoken()              { awoken++; }
  static void incrementNumDidNothing()          { didNothing++; }
  static void incrementNumMethodsConsidered()   { numMethodsConsidered++; }
  static void incrementNumMethodsScheduledForRecomp()  
    { numMethodsScheduledForRecomp++; }
  static void incrementNumBase()                { numBase++; }
  static void incrementNumOpt0()                { numOpt0++; }
  static void incrementNumOpt1()                { numOpt1++; }
  static void incrementNumOpt2()                { numOpt2++; }
  static void incrementNumOpt3()                { numOpt3++; }
  static void incrementNumOpt4()                { numOpt4++; }

  static void init() {
    table = new Hashtable();
  }
 
  /**
   *  Inserts a controller plan keyed on the underlying method
   *
   *  @param plan the controller plan to insert
   */
  static void insert(VM_ControllerPlan plan) {

    if (VM.LogAOSEvents) {
      VM_Method method = plan.getCompPlan().getMethod();
      numMethodsScheduledForRecomp++;
      int optLevel = plan.getCompPlan().options.getOptLevel();
      switch (optLevel) {
      case 0:  numOpt0++; break;
      case 1:  numOpt1++; break;
      case 2:  numOpt2++; break;
      case 3:  numOpt3++; break;
      case 4:  numOpt4++; break; 
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown Opt Level");
      }
    }

    // first check to see if there is a plan list for this method
    LinkedList planList = findPlan(plan.getCompPlan().method);

    if (planList == null) {
      // create a plan list, with the single element being this plan
      planList = new LinkedList();

      // no synch needed here because the planList is not in the table yet
      planList.addLast(plan);

      // insert in the hashtable using the method ID as the hash value
      table.put(new Integer(plan.getCompPlan().method.getDictionaryId()), 
		planList);
    } else {
      // add the current plan to the end of the list
      synchronized(planList) {
	planList.addLast(plan);
      }
    }

    // tell the plan what list it is on
    plan.setPlanList(planList);

  }

  /**
   * Looks for a controller plan for the passed method
   *
   * @param VM_Method the method to look for
   * @return the list of controller plans for this method if one exists, 
   *         otherwise, null
   */
  static LinkedList findPlan(VM_Method method) {
    return (LinkedList) table.get(new Integer(method.getDictionaryId()));
  }

  /**
   *  Find the plan for the compiled method that is passed
   *  @param cmpMethod the compiled method of interest
   *  @return the matching plan or null if none exists.
   */
  static VM_ControllerPlan findMatchingPlan(VM_CompiledMethod cmpMethod) {
    VM_Method method = cmpMethod.getMethod();

    LinkedList planList = findPlan(method);
    if (planList == null) {
      return null;
    } else{
      // iterate over the planList until we get to this item
      boolean found = false;
      VM_ControllerPlan curPlan = null;
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  curPlan = (VM_ControllerPlan) iter.next();

	  // exit when we find ourselves
	  if (curPlan.getCMID() == cmpMethod.getId()) {
	    found = true;
	    break;
	  }
	} // more to process
      }

      if (!found) {
	// there was a plan for this method, but not for this compiled method
	curPlan = null;
      }

      return curPlan;
    }
  }

  /**
   *  Determine if the passed method should be considered as a candidate
   *  for _initial_ AOS recompilation. 
   *  A method should not be reconsider for initial AOS recompilation if 
   *  a plan already exists for the method whose status is IN_PROGRESS, 
   *  COMPLETED, OUTDATED, or ABORTED because of compilation error.
   * 
   *  @param method the method of interest
   *  @return whether the method should be considered or not
   */
  static boolean shouldConsiderForInitialRecompilation(VM_Method method) {
    LinkedList planList = findPlan(method);
    if (planList == null) {
      return true;
    } else {
      // iterate over the planList until we find a plan whose status is
      // inprogress, completed, 
      synchronized(planList) {
	boolean found = false;
	VM_ControllerPlan curPlan = null;
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  curPlan = (VM_ControllerPlan) iter.next();

	  // exit when we find ourselves
	  byte status = curPlan.getStatus();
	  if (status == VM_ControllerPlan.COMPLETED || 
	      status == VM_ControllerPlan.IN_PROGRESS || 
	      status == VM_ControllerPlan.ABORTED_COMPILATION_ERROR || 
	      status == VM_ControllerPlan.OUTDATED) {
	    return false;
	  }
	}
      }
      return true;  // we didn't find any, so return true
    }
  }


  /**
   * Return true if there is a plan with the given status for the given method
   *
   * @param method the method of interest
   * @param status the status of interest
   * @return whether or not there is plan with that status for the method
   */
  static boolean planWithStatus(VM_Method method, byte status) {
    LinkedList planList = findPlan(method);
    if (planList != null) {
      // iterate over the planList until we find a plan with status 'status'
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();
	  if (curPlan.getStatus() == status) {
	    return true;
	  }
	}
      }
    }
    return false;
  }

  /**
   * Return true if there is a completed plan with the given opt level for 
   * the give method
   *
   * @param method the method of interest
   * @param optLevel the opt level of interest
   * @return whether or not there is completed plan with that level 
   *             for the method
   */
  static boolean completedPlanWithOptLevel(VM_Method method, int optLevel) {
    LinkedList planList = findPlan(method);
    if (planList != null) {
      // iterate over the planList until we find a completed plan with the
      // opt level passed
      synchronized(planList) {
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	  VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();
	  if (curPlan.getStatus() == VM_ControllerPlan.COMPLETED &&
	      curPlan.getCompPlan().options.getOptLevel() == optLevel) {
	    return true;
	  }
	}
      }
    }
    return false;
  }


  /**
   * Looks for the last controller plan for the passed method
   *
   * @param VM_Method the method to look for
   * @return the last controller plan for this method if it exists, 
   *         otherwise, null
   */
  static VM_ControllerPlan findLatestPlan(VM_Method method) {
    LinkedList planList = findPlan(method);
    if (planList == null) {
      return null;
    } else {
      return (VM_ControllerPlan) planList.getLast();
    }
  }

  /**
   * This method summarizes the recompilation actions taken for all methods
   * in this object and produces a report to the passed PrintStream.
   * @param log the stream to print to
   */
  static void printFinalMethodStats(PrintStream log) {
    // We will traverse the hash table and for each method record its status as
    // one of the following
    //    B -> 0 -> 1 -> 2
    //    B -> 0 -> 1 
    //    B -> 0 
    //    B      -> 1 -> 2
    //    B -> 0      -> 2
    //    B           -> 2
    //    B      -> 1
    //
    //  We encode these possibilities by turning on 1 of three bits for 0, 1, 2
    //  Also, for all methods that eventually get to level 2, they can be 
    //  recompiled an arbitrary amount of times.  We record this in in a counter.

    final int MAX_BIT_PATTERN = 7;
    int summaryArray[] = new int[MAX_BIT_PATTERN+1];
    int totalRecompsAtLevel2 = 0;

    // traverse table and give a summary of all actions that have occurred
    if (table != null) {
      for (Enumeration enum = table.keys(); enum.hasMoreElements() ;) {
        Integer intObject = (Integer) enum.nextElement();
	LinkedList planList = (LinkedList) table.get(intObject);

	int bitPattern = 0;
	int recompsAtLevel2 = 0;
	VM_ControllerPlan plan = null;
	
	ListIterator iter = planList.listIterator();
	while (iter.hasNext()) {
	plan = (VM_ControllerPlan) iter.next();

	// only process plans that were completed or completed and outdated 
	// by subsequent plans for this method
	byte status = plan.getStatus();
	if (status == VM_ControllerPlan.COMPLETED || 
	    status == VM_ControllerPlan.OUTDATED) {
	    int optLevel = plan.getCompPlan().options.getOptLevel();

	    // check for recomps at level 2
	    if (optLevel == 2 && bitIsSet(bitPattern, 2)) {
	      recompsAtLevel2++;
	    }

	    bitPattern = setBitPattern(bitPattern, optLevel);
	  } // if
	} // while
      
	if (VM_Controller.options.LOGGING_LEVEL >= 2) {
	    log.println("Method: "+ plan.getCompPlan().getMethod() 
			+", bitPattern: "+ bitPattern
			+", recompsAtLevel2: "+ recompsAtLevel2);
	}

	summaryArray[bitPattern]++;
	totalRecompsAtLevel2 = totalRecompsAtLevel2 + recompsAtLevel2;
      }
    }

    // Print the summary
    int totalUniqueMethods = 0;
    for (int i=1; i<=MAX_BIT_PATTERN; i++) {
      log.print("    Base");
      for (int optLevel=0;  optLevel <=2; optLevel++) {
	if (bitIsSet(i, optLevel)) {
	  log.print(" -> "+ optLevel);
	}
      }
      log.println(": "+ summaryArray[i]);
      totalUniqueMethods = totalUniqueMethods + summaryArray[i];
    }
    log.println("  Num recompilations At level 2: "+ totalRecompsAtLevel2);
    log.println("  Num unique methods recompiled: "+ totalUniqueMethods +"\n");
  }

  /**
   *  set the optLevel bit in the passed bitPattern and return the result
   *  @param bitPattern
   *  @param optLevel
   */
  static int setBitPattern(int bitPattern, int optLevel) {
    int newPattern = 1;
    newPattern = newPattern << optLevel;
    return newPattern | bitPattern;
  }

  /**
   * check if the bit position defined by the 2nd parm is set in the first parm
   * @param bitPattern
   * @param optLevel
   * @return whether the passed bit is set
   */
  static boolean bitIsSet(int bitPattern, int optLevel) {
    int newPattern = 1;
    newPattern = newPattern << optLevel;
    return (newPattern & bitPattern) > 0;
  }

  static String asString() {
    return table.toString();
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.LinkedList;
import java.util.ListIterator;

/**
 * An instance of this class describes a compilation decision made by
 * the controller
 *
 * Constraints:
 *   Given the plan list of a method:
 * 	Only one plan will have status COMPLETED
 * 	Multiple plans may have status OUTDATED
 *	Only one plan will have status IN_PROGRESS
 *
 * status states:
 * UNINITIALIZED -> IN_PROGRESS -> COMPLETED -> OUTDATED
 *             \              \--> ABORTED_COMPILATION_ERROR (never recompile method)
 *             \--> ABORTED_QUEUE_FULL
 *
 * @author Michael Hind
 */
final class VM_ControllerPlan {

  // The plan was created, but the setStatus method was never called
  static final byte UNINITIALIZED = 0;

  // The plan was successfully completed, i.e., the method was recompiled
  static final byte COMPLETED = 1;

  // Compilation began the method, but failed in an error
  static final byte ABORTED_COMPILATION_ERROR = 2;

  // The plan was aborted because the compilation queue was full
  static final byte ABORTED_QUEUE_FULL = 3;

  // The compilation is still in progress
  static final byte IN_PROGRESS = 4;

  // The compilation completed, but a new plan for the same method also 
  // completed, so this is not the most recent completed plan
  static final byte OUTDATED = 5;

  // This is used by clients to initialize local variables for Java semantics
  static final byte UNKNOWN = 99;   
  
  /**
   *  The associate compilation plan 
   */
  private OPT_CompilationPlan compPlan; 

  /**
   *  The time we created this plan
   */
  private int timeCreated;

  /**
   *  The time compilation began
   */
  private int timeInitiated = -1;

  /**
   *  The time compilation end
   */
  private int timeCompleted = -1;

  /**
   *  The CPU time it took for the compilation
   */
  private double compilationCPUTime;

  /**
   *  The speedup we were expecting
   */
  private double expectedSpeedup;

  /**
   *  The priority associated with this plan
   */
  private double priority;

  /**
   *  The compiled method ID for this plan
   */
  private int CMID;

  /**
   *  The compiled method ID for the previous plan for this method
   */
  private int prevCMID;

  /**
   *  The status of this plan
   */
  private byte status;

  /**
   *  The list that we are onstatus of this plan
   */
  private LinkedList planList; 


  /**
   * Construct a controller plan
   *
   * @param compPlan     The compilation plan
   * @param timeCreated  The "time" this plan was created
   * @param prevCMID     The previous compiled method ID
   * @param expectedSpeedup	Expected recompilation benefit
   * @param priority     How important is executing this plan?
   */
  public VM_ControllerPlan(OPT_CompilationPlan compPlan, 
			   int timeCreated, 
			   int prevCMID, 
			   double expectedSpeedup,
			   double priority) {
    this.compPlan = compPlan;
    this.timeCreated = timeCreated;
    this.prevCMID = prevCMID;
    this.status = VM_ControllerPlan.UNINITIALIZED;
    this.expectedSpeedup = expectedSpeedup;
    this.priority = priority;
  }
  

  /**
   * Execute the plan.
   * 
   * @return true on success, false on failure
   */
  boolean execute() {
    // mark plan as in progress and insert it into controller memory
    setStatus(VM_ControllerPlan.IN_PROGRESS);
    VM_ControllerMemory.insert(this);
      
    // Attempt to add plan to compilation queue.
    boolean succeeded = 
      VM_Controller.compilationQueue.insert(getPriority(), this);

    // Logging, and record failure in plan if necessary.
    if (succeeded) {
      if (VM.LogAOSEvents) 
	VM_AOSLogging.recompilationScheduled(getCompPlan(), getPriority()); 
    } else {
      if (VM.LogAOSEvents) 
	VM_AOSLogging.recompilationQueueFull(getCompPlan()); 
      setStatus(VM_ControllerPlan.ABORTED_QUEUE_FULL); 
    }

    return succeeded;
  }



  /**
   * The compilation plan
   */
  public OPT_CompilationPlan getCompPlan() { return compPlan; }

  /**
   * The expected speedup <em>for this method </em> due to this recompilation
   */
  public double getExpectedSpeedup() { return expectedSpeedup; }

  /**
   * The priority (how important is it that this plan be executed)
   */
  public double getPriority() { return priority; }

  /**
   * The time this plan was created
   */
  public int getTimeCreated() { return timeCreated;  }
   
  /**
   * The time (according to the controller clock) compilation of this plan 
   * began.
   */
  public int getTimeInitiated() { return timeInitiated; }
  public void setTimeInitiated(int t) { timeInitiated = t; }
  

  /**
   * The time (according to the controller clock) compilation of this plan 
   * completed.
   */
  public int getTimeCompleted() { return timeCompleted; }
  public void setTimeCompleted(int t) { timeCompleted = t; }


  /**
   * The CPU time in milliseconds actually consumed by the compilation
   * thread to execute this plan. 
   */
  public double getCompilationCPUTime() { return compilationCPUTime; }
  public void setCompilationCPUTime(double t) { compilationCPUTime = t; }


  /**
   * CMID (compiled method id) associated with the code produced 
   * by executing this plan
   */
  public int getCMID() { return CMID; }
  public void setCMID(int x) { CMID = x; }


  /**
   * CMID (compiled method id) associated with the *PREVIOUS* compiled 
   * version of this method
   */
  public int getPrevCMID() { return prevCMID; }


  /**
   * Status of this compilation plan, choose from the values above
   */
  public byte getStatus() { return status; }

  public void setStatus(byte newStatus) { 
    status = newStatus; 

    // if we are marking this plan as completed, all previous completed plans
    // for this method should be marked as OUTDATED
    if (newStatus == COMPLETED) {
      // iterate over the planList until we get to this item
      ListIterator iter = planList.listIterator();
      while (iter.hasNext()) {
	VM_ControllerPlan curPlan = (VM_ControllerPlan) iter.next();

	// exit when we find ourselves
	if (curPlan == this) break;

	if (curPlan.getStatus() == COMPLETED) {
	  curPlan.status = OUTDATED;
	}
      } // more to process
    }
  }

  /**
   * List of plans for a source method
   */
  public LinkedList getPlanList() { return planList; }
  public void setPlanList(LinkedList list) { planList = list; }

  public String getStatusString() {
    switch (status) {
    case UNINITIALIZED:             return "UNINITIALIZED";
    case COMPLETED:                 return "COMPLETED";
    case ABORTED_COMPILATION_ERROR: return "ABORTED_COMPILATION_ERROR";
    case ABORTED_QUEUE_FULL:        return "ABORTED_QUEUE_FULL";
    case IN_PROGRESS:               return "IN_PROGRESS";
    case OUTDATED:                  return "OUTDATED";
    case UNKNOWN:                   return "UNKNOWN (not error)";
    default:                        return "**** ERROR, UNKNOWN STATUS ****";
    }
  }

  public String toString() {
    StringBuffer buf = new StringBuffer();

    buf.append("Method: "+ getCompPlan().method
	       +"\n\tCompiled Method ID: " + CMID
	       +"\n\tPrevious Compiled Method ID: " + prevCMID
	       +"\n\tCreated at "+ timeCreated
	       +"\n\tInitiated at "+ timeInitiated
	       +"\n\tCompleted at "+ timeCompleted
	       +"\n\tCPU Time Consumed: "+ compilationCPUTime
	       +"\n\tExpected Speedup: "+ expectedSpeedup
	       +"\n\tPriority: "+ priority
	       +"\n\tStatus: "+ getStatusString()
	       +"\n\tComp. Plan Level: "+compPlan.options.getOptLevel() +"\n");
    return buf.toString();
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * This class implements the controller thread.  This entity is the brains of 
 * the adaptive optimization system.  It communicates with the runtime 
 * measurements subsystem to instruct and gather profiling information.  
 * It also talks to the compilation threads to generate 
 *     a) instrumented executables;
 *     b) optimized executables; 
 *     c) static information about a method; or 
 *     d) all of the above.
 *
 *  @author Michael Hind
 *  @author David Grove
 *  @author Stephen Fink
 *  @author Peter Sweeney
 */
class VM_ControllerThread extends VM_Thread {

  /**
   * constructor
   * @param sentinel: an object to signal when up and running
   */
  VM_ControllerThread(Object sentinel)  { this.sentinel = sentinel; }
  private Object sentinel;


  /**
   * This method is the entry point to the controller, it is called when
   * the controllerThread is created.
   */
  public void run() {
    // save this object so others can access it, if needed
    VM_Controller.controllerThread = this;

    // Bring up the logging system
    if (VM.LogAOSEvents) VM_AOSLogging.boot();
    if (VM.LogAOSEvents) VM_AOSLogging.controllerStarted();

    // Create measurement entities that are NOT related to 
    // adaptive recompilation
    createProfilers();

    if (!VM_Controller.options.adaptive()) {
      // We're running an AOS bootimage with a non-adaptive primary strategy. 
      // We already set up any requested profiling infrastructure, so nothing
      // left to do but exit.
      controllerInitDone();
      VM.sysWrite("\nAOS: In non-adaptive mode; controller thread exiting.\n");
      return; // controller thread exits.
    }

    // Create the organizerThreads and schedule them
    createOrganizerThreads();

    // Create the compilationThread and schedule it
    createCompilationThread();

    // Initialize the controller "memory"
    VM_ControllerMemory.init();

    // Initialize the CompilerDNA class
    VM_CompilerDNA.init();

    // Create our set of standard optimization plans.
    VM_Controller.recompilationStrategy.init();

    controllerInitDone();

    // Enter main controller loop.
    // Pull an event to process off of 
    // VM_Controller.controllerInputQueue and handle it.  
    // If no events are on the queue, then the deleteMin call will 
    // block until an event is available.
    // Repeat forever.
    while (true) {
      if (VM_Controller.options.EARLY_EXIT &&
	  VM_Controller.options.EARLY_EXIT_TIME < VM_Controller.controllerClock) {
	VM_Controller.stop();
      }
      Object event = VM_Controller.controllerInputQueue.deleteMin();
      ((VM_ControllerInputEvent)event).process();
    }
  }

  // Now that we're done initializing, signal the sentinel object.
  private void controllerInitDone() {
    try {
      synchronized(sentinel) {
	sentinel.notify();
      }
    } catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to start up controller subsystem");
    }
  }

  
  /**
   * Called when the controller thread is about to wait on 
   * VM_Controller.controllerInputQueue
   */
  public void aboutToWait() {
  }


  /**
   * Called when the controller thread is woken after waiting on 
   * VM_Controller.controllerInputQueue
   */
  public void doneWaiting() {
    if (VM.LogAOSEvents) VM_ControllerMemory.incrementNumAwoken();
  }


  ///////////////////////
  // Initialization.
  //  Create AOS threads.
  //  Initialize AOS data structures that depend on command line arguments.
  ///////////////////////

  /**
   *  Create the compilationThread and schedule it
   */
  private void createCompilationThread() {
    VM_CompilationThread ct = new VM_CompilationThread();
    VM_Controller.compilationThread = ct;
    ct.makeDaemon(true);
    ct.start();
  }

  /**
   * Create profiling entities that are NOT used for adaptive recompilation
   */
  private void createProfilers() {
    VM_AOSOptions opts = VM_Controller.options;

    if (opts.GATHER_PROFILE_DATA) {
      VM_MethodCountData tmp = new VM_MethodCountData();
      VM_RuntimeMeasurements.registerReportableObject(tmp);
      VM_MethodListener methodListener = 
        new VM_AccumulatingMethodListener(opts.INITIAL_SAMPLE_SIZE, 
					  false, tmp);
      VM_RuntimeMeasurements.installMethodListener(methodListener);
      methodListener.activate();
    }
  }


  /**
   *  Create the organizerThreads and schedule them
   */
  private void createOrganizerThreads() {
    VM_AOSOptions opts = VM_Controller.options;

    // Primary backing store for method sample data
    VM_Controller.methodSamples = new VM_MethodCountData();

    // Select organizers to drive method recompilation 
    int filterOptLevel = opts.ADAPTIVE_RECOMPILATION ? 
      opts.FILTER_OPT_LEVEL : opts.DEFAULT_OPT_LEVEL;
    VM_Organizer methodOrganizer = null;
    if (opts.windowing()) {
      VM_BasicMethodListener methodListener = 
	new VM_BasicMethodListener(opts.INITIAL_SAMPLE_SIZE);
      methodOrganizer = 
	new VM_MethodSampleOrganizer(methodListener, filterOptLevel);
    } else if (opts.windowingWithHistory()) {
      VM_BasicMethodListener methodListener = 
	new VM_BasicMethodListener(opts.INITIAL_SAMPLE_SIZE);
      methodOrganizer = 
	new VM_SlopeDetectingMethodSampleOrganizer(methodListener,
						   filterOptLevel,
						   opts.MSO_ADJUST_BOUNDS,
						   opts.MSO_NUM_EPOCHS);
    } else {
      VM.sysFail("Unimplemented selection of method organizer");
    }
    VM_Controller.organizers.addElement(methodOrganizer);


    // Decay runtime measurement data 
    if (opts.ADAPTIVE_INLINING) {
      VM_Organizer decayOrganizer = 
	new VM_DecayOrganizer(new VM_YieldCounterListener(opts.DECAY_FREQUENCY));
      VM_Controller.organizers.addElement(decayOrganizer);
    }
    
    if (opts.ADAPTIVE_INLINING) {
      VM_Organizer AIOrganizer = 
	new VM_AIByEdgeOrganizer(new VM_EdgeListener());
      VM_Controller.organizers.addElement(AIOrganizer);
    }

    for (Enumeration e = VM_Controller.organizers.elements(); 
	 e.hasMoreElements(); ) {
      VM_Organizer o = (VM_Organizer)e.nextElement();
      o.makeDaemon(true);
      o.start();
    }
  }


  /**
   * Final report
   */
  public static void report() {
    if (VM.LogAOSEvents) VM_AOSLogging.controllerCompleted();
  }

} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Abstract parent class for events from organizers to the controller 
 * used to communicate that a method should be considered as a candidate
 * for recompilation.
 *
 * @author Dave Grove 
 */
abstract class VM_HotMethodEvent {

  /**
   * The compiled method associated querries.
   */
  private VM_CompiledMethod cm;
  public final int getCMID() { return cm.getId(); }
  public final VM_CompiledMethod getCompiledMethod() { return cm; }
  public final VM_Method getMethod() { return cm.getMethod();  }
  public final boolean isOptCompiled() {
    return cm.getCompilerType() == VM_CompiledMethod.OPT;
  }
  public final int getOptCompiledLevel() {
    if (!isOptCompiled()) return -1;
    return ((VM_OptCompiledMethod)cm).getOptLevel();
  }


  /**
   * Number of samples attributed to this method.
   */
  private double numSamples;
  public final double getNumSamples() { return numSamples; }

  /**
   * @param _cm the compiled method 
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodEvent(VM_CompiledMethod _cm, double _numSamples) {
    if (VM.VerifyAssertions) {
      VM.assert(_cm != null, "Don't create me for null compiled method!");
      VM.assert(_numSamples >= 0.0, "Invalid numSamples value");
    }
    cm = _cm;
    numSamples = _numSamples;
  }

  /**
   * @param _cm the compiled method 
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodEvent(VM_CompiledMethod _cm, int _numSamples) {
    this(_cm, (double)_numSamples);
  }

  public String toString() {
    return getMethod()+ " = " +getNumSamples();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Event used by the basic recompilation organizer
 * to notify the controller that a method is hot.
 *
 * @author Dave Grove 
 * @author Stephen Fink
 */
public final class VM_HotMethodRecompilationEvent extends VM_HotMethodEvent 
  implements VM_ControllerInputEvent {

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodRecompilationEvent(VM_CompiledMethod _cm, double _numSamples) {
    super(_cm, _numSamples);
  }

  /**
   * @param _cm the compiled method
   * @param _numSamples the number of samples attributed to the method
   */
  VM_HotMethodRecompilationEvent(VM_CompiledMethod _cm, int _numSamples) {
    this(_cm, (double)_numSamples);
  }

  public String toString() {
    return "HotMethodRecompilationEvent: "+super.toString();
  }


  /**
   * This function defines how the controller handles a
   * VM_HotMethodRecompilationEvent.  Simply passes the event to the
   * recompilation strategy.
   */
  public void process() {
    VM_ControllerPlan plan =
      VM_Controller.recompilationStrategy.considerHotMethod(getCompiledMethod(), this);

    if (VM.LogAOSEvents) VM_ControllerMemory.incrementNumMethodsConsidered();

    // If plan is still null we decided not to recompile.
    if (plan != null) {
      plan.execute();
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Implements the multi-level adaptive strategy using an analytic
 * model, as described in the OOPSLA 2000 paper.  Most behavior
 * inherited from VM_AnalyticModel.  This class defines the the specific 
 * recompilation choices that should be considered by the analytic model.
 *
 * @author Matthew Arnold 
 */
class VM_MultiLevelAdaptiveModel extends VM_AnalyticModel {


  /**
   * Initialize the set of "optimization choices" that the
   * cost-benefit model will consider.
   *
   * This method is conceptually simply, but becomes more complex
   * because sets of choices are precomputed and stored in a table so
   * they do not need to be recomputed to answer queries.
   * */

  void populateRecompilationChoices() {

    int maxOptLevel =  VM_Controller.options.MAX_OPT_LEVEL;
    int maxCompiler =  VM_CompilerDNA.getCompilerConstant(maxOptLevel);
    allOptLevelChoices = new VM_RecompileOptChoice[maxOptLevel+1];

    // Create one main list of all possible recompilation choices that
    // will be considered.  For each opt-level, create a recompilation
    // choice for that opt-level and record it indexed by opt-level
    for (int optLevel=0; optLevel <= maxOptLevel; optLevel++) {
      allOptLevelChoices[optLevel] = 
	  new VM_RecompileOptChoice(optLevel);
    }

    // Given the above choices, create lookup table so that the
    // controller's calls to
    // getViableRecompilationChoices(prevCompiler) are answered as
    // efficiently as possible.
    createViableOptionLookupTable(maxCompiler);

  }


  /**
   * Compute the set of optimization choices that should be
   * considered by the cost-benefit model.  
   *
   * @param prevCompiler The compiler compiler that was used to 
   *                     comile cmpMethod
   * @param cmpMethod The compiled method being considered
   */
  VM_RecompilationChoice[] getViableRecompilationChoices(int prevCompiler,
							  VM_CompiledMethod cmpMethod) {

     // Return the precomputed set of choices given the previous compiler
     return viableChoices[prevCompiler];
  }


  //----- Implementaiton ------

  
  /**
   * Setup a lookup table that maps a "previous compiler" to a set
   * of viable recompilation choices.  In this case, a viable choice
   * is any compiler > prevCompiler. 
   */
  protected void createViableOptionLookupTable(int maxCompiler) {
    
    viableChoices = new 
      VM_RecompilationChoice[maxCompiler][];

    // A temp place to store the list of viable choices
    VM_RecompilationChoice[] temp = new VM_RecompilationChoice[maxCompiler];

    // For each potential value of the previous compiler
    for (int prevCompiler=VM_CompilerDNA.BASELINE;
	 prevCompiler < maxCompiler;
	 prevCompiler++) {
      
      // Consider each choice in the list of all choices.
      // If it is greater than cur compiler, add it.
      int curSlot=0;
      for (int i=0; i<allOptLevelChoices.length; i++) {
	if (allOptLevelChoices[i].getCompiler() > prevCompiler) {
	  // Add the current opt-level as a choice to consider when
	  // the previous compiler is prevCompiler
	  temp[curSlot++] = allOptLevelChoices[i];
	}
      }
      
      // Now that you know how many choices there are, create an array
      // of them and copy the choices in.
      viableChoices[prevCompiler] = new VM_RecompilationChoice[curSlot];
      for (int i=0; i<curSlot; i++) {
	viableChoices[prevCompiler][i] = temp[i];
	temp[i]=null;
      }
    }
  }


  /** 
   * List of all opt-level choices that can be considered by the
   * cost-benefit model 
   */
  protected  VM_RecompileOptChoice[] allOptLevelChoices;

  /**
   * Keep a map from previous compiler to a set of recompilation
   * choices.  After initialization, viableChoices[x][y] means that if
   * x is the previous compiler, y makes sense as a possible
   * recompilation choice.
   */
  protected  VM_RecompilationChoice[][] viableChoices;



}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A recompilation choice represents an action (or a set of actions)
 * that can be considered by the controller's analytic model.
 *
 * @author Matthew Arnold
 */

abstract class VM_RecompilationChoice {

  //--- Interface ---

  /**
   * What is the cost of selecting this recompilation choice
   *
   * @param prevCompiler The previous compiler 
   * @param prevCompileTime The compile time when compiled with the
   *        previous compiler
   * @return The expected cost of exeuting this recompilation choice
   */
  abstract double getCost(int prevCompiler, double prevCompileTime);

  /**
   * What is the benefit of executing this recompilation choice, given
   * the estimated future time for the method if nothing changes?  
   *
   * @param prevCompiler The previous compiler 
   * @param futureExecutionTime The expected future execution time of
   *        the method if left running with the previous compiler.
   * @return The expected future execution time if this choice were selected 
   */
  abstract double getFutureExecutionTime(int prevCompiler, 
					 double futureExecutionTime);

  /**
   * Return a controller plan that will start this recompilation choice 
   * in action
   *
   * @param cmpMethod The method in question
   * @param prevCompiler The previous compiler
   * @param prevTimeFormethod The estimated future time had nothing been done
   * @param bestActionTime The estimated total time implementing this choice
   * @return The controller plan implementing this recompilation choice
   */
  abstract VM_ControllerPlan makeControllerPlan(VM_CompiledMethod cmpMethod,
						int prevCompiler,
						double prevTimeFormethod,
						double bestActionTime);

}







/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_Recompilation Strategy
 *
 * An abstract class providing the interface to the decision making
 * component of the controller.  There are currently two types
 * recompilation strategies implemented for the Jikes RVM, 
 * (both discussed in OOPSLA 2000 paper)
 *
 *  1) Multi-level adaptive strategy using an analytic model (see
 *     VM_AnalyticModel.java and VM_MultiLevelAdaptiveModel.java) 
 *
 *  2) Single level strategy not using a model 
 *     (See VM_SingleLevelAdaptive.java)
 *
 * @author Matthew Arnold
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

abstract class VM_RecompilationStrategy {

  //------  Interface -------

  /**
   * A hot method has been passed to the controller by an organizer
   */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {
    // Default behavior, do nothing.
    return null;
  }


  /**
   * A hot call edge has been passed to the controller by an organizer
   */
  void considerHotCallEdge(VM_CompiledMethod cmpMethod, 
			   VM_AINewHotEdgeEvent event) {
    // Default behavior, do nothing.
  }


  // -- Functionality common to all recompilation strategies (at least
  //    for now) --


  /**
   *  Initialize the recompilation strategy.
   *
   *  Note: This uses the command line options to set up the
   *  optimization plans, so this must be run after the command line
   *  options are available.  
   */
  void init() {
    createOptimizationPlans();
  }


  /**
   * This helper method creates a ControllerPlan, which contains a 
   * CompilationPlan, for the passed method using the passed optimization 
   * level and instrumentation plan.
   * 
   * @param method the VM_Method for the plan
   * @param optLevel the optimization level to use in the plan
   * @param instPlan the instrumentation plan to use
   * @param prevCMID the previous compiled method ID
   * @param expectedSpeedup  expected speedup from this recompilation
   * @param priority a measure of the oveall benefit we expect to see
   *                 by executing this plan.
   * @return the compilation plan to be used 
   */

   VM_ControllerPlan createControllerPlan(VM_Method method, 
					  int optLevel,
					  OPT_InstrumentationPlan instPlan,
					  int prevCMID,
					  double expectedSpeedup,
					  double priority) {

     // Construct the compilation plan (varies depending on strategy)
     OPT_CompilationPlan compPlan = createCompilationPlan(method,
							optLevel,
							instPlan);

    if (VM_Controller.options.ADAPTIVE_INLINING) {
      OPT_InlineOracle inlineOracle = 
	VM_AdaptiveInlining.getInlineOracle(method);
      compPlan.setInlineOracle(inlineOracle);
    }

    // Create the controller plan
    return new VM_ControllerPlan(compPlan, VM_Controller.controllerClock, 
				 prevCMID, expectedSpeedup, priority);
  }


  /**
   * Construct a compilation plan that will compile the given method
   * with instrumentation.  
   *
   * @param method The method to be compiled with instrumentation
   * @param optLevel The opt-level to recompile at 
   * @param instPlan The instrumentation plan
   */
  OPT_CompilationPlan createCompilationPlan(VM_Method method, 
					    int optLevel,
					    OPT_InstrumentationPlan instPlan) {

     // Construct a plan from the basic pre-computed opt-levels
     return new OPT_CompilationPlan(method, _optPlans[optLevel],
				    null, _options[optLevel]);
   }

  /**
   * Compute the previous compile time for the compiled code
   * associated with the argument VM_HotMethodEvent.
   *
   * @param hme the VM_HotMethodEvent
   * @param plan the VM_ControllerPlan for the compiled method (may be null)
   * @param prevCompiler the previous compiler 
   * 
   * @return previous compile time.  A value < 0.0 encodes that the 
   *         method should not be considered for recompilation.
   */
   double getPreviousCompilationTime(VM_HotMethodEvent hme, 
				     VM_ControllerPlan plan, 
				     int prevCompiler) {
    VM_Method method = hme.getMethod();
    if (plan == null) {
      // Our caller did not find a matching plan for this compiled method.
      // Therefore the code was not generated by 
      // the AOS recompilation subsystem. 
      if (VM_ControllerMemory.shouldConsiderForInitialRecompilation(method)) {
	// AOS has not already taken action to address the situation 
	// (or it attempted to take action, and the attempt failed in a way
	//  that doesn't preclude trying again,
	//  for example the compilation queue could have been full).  
	// But, since the compiled method wasn't generated by AOS, we don't 
	// have a measured compilation time, so we'll have to  
	// approximate prevCompileTime based on prevCompiler and the size of
	// the method's bytecodes in order to let the model figure out what
	// recompilation action should be taken.
	double baselineCompilationTime = 
	  ((double)method.getBytecodes().length) / 
	  VM_CompilerDNA.getBaselineCompilationRate();

	return baselineCompilationTime * 
	  VM_CompilerDNA.getCompileTimeRatio(VM_CompilerDNA.BASELINE, 
					     prevCompiler);
      } else {
	// AOS has already taken action to address the situation, and thus
	// we need to handle this as an old compiled version of a 
        // method still being live on some thread's stack.
	if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme); 
	VM_Controller.methodSamples.reset(hme.getCMID());
	return -1.0;
      }
    } else {
      // A matching plan was found.
      if (plan.getStatus() == VM_ControllerPlan.OUTDATED || 
	  VM_ControllerMemory.planWithStatus(method, 
					     VM_ControllerPlan.IN_PROGRESS)) {
	// (a) The HotMethodEvent actually corresponds to an 
        // old compiled version of the method
	// that is still live on some thread's stack or 
        // (b) AOS has already initiated a plan that hasn't
	// completed yet to address the situation. 
        // Therefore don't initiate a new recompilation action.
	if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme); 
	VM_Controller.methodSamples.reset(hme.getCMID());
	return -1.0;
      }
      if (VM_ControllerMemory.planWithStatus(method, 
			     VM_ControllerPlan.ABORTED_COMPILATION_ERROR)) {
	// AOS failed to successfully recompile this method before.  
        // Don't try it again.
	return -1.0;
      }
      // use the measured compilation time of the previous AOS 
      // recompilation of method.
      return plan.getCompilationCPUTime();
    }
  }


  /**
   *  This method returns true if we've already tried to recompile the
   *  passed method.  It does not guarantee that the compilation was
   *  successful.
   * 
   *  @param method the method of interest
   *  @return whether we've tried to recompile this method
   */
   boolean previousRecompilationAttempted(VM_Method method) {
    return  VM_ControllerMemory.findLatestPlan(method) != null;
  }

  /**
   *  This method retrieves the previous compiler constant.
   */
   int getPreviousCompiler(VM_CompiledMethod cmpMethod) {
    switch(cmpMethod.getCompilerType()) {
    case VM_CompiledMethod.TRAP: 
    case VM_CompiledMethod.JNI:
      return -1; // don't try to optimize these guys!
    case VM_CompiledMethod.BASELINE:
      { 
	// Prevent the adaptive system from recompiling certain classes
	// of baseline compiled methods.
	if (cmpMethod.getMethod().getDeclaringClass().isDynamicBridge()) {
	  // The opt compiler does not implement this calling convention.
	  return -1;
	} 
	if (cmpMethod.getMethod().getDeclaringClass().isBridgeFromNative()) {
	  // The opt compiler does not implement this calling convention.
	  return -1;
	}
	if (VM_Collector.MOVES_OBJECTS && !cmpMethod.getMethod().isInterruptible()) {
	  // A crude filter to identify the subset of core VM methods that 
	  // can't be recompiled because we require their code to be non-moving.
	  // We really need to do a better job of this to avoid missing too many opportunities.
	  return -1;
	}
	return 0;
      }
    case VM_CompiledMethod.OPT:
      VM_OptCompiledMethod optMeth = (VM_OptCompiledMethod)cmpMethod;
      return VM_CompilerDNA.getCompilerConstant(optMeth.getOptLevel());
    default:
      if (VM.VerifyAssertions) VM.assert(false, "Unknown Compiler");
      return -1;
    }
  }

  /**
   * What is the maximum opt level that is vallid according to this strategy?
   */
  int getMaxOptLevel() {
    return Math.max(VM_Controller.options.DEFAULT_OPT_LEVEL, 
		    VM_Controller.options.MAX_OPT_LEVEL);
  }


  /**
   * Create the default set of <optimization plan, options> pairs
   * Process optimizing compiler command line options.
   * <p>
   * If VM_Controller.options.ADAPTIVE_RECOMPILATION is False, 
   * then don't use cost benefit model, but recompile all methods 
   * at VM_Contoller.options.DEFAULT_OPT_LEVEL (single 
   * level adaptive).  If this is the case, then generate warning if  
   * optimizing compiler command line options for optimization levels other  
   * than VM_Controller.options.DEFAULT_OPT_LEVEL.
   */
  private  OPT_OptimizationPlanElement[][] _optPlans;
  private  OPT_Options[] _options;

   void createOptimizationPlans() {
    OPT_Options options = new OPT_Options();

    int maxOptLevel = getMaxOptLevel();
    _options = new OPT_Options[maxOptLevel+1];
    _optPlans = new OPT_OptimizationPlanElement[maxOptLevel+1][];
    String[] optCompilerOptions = VM_Controller.getOptCompilerOptions();
    for (int i=0; i<= maxOptLevel; i++) {
      _options[i] = (OPT_Options)options.clone();
      _options[i].setOptLevel(i);		// set optimization level specific optimiations
      processCommandLineOptions(_options[i],i,maxOptLevel,optCompilerOptions);
      _optPlans[i]=OPT_OptimizationPlanner.createOptimizationPlan(_options[i]);
      if (_options[i].PRELOAD_CLASS != null) {
	VM.sysWrite("In an adaptive system, PRELOAD_CLASS should be specified with -X:aos:irc not -X:aos:opt\n");
	VM.sysExit(1);
      }
    }
  }

  /**
   * Process the command line arguments and pass the appropriate ones to the 
   * OPT_Options
   * 
   * @param options The options being constructed
   * @param optLevel The level of the options being constructed
   * @param maxOptLevel The maximum valid opt level
   * @param optCompilerOptions The list of command line options
   */
  void processCommandLineOptions(OPT_Options options, int optLevel, int maxOptLevel,
			 String optCompilerOptions[]) {

    String prefix = "opt"+optLevel+":";
    for (int j=0; j<optCompilerOptions.length; j++) {
      if (optCompilerOptions[j].startsWith("opt:")) {
	String option = optCompilerOptions[j].substring(4);
	if (!options.processAsOption("-X:aos:opt:", option)) {
	  VM.sysWrite("vm: Unrecognized optimizing compiler command line argument: \""
		      +option+"\" passed in as "
		      +optCompilerOptions[j]+"\n");
	}
      } else if (optCompilerOptions[j].startsWith(prefix)) {
	if (!VM_Controller.options.ADAPTIVE_RECOMPILATION &&
	    VM_Controller.options.DEFAULT_OPT_LEVEL != optLevel) {
	  VM.sysWrite("***WARNING: a command line option for optimization "+
		      "level "+optLevel+
		      " is not allowed when single level adaptivity is "+
		      VM_Controller.options.DEFAULT_OPT_LEVEL+"\n");
	  continue;
	}
	String option = optCompilerOptions[j].substring(5);
	if (!options.processAsOption("-X:aos:"+prefix, option)) {
	  VM.sysWrite("vm: Unrecognized optimizing compiler command line argument: \""
		      +option+"\" passed in as "
		      +optCompilerOptions[j]+"\n");
	}
      }
    }
    // TODO: check for optimization levels that are invalid; that is, 
    // greater than optLevelMax.
    //
    for (int j=0; j<optCompilerOptions.length; j++) {
      if (!optCompilerOptions[j].startsWith("opt")) {
	// This should never be the case!
	continue;
      }
      if (! optCompilerOptions[j].startsWith("opt:")) {
	// must specify optimization level!
	int endPoint = optCompilerOptions[j].indexOf(":");
	if (endPoint == -1) {
	  VM.sysWrite("vm: Unrecognized optimization level in optimizing compiler command line argument: \""
		      +optCompilerOptions[j]+"\"\n");
	}
	String optLevelS;
	try {
	  optLevelS = optCompilerOptions[j].substring(3,endPoint);
	} catch (IndexOutOfBoundsException e) {
	  VM.sysWrite("vm internal error: trying to find opt level has thrown indexOutOfBoundsException\n");
	  e.printStackTrace();
	  continue;
	}
	try {
	  Integer optLevelI = new Integer(optLevelS);
	  int cmdOptLevel = optLevelI.intValue();
	  if (cmdOptLevel > maxOptLevel) {
	    VM.sysWrite("vm: Invalid optimization level in optimizing compiler command line argument: \""
			+optCompilerOptions[j]+"\"\n"+
			"  Specified optimization level "+cmdOptLevel+
			" must be less than "+maxOptLevel+"\n");
	  }
	} catch (NumberFormatException e) {
	  VM.sysWrite("vm: Unrecognized optimization level in optimizing compiler command line argument: \""
		      +optCompilerOptions[j]+"\"\n");
	}
      }
    }
    VM_RuntimeOptCompilerInfrastructure.setNoCacheFlush(options);
  }
}




/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_RecompileOptChoice 
 *
 * Represents the recompilation choice of simply recompiling the
 * method in question at a particular opt-level.  The cost is the
 * expected compilation time at that level, and the benefit is the
 * execution improvement of executing at that level.
 *
 *
 * @author Matthew Arnold */

class VM_RecompileOptChoice extends VM_RecompilationChoice {

  /**
   * Constructor
   */
  VM_RecompileOptChoice(int level) {
    this.thisChoiceOptLevel = level;
    this.thisChoiceCompiler = 
      VM_CompilerDNA.getCompilerConstant(level); 
  }

  /**
   * What is the cost of executing this plan?
   *
   * @param prevCompiler The previous compiler 
   * @param prevCompileTime The compile time when compiled with the
   *        previous compiler
   * @return The expected cost of exeuting this recompilation choice
   */
  double getCost(int prevCompiler, double prevCompileTime) {

    double compileTimeFactor = 
      VM_CompilerDNA.getCompileTimeRatio(prevCompiler, getCompiler());

    return prevCompileTime * compileTimeFactor   // compile time
      + VM_Controller.options.FIXED_RECOMPILATION_OVERHEAD;   // fixed cost "brake" 
  }


  /**
   * What is the benefit of executing this plan, given the estimated
   * future time for the method if nothing changes?
   *
   * @param prevCompiler The previous compiler 
   * @param futureExecutionTime The expected future execution time of
   *        the method if left running with the previous compiler.
   * @return The expected future execution time if this choice were selected 
   */
  double getFutureExecutionTime(int prevCompiler, 
				double futureTimeForMethod) {

    double rtFactor = 
      VM_CompilerDNA.getBenefitRatio(prevCompiler, 
				     getCompiler());
    
    return futureTimeForMethod / rtFactor; // future execution time

  }

  /**
   * Return a controller plan that will start this recompilation
   * choice in action.  In this case, simply create a plan to
   * recompile at level "optLevel"
   *
   * @param cmpMethod The method in question
   * @param prevCompiler The previous compiler
   * @param prevTimeFormethod The estimated future time had nothing been done
   * @param bestActionTime The estimated total time implementing this choice
   * @return The controller plan implementing this recompilation choice
   */
  VM_ControllerPlan makeControllerPlan(VM_CompiledMethod cmpMethod,
				       int prevCompiler, 
				       double prevTimeForMethod,
				       double bestActionTime) {
    double speedup = 
      VM_CompilerDNA.getBenefitRatio(prevCompiler, getCompiler());
    double priority = prevTimeForMethod - bestActionTime;
    return VM_Controller.recompilationStrategy.
      createControllerPlan(cmpMethod.getMethod(), thisChoiceOptLevel, 
			   null, cmpMethod.getId(), speedup, priority);

    
  }

  /**
   * How should this choice be displayed?
   */
  public String toString() {
    return "O" + getOptLevel();
  }

  /**
   * Which opt-level is associated with this choice?
   */
  int getOptLevel() {
    return thisChoiceOptLevel;
  }

  /**
   * Which "compiler" (@see VM_CompilerDNA) is associated with this choice?
   */
  int getCompiler() {
    return thisChoiceCompiler;
  }

  //----  Implementation -----

  /** The opt level associated with this recompilation choice */ 
  private int thisChoiceOptLevel;

  /** The "compiler" (see VM_CompilerDNA) that is associated with this choice */
  private int thisChoiceCompiler;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A single-level adaptive model that behaves like the "SLA"
 * strategies in the OOPSLA 2000 paper.  If a method is hot enough to
 * make it to the controller (ie, get past the organizer), it is
 * recompiled at the fixed level.
 *
 * NOTE: This strategy does NOT use a cost-benefit model.
 *
 * @author Matthew arnold 
 * @author Dave Grove 
 * @author Stephen Fink
 */

class VM_SingleLevelAdaptive extends VM_RecompilationStrategy {


  /**
   * A hot method has been passed to the controller by an organizer.
   * Optimize it!  
   */
  VM_ControllerPlan considerHotMethod(VM_CompiledMethod cmpMethod,
				      VM_HotMethodEvent hme) {

    VM_Method method = cmpMethod.getMethod();

    VM_ControllerPlan plan = null;

    // Don't try to recompile if we know it won't work
    if (VM_ControllerMemory.shouldConsiderForInitialRecompilation(method)) {
      int optLevel = VM_Controller.options.DEFAULT_OPT_LEVEL;
      int prevCompiler = VM_CompilerDNA.BASELINE;
      int newCompiler = VM_CompilerDNA.getCompilerConstant(optLevel);
      double speedup = 
	VM_CompilerDNA.getBenefitRatio(prevCompiler, newCompiler);
      plan = createControllerPlan(method, optLevel,
				  null, cmpMethod.getId(),
				  speedup, 1.0);
    } else {
      if (VM.LogAOSEvents) VM_AOSLogging.oldVersionStillHot(hme);
    }
    return plan;
  } 


  /**
   * What is the maximum opt level that is vallid according to this strategy?
   */
  int getMaxOptLevel() {
    return VM_Controller.options.DEFAULT_OPT_LEVEL;
  }



}


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * The representation of a call site as a triple: a caller, 
 * call site (as bytecode offset), and callee.
 *
 * SJF: I've modified this class to also hold an integer edge weight, rather
 * than keeping a lookaside hash table.  This may compromise OO design
 * a little, but should be more efficient for current clients.
 *
 * @author Peter F. Sweeney
 * @author Stephen Fink
 * @date   23 May 2000
 */

public final class VM_CallSiteTriple {

  public static final boolean DEBUG = false;
  
  /**
   * Current method.
   */
  VM_Method caller;
  /**
   * Get caller
   * @return call site's caller
   */
  public VM_Method getCaller() { return caller; }
  /**
   * Who is called.  null if not known.
   */
  VM_Method callee;
  /**
   * Get callee
   * @return call site's callee
   */
  public VM_Method getCallee() { return callee; }
  /**
   * Bytecode index of callsite in caller.
   * @return call site's bytecode index in caller
   */
  int bcIndex;		// bytecode index (in caller) of call site
  /**
   * Get call site's bytecode index
   * @ return call site's bytecode index
   */
  public int getBytecodeIndex() {return bcIndex;}
  
  /**
   * Edge weight
   */
  double weight;
  public double getWeight() { return weight; }
  public void setWeight(double w) { weight = w; }
  public void incrementWeight() { weight+=1.0; }
  
  /**
   * Decay the weight
   * @param rate the value to decay by
   */
  public void decayWeight(double rate) { 
    weight /= rate; 
  }
  
  /**
   * Constructor
   * @param caller call site caller
   * @param bcIndex bytecode index of call site
   * @param callee call site callee
   */
  VM_CallSiteTriple(VM_Method caller, int bcIndex, VM_Method callee) 
  {
    this.caller  = caller;
    this.bcIndex = bcIndex;
    this.callee  = callee;
  }
   
  /**
   * Generate string representation of a call site
   * @return string representation of call site
   */
  public String toString() {
    return " <"+caller+", "+bcIndex+", "+callee+">"+"(wt:"+weight+")";
  }
  /**
   * Determine if two call sites are the same.  Exact match: no wild cards.
   *
   * @param obj call site to compare to
   * @return true if call sites are the same; otherwise, return false
   */
  public boolean equals(Object obj) {
    if (!(obj instanceof VM_CallSiteTriple)) return false;
    if (obj == null) return false;
    VM_CallSiteTriple triple = (VM_CallSiteTriple)obj;
    boolean returnValue = false;
    try {
      returnValue = (caller.toString().compareTo(triple.caller.toString())==0);
      if (returnValue == true) {
	returnValue = (callee.toString().compareTo(triple.callee.toString())==0);
	if (returnValue == true) {
	  if (bcIndex != triple.bcIndex && bcIndex != -1 && triple.bcIndex != -1) {
	    returnValue = false;
	  }
	}
      }
    } catch (NullPointerException e) {
      VM.sysWrite("***VM_CallSiteTriple.equals("+obj+
		  ") compareTo of names failed!\n");
      returnValue = false;
    }
    return returnValue;
  }
   
  /**
   * Compute a call site's hash code
   *
   * @return hash code
   */
  public int hashCode() {
    int result = 7;
    if (caller != null) result += caller.hashCode();
    if (callee != null) result += callee.hashCode();
    result += bcIndex;
    return result;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
  * This class implements the Comparator comparing two 
  * VM_CallSiteTriple objects.
  *
  * We can either choose to implement a partial order based on
  * the String representation, or by weight.
  *
  * @author Peter Sweeney
  * @modified Stephen Fink
  * @modified Michael Hind
  * @date   25 May 2000
  */

import java.util.*;

class VM_CallSiteTripleComparator implements java.util.Comparator {

  public static boolean debug = false;
  
  /**
   * Boolean flag that determines if the comparison of two call sites
   * takes into account their weights.
   */
  private boolean byWeight = false;

  /** Interface */
    
  /**
   * Constructor
   */
  VM_CallSiteTripleComparator(boolean byWeight) {
    this.byWeight = byWeight;
  }
  /**
   * Constructor
   */
  VM_CallSiteTripleComparator() {
    this.byWeight = false;
  }

  /** 
   * Compare the string representation of two triples
   *
   * @returns -1 iff o1 < o2
   * @returns +1 iff o1 > o2
   * @returns 0 if o1 and o2 are the same callsites or 
   *		if they only differ by their bytecode indices and 
   *		one index is -1.
   */
  private int compareByName(VM_CallSiteTriple t1, VM_CallSiteTriple t2) 
  {
    String s1 = t1.toString();
    String s2 = t2.toString();

    return s1.compareTo(s2);
  }
  /** 
   * Compare the weight of two triples
   *
   * @returns -1 iff w(t1) < w(t2)
   * @returns +1 iff w(t1) > w(t2)
   * @returns compareByName(t1,t2) otherwise
   */
  private int compareByWeight(VM_CallSiteTriple t1, VM_CallSiteTriple t2) 
  {
    double w1 = t1.getWeight();
    double w2 = t2.getWeight();

    if (w1 < w2) return -1;
    if (w1 > w2) return 1;
    return compareByName(t1,t2);
  }
  /** 
   * @param o1, o2 two VM_CallSiteTriple to be compared.
   * @returns -1 iff o1 < o2
   * @returns +1 iff o1 > o2
   * @returns 0 if o1 and o2 are the same callsites or 
   *		if they only differ by their bytecode indices and 
   *		one index is -1.
   */
  public int compare(Object o1, Object o2) 
  {
    // This should never happen!
    if (!(o1 instanceof VM_CallSiteTriple) || 
	!(o2 instanceof VM_CallSiteTriple)) {
      VM.sysWrite("***VM_CallSiteTripleComparator.compare():"+
		  " one object is not of type VM_CallSiteTriple\n");
      VM.sysExit(-1);
    }

    if (o1.equals(o2)) return 0;

    VM_CallSiteTriple t1 = (VM_CallSiteTriple)o1;
    VM_CallSiteTriple t2 = (VM_CallSiteTriple)o2;
    if (byWeight) return compareByWeight(t1,t2);
    else return compareByName(t1,t2);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A partial call graph (PCG) is implemented as a set of edges and each
 * edge is represented as a VM_CallSiteTriple object.
 * A PCG's internal representation is a HashMap of edges:
 *	hashCodeOf(E) -> E where 
 *             E is VM_CallSiteTriple<caller,bytecodeOffset,callee>
 *
 * @author Peter F. Sweeney
 * @modified Michael Hind
 * @date   24 May 2000
 */

public final class VM_PartialCallGraph implements VM_Decayable {

  public static final boolean DEBUG = false;
  
  /**
   * Constructor
   */
  VM_PartialCallGraph() {
    findTriples = new java.util.HashMap();
    totalEdgeWeights = 0.0;
  }
  
  /**
   * Return an iterator over the edges in the PCG
   *
   * @return iterator over edges 
   */
  java.util.Iterator getEdges() {
    return findTriples.values().iterator();
  }
  
  /**
   *  Visit each edge and decay its weight. 
   */
  public void decay() { 
    if (DEBUG) {VM.sysWrite(" Before decay\n");  dump(); }
    
    double rate = VM_Controller.options.AI_DECAY_RATE;
    
    synchronized(findTriples) {
      for (java.util.Iterator iterator = getEdges(); iterator.hasNext();) {
	VM_CallSiteTriple triple = (VM_CallSiteTriple)iterator.next();
	triple.decayWeight(rate);
      }
      totalEdgeWeights /= rate;
    }
    
    if (DEBUG) {VM.sysWrite(" After decay\n");  dump(); }
  }
  
  /**
   * Increment the edge represented by the input parameters, 
   * creating it if needed.
   *
   * @param caller   method making the call
   * @param bytecode call site, if -1 then no call site is specified.
   * @param callee   method called
   */
  public void incrementEdge(VM_Method caller, int bcIndex, VM_Method callee) {
    
    VM_CallSiteTriple triple = findOrCreateEdge(caller, bcIndex, callee);
    
    triple.incrementWeight();
    totalEdgeWeights += 1.0;
  }
  
  /**
   * Find the edge in the partial call graph, if not found add it.
   *
   * @param caller   method making the call
   * @param bytecode call site, if -1 then no call site is specified.
   * @param callee   method called
   * @return         edge
   */
  public VM_CallSiteTriple findOrCreateEdge(VM_Method caller, 
					    int bcIndex, 
					    VM_Method callee) 
  {
    if (findTriples == null) {
      VM.sysWrite("FIND TRIPLES NULL");
    }
    if(DEBUG)VM.sysWrite(" VM_PartialCallGraph.findEdge("+caller+", "+
			 callee+", "+bcIndex+") entered\n");
    if (caller == null) {
      VM.sysWrite("***Error: VM_PartialCallGraph.findEdge("+caller+", "+
		  callee+") has null caller!\n");
      new Exception().printStackTrace();
    }
    if (callee == null) {
      VM.sysWrite("***Error: VM_PartialCallGraph.findEdge("+caller+", "+
		  callee+") has null callee!\n");
      new Exception().printStackTrace();
    }
    VM_CallSiteTriple triple = new VM_CallSiteTriple(caller, bcIndex, callee);
    
    synchronized(findTriples) {
      if (findTriples.containsKey(triple)) {
	if(DEBUG) VM.sysWrite
		    ("  VM_PartialCallGraph.findEdge() edge already called!\n");
	triple = (VM_CallSiteTriple)findTriples.get(triple);
      } else {
	if(DEBUG) VM.sysWrite
		    ("  VM_PartialCallGraph.findEdge() FIRST time edge called!\n");
	findTriples.put(triple,triple);
      }
    }
    
    if(DEBUG)VM.sysWrite(" VM_PartialCallGraph.increment() exit\n");
    return triple;
  }
  
  /**
   * Dump out set of edges in sorted order.
   */
  public void dump() {
    VM.sysWrite("VM_PartialCallGraph.dump()\n");
    VM.sysWrite("  Number of edges "+findTriples.size()+", total weight: "+totalEdgeWeights+"\n");
    java.util.TreeSet treeSet = new java.util.TreeSet(new VM_CallSiteTripleComparator(true));
    try {
      synchronized(findTriples) {
	treeSet.addAll(findTriples.values());
      }
    } catch (ClassCastException e) {
      VM.sysWrite("***VM_PartialCallGraph.dump(): addAll threw CallCastException!\n");
      VM.sysExit(-1);
    }
    int i=0;
    for (java.util.Iterator iterator = treeSet.iterator(); iterator.hasNext();) {
      VM_CallSiteTriple triple = null;
      try {
	triple = (VM_CallSiteTriple)iterator.next();
      } catch (java.util.NoSuchElementException e) {
	VM.sysWrite("***OPT_PCG.dump(): iterator.next() returns NoSuchElementException!\n");
	VM.sysExit(-1);
      }
      i++;
      VM.sysWrite(i+": "+triple.toString()+"\n");
    }
  }
  
  /**
   * Get sum of all edge weights in the partial call graph
   * @return edge weight sum
   */
  double getTotalEdgeWeights() { return totalEdgeWeights; }
  /*
   * hashcodeOf(callers,call site,callees) -> triple
   */
  private java.util.HashMap findTriples;
  /*
   * sum of all edge weights in the graph
   */
  private double totalEdgeWeights;
  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_AOSDatabase.java
 * 
 * Used to keep track of the various data structures that make up the
 * AOS database.  
 *
 * @author Matthew Arnold 
 */
public final class VM_AOSDatabase 
{
  /** 
   * Static links to data objects that are "whole-program" (as opposed
   * to per-method)
    */
  static VM_MethodInvocationCounterData methodInvocationCounterData;
  static VM_YieldpointCounterData yieldpointCounterData;
  static VM_StringEventCounterData instructionCounterData;
  static VM_StringEventCounterData debuggingCounterData;
 
  /**
   * Called at startup
   **/
  static void boot(VM_AOSOptions options)
  {
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;

/**
 * A container for recording how often a method is executed.
 *
 * @author Dave Grove
 * @author Michael Hind
 * @modified Peter Sweeney
 */
public final class VM_MethodCountData 
  implements VM_Decayable, VM_Reportable {

  private static final boolean DEBUG = false;
  
  /**
   * Sum of values in count array that are decayed over time.
   */
  private double totalCountsTaken;

  /**
   * How many counts have really been taken (ignoring decay).
   */
  private double undecayedTotalCountsTaken;

  /**
   * Count array: counts how many times a method is executed.
   * Constraint: counts[0] is not used.
   */
  private double[] counts;
  /**
   * Maps count array index to compiled method id.
   * Constraint: cmids[0] is not used.
   */
  private int[] cmids;
  /**
   * Maps compiled method id to count array index.
   * '0' implies that there is no entry in the count array for this cmid
   */
  private int[] map;
  /**
   * Next available count array entry.
   */
  private int nextIndex;
  
  /**
   * Constructor
   */
  VM_MethodCountData() {
    initialize();
  }

  /**
   * Reset fields.
   */
  private void initialize() {
    int numCompiledMethods = VM_CompiledMethods.numCompiledMethods();
    map = new int[ numCompiledMethods + (numCompiledMethods >>> 2) ];
    counts = new double[256];
    cmids = new int[256];
    nextIndex = 1;
    totalCountsTaken = 0;
    undecayedTotalCountsTaken = 0;
  }

  /** 
   * Drain a buffer of compiled method id's and update the count array.
   *
   * @param countBuffer a buffer of compiled method id's
   * @param numCounts the number of valid entries in the buffer
   */
  public final synchronized void update(int[] countBuffer, int numCounts) {
    for (int i=0; i<numCounts; i++) {
      int cmid = countBuffer[i];
      int index = findOrCreateHeapIdx(cmid);
      counts[index]++;      // Record count
      heapifyUp(index);     // Fix up the heap
    }
    totalCountsTaken += numCounts;
    undecayedTotalCountsTaken += numCounts;
    if (DEBUG) validityCheck();
  }

  /**
   * Increment the count for a compiled method id.
   *
   * @param cmid compiled method id
   * @param numCounts number of counts
   */
  public final synchronized void update(int cmid, double numCounts) {
    int index = findOrCreateHeapIdx(cmid);
    counts[index] += numCounts;       // Record counts
    heapifyUp(index);                 // Fix up the heap
    totalCountsTaken += numCounts;
    undecayedTotalCountsTaken += numCounts;
    if (DEBUG) validityCheck();
  }

  /**
   * Decay the method counts.
   */
  public final synchronized void decay() {
    double rate = VM_Controller.options.DECAY_RATE;
    for (int i=1; i<nextIndex; i++) {
      counts[i] /= rate;
    }
    totalCountsTaken /= rate;
  }


  /** 
   *  Print the counted (nonzero) methods.
   *  To get a sorted list, pipe the output through sort -n -r.
   */
  public final synchronized void report() {
    VM.sysWrite("Method counts: A total of "+totalCountsTaken+
		" times counted (undecayed  "+undecayedTotalCountsTaken+")\n");
    for (int i=1; i<nextIndex; i++) {
      double percent = 100 * countsToHotness(counts[i]);
      VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmids[i]);
      VM.sysWrite(counts[i] + " ("+percent+"%) ");
      if ( cm == null ) {
        VM.sysWrite("OBSOLETE");		// Compiled Method Obsolete
      } else {
        VM_Method m = cm.getMethod();
        VM.sysWrite(m);
        if (m.getDeclaringClass().isInBootImage()) {
	  VM.sysWrite("\n\tBOOT");
        }
      }
      VM.sysWrite("\n");
    }    
  }

  /**
   * Reset (clear) the method counts
   */
  public final synchronized void reset() {
    initialize();
  }

  /**
   * Get the current count for a given compiled method id.
   *
   * @param cmid compiled method id
   */
  public final synchronized double getData(int cmid) {
    int index = findHeapIdx(cmid);
    if (index > 0) {
      return counts[index];
    } else {
      return 0.0;
    }
  }

  /**
   * Reset (set to 0.0) the count for a given compiled method id.
   *
   * @param cmid compiled method id
   */
  public final synchronized void reset(int cmid) {
    int index = findHeapIdx(cmid);
    if (index > 0) {
      // Cmid does have a value in the heap. 
      // (1) clear map[cmid].
      // (2) shrink the heap by one slot.  
      //     (a) If index is the last element in the heap we have nothing 
      //         to do after we decrement nextIndex.
      //     (b) If index is not the last element in the heap, then move the
      //         last heap element to index and heapify.
      map[cmid] = 0;
      nextIndex--;
      if (index < nextIndex) {
	double oldValue = counts[index];
	counts[index] = counts[nextIndex];
	cmids[index] = cmids[nextIndex];
	map[cmids[index]] = index;
	if (counts[index] > oldValue) {
	  heapifyUp(index);
	} else {
	  heapifyDown(index);
	}
      } 
    }
    if (DEBUG) validityCheck();
  }

  /**
   * Set the raw data for a given cmid to the specified value
   *
   * @param cmid compiled method id
   * @param newVal new value for compiled method id count
   */
  public final synchronized void setData(int cmid, double newVal) {
    if (newVal == 0.0) {
      reset(cmid); // Prefer reset, since it frees up a slot in the heap.
    } else {
      int index = findOrCreateHeapIdx(cmid);
      double oldVal = counts[index];
      counts[index] = newVal;
      if (newVal > oldVal) {
	heapifyUp(index);
      } else {
	heapifyDown(index);
      }
    }
    if (DEBUG) validityCheck();
  }

  /**
   * Enqueue events describing the "hot" methods on the organizer's event queue.
   *
   * @param filterOptLevel filter out all methods already compiled at 
   *                       this opt level (or higher)
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   */
  public final synchronized void insertHotMethods(int filterOptLevel, 
						  double threshold) {
    if (DEBUG) validityCheck();
    insertHotMethodsInternal(1, filterOptLevel, hotnessToCounts(threshold));
  }


  /**
   * Collect the hot methods that have been compiled at the given opt level.
   *
   * @param optLevel  target opt level
   * @param threshold hotness value above which the method is considered to
   *                  be hot. (0.0 to 1.0)
   * @return a VM_MethodCountSet containing an
   * 		array of compiled methods and an array of their counts.
   * 
   */
  public final synchronized VM_MethodCountSet collectHotMethods(int optLevel, 
								double threshold) {
    if (DEBUG) validityCheck();
    Vector collect = new Vector();
    collectHotOptMethodsInternal(1, collect, hotnessToCounts(threshold), optLevel);

    // now package the data into the form the caller expects.
    int numHotMethods = collect.size();
    double[] numCounts = new double[numHotMethods];
    VM_CompiledMethod[] hotMethods = new VM_CompiledMethod[numHotMethods];
    for (int i=0; i<numHotMethods; i++) {
      VM_HotMethodEvent event = (VM_HotMethodEvent)collect.elementAt(i);
      hotMethods[i] = event.getCompiledMethod();
      numCounts[i] = event.getNumSamples();
    }
    return new VM_MethodCountSet(hotMethods, numCounts);
  }

  /** 
   * Convert from a [0.0...1.0] hotness value to the number of counts
   * that represents that fraction of hotness
   *
   * @param hotness a value [0.0...1.0]
   * @return a number of counts
   */
  private double hotnessToCounts(double hotness) {
    return totalCountsTaken * hotness;
  }

  /**
   * Convert a value to a [0.0...1.0] fractional hotness value
   *
   * @param numCounts number of counts
   * @return a value [0.0...1.0]
   */
  private double countsToHotness(double numCounts) {
    if (VM.VerifyAssertions) VM.assert(numCounts <= totalCountsTaken);
    return numCounts / totalCountsTaken;
  }

  /**
   * Convert a possibly decayed numCounts into an
   * "undecayed" count value for use in model calculations of the 
   * time spent executing in this method so far.
   *
   * @param numCounts number of decayed counts
   * @return number of undecayed counts
   */
  private double numCountsForModel(double numCounts) {
    return undecayedTotalCountsTaken * countsToHotness(numCounts);
  }

  /**
   * Recursive implementation of insertHotMethods. Exploit heap property.
   * Note threshold has been converted into a count value by my caller!
   *
   * @param index count array index
   * @param filterOptLevel filter out all methods already compiled at 
   *                       this opt level (or higher)
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   */
  private void insertHotMethodsInternal(int index, 
					int filterOptLevel, 
					double threshold) {
    if (index < nextIndex) {
      if (counts[index] > threshold) {
	int cmid = cmids[index];
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm == null) {			// obsolete and deleted
	  reset(cmid);				// free up this slot
	  // Visit new one in the slot
	  insertHotMethodsInternal(index, filterOptLevel, threshold);
	} else {
	  int compilerType = cm.getCompilerType();
	  // Enqueue it unless it's either a trap method or already 
	  // opt compiled at filterOptLevel or higher.
	  if (!(compilerType == VM_CompiledMethod.TRAP ||
		(compilerType == VM_CompiledMethod.OPT && 
		 (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {
	    double ns = numCountsForModel(counts[index]);
	    VM_HotMethodRecompilationEvent event = 
	      new VM_HotMethodRecompilationEvent(cm, ns);
	    if (VM_Controller.controllerInputQueue.prioritizedInsert(ns, event)){
	      if (VM.LogAOSEvents) {
		VM_AOSLogging.controllerNotifiedForHotness(cm, ns);
	      }
	    } else {
	      if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	    }
	  }
	
	  // Since I was hot enough, also consider my children.
	  insertHotMethodsInternal(index * 2, filterOptLevel, threshold);
	  insertHotMethodsInternal(index * 2 + 1, filterOptLevel, threshold);
	}
      }
    }
  }

  /**
   * Recursive implementation of collectHotOptNMethods. 
   * Exploit heap property. 
   * Constraint: threshold has been converted into a count value by my caller!
   *
   * @param index count array index
   * @param collect vector used to collect output.
   * @param threshold hotness value above which the method is considered
   *                  to be hot. (0.0 to 1.0)
   * @param optLevel target opt level to look for.
   */
  private void collectHotOptMethodsInternal(int index, 
					    Vector collect, 
					    double threshold, 
					    int optLevel) {
    if (index < nextIndex) {
      if (counts[index] > threshold) {
	int cmid = cmids[index];
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm == null) {			// obsolete and deleted
	  reset(cmid);				// free up this slot
	  // Visit new one in the slot
	  collectHotOptMethodsInternal(index, collect, threshold, optLevel);
	} else {
	  int compilerType = cm.getCompilerType();
	  if (compilerType == VM_CompiledMethod.OPT && 
	      ((VM_OptCompiledMethod)cm).getOptLevel() == optLevel) {
	    double ns = numCountsForModel(counts[index]);
	    collect.add(new VM_HotMethodRecompilationEvent(cm, ns));
	  }
	
	  // Since I was hot enough, also consider my children.
	  collectHotOptMethodsInternal(index * 2, collect, threshold, optLevel);
	  collectHotOptMethodsInternal(index * 2 + 1, collect, threshold, optLevel);
	}
      }
    }
  }

  /**
   * Either find the index that is already being used to hold the counts
   * for cmid or allocate a new entry in the heap for cmid.
   *
   * @param cmid compiled method id
   * @return count array index
   */
  private int findOrCreateHeapIdx(int cmid) {
    if (cmid >= map.length) {
      growHeapMap(cmid);
    }
    int index = map[cmid];
    if (index == 0) {
      // A new cmid. Allocate a heap entry for it.
      index = nextIndex++;
      if (index >= counts.length) {
	growHeap();
      }
      counts[index] = 0.0;
      cmids[index] = cmid;
      map[cmid] = index;
    }
    return index;
  }
    
    
    /**
   * Find the index that is already being used to hold the counts for cmid.
   * If no such index exists, return 0.
   *
   * @param cmid compiled method id
   */
  private int findHeapIdx(int cmid) {
    if (cmid < map.length) {
      int index = map[cmid];
      return index;
    } else {
      return 0;
    }
  }


  /**
   * Grow the map to be at least as large as would be required to map cmid
   *
   * @param cmid compiled method id
   */
  private void growHeapMap(int cmid) {
    int[] newMap = new int[Math.max((int)(map.length * 1.25), cmid+1)];
    for (int j=0; j<map.length; j++) {
      newMap[j] = map[j];
    }
    map = newMap;
  }

  /**
   * Increase the size of the count's backing arrays
   */
  private void growHeap() {
    double[] tmp1 = new double[counts.length * 2];
    for (int i=1; i< counts.length; i++) {
      tmp1[i] = counts[i];
    }
    counts = tmp1;
    int[] tmp2 = new int[cmids.length * 2];
    for (int i=1; i< cmids.length; i++) {
      tmp2[i] = cmids[i];
    }
    cmids = tmp2;
  }

  /**
   * Restore the heap property after increasing a count array entry's value
   *
   * @param index of count array entry
   */
  private void heapifyUp(int index) {
    int current = index;
    int parent = index / 2;
    while (parent > 0 && counts[parent] < counts[current]) {
      swap(parent, current);
      current = parent;
      parent = parent / 2;
    }
  }

  /**
   * Restore the heap property after decreasing a count array entry's value
   *
   * @param index of count array entry
   */
  private void heapifyDown(int index) {
    int current = index;
    int child1 = current * 2;
    while (child1<nextIndex) {
      int child2 = current * 2 + 1;
      int larger = 
	(child2<nextIndex && counts[child2]>counts[child1]) ? child2 : child1;
      if (counts[current] >= counts[larger]) break; // done
      swap(current, larger);
      current = larger;
      child1 = current * 2;
    }
  }

  /**
   * Swap the heap entries at i and j.
   *
   * @param i count array index
   * @param j count array index
   */
  private void swap(int i, int j) {
    double tmpS = counts[i];
    counts[i] = counts[j];
    counts[j] = tmpS;
    int tmpC = cmids[i];
    cmids[i] = cmids[j];
    cmids[j] = tmpC;
    map[cmids[i]] = i;
    map[cmids[j]] = j;
  }

  /**
   * Validate that internal fields are consistent.
   * This is very expensive.  Only use for debugging purposes.
   */
  private void validityCheck() {
    if (DEBUG && VM.VerifyAssertions) {
      // (1) Verify map and cmids are in synch
      for (int i=0; i<map.length; i++) {
	VM.assert(map[i] == 0 || cmids[map[i]] == i);
      }
      for (int i=1; i<nextIndex; i++) {
	VM.assert(map[cmids[i]] == i);
      }

      // Verify that heap property holds on data.
      for (int i=2; i<nextIndex; i++) {
	VM.assert(counts[i] <= counts[i/2]);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$
/**
 * Wrapper around a pair of parallel arrays:
 *  (1) an array of compiled method id's
 *  (2) an array of counts: how many times each compiled method id is counted
 *
 * @author Dave Grove 
 * @modified Peter Sweeney
 */
public final class VM_MethodCountSet {
  /**
   * array of compiled methods
   */
  VM_CompiledMethod[] cms;
  /**
   * array of counts
   */
  double[] counters;

  /**
   * Constructor
   *
   * @param _cms array of compiled method ids
   * @param _counters array of counters
   */
  VM_MethodCountSet(VM_CompiledMethod[] _cms, double[] _counters) {
    if (VM.VerifyAssertions) VM.assert(_cms.length == _counters.length);
    cms = _cms;
    counters= _counters;
  }

  /**
   * String representation of fields
   * 
   * @return string representation of compiled method id's and thier counts
   */
  public String toString() {
    String ans = "";
    for (int i=0; i<cms.length; i++) {
      ans += cms[i] + " = " + counters[i] + "\n";
    }
    return ans;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Vector;
import java.util.Enumeration;


/** 
 *
 * OPT_InsertInstructionCounters.java
 *
 * The following OPT phase inserts counters on all instructions in the
 * IR.  It maintians one counter for each operand type, so it output
 * how many loads were executed, how many int_add's etc.  This is
 * useful for debugging and assessing the accuracy of optimizations.
 *
 * Note: The counters are added at the end of HIR, so the counts will
 * NOT reflect any changes to the code that occur after HIR.
 * 
 * @author Matthew Arnold 
 *
 **/

class OPT_InsertInstructionCounters  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_INSTRUCTION_COUNTERS;
   }

   final String getName() { return "InsertInstructionCounters"; }

   /**
    * Insert a counter on every instruction, and group counts by
    * opcode type.  
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // the boot image, or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 ir.method.getDeclaringClass().isInBootImage() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     // Get the data object that handles the counters
     VM_StringEventCounterData data = 
       VM_AOSDatabase.instructionCounterData;

     // Create a vector of basic blocks up front because the blocks
     // are modified as we iterate below.
     Vector bbList = new Vector();
     for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	  bbe.hasMoreElements(); ) {
       OPT_BasicBlock bb = bbe.next();
       bbList.add(bb);
     }
     
     // Iterate through the basic blocks
     for (Enumeration e = bbList.elements();
	  e.hasMoreElements(); ) {
       OPT_BasicBlock bb = (OPT_BasicBlock) e.nextElement();
       
       // Add instructions to vector so enumeration doesn't mess
       // things up.  There is probably a better way to do this, but
       // it doesn't matter because this is a debugging phase.
       Vector iList = new Vector();
       OPT_Instruction i = bb.firstInstruction();
       while (i!=null && i!=bb.lastInstruction()) {
	 iList.add(i);
	 i = i.nextInstructionInCodeOrder();
       }
       
       // Iterate through all the instructions in this block.
       for (Enumeration instructions = iList.elements();
	    instructions.hasMoreElements();) {
	 i = (OPT_Instruction) instructions.nextElement();

	 // Skip dangerous instructions
	 if (i.operator() == LABEL ||
	     Prologue.conforms(i))
	   continue;
	 
	 if (i.isBranch() || 
	     i.operator() == RETURN) {
	   
	   // It's a branch, so you need to be careful how you insert the 
	   // counter.
	   OPT_Instruction prev = i.prevInstructionInCodeOrder();
	   
	   // If the instruction above this branch is also a branch,
	   // then we can't instruction as-is because a basic block
	   // must end with branches only.  Solve by splitting block.
	   if (prev.isBranch()) {
	     OPT_BasicBlock newBlock = bb.splitNodeWithLinksAt(prev,ir);
	     bb.recomputeNormalOut(ir);
	   }
	   
	   // Use the name of the operator as the name of the event
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(i.operator().toString());
	   
	   // Insert the new instruction into the code order
	   i.insertBefore(counterInst);      
	 }
	 else {
	   // It's a non-branching instruction.  Insert counter after
	   // the instruction.
	   
	   // Use the name of the operator as the name of the event
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(i.operator().toString());

	     i.insertBefore(counterInst);
	 }
       }
     }
   }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 *
 * OPT_InsertMethodInvocationCounter.java
 *
 * An OPT_Phase that inserts a method invocation counter on the first
 * basic block of the method.  It uses a
 * VM_InstrumentedEventCounterManager to obtain the space to put the
 * counters.
 *
 * Note: one counter data, (VM_MethodInvocationCounterData) is shared
 * across all methods, and is initialized at boot time.  This is
 * unlike other kinds of instrumentation (such as basic block
 * counters) where a separate data object is maintained for each
 * method.
 *
 * @author Matthew Arnold 
 *
 **/

class OPT_InsertMethodInvocationCounter  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {
   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_METHOD_COUNTERS_OPT;
   }

   final String getName() { return "InsertMethodInvocationCounters"; }

   /**
    * Insert basic block counters
    * 
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     OPT_BasicBlock firstBB = ir.cfg.entry();

     VM_MethodInvocationCounterData data = 
       VM_AOSDatabase.methodInvocationCounterData;

     int cmid = ir.compiledMethod.getId();

     // Create a dummy instruction that is later converted into an
     // increment of the appropriate VM_CounterArray element.
     OPT_Instruction c = data.createEventCounterInstruction(cmid);

     // Insert it at the beginnging of the basic block
     firstBB.prependInstructionRespectingPrologue(c);
   }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/** 
 *
 * OPT_InsertYieldPointCounters.java
 *
 *
 * An opt compiler phase that inserts yieldpoint counters.  Searches
 * for all yieldpoint instructions and inserts an increment after
 * them, using the VM_CounterArrayManager counter manager to implement
 * the counters.
 *
 * @author Matthew Arnold 
 */

class OPT_InsertYieldpointCounters  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     return options.INSERT_YIELDPOINT_COUNTERS;
   }

   final String getName() { return "InsertYieldpointCounters"; }

   /**
    * counters after all yieldpoint instructions
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {

     // Don't insert counters in uninterruptible methods, 
     // the boot image, or when instrumentation is disabled
     if (!ir.method.isInterruptible() ||
	 ir.method.getDeclaringClass().isInBootImage() ||
	 !VM_Instrumentation.instrumentationEnabled())
       return;

     VM_YieldpointCounterData data = 
       VM_AOSDatabase.yieldpointCounterData;

     if (OPT_InsertYieldpointCounters.DEBUG) {
       VM.sysWrite("OPT_InsertYieldpointCounters.perform() " + 
		   ir.method + "\n");
     }
     // For each yieldpoint, insert a counter.
     for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	  bbe.hasMoreElements(); ) {
       OPT_BasicBlock bb = bbe.next();

       if (OPT_InsertYieldpointCounters.DEBUG) {
	 VM.sysWrite("Considering basic block " + bb.toString() + "\n");
       	  bb.printExtended();
       }
       
       OPT_Instruction i =  bb.firstInstruction();
       while (i!=null && i!=bb.lastInstruction()) {

	 if (i.operator() == YIELDPOINT_PROLOGUE ||
             i.operator() == YIELDPOINT_EPILOGUE ||
             i.operator() == YIELDPOINT_BACKEDGE) {
 	   String prefix = yieldpointPrefix(i.operator());
	   double incrementValue = 1.0;
 	   if (bb == ir.cfg.entry()) {
 	     prefix = "METHOD ENTRY ";
	   }
	   else {
	     prefix = "BACKEDGE ";
	     incrementValue=1.0;  
	   }

	   // Create an instruction to increment the counter for this
	   // method.  By appending the prefix and method name, it
	   // maintains a separate counter for each method, and
	   // separates between method entry and backedges.
	   OPT_Instruction counterInst = data.
	     getCounterInstructionForEvent(prefix+ir.method.toString(),
					   incrementValue);

	   // Insert the new instruction into the code order
	   i.insertAfter(counterInst);      
	 }

	 i = i.nextInstructionInCodeOrder();
       }
     }
   }

  /**
   * Return a string based version of the passed yieldpoint operator
   * @param op the yieldpoint operator
   * @return a string based on the type of yieldpoint operator
   */
  private static String yieldpointPrefix(OPT_Operator op) {
    if (op == YIELDPOINT_PROLOGUE) return "Prologue";
    if (op == YIELDPOINT_EPILOGUE) return "Epilogue";
    if (op == YIELDPOINT_BACKEDGE) return "Backedge";
    return "ERROR";
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.util.Enumeration;
import java.util.Vector;
import instructionFormats.*;

/** 
 *  OPT_LowerInstrumentation
 *
 *  This phase takes converts "instrumentation instructions" that were
 *  inserted by previous instrumentation phases and "lowers" them,
 *  converting them to the actual instructions that perform the
 *  instrumentation.
 *
 *  @author Matthew Arnold
 *
 **/

class OPT_LowerInstrumentation  extends OPT_CompilerPhase
  implements OPT_Operators, VM_Constants, OPT_Constants {

   static final boolean DEBUG = false;

   final boolean shouldPerform(OPT_Options options) {
     if (options.INSERT_INSTRUCTION_COUNTERS ||
	 options.INSERT_METHOD_COUNTERS_OPT ||
	 options.INSERT_DEBUGGING_COUNTERS ||
	 options.INSERT_YIELDPOINT_COUNTERS)
     return true;
    return false;
   }

   final String getName() { return "LowerInstrumentation"; }

   /**
    * Finds all instrumented instructions and calls the appropriate code to 
    * convert it into the real sequence of instrumentation instructions.
    *
    * @param ir the governing IR
    */
   final public void perform(OPT_IR ir) {
     // Convert all instrumentation instructions into actual counter code
     lowerInstrumentation(ir);

     // TODO: For efficiency, should proably call OPT_Simple, or
     // branch optimizations or something.
   }
  
  /**
   * Actually perform the lowering
   *
    * @param ir the governing IR
   */ 
  static final void lowerInstrumentation(OPT_IR ir) {
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      //bb.printExtended();
    }
    
    Vector vector = new Vector();
    
    // Go through all instructions and find the instrumented ones.
    // We put them in a vector and expand them later because if we
    // expanded them on the fly we mess up the enumeration.
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      
      OPT_Instruction i = bb.firstInstruction();
      while (i!=null && i!=bb.lastInstruction()) {
	
	if (i.operator() == INSTRUMENTED_EVENT_COUNTER) {
	  vector.add(i);
	}
	i = i.nextInstructionInCodeOrder();
      }
    }
    
    // Now go through the instructions and "lower" them by calling
    // the counter manager to convert them into real instructions
    boolean didSomething = false;
    Enumeration e = vector.elements();
    while (e.hasMoreElements()) {
      OPT_Instruction i = (OPT_Instruction) e.nextElement();
      
      // Have the counter manager for this data convert this into the
      // actual counting code.  For now, we'll hard code the counter
      // manager.  Ideally it should be stored in the instruction,
      // (to allow multipe counter managers.  It would also make this
      // code independant of the adaptive system..)
      OPT_InstrumentedEventCounterManager counterManager = 
	VM_Instrumentation.eventCounterManager;
      
      counterManager.mutateOptEventCounterInstruction(i,ir);
      didSomething=true;
    }
    
    for (OPT_BasicBlockEnumeration bbe = ir.getBasicBlocks(); 
	 bbe.hasMoreElements(); ) {
      OPT_BasicBlock bb = bbe.next();
      //       bb.printExtended();
    }
  } // end of perform
  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * VM_AOSInstrumentationPlan.java
 *
 * Defines: 
 * class VM_AOSInstrumentationPlan
 *
 * An instance of this class is created for each method that is
 * instrumented by the adaptive system.  It serves as a place to put
 * information that is needed by the instrumentation phases.  Is is
 * different from an OPT_InstrumentationPlan because it contains
 * information that the non-adaptive opt-compiler can't see.
 *
 *
 * @author Matthew Arnold
 *
 **/

class VM_AOSInstrumentationPlan extends OPT_InstrumentationPlan
{

  
  /**
   * Construct empty plan, must setup manually
   **/ 
  VM_AOSInstrumentationPlan(VM_Method method) {
    this.method = method;
  }

  /**
   * Construct based on options
   **/ 
  VM_AOSInstrumentationPlan(VM_AOSOptions options, VM_Method method)
  {
    // If we want to collect method invocation counts.
    if (options.INSERT_METHOD_COUNTERS_OPT) {
    }
  }

  /** 
   * Initialize instrumentation by the opt compiler immediately before
   * compilation begins.
   **/
  void initInstrumentation(VM_Method method)
  {
  }

  /** 
   * Called after compilation is complete.  If instrumentation has
   * occured, perform some cleanup/finalization
   **/

  void finalizeInstrumentation(VM_Method method)
  {

  }

  /** The method that this plan is for */
  private VM_Method method;
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  This class is a separate thread whose job is to monitor a (priority)
 *  queue of compilation plans.  Whenever the queue is nonempty, this
 *  thread will pick the highest priority compilation plan from the queue
 *  and invoke the OPT compiler to perform the plan.
 *
 *  No intelligence is contained in this class.  All policy decisions are
 *  made by the controllerThread.
 *
 *  @author Michael Hind
 *  @author David Grove
 */
class VM_CompilationThread extends VM_Thread {

  /**
   * This is the main loop of the compilation thread. It's job is to 
   * remove controller plans from the compilation queue and perform
   * them.
   */
  public void run() {
    if (VM.LogAOSEvents) VM_AOSLogging.compilationThreadStarted();

    // Make a blocking call to deleteMin to get a plan and then execute it. 
    // Repeat...
    while (true) {
      VM_ControllerPlan plan = 
	(VM_ControllerPlan)VM_Controller.compilationQueue.deleteMin();
      recompile(plan);
    }
  }

  /**
   * This method will recompile the method designated by the passed 
   * controller plan.  It also 
   *  1) credits the samples associated with the old compiled method
   *     ID to the new method ID and clears the old value.
   *  2) clears inlining information
   *  3) updates the status of the controller plan
   * @param plan the controller plan to use for the recompilation
   */
  private void recompile(VM_ControllerPlan plan) {
    OPT_CompilationPlan cp = plan.getCompPlan();

    plan.setTimeInitiated(VM_Controller.controllerClock);
    if (VM.LogAOSEvents) VM_AOSLogging.recompilationStarted(cp); 

    if (cp.options.PRINT_METHOD) {
      VM.sysWrite("-oc:O"+cp.options.getOptLevel()+" \n");
    }
    
    // must hold classloader lock while compiling.
    // Update compilation thread timing information to prepare for new run.
    double now = VM_Time.now();
    cpuTotalTime += (now - cpuStartTime);
    cpuStartTime = now;
    double start = cpuTotalTime;

    // Compile the method.
    int newCMID = VM_RuntimeOptCompilerInfrastructure.recompileWithOpt(cp);

    // Update compilation thread timing information and compute time 
    // taken during this compilation.
    now = VM_Time.now();
    cpuTotalTime += (now - cpuStartTime);
    cpuStartTime = now;
    double end = cpuTotalTime;
    double compileTime = (end - start) * 1000.0; // Convert seconds to milliseconds.
      
    // transfer the samples from the old CMID to the new CMID.
    // scale the number of samples down by the expected speedup 
    // in the newly compiled method.
    int prevCMID = plan.getPrevCMID();
    double expectedSpeedup = plan.getExpectedSpeedup();
    double oldNumSamples = VM_Controller.methodSamples.getData(prevCMID);
    double newNumSamples = oldNumSamples / expectedSpeedup;
    VM_Controller.methodSamples.reset(prevCMID);
    if (newCMID > -1) {
      VM_Controller.methodSamples.setData(newCMID, newNumSamples);
    }

    // set the status of the plan accordingly
    if (newCMID != -1) {
      plan.setStatus(VM_ControllerPlan.COMPLETED);
      VM_AdaptiveInlining.clearNonInlinedEdges(prevCMID);
    } else {
      plan.setStatus(VM_ControllerPlan.ABORTED_COMPILATION_ERROR);
    }

    plan.setCMID(newCMID);
    plan.setCompilationCPUTime(compileTime);
    plan.setTimeCompleted(VM_Controller.controllerClock);
    if (VM.LogAOSEvents) {
      if (newCMID == -1) {
	VM_AOSLogging.recompilationAborted(cp);
      } else {
	VM_AOSLogging.recompilationCompleted(cp);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.*;
import java.util.*;

/**
 * This class codifies the cost/benefit properties of the various compilers
 * used in the adaptive optimization system.
 *
 * @author: Michael Hind
 */
class VM_CompilerDNA implements VM_Constants {

  private static final String[] compilerNames = {"Baseline", "Opt0", "Opt1", "Opt2"};
  final static int BASELINE = 0;
  final static int OPT0 = 1;
  final static int OPT1 = 2;
  final static int OPT2 = 3;

  /**
   *  The number of compilers available
   */
  private static int numCompilers;

  /**
   *  Average bytecodes compiled per millisec
   */
  //-#if RVM_FOR_AIX
  /*
   *  These numbers were from a shadow on June 28, 2002 on AIX/PPC (munchkin)
   */
  private static final double[] compilationRates = {431.78, 9.72, 4.04, 1.44};
  //-#else
  /*
   *  These numbers were from a shadow on July 7, 2002 on Linux/IA32 (turangalila)
   */
  private static final double[] compilationRates = {742.78, 15.68, 6.25, 2.52};
  //-#endif

  /**
   * What is the execution rate of each compiler normalized to the 1st compiler
   */
  //-#if RVM_FOR_AIX
  /*
   *  These numbers were from a shadow on June 28, 2002 on AIX/PPC (munchkin)
   */
  private static final double[] speedupRates = {1.00, 4.09, 5.27, 5.66};
  //-#else
  /*
   *  These numbers were from a shadow on July 7, 2002 on Linux/IA32 (turangalila)
   */
  private static final double[] speedupRates = {1.00, 3.55, 4.68, 4.75};
  //-#endif

  /**
   * Benefits of moving from one compilation level to another
   * USAGE NOTE: The data is layed out in a upper triangular matrix
   */
  private static double[][] benefitRatio;

  /**
   * Compile time ratio of one compilation level to another
   * For example, if compiler1 (say OPT1) compiles at 50 bc/msec
   * and compiler2 (say OPT2) compiles at 100 bc/msec, 
   *    compileTimeRatio[OPT1][OPT2] = 2
   * USAGE NOTE: The data is layed out in a upper triangular matrix 
   */
  private static double[][] compileTimeRatio;

  /**
   * This method returns the expected speedup from going from compiler1 to compiler2
   * @param compiler1
   * @param compiler2
   * @return the benefit ratio (speedup) of moving from compiler1 to compiler2
   */
  static public double getBenefitRatio(int compiler1, int compiler2) {
    return benefitRatio[compiler1][compiler2];
  }

  /**
   * What is the additional overhead (relative to compiler1 compile time)
   * of compile2 compile time.  For example, if compiler1 compiles at
   * 50 bc/msec and compiler2 compiles at 100 bc/msec, this method returns 2
   * @param compiler1 the compiler whose compile time we compare to
   * @param compiler2 the compiler's compile time we care about 
   * @return the additional overhead (relative to compiler1 compile time)
   * of compile2 compile time
   */
  static public double getCompileTimeRatio(int compiler1, int compiler2) {
    return compileTimeRatio[compiler1][compiler2];
  }

  /**
   * Returns the compilation rates of the baseline compiler in 
   *  bytecodes/millisecond
   * @return the compilation rates of the baseline compiler in 
   *   bytecodes/millisecond
   */
  static public double getBaselineCompilationRate() {
    return compilationRates[BASELINE];
  }

  /**
   * initialize static fields
   */
  static void init()  { 
    // check to see if the raw rates are specified during boot time
    if (VM_Controller.options.USE_COMPILER_DNA_FILE) {
      //  Read the DNA values from disk
      readDNA();
    }

    numCompilers = compilerNames.length;

    benefitRatio = new double[numCompilers][numCompilers];
    compileTimeRatio = new double[numCompilers][numCompilers];

    if (VM.LogAOSEvents) {
      for (int i=0; i < compilationRates.length; i++) {
	VM_AOSLogging.reportCompilationRate(i, compilationRates[i]);
      }
      for (int i=0; i < speedupRates.length; i++) {
	VM_AOSLogging.reportSpeedupRate(i, speedupRates[i]);
      }
    }

    // fill in the upper triangular matrices
    for (int prevCompiler = 0; 
	 prevCompiler < numCompilers; 
	 prevCompiler++) {

      benefitRatio[prevCompiler][prevCompiler] = 1.0;
      compileTimeRatio[prevCompiler][prevCompiler] = 1.0;

      for (int nextCompiler = prevCompiler+1; 
	   nextCompiler < numCompilers; 
	   nextCompiler++) {

	benefitRatio[prevCompiler][nextCompiler] = 
	  speedupRates[nextCompiler] / speedupRates[prevCompiler];

	// Since compilation rates are not relative to the 1st compiler
	//  we invert the division.
	compileTimeRatio[prevCompiler][nextCompiler] = 
	  compilationRates[prevCompiler] / compilationRates[nextCompiler];  

	if (VM.LogAOSEvents) {
	  VM_AOSLogging.reportBenefitRatio(
			   prevCompiler, nextCompiler,
			   benefitRatio[prevCompiler][nextCompiler]);

	  VM_AOSLogging.reportCompileTimeRatio(
			   prevCompiler, nextCompiler,
			   compileTimeRatio[prevCompiler][nextCompiler]);
	}
	
      }
    }
  }


  /** 
   * Read a serialized representation of the DNA info
   */
  static private void readDNA() {
    try {

      LineNumberReader in =
	new LineNumberReader(new FileReader(VM_Controller.options.COMPILER_DNA_FILE_NAME));

      // Expected Format
      //   CompilationRates  aaa.a  bbbb.b cccc.c dddd.d ....
      //   SpeedupRates      aaa.a  bbbb.b cccc.c dddd.d ....
      processOneLine(in, "CompilationRates", compilationRates);
      processOneLine(in, "SpeedupRates", speedupRates);
    }
    catch (Exception e) {
      e.printStackTrace();
      VM.sysFail("Failed to open controller DNA file");
    }
  }

  /**
   *  Helper method to read one line of the DNA file
   *  @param in the LineNumberReader object
   *  @param title the title string to look for
   *  @param valueHolder the array to hold the read values
   */
  static private void processOneLine(LineNumberReader in, String title,
				     double[] valueHolder) throws IOException {

    String s = in.readLine();
    if (VM.VerifyAssertions) VM.assert(s != null);
    
    // parse the string
    StringTokenizer parser = new StringTokenizer(s);
    
    // make sure the title matches
    String token = parser.nextToken();
    if (VM.VerifyAssertions) VM.assert(token.equals(title));
    
    // walk through the array, making sure we still have tokens
    for (int i=0;
	 parser.hasMoreTokens() && i < valueHolder.length;
	 i++) {

      // get the available token
      token = parser.nextToken();
      
      // convert token to a double
      valueHolder[i] = Double.valueOf(token).doubleValue();
    }
  }

  /**
   * returns the number of compilers 
   * @return the number of compilers 
   */
  static public int getNumberOfCompilers() {
    return numCompilers;
  }


  /**
   * A mapping from an Opt compiler number to the corresponding Opt level
   * @param compiler the compiler constant of interest
   * @return the Opt level that corresponds to the Opt compiler constant passed
   */
  public static int getOptLevel(int compiler) {
    switch (compiler) {
      case BASELINE: return -1;
      case OPT0: return 0;
      case OPT1: return 1;
      case OPT2: return 2;
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown compiler constant\n");
	return -99;
    }
  }

  /**
   * maps a compiler constant to a string
   * @param compiler
   * @return the string that represents the passed compiler constant
   */
  public static String getCompilerString(int compiler) {
    return compilerNames[compiler];
  }

  /**
   * maps opt levels to the compiler
   * @param optLevel opt level
   * @return the opt level that corresponds to the passed compiler constant
   */
  public static int getCompilerConstant(int optLevel) {
    switch (optLevel) {
      case 0: return OPT0;
      case 1: return OPT1;
      case 2: return OPT2;
      default:
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED, "Unknown Opt Level\n");
	return -99;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import instructionFormats.*;

/**
 * VM_CounterArrayManager.java
 *
 * An implementation of a OPT_InstrumentedEventCounterManager .  It
 * uses an unsynchronized two dimensional array of doubles to allocate
 * it's counters. (see OPT_InstrumentedEventCounterManager.java for a
 * description of a counter manager)
 * 
 * NOTE: Much of this class was stolen from VM_CounterArray.java, which
 * is now gone.
 *
 * @author Matthew Arnold
 *
 **/


final class VM_CounterArrayManager extends OPT_InstrumentedEventCounterManager
  implements OPT_Operators, OPT_Constants {

  static final boolean DEBUG=false;

  /**
   *  This method is called my a VM_ManagedData object to obtain space
   *  in the counter manager.  A handle or "ID" is returned for the
   *  data to identify it's counter space.
   *
   * @param countersNeeded The number of counters being requested 
   * @return The handle for this data's counter space.
   **/
  synchronized public int registerCounterSpace(int countersNeeded) {
    if (counterArrays.length == numCounterArrays) {
      expandCounterArrays();
    }

    // return the handle of the next available counter array
    int handle = numCounterArrays;

    // resize the appropriate counter array
    resizeCounterSpace(handle,countersNeeded);

    numCounterArrays++;

    return handle;
  }

  /**
   *  This method is called to change the number of counters needed by
   *  a particular data.
   *
   * @param handle  The handle describing which the data to be resized
   * @param countersNeeded The number of counters being requested 
   **/
  synchronized public void resizeCounterSpace(int handle, int countersNeeded) {
    // allocate the new array
    double[] temp = new double[countersNeeded];
    
    // transfer the old data to the new array
    if (counterArrays[handle] != null) {
      for (int i=0; i<counterArrays[handle].length; i++) {
	temp[i] = counterArrays[handle][i];
      }
    }
    
    // switch to the new counter array
    counterArrays[handle] = temp;
  }


  /**
   * Return the value of a particular counter
   *
   * @param handle The handle describing which the data to look in
   * @param index The relative index number of the counter
   * @return The value of the counter
   */
  public double getCounter(int handle, int index) {
    return counterArrays[handle][index];
  }

  /**
   * Set the value of a particular counter
   *
   * @param handle The handle describing which the data to look in
   * @param index The relative index number of the counter
   * @param value The new value of the counter
   */
  public void setCounter(int handle, int index, double value) {
    counterArrays[handle][index] = value;
  }


  /**
   * Create a place holder instruction to represent the counted event.
   *
   * @param counterHandle  The handle of the array for the method
   * @param index  Index within that array
   * @param incrementValue The value to add to the counter
   * @return The counter instruction
   **/
  public OPT_Instruction createEventCounterInstruction(int handle, int index,
						       double incrementValue) {

    // Doubles are annoying. They are too big to fit into the
    // instruction, so they must be loaded from the JTOC.  That means
    // we need to make sure the increment value is actually in the
    // JTOC.

    long l = Double.doubleToLongBits(incrementValue);
    int offset = VM_Statics.findOrCreateDoubleLiteral(l);

    // Now create the instruction to be returned.
    OPT_Instruction c = 
      InstrumentedCounter.create(INSTRUMENTED_EVENT_COUNTER, 
				 new OPT_IntConstantOperand(handle),
				 new OPT_IntConstantOperand(index),
				 new OPT_DoubleConstantOperand(incrementValue,
							       offset));
    c.bcIndex = INSTRUMENTATION_BCI;

    return c;
  }


  /**
   *  Take an event counter instruction and mutate it into IR
   *  instructions that will do the actual counting.
   *
   *  Precondition: IR is in LIR
   *
   * @param s The counter instruction to mutate
   * @param ir The governing IR
   **/
  public void mutateOptEventCounterInstruction(OPT_Instruction counterInst, 
					       OPT_IR ir) {
    if (VM.VerifyAssertions)
      VM.assert(InstrumentedCounter.conforms(counterInst));

    OPT_IntConstantOperand intOp =
      InstrumentedCounter.getData(counterInst);
    int handle = intOp.value;
    intOp = InstrumentedCounter.getIndex(counterInst);
    int index = intOp.value;

    // Get the base of array
    OPT_RegisterOperand counterArray =  OPT_ConvertToLowLevelIR.
      getStatic(counterInst, ir, VM_Entrypoints.counterArrayManagerCounterArraysField);

    // load counterArrays[handle]
    OPT_RegisterOperand array2 =
      InsertALoadOffset(counterInst,
                        ir, REF_ALOAD,
                        VM_Type.JavaLangObjectType,
                        counterArray, handle);
    OPT_ConvertToLowLevelIR.
      doArrayLoad(counterInst.prevInstructionInCodeOrder(), ir, INT_LOAD, 2);
                                                                               
    // load counterArrays[handle][index]
    OPT_RegisterOperand origVal =
      InsertALoadOffset(counterInst,
                        ir, DOUBLE_ALOAD,
                        VM_Type.DoubleType,
                        array2, index);
    OPT_ConvertToLowLevelIR.
      doArrayLoad(counterInst.prevInstructionInCodeOrder(),ir, DOUBLE_LOAD, 3);

    
    OPT_Operand incOperand = InstrumentedCounter.getIncrement(counterInst);
    // Insert increment instruction
    OPT_RegisterOperand newValue =
      OPT_ConvertToLowLevelIR.InsertBinary(counterInst, ir, DOUBLE_ADD,
                                           VM_Type.DoubleType, origVal,
                                           incOperand.copy());

    // Store it
    OPT_Instruction store = AStore.mutate(counterInst,DOUBLE_ASTORE,
                                         newValue, array2.copyU2D(),
                                         OPT_IRTools.I(index),null,null);
    OPT_ConvertToLowLevelIR.doArrayStore(store, ir, DOUBLE_STORE, 3);
                                       
  }

  /**
   * Insert array load off before s in the instruction stream.
   * @param s the instruction to insert before
   * @param ir the containing IR
   * @param operator the operator to insert
   * @param type the type of the result
   * @param reg2 the base to load from
   * @param offset the offset to load at
   * @return the result operand of the inserted instruction
   */
  static OPT_RegisterOperand InsertALoadOffset (OPT_Instruction s, OPT_IR ir,
                                                OPT_Operator operator,
                                                VM_Type type,
                                                OPT_Operand reg2,
                                                int offset){
    OPT_RegisterOperand regTarget = ir.regpool.makeTemp(type);
    OPT_Instruction s2 = ALoad.create(operator, regTarget, reg2,
                                      OPT_IRTools.I(offset),
                                      null,null);
    s.insertBack(s2);
    return  regTarget.copyD2U();
  }    


  /**
   * Still  under construction.
   */
  public void insertBaselineCounter()
  {
  }

  /**
   * decay counters
   *
   * @param handle, the identifier of the counter array to decay
   * @param rate, the rate at which to decay, i.e. a value of 2 will divide
   *                all values in half
   */
  static void decay(int handle, double rate) {
    int len = counterArrays[handle].length;
    for (int i=0; i<len; i++) {
      counterArrays[handle][i] /= rate;
    }
  }
 
  /** Implementation */
  static final int INITIAL_COUNT = 10;
  static final int INCREMENT = 10;
  static int numCounterArrays = 0;
  static double[][] counterArrays = new double[INITIAL_COUNT][];

  /**
   * increment the number of counter arrays
   */
  private static void expandCounterArrays() {
    // expand the number of counter arrays
    double[][] temp = new double[counterArrays.length*2][];

    // transfer the old counter arrays to the new storage
    for (int i=0; i<counterArrays.length; i++) {
      temp[i] = counterArrays[i];
    }
    counterArrays = temp;
  }

} // end of class



/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_CounterNameFunction.java
 *
 * @author Stephen Fink
 *
 * This interface defines a function that takes an integer and
 * returns a string corresponding to that integer.
 *
 **/

interface VM_CounterNameFunction {

   String getName(int key);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_Instrumentation.java
 *
 * This class is used to provide general functionality useful to
 * instrumenting methods.
 *
 * @author Matthew Arnold
 *
*/

final class VM_Instrumentation
{

  /**
   * A pointer to a OPT_InstrumentedEventCounterManager, (See
   * VM_InstrumentedEventCounterManager.java for the idea behind a
   * counter manager) There can be multiple managers in use at the
   * same time (for example, one per method)., but for now we just use
   * one for everything.
   **/
  public static OPT_InstrumentedEventCounterManager eventCounterManager;


  /**
   * Called at boot time
   **/
  static void boot(VM_AOSOptions options) 
  {
    
    // If the system may perform any instrumentation that uses managed
    // event counters, initialize a counter manager here.  
    if (options.INSERT_INSTRUCTION_COUNTERS ||
	options.INSERT_METHOD_COUNTERS_OPT ||
	options.INSERT_YIELDPOINT_COUNTERS ||
	options.INSERT_DEBUGGING_COUNTERS) {
      eventCounterManager = new VM_CounterArrayManager();
    }

    // If inserting method counters, initialize the counter space for
    // the invocation counters, using the eventCounterManager from above.
    if (options.INSERT_METHOD_COUNTERS_OPT) {
      VM_AOSDatabase.methodInvocationCounterData = 
	new VM_MethodInvocationCounterData(eventCounterManager);

      // Method Counters have only one array of counters for the whole
      // program, so initialize it here. Make it automitacally double
      // in size when needed.
      VM_AOSDatabase.methodInvocationCounterData.
	automaticallyGrowCounters(true);

      // Report at end
      VM_RuntimeMeasurements.
	registerReportableObject(VM_AOSDatabase.methodInvocationCounterData);
    }

    /**
     * If collecting yieldpoint counts, initialize the 
     * data here.
     **/
    if (options.INSERT_YIELDPOINT_COUNTERS) {
      // Create it here, because we need only one array of numbers,
      // not one per method.
      VM_AOSDatabase.yieldpointCounterData = 
	new VM_YieldpointCounterData(eventCounterManager);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.yieldpointCounterData);

    }

    /**
     * If collecting instruction counts, initialize the 
     * data here.
     **/
    if (options.INSERT_INSTRUCTION_COUNTERS) {
      VM_AOSDatabase.instructionCounterData = 
	new VM_StringEventCounterData(eventCounterManager,
				      "Instruction Counter");
      VM_AOSDatabase.instructionCounterData.automaticallyGrowCounters(true);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.instructionCounterData);
    }

    /**
     * If collecting instruction counts, initialize the 
     * data here.
     **/
    if (options.INSERT_DEBUGGING_COUNTERS) {
      VM_AOSDatabase.debuggingCounterData = 
	new VM_StringEventCounterData(eventCounterManager,
				      "Debugging Counters");
      VM_AOSDatabase.debuggingCounterData.automaticallyGrowCounters(true);

      // We want to report everything at the end.
      VM_RuntimeMeasurements.
       	registerReportableObject(VM_AOSDatabase.debuggingCounterData);
    }

  }

  /**
   * Calling this routine causes all future compilations not to insert
   * instrumentation, regardless of what the options say.  Used during
   * system shutdown.  Note, this method will not stop instrumentation
   * in currently compiled methods from executing.
   * 
   */
  static void disableInstrumentation() {
    instrumentationEnabled=false;
  }

  /**
   * Enable instrumentations, so that future compilations will not
   * perform any instrumentation.
   * 
   */
  static void enableInstrumentation() {
    instrumentationEnabled=true;
  }

  /**
   * Is it currently O.K. to compile a method and insert instrumentation?
   */
  static boolean instrumentationEnabled() {
    return instrumentationEnabled;
  }
  static private boolean instrumentationEnabled=true;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import java.util.Vector;

/**
 * VM_ManagedCounterData
 *
 * This class provides the basic functionality for instrumented data
 * that use counters allocated from a VM_InstrumentedEventCounterManager.  
 * It provides the basic interface to access counters,  forwarding
 * those requests to the counter manager.
 *
 * @author Matthew Arnold
 *
**/

class VM_ManagedCounterData
{

  static final boolean DEBUG=false;

  /**
   *  Constructor
   *
   * @param counterManager The counterManager that will provide the counter space
   */
  VM_ManagedCounterData(OPT_InstrumentedEventCounterManager counterManager) 
  {
    // Basic block instrumentation is performed using a common counter
    // allocation for the whole method.  It requests that space here.
    this.counterManager = counterManager;
  }


  /**
   * This method must be called before creating any counters for this
   * data.  It registers this data with the counter manager and gets a
   * "handle" that is coded into the counter instruction.  If you need
   * to change the number of counters in this data AFTER you have
   * created counters, use void
   * VM_ManagerdCounterData.resizeCounters(int) instead.
   *
   * @param countersNeeded How many counters are needed by this data
   */
  public void initializeCounters(int countersNeeded)
  {
    // Confirm that this method is called only once.  Once a handle is
    // assigned, it should not be changed.  Use resizeCounters(int) to
    // change the size of the data.
    if (VM.VerifyAssertions)
      VM.assert(handle == -1);

    this.numCounters = countersNeeded;
    // Register  this many counters with the counter manager
    this.handle = counterManager.registerCounterSpace(countersNeeded);
  }

  /** 
   * Tell the data to automatically expand the counters if there is a
   * request to count an event that is greater than the current size.
   *
   * @param autoGrow Whether the counters should grow automatically. 
   */
  public void automaticallyGrowCounters(boolean autoGrow){

    final int INITIAL_COUNTER_SIZE = 20;

    automaticallyGrowCounters = autoGrow;
    if (automaticallyGrowCounters)
      initializeCounters(INITIAL_COUNTER_SIZE);
  }

  /**
   * Used to reset the number of counters for this data
   * 
   * @param countersNeeded The number of counters needed
   */
  public void resizeCounters(int countersNeeded)
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    
    counterManager.resizeCounterSpace(this.getHandle(),countersNeeded);
    numCounters = countersNeeded;
  }

  /**
   * Return the count for the given (relative) index
   * 
   * @param counterNumber The event number within the data
   * @return The count associated with this counter
   */
  public double getCounter(int counterNumber)
  {
    // Confirm that counters have been initialized 
    //  (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    return counterManager.getCounter(this.getHandle(), counterNumber);
  }

  /**
   * Set the count for the given index
   * 
   * @param counterNumber The event number within the data
   * @param value The new value of the counter
   */
  public void setCounter(int counterNumber, double value)
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions) {
      VM.assert(handle != -1);
    }
    if (counterNumber >= getNumCounters()) {
      if (automaticallyGrowCounters) {
	while (counterNumber >= getNumCounters()) 
	  resizeCounters(getNumCounters()*2);
      }
      else {
	VM.assert(false);
      }
    }

    counterManager.setCounter(this.getHandle(), counterNumber, value);
  }


  /**
   * Return the number of counters currently allocated for this data
   *
   *  @return the number of counters 
   */
  public int getNumCounters()
  {
    // Confirm that counters have been initialized (using initializeCounters(int))
    if (VM.VerifyAssertions)
      VM.assert(handle != -1);
    return numCounters;
  }

  /**
   * Counter Managers give id's that identify the counter space they
   * have given to each data. This method returns that ID. 
   *
   * @return The handle given to this data object by the counter manager.
   **/
  public int getHandle()
  {
    return handle;
  }
  
  /**
   * Return the counter manager for this data.
   * 
   * @return the counter manager object
   */
  public OPT_InstrumentedEventCounterManager getCounterManager()
  {
    return counterManager;
  }

  /**
   * Create a place holder instruction to represent an increment of a
   * particular counted event.  Simply forwards the request to the
   * counter manager.
   *
   * @param counterNumber The number of the counter to increment 
   * @return The instruction that will update the given counter
   */
  OPT_Instruction createEventCounterInstruction(int counterNumber) {
    return createEventCounterInstruction(counterNumber,1.0);
  }

  /**
   * Create a place holder instruction to represent the counted event.
   * Simply forwards the request to the counter manager.
   *
   * @param counterNumber The number of the counter to increment 
   * @param incrementValue The value to add to the given counter
   * @return The instruction that will update the given counter
   */
  OPT_Instruction createEventCounterInstruction(int counterNumber,
						double incrementValue)
  {
    // Confirm that counters have been initialized 
    if (VM.VerifyAssertions) {
      VM.assert(handle != -1);
    }

    // If we automatically growing counters, see if we need to.
    if (counterNumber >= numCounters) {
      if (automaticallyGrowCounters) {
	while (counterNumber >= numCounters)
	  resizeCounters(getNumCounters()*2);
      }
      else{
	// Should we put a warning here?? Not sure.  
      }
    }
    return getCounterManager().createEventCounterInstruction(getHandle(),
							     counterNumber,
							     incrementValue);
  }

   /**
    *  This method prints the (sorted) nonzero elements a counter
    *  array.
    *
    * @param handle identifier of the counter array to print
    * @param f a function that gets the "name" for each counter
    */
   final void report(VM_CounterNameFunction f) {
    double sum = 0;
    Vector vec = new Vector();

    // set up a vector of non-zero counts
    for (int i=0; i < getNumCounters(); i++) {
      double count = getCounter(i);
      if (count > 0.0) {
	sum += count;
	String s = f.getName(i);
	vec.addElement(new Counter(s,count));
      }
    }

    // sort the vector in decreasing order
    sort(vec);

    // print
    for (Enumeration e = vec.elements(); e.hasMoreElements(); ) {
       Counter c = (Counter)e.nextElement();
       String s = c.name;
       double count = c.count;
       double percent = (100 * count) / sum;
       VM.sysWrite(count + "/" + sum + " = " + percent + "% " + s + "\n");
    }
  }

  /**
   * Sort a Vector<Counter> by decreasing count.
   * (code borrowed from InstructionSampler.java)
   * Shell sort
   * Reference: "The C Programming Language", Kernighan & Ritchie, p. 116
   */
  private void sort(Vector v) {
     int n = v.size();
     for (int gap = n/2; gap > 0; gap /= 2) {
       for (int i = gap; i<n; ++i) {
          for (int j = i-gap; j >=0; j-=gap) {
	     double a = ((Counter)v.elementAt(j)).count;
	     double b = ((Counter)v.elementAt(j+gap)).count;
	     if (a>=b) break;
	     swap(v,j,j+gap);
	  }
       }
     }
  }
  
  // Interchange vec[i] with vec[j]
  private void swap (Vector vec, int i, int j) {
     Object t = vec.elementAt(i);
     vec.setElementAt(vec.elementAt(j),i);
     vec.setElementAt(t,j);
  }

 

  /* -----   Implementation   ---- */

  /**
   * How many counters are needed by this data?
   **/
  protected int numCounters=0;

  /**
   *  When a data object is registered with a counter manager, it is
   *  given an id, which is stored here.
   **/
   
   protected int handle=-1;

  /**
   * Basic block instrumentation stores its counters using an
   * abstracted counter allocation technique (a counterManager)
   **/
  protected OPT_InstrumentedEventCounterManager counterManager=null;
  
  protected boolean automaticallyGrowCounters = false;

 /**
  * Auxiliary class
  */
 class Counter {
     String name;
     double count;
     Counter(String s, double c) { name=s; count = c; }
 }

}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_MethodInvocationCounterData.java
 *
 * An instance of this class is used to store method counters.  It is
 * initialized at startup, and instrumentation phase
 * OPT_InsertMethodInvocationCounter.java inserts instrumentation that
 * writes into this data.
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;

final class VM_MethodInvocationCounterData extends VM_ManagedCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_MethodInvocationCounterData(OPT_InstrumentedEventCounterManager manager)
  {
    // Call superclass constructor
    super(manager);
  }

  /**
   *  Part of VM_Reportable interface.  Called on system exit
   **/
  public void report()
  {
    super.report(new VM_MethodNameFunction());
  }

  /**
   *  Part of VM_Reportable interface
   **/
  public void reset()  
  { 
    VM.assert(false, "TODO: implement reset for VM_BasicBlockCounterDatabase"); 
  }

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_MethodNameFunction.java
 *
 * @author Stephen Fink
 *
 * This class takes a compiled method id and returns a string
 * representation of the method name.
 *
 **/

class VM_MethodNameFunction implements VM_CounterNameFunction {

   /**
    * @param key the compiled method id of a method
    */
   public String getName(int key) {
     VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(key);
     if (cm == null) {
       return "OBSOLETE";
     } else {
       return cm.getMethod().toString();
     }
   }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_StringEventCounterData.java
 * 
 * A generic data object that maps strings to counters.  The key
 * method is "OPT_Instruction getCounterInstructionForEvent(String)"
 * which, given a string, returns a counter instruction that
 * increments the corresponding counter for that string.
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;
import java.util.Enumeration;

class VM_StringEventCounterData extends VM_ManagedCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_StringEventCounterData(OPT_InstrumentedEventCounterManager manager,
			    String name)
  {
    // Call superclass constructor
    super(manager);
    
    dataName = name;
  }

  /**
   * Given a string, find or create the counter associated and return
   * and instruction to increment that counter.  
   *
   * @param event The name of the event
   * @return An instruction to increment the count associated with the event.
   */
  OPT_Instruction getCounterInstructionForEvent(String event) {
    return getCounterInstructionForEvent(event,1.0);
  }

  /**
   * Given a string, find or create the counter associated and return
   * and instruction to increment that counter.  
   *
   * @param event The name of the event
   * @param incrementValue The value to add to counter
   * @return An instruction that will update the count associated with the event.
   *
   */
  OPT_Instruction getCounterInstructionForEvent(String event, 
						double incrementValue) {

    // Get (or create) the counter for this string and return it.
    int counterIdx = getOrCreateCounterIndexForString(event);

    return createEventCounterInstruction(counterIdx,incrementValue);
  }

  /**
   * Convert a double to string with maximum precision.
   * @param num double to convert
   */
  protected static String doubleToString(double num) {
    long whole = (long)num;
    if (whole == Long.MAX_VALUE || whole == Long.MIN_VALUE)
      return Double.toString(whole);
    double fract = Math.abs(num - (double)whole);
    String res = Long.toString(whole);
    if (fract != 0.0) {
      String f2s = Double.toString(fract + 1.0);
      res += f2s.substring(1);
    }
    return res;
  }

  /**
   * Part of VM_Reportable interface
   * Print a report at the end of execution
   */
  public void report()
  {
    // Turn off future instrumentation to avoid hanging during 
    // iteration
    VM_Instrumentation.disableInstrumentation();

    VM.sysWrite("Printing " + dataName + ":\n");
    VM.sysWrite("--------------------------------------------------\n");
    double total=0;
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();

      int counterIdx = getCounterIndexForString(stringName);
      double counterVal = getCounter(counterIdx);
      VM.sysWrite(doubleToString(counterVal) + " " + stringName + "\n");
      total += counterVal;
    }
    VM.sysWrite("Total: " + doubleToString(total) + "\n");
  }

  /**
   * For a given string, return the number of the counter associated
   * with this string.  If this string doesn't already have a counter, 
   * reserve one. 
   *
   * @param str The string for which you want the counter number
   * @return The counter number for this string

   */
  public int getOrCreateCounterIndexForString(String str) {

    int counterIdx = getCounterIndexForString(str);
    if (counterIdx == -1) {
      // Use new counter
      counterIdx = ++ eventNumber;
      // remember it, and return it
      stringToCounterMap.put(str,new Integer(eventNumber));
    }

    return counterIdx;
  }


  /**
   * For a given string, return the number of the counter associated
   * with this string.  Ideally this number would be completely hidden
   * from the outside world, but for efficiency it is made public.
   *
   * @param str The string for which you want the counter number
   * @return The counter number for this string, or -1 if the string has no 
             counter associated with it. 
   */
  public int getCounterIndexForString(String str) {

    int counter = -1;
    Integer counterNum = (Integer) stringToCounterMap.get(str);
    if (counterNum != null) 
      counter = counterNum.intValue();

    return counter;
  }

  /**
   *  Part of VM_Reportable interface
   **/
  public void reset() { 
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();
      int counterIdx = getCounterIndexForString(stringName);
      setCounter(counterIdx, 0.0);
    }
  }

 /** 
  *  Map strings to a counter location
  */
  protected  Hashtable stringToCounterMap = new Hashtable();

  /**
   * A string description of this data;
   */
  String dataName= "";

  /** 
   * Used to keep track of how many counters have been used so far.
   */ 
  int eventNumber=-1;

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * VM_YieldpointCounterData.java
 *
 * An extension of VM_StringEventCounterData so that the printing can
 * be specialized for yieldpoints.  Otherwise, the functionality is
 * identical.
 *
 * @see VM_StringEventCounterData.java
 *
 * @author Matthew Arnold
 *
**/

import java.util.Hashtable;
import java.util.Enumeration;

final class VM_YieldpointCounterData extends VM_StringEventCounterData
  implements VM_Reportable 
{

  static final boolean DEBUG=false;


  /**
   *  Constructor
   *
   * @manager The manager that will provide the counter space
   **/
  VM_YieldpointCounterData(OPT_InstrumentedEventCounterManager manager)
  {
    // Call superclass constructor
    super(manager,"Yieldpoint Counter");

    automaticallyGrowCounters(true);
  }

  /**
   *  Called at end when data should dump it's contents.
   */
  public void report()
  {
    // Turn off future instrumentation so that the data structures do
    // not change while we are iterating over them
    VM_Instrumentation.disableInstrumentation();

    VM.sysWrite("Printing " + dataName + ":\n");
    VM.sysWrite("--------------------------------------------------\n");
    double total=0;
    double methodEntryTotal=0;
    double backedgeTotal=0;
    for (Enumeration e = stringToCounterMap.keys();
	 e.hasMoreElements();) {
      String stringName = (String) e.nextElement();

      Integer counterNum = (Integer) stringToCounterMap.get(stringName);
      double count = getCounter(counterNum.intValue());

      VM.sysWrite(count + " " + stringName + "\n");
      total += count;
      
      // If it's a method entry event
      if (stringName.indexOf("METHOD ENTRY") != -1)
	methodEntryTotal += count;
      
      if (stringName.indexOf("BACKEDGE") != -1)
	backedgeTotal += count;

    }
    VM.sysWrite("Total backedges: " + backedgeTotal + "\n");
    VM.sysWrite("Method Entry Total: " + methodEntryTotal + "\n");
    VM.sysWrite("Total Yieldpoints: " + total + "\n");
  }

} // end of class


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  This interface defines the decay method.  Implementors are 
 *  eligible for decay if they register with the 
 *  VM_RuntimeMeasurements class.
 *
 *  @author Michael Hind
 */

interface VM_Decayable {

  /**
   *  Called periodically when it is time to decay runtime mesaurment data
   */
  public void decay();

}





/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Interface for all reportable objects that are managed by the runtime
 * measurements.
 *
 * @author Peter Sweeney
 */

interface VM_Reportable { 
  /**
   * generate a report
   */
  void report(); 
  /**
   * reset (clear) data set being gathered
   */
  void reset();  
}







/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;
import java.util.Enumeration;

/**
 * RuntimeMeasurements manages listeners, decayable objects, and 
 * reportable objects.
 *
 * A listener is installed by an organizer, and activated at thread
 * switch time by VM_Thread.  Depending on the update method that the
 * listener supports, it can be either a method, context, or a null 
 * listener.  Currently we have different registries for different 
 * listeners.  An alternative design is to have one register with where 
 * entries are tagged.
 *
 * A decayable object implements the VM_Decayable interface.
 * Anyone can register a decayable object,
 * The VM_DecayOrganizer periodically decays all objects that have 
 * been registers.
 *
 * A reportable object implements the Reportable interface, and 
 * is typically registered and used by the instrumentation subsystem. 
 * A reportReporableObject can be reset, and reported.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 * @modified Peter Sweeney
 */
abstract class VM_RuntimeMeasurements {

  /**
   * listeners for methods
   */
  static VM_MethodListener[] methodListeners = new VM_MethodListener[0];
  /**
   * listeners for contexts
   */
  static VM_ContextListener[] contextListeners = new VM_ContextListener[0];
  /**
   * listeners for nulls
   */
  static VM_NullListener[] nullListeners = new VM_NullListener[0];

  private static int activateMethodListeners_count = 0;
  private static int activateContextListeners_count = 0;
  private static int activateNullListeners_count = 0;
  /**
   * Install a method listener
   * @param s method listener to be installed
   */
  static synchronized void installMethodListener(VM_MethodListener s) { 
    int numListeners = methodListeners.length;
    VM_MethodListener[] tmp = new VM_MethodListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = methodListeners[i];
    }
    tmp[numListeners] = s;
    methodListeners = tmp;
  }

  /**
   * Install a context listener
   * @param s context listener to be installed
   */
  static synchronized void installContextListener(VM_ContextListener s) { 
    int numListeners = contextListeners.length;
    VM_ContextListener[] tmp = new VM_ContextListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = contextListeners[i];
    }
    tmp[numListeners] = s;
    contextListeners = tmp;
  }

  /**
   * Install a null listener
   * @param s null listener to be installed
   */
  static synchronized void installNullListener(VM_NullListener s) { 
    int numListeners = nullListeners.length;
    VM_NullListener[] tmp = new VM_NullListener[numListeners+1];
    for (int i=0; i<numListeners; i++) {
      tmp[i] = nullListeners[i];
    }
    tmp[numListeners] = s;
    nullListeners = tmp;
  }

  /**
   * Determine if at least one active method listener exists
   * @return true if at least one active method listener
   */
  static boolean hasMethodListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = methodListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }
  /**
   * Determine if at least one active context listener exists
   * @return true if at least one active context listener
   */
  static boolean hasContextListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = contextListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }
  /**
   * Determine if at least one active null listener exists
   * @return true if at least one active null listener
   */
  static boolean hasNullListener() throws VM_PragmaUninterruptible { 
    VM_Listener[] tmp = nullListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) return true;
    }
    return false;
  }

  /**
   * Notify RuntimeMeasurements that method listeners should be activated
   *
   * @param cmid a compiled method id
   * @param callerCmid a compiled method id for the caller, -1 if none
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *           EPILOGUE?
   */
  static void activateMethodListeners(int cmid, int callerCmid, int whereFrom) throws VM_PragmaUninterruptible {
    activateMethodListeners_count++;     
    VM_MethodListener[] tmp = methodListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
 	tmp[i].update(cmid, callerCmid, whereFrom);
      }
    }
  }

  /**
   * Notify RuntimeMeasurements that context listeners should be activated.
   *
   * @param sfp		a pointer to a stack frame
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  static void activateContextListeners(VM_Address sfp, int whereFrom) throws VM_PragmaUninterruptible {
    activateContextListeners_count++;     
    VM_ContextListener[] tmp = contextListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
 	tmp[i].update(sfp, whereFrom);
      }
    }
  }

  /**
   * Notify RuntimeMeasurements that null listeners should be activated.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  static void activateNullListeners(int whereFrom) throws VM_PragmaUninterruptible {
    activateNullListeners_count++;     
    VM_NullListener[] tmp = nullListeners; // side-step dangerous race condition
    for (int i=0; i<tmp.length; i++) {
      if (tmp[i].isActive()) {
	tmp[i].update(whereFrom);
      }
    }
  }

  /**
   * The currently registered decayable objects
   */
  static Vector decayObjects = new Vector();

  /**
   * Counts the number of decay events
   */
  static int decayEventCounter = 0;

  /**
   *  Register an object that should be decayed.
   *  The passed object will have its decay method called when the
   *  decaying thread decides it is time for the system to decay.
   */
  static void registerDecayableObject(VM_Decayable obj) {
    decayObjects.add(obj);
  }

  /**
   * Decay all registered decayable objects.
   */
  static void decayDecayableObjects() {
    decayEventCounter++;
    if (VM.LogAOSEvents) VM_AOSLogging.decayingCounters();
    
    for (Enumeration e=decayObjects.elements(); e.hasMoreElements();) {
      VM_Decayable obj = (VM_Decayable) e.nextElement();
      obj.decay();
    }
  }

  /**
   * The currently registered reportable objects
   */
  static Vector reportObjects = new Vector();

  /** 
   * Register an object that wants to have its report method called
   * whenever VM_RuntimeMeasurements.report is called
   */
  static void registerReportableObject(VM_Reportable obj) {
    reportObjects.add(obj);
  }

  /**
   * Reset to all registered reportable objects
   */
  public static void resetReportableObjects() {
    for (Enumeration e=reportObjects.elements(); e.hasMoreElements();) {
      VM_Reportable obj = (VM_Reportable)e.nextElement();
      obj.reset();
    }
  }    
  /**
   * Report to all registered reportable objects
   */
  private static void reportReportableObjects() {
    for (Enumeration e=reportObjects.elements(); e.hasMoreElements();) {
      VM_Reportable obj = (VM_Reportable)e.nextElement();
      obj.report();
    }
  }    
  
  /**
   * Report the current state of runtime measurements
   */
  static void report() {
    reportReportableObjects();
    
    if (VM.LogAOSEvents) {
      VM_AOSLogging.listenerStatistics(activateMethodListeners_count,
				       activateContextListeners_count,
				       activateNullListeners_count);
      VM_AOSLogging.decayStatistics(decayEventCounter);

      for (int i = 0, n = VM_Scheduler.threads.length; i < n; i++) {
	VM_Thread t = VM_Scheduler.threads[i];
	if (t != null) {
	  VM_AOSLogging.threadExiting(t);
	}
      }
    }
  }

  /**
   * Stop the runtime measurement subsystem
   */
  static synchronized void stop() {
    methodListeners = new VM_MethodListener[0];
    contextListeners = new VM_ContextListener[0];
    nullListeners = new VM_NullListener[0];
  }
    
  /**
   * Called from VM_Thread.terminate.
   */
  public static void monitorThreadExit() {
    if (VM.LogAOSEvents) VM_AOSLogging.threadExiting(VM_Thread.getCurrentThread());
  }
  
  /**
   * Called when the VM is booting
   */
  static void boot() { }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener that accumulates samples into a VM_MethodCountData.
 *
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
final class VM_AccumulatingMethodListener extends VM_MethodListener 
  implements VM_Uninterruptible {

  /**
   * @param sampleSize the initial sampleSize for the listener
   * @param notifyOrganizer should the listener notify an organizer
   *                    when its threshold is reached?
   * @param data the VM_MethodCountData object to use to accumulate the samples
   */
  public VM_AccumulatingMethodListener(int sampleSize, 
				       boolean notifyOrganizer,
				       VM_MethodCountData data) {
    super(sampleSize, notifyOrganizer);
    this.data = data;
  }

  /** 
   * Update data with the current samples and generate a cumulative report.
   * Reset ourselves, since the sample buffer has been drained into data.
   */
  public void report() throws VM_PragmaInterruptible {
    processSamples();
    reset();
    VM.sysWrite("\nMethod sampler report");
    data.report();
  }

  /**
   * Process the samples.
   */
  public void processSamples() {
    data.update(samples, getNumSamples());
  }

  /**
   * @return the method data being updated by the listener.
   */
  public final VM_MethodCountData getData() { return data; }

  // The cummulative method sample data associated with this listener
  protected VM_MethodCountData data;
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener that relies on its organizer alone to process
 * the sample data.
 *
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
final class VM_BasicMethodListener extends VM_MethodListener 
  implements VM_Uninterruptible {

  /**
   * @param sampleSize the initial sampleSize for the listener
   */
  public VM_BasicMethodListener(int sampleSize) {
    super(sampleSize, true);
  }

  /** 
   * Nothing to report.
   */
  public void report() {
    VM.sysWrite("BasicMethodListener has nothing to report!");
  }


  /**
   * We rely on the organizer to process the samples, therefore
   * nothing to do.
   */
  public void processSamples() { }

} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This object that is invoked when online measurement information must 
 * be collected.
 *
 * @author Peter Sweeney
 * @date   2 June 2000
 */
abstract class VM_ContextListener extends VM_Listener implements VM_Uninterruptible {

  /**
   * Entry point when listener is awoken.
   *
   * @param sfp  pointer to stack frame where call stack should start 
   *             to be examined.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *            EPILOGUE?
   */
  abstract public void update(VM_Address sfp, int whereFrom);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * A VM_EdgeListener defines a listener 
 * that computes a call graph edge from the call stack.
 * After a parameterized number of edges are collected, 
 * it notifies its organizer that the threshold is reached.
 *
 * Defines update's interface.
 *
 * VM_EdgeListener communicates with an organizer through a 
 * integer array, buffer.  Each time this listener is called, 
 * it places a triple of integers in buffer that correspond to
 * the callee, caller, and machine code offset of the call site
 *
 * @author Peter Sweeney
 * @author Michael Hind
 * @date   May 18, 2000
 *
 */

class VM_EdgeListener extends VM_ContextListener 
  implements VM_Uninterruptible, VM_StackframeLayoutConstants {

  protected static final boolean DEBUG = false;

  /**
   * buffer provides the communication channel between the listener and the
   * organizer.
   * The buffer contains an array of triples <callee, caller, address> where
   * the caller and callee are VM_CompiledMethodID's.
   * Initially, buffer contains zeros.  The listener adds triples.
   * When the listener hits the end of the buffer, notify the organizer.
   * (Alternatively, could make the buffer circular.)
   */
  private int[] buffer;

  /**
   * the index in the buffer of the next free triple
   */
  private int nextIndex;

  /**
   * Number of samples to be taken before issuing callback to controller 
   */
  private int desiredSamples;

  /**
   *  Number of samples taken so far
   */
  protected int samplesTaken = 0;

  /**
   * Number of times update is called
   */
  protected int calledUpdate = 0;

  /**
   * Constructor
   */
   public VM_EdgeListener() {
      buffer         = null;
      desiredSamples = 0;
  }

  /**
   * returns the number of times that update is called
   * @returns the number of times that update is called
   */
  int getTimesUpdateCalled() { 
    return calledUpdate; 
  }

  /**
   * Setup buffer and buffer size.  
   * This method must be called before any data can be written to
   * the buffer.
   *
   * @param buffer the allocated buffer to contain the samples, size should
   *      be a muliple of 3
   */
  public void setBuffer(int[] buffer) {
    // ensure buffer is proper length
    if (VM.VerifyAssertions) {
      VM.assert(buffer.length%3 == 0);
    }

    if (DEBUG) {
      VM.sysWrite("VM_EdgeListener.setBuffer("+buffer.length+"): enter\n");     
    }

    this.buffer    = buffer;
    desiredSamples = buffer.length / 3;
    resetBuffer();
  }

  /**
   * This method is called when a call stack edge needs to be 
   * sampled.  Expect the sfp argument to point to the stack frame that
   * contains the target of the edge to be sampled.
   *
   * RESTRICTION: the execution time of this method is time critical 
   * (we don't want another thread switch to occur inside of it).  
   * Therefore, this method simple stuffs integers into buffer.
   *
   * RESTRICTION: while GC is disabled, do not preform any operation that can
   * allocate space!
   *
   * @param sfp  a pointer to the stack frame that corresponds to the callee of
   *             the call graph edge that is to be sampled.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  public final void update(VM_Address sfp, int whereFrom) {
    if (DEBUG) {
      VM.sysWrite("VM_EdgeListener.update("+sfp.toInt()+","+whereFrom+
		  "): enter "+samplesTaken+"\n");     
    }

    calledUpdate++;

    // don't take a sample for back edge yield points
    if (whereFrom == VM_Thread.BACKEDGE) return; 

    if (buffer == null) {
      VM.sysWrite("***Error: VM_EdgeListener.update() called "+
		  "before setBuffer() is called!\n");
      VM.sysExit(-1);
    }

    int calleeCMID    = 0;
    int callerCMID    = 0;
    VM_Address returnAddress = VM_Address.zero();

    // While GC is disabled, don't do string concatenation!
    VM_Processor.getCurrentProcessor().disableThreadSwitching();
     
    if (VM_Magic.getMemoryWord(sfp) == STACKFRAME_SENTINAL_FP) {
      if (DEBUG) VM.sysWrite(" Walking off end of stack!\n");	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    calleeCMID = VM_Magic.getCompiledMethodID(sfp);
    if (calleeCMID == INVISIBLE_METHOD_ID) {
      if (DEBUG){
	VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	VM.sysWrite(calleeCMID); VM.sysWrite("\n");       
      } 
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    if (VM_CompiledMethods.getCompiledMethod(calleeCMID) == null) {
      VM.sysWrite("VM_EdgeListener:update: Found a callee cmid (");
      VM.sysWrite(calleeCMID, false);
      VM.sysWrite(") with a null compiled method. ");
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      VM.sysFail("Exiting VM");
    }

    returnAddress = VM_Magic.getReturnAddress(sfp); // return address in caller
    sfp = VM_Magic.getCallerFramePointer(sfp);      // caller's frame pointer
    if(VM_Magic.getMemoryWord(sfp) == STACKFRAME_SENTINAL_FP) {
      if (DEBUG) VM.sysWrite(" Walking off end of stack\n");	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }
    callerCMID = VM_Magic.getCompiledMethodID(sfp);
    if (callerCMID == INVISIBLE_METHOD_ID) {
      if (DEBUG) { 
	VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	VM.sysWrite(callerCMID); VM.sysWrite("\n"); 
      }	
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      return;
    }

    if (VM_CompiledMethods.getCompiledMethod(callerCMID) == null) {
      VM.sysWrite("VM_EdgeListener:update: Found a caller cmid (");
      VM.sysWrite(calleeCMID, false);
      VM.sysWrite(") with a null compiled method, exiting");
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
      VM.sysFail("Exiting VM");
    }

    // store the offset of the return address from the beginning of the 
    // instruction
    VM_CompiledMethod callerCM = VM_CompiledMethods.getCompiledMethod(callerCMID);
    VM_Address beginningOfMachineCode = VM_Magic.objectAsAddress(callerCM.getInstructions());
    int callSite = returnAddress.diff(beginningOfMachineCode);

    if (DEBUG){ 
      VM.sysWrite("  <");VM.sysWrite(calleeCMID);VM.sysWrite(",");
      VM.sysWrite(callerCMID);VM.sysWrite(",");VM.sysWrite(returnAddress);
      VM.sysWrite(">\n");
    }

    if (DEBUG) { 
      VM_Address fp = VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());
      int compiledMethodID = 0;
      for(int i=1; i<6; i++) {
	if (VM_Magic.getMemoryWord(fp) == STACKFRAME_SENTINAL_FP) {
	  VM.sysWrite(" Walking off end of stack\n"); break;
	}
	compiledMethodID = VM_Magic.getCompiledMethodID(fp);
	if (compiledMethodID == INVISIBLE_METHOD_ID) {
	  VM.sysWrite(" INVISIBLE_METHOD_ID  (assembler code) ");
	  VM.sysWrite(compiledMethodID); VM.sysWrite("\n");     continue;
	}
	VM.sysWrite("   Stack frame ");VM.sysWrite(i);VM.sysWrite(": ");
	VM.sysWrite(compiledMethodID);
	if (true) {
	  VM_CompiledMethod compiledMethod = null;
	  compiledMethod = VM_CompiledMethods.getCompiledMethod(compiledMethodID);
	  VM_Method  method = compiledMethod.getMethod();
	  VM.sysWrite(method);
	}
	VM.sysWrite("\n");
	fp = VM_Magic.getCallerFramePointer(fp);
      }
    }

    // done with stack inspection, re-enable GC
    VM_Processor.getCurrentProcessor().enableThreadSwitching();

    // Try to get 3 buffer slots and update nextIndex appropriately
    int idx = VM_Synchronization.fetchAndAdd(this, 
					     VM_Entrypoints.edgeListenerNextIndexField.getOffset(),
					     3);

    // Ensure that we got slots for our sample, if we don't (because another
    // thread is racing with us) we'll just ignore this sample
    if (idx < buffer.length) {
      buffer[idx+0] = calleeCMID;
      buffer[idx+1] = callerCMID;
      buffer[idx+2] = callSite;

      // Determine which sample we just completed.
      // fetchAndAdd returns the value before the increment, add one to
      // determine which sample we were
      int sampleNumber = 
	VM_Synchronization.fetchAndAdd(this, 
				       VM_Entrypoints.edgeListenerSamplesTakenField.getOffset(),
				       1) + 1;

      // If we are the last sample, we need to take action
      if (sampleNumber == desiredSamples) {
	thresholdReached();
      } 
    } 
  }

  /** 
   *  report() noop
   */
  public final void report() {}

  /**
   * Called when threshold is reached.
   */
  public void thresholdReached() {
    if (DEBUG) VM.sysWrite("VM_EdgeListener.thresholdReached(): enter\n");

    passivate();
    
    // Notify the organizer thread
    notifyOrganizer();
    if (DEBUG) VM.sysWrite("VM_EdgeListener.thresholdReached(): exit\n");
  }

  /**
   * Reset (in preparation of starting a new sampling window)
   */
  public void reset() {
     if (DEBUG) VM.sysWrite("VM_EdgeListener.reset(): enter\n");     
     samplesTaken = 0;
     calledUpdate = 0;
     resetBuffer();
  }

  /**
   *  Resets the buffer
   */
  private void resetBuffer() {
    for (int i=0; i<buffer.length; i++) {
      buffer[i] = 0;
    }
    nextIndex = 0;
  }
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_Listener object is invoked when online measurement information 
 * needs to be collected.
 *
 * This class does not define the update() method, the call back method from
 * the runtime when a sample should be taken.
 * The expectation is that immediately derived classes define an interface to
 * the update() method from which classes may be further derived.
 *
 * CONSTRAINTS:
 * Classes that are derived from VM_Listener 
 * must inherit directly from VM_Uninterruptible to ensure that they
 * are not interrupted by a thread switch.  
 * Since thread switching is disabled, listeners are 
 * expected to complete execution quickly, and therefore, 
 * must do a minimal amount of work.
 *
 * @author Peter Sweeney
 * @modified Dave Grove
 */
abstract class VM_Listener implements VM_Uninterruptible {

  /**
   * Entry point to dump what has been collected.
   */
  abstract public void report() throws VM_PragmaInterruptible;

  /**
   * Is the listener currently active (interested in getting "update" calls)
   */
  public final boolean isActive() { return active; }

  /**
   * Transition listener to active state
   */
  public final void activate() { active = true; }
  
  /**
   * Transition listener to passive state 
   */
  public final void passivate() { active = false; }

  /**
   * Organizer associated with this listener.
   */
  public final void setOrganizer(VM_Organizer organizer) {
    this.organizer = organizer;
  }

  /**
   * Wake up the organizer thread (if any) associated with the listener
   */
  public final void notifyOrganizer() {
    if (organizer != null) {
      synchronized(organizer) {
	try {
	  organizer.notify();
	} catch (Exception e) {
	  e.printStackTrace();
	}
      }
    }
  }

  // Is the listener active or passive?
  private boolean active = false;
  // My organizer.
  private VM_Organizer organizer;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_MethodListener defines a listener to collect method invocation samples.
 *
 * Samples are collected in a buffer.  
 * When sampleSize samples have been collected, thresholdReached is called.
 *  
 * Defines update's interface to be a compiled method identifier, CMID.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind
 * @author Peter Sweeney
 */
abstract class VM_MethodListener extends VM_Listener 
  implements VM_Uninterruptible {

  /**
   * Number of samples to be processed before calling thresholdReached
   */
  protected int sampleSize;  
  
  /**
   * Next available index in the sample array
   */
  protected int nextIndex;
  
  /**
   * Number of samples taken so far
   */
  protected int numSamples;
  
  /**
   * The sample buffer
   * Key Invariant: samples.length >= sampleSize
   */
  protected int[] samples;
  
  /**
   * Is this listener supposed to notify its organizer when thresholdReached?
   */
  protected boolean notifyOrganizer;
  

  /**
   * @param sampleSize the initial sampleSize for the listener
   * @param notifyOrganizer should the listener notify an organizer
   *                    when its threshold is reached?
   */
  public VM_MethodListener(int sampleSize, boolean notifyOrganizer) {
    this.sampleSize = sampleSize;
    this.notifyOrganizer = notifyOrganizer;
    samples = new int[sampleSize];
  }


  /** 
   * This method is called when a time based sample occurs.
   * It parameter "cmid" represents the compiled method ID of the method
   * which was executing at the time of the sample.  This method
   * bumps the counter and checks whether a threshold is reached.
   * <p>
   * NOTE: There can be multiple threads executing this method at the 
   *       same time. We attempt to ensure that the resulting race conditions
   *       are safely handled, but make no guarentee that every sample is
   *       actually recorded. We do try to make it somewhat likely that 
   *       thresholdReached is called exactly once when numSamples reaches 
   *       sampleSize, but there are still no guarentees.
   *
   * @param cmid the compiled method ID to update
   * @param callerCmid a compiled method id for the caller, -1 if none
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *         EPILOGUE?
   */
  public final void update(int cmid, int callerCmid, int whereFrom) {
    if (VM.UseEpilogueYieldPoints) {
      // Use epilogue yieldpoints.  We increment one sample
      // for every yieldpoint.  On a prologue, we count the caller.
      // On backedges and epilogues, we count the current method.
      if (whereFrom == VM_Thread.PROLOGUE) {
	// Before getting a sample index, make sure we have something to insert
	if (callerCmid != -1) {
	  int sampleNumber = recordSample(callerCmid);
	  checkSampleSize(sampleNumber);
        } // nothing to insert
      } else { 
        // loop backedge or epilogue.  
	int sampleNumber = recordSample(cmid);
	checkSampleSize(sampleNumber);
      }
    } else {
      // Original scheme: No epilogue yieldpoints.  We increment two samples
      // for every yieldpoint.  On a prologue, we count both the caller
      // and callee.  On backedges, we count the current method twice.
      if (whereFrom == VM_Thread.PROLOGUE) {
        // Increment both for this method and the caller
	int sampleNumber = recordSample(cmid);
	if (callerCmid != -1) {
	  sampleNumber = recordSample(callerCmid);
	}
	checkSampleSize(sampleNumber);
      } else { 
        // loop backedge.  We're only called once, so need to take
        // two samples to avoid penalizing methods with loops.
	int sampleNumber = recordSample(cmid);
	sampleNumber = recordSample(cmid);
	checkSampleSize(sampleNumber);
      }
    }
  }

  /**
   * This method records a sample containing the CMID (compiled method ID)
   * passed.  Since multiple threads may be taking samples concurrently,
   * we use fetchAndAdd to distribute indices into the buffer AND to record
   * when a sample is taken.  (Thread 1 may get an earlier index, but complete
   * the insertion after Thread 2.)
   *
   * @param CMID compiled method ID to record
   * @return the sample number that was inserted or -1, if we were unlucky
   *
   * NOTE: if we are unlucky that we didn't get to insert the sample (because
   *  there was a slot when we started, but another thread stole it from us)
   *  we simply don't insert the sample.
   */
  private int recordSample(int CMID) {  
    // reserved the next available slot
    int idx = VM_Synchronization.fetchAndAdd(this, VM_Entrypoints.methodListenerNextIndexField.getOffset(), 1);
    // make sure it is valid
    if (idx < sampleSize) {
      samples[idx] = CMID;

      // sampleNumber has the value before we incremented, add one to 
      // determine which sample we were
      int sampleNumber =  VM_Synchronization.fetchAndAdd(this, VM_Entrypoints.methodListenerNumSamplesField.getOffset(), 1) + 1;
      return sampleNumber;
    } else {
      return -1;
    }
  }

  /**
   * This method checks to see if the parameter passed was the last sample
   * If so, it passivates this listener and reports that the threshold has
   * been reached.
   *
   * @param sampleNumber the sample number was just taken 
   *      valid samples will be in [1..sampleSize]
   *      invalid sample will be -1 
   */
  private void checkSampleSize(int sampleNumber) {
    if (sampleNumber == sampleSize) { 
      passivate();
      thresholdReached();
    }
  }

  /**
   * When the threshold is reached either notify our organizer or
   * handle it ourselves by processing the samples, resetting, and 
   * activating ourselves again.
   */
  public void thresholdReached() {
    int numSamples = getNumSamples();
    for (int i=0; i<numSamples; i++) {
      int id = samples[i];
    }

    if (notifyOrganizer) {
      notifyOrganizer();
    } else {
      processSamples();
      reset();
      activate();
    }
  }


  /**
   * process the buffer of samples
   */
  public abstract void processSamples(); 


  /**
   * Reset the buffer to prepare to take more samples.
   */
  public void reset() {
    nextIndex = 0;
    numSamples = 0;
  }


  /**
   * updates the sample size for this listener
   * doesn't worry about any samples in the current buffer
   * @param newSampleSize the new sample size value to use, 
   */
  public final void setSampleSize(int newSampleSize) throws VM_PragmaInterruptible {
    sampleSize = newSampleSize; 
    if (sampleSize > samples.length) {
      samples = new int[newSampleSize];
      nextIndex = 0;
      numSamples = 0;
    }
  }

  /**
   * @return the current sample size/threshold value
   */
  public final int getSampleSize() { return numSamples; }

  /**
   * @return the buffer of samples
   */
  public final int[] getSamples() { return samples; }

  /**
   * @return how many samples have been taken
   */
  public final int getSamplesTaken() { return numSamples; }

  /**
   * @return how many samples in the array returned by getSamples
   *         are valid (min(getSamplesTaken(), getSampleSize())).
   */
  public final int getNumSamples() {
    int x = getSamplesTaken();
    int y = getSampleSize();
    return (x < y) ? x : y;
  }
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A VM_NullListener is an object that is invoked when
 * online measurement information must be collected.
 *
 * Defines update's interface.
 *
 * @author Peter Sweeney
 * @date   2 June 2000
 */

abstract class VM_NullListener extends VM_Listener implements VM_Uninterruptible {
  /**
   * Entry point when listener is awoken.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *             EPILOGUE?
   */
  abstract public void update(int whereFrom);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * A VM_YieldCounterListener samples yield points, and
 * notifies an Organizer when a threshold is reached.
 *
 * In effect, this class provides a way to "wake up" an infrequent
 * service periodically.
 *
 * @author Stephen Fink
 * @modified Peter Sweeney
 */
class VM_YieldCounterListener extends VM_NullListener implements VM_Uninterruptible {

  /**
   * Constructor
   *
   * @param yieldThreshold  the threshold of when to call organizer
   */
  public VM_YieldCounterListener(int yieldThreshold) {
    this.yieldThreshold = yieldThreshold;
  }

  /** 
   * This method is called when its time to record that a 
   * yield point has occurred.
   * @param whereFrom Was this a yieldpoint in a PROLOGUE, BACKEDGE, or
   *             EPILOGUE?
   */
  public void update(int whereFrom) {
     nYields++;
     if (nYields >= yieldThreshold) {
	synchronized(this) {
	  // now that we're in a critical section, double-check that
	  // another thread has not yet processed this threshold reached event.
     	  if (nYields >= yieldThreshold) {
	    passivate();
            notifyOrganizer();
            totalYields += nYields;
            nYields = 0;
	  }
        }
     }
  }

  public void report() {
     VM.sysWriteln("Yield points counted: ", totalYields);
  }

  private int yieldThreshold;
  private int nYields = 0;
  private int totalYields = 0;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.*;

/**
 * An organizer of call graph edge information that is used for 
 * adaptive inlining.
 *
 * VM_AIByEdgeOrganizer communicates with an edge listener through a 
 * integer array, denoted buffer.  When this organizer is woken up
 * via threshold reached, it processes the sequence of triples 
 * that are contained in buffer.
 * After the buffer is processed, the organizer checks to see
 * if any methods compiled at VM_Controller.options.MAX_OPT_LEVEL
 * should be recompiled due to inlining opportunities.
 * 
 * EXPECTATION: buffer is filled all the way up with triples.
 * 
 * Potential problems: The system does not retain new edges between
 * organizer runs.  There could be a race condition where a
 * ??? where what?? --dave
 * 
 * @author Peter Sweeney 
 * @author Dave Grove
 * @modified Stephen Fink
 * @modified Michael Hind
 * @modified Matthew Arnold
 */
class VM_AIByEdgeOrganizer extends VM_Organizer implements VM_Decayable {

  private final static boolean DEBUG = false;

  /*
   * buffer provides the communication channel between the edge listener
   * and the organizer.
   * The buffer contains an array of triples <callee, caller, address> where
   * the caller and callee are VM_CompiledMethodID's, and address identifies
   * the call site.
   * bufferSize is the number of triples contained in buffer.
   * The edge listener adds triples.  
   * At some point the listener deregisters itself and notifies the organizer 
   * by calling thresholdReached().
   */
  private int[] buffer;
  private int   bufferSize;
  private int   numberOfBufferTriples;

  /**
   *	Representation of call graph.
   */
  private VM_PartialCallGraph callGraph;
  /*
   *  The edge listener
   */
  VM_EdgeListener edgeListener;

  /**
   * Constructor
   */
  VM_AIByEdgeOrganizer(VM_EdgeListener edgeListener) {
     if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.<init>(): enter\n");     
     this.edgeListener = edgeListener;
     edgeListener.setOrganizer(this);
  }

  /**
   */
  public void decay() {
     VM_AdaptiveInlining.decay();
  }

  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.initialize(): enter\n");

    if (VM.LogAOSEvents) VM_AOSLogging.AIByEdgeOrganizerThreadStarted();

    numberOfBufferTriples = VM_Controller.options.AI_SAMPLE_SIZE;

    bufferSize = numberOfBufferTriples * 3;
    buffer     = new int[bufferSize];

    edgeListener.setBuffer(buffer); 

    // allocate internal data structures.
    callGraph   = VM_AdaptiveInlining.getPartialCallGraph();

    // Install and activate the edge listener
    VM_RuntimeMeasurements.installContextListener(edgeListener);
    edgeListener.activate();

    // register as decayable
    VM_RuntimeMeasurements.registerDecayableObject(this);

    if (DEBUG) VM.sysWrite("VM_AIByEdgeOrganizer.initialize(): exit\n");
  }

  /**
   * Method that is called when the sampling threshold is reached.
   * Process contents of buffer: 
   *    add call graph edges and increment their weights.
   */
  void thresholdReached() {
    if(DEBUG)
      VM.sysWrite("VM_AIByEdgeOrganizer.thresholdReached(): enter and reregister.\n");

    VM_AdaptiveInlining.incrementNumYieldPoints(edgeListener.
                getTimesUpdateCalled());
    for (int i=0; i<bufferSize; i=i+3) {
      int calleeCMID = buffer[i+0];
      VM_CompiledMethod compiledMethod   = VM_CompiledMethods.getCompiledMethod(calleeCMID);
      if (compiledMethod == null) continue;
      VM_Method callee = compiledMethod.getMethod();
      int callerCMID = buffer[i+1];
      compiledMethod   = VM_CompiledMethods.getCompiledMethod(callerCMID);
      if (compiledMethod == null) continue;
      VM_Method stackFrameCaller = compiledMethod.getMethod();
       
      int MCOffset = buffer[i+2];
      int bytecodeIndex = -1;
      VM_Method caller = null;

      switch (compiledMethod.getCompilerType()) {
      case VM_CompiledMethod.TRAP:
      case VM_CompiledMethod.JNI:
	if (DEBUG) VM.sysWrite("Skipping sample with TRAP/JNI caller");
	continue;
      case VM_CompiledMethod.BASELINE:
	{
	  VM_BaselineCompiledMethod baseCompiledMethod = 
	    (VM_BaselineCompiledMethod)compiledMethod;
	  // note: the following call expects the offset in INSTRUCTIONS!
	  bytecodeIndex = baseCompiledMethod.findBytecodeIndexForInstruction
	    (MCOffset>>VM.LG_INSTRUCTION_WIDTH);
	  caller = stackFrameCaller;
	}
	break;
      case VM_CompiledMethod.OPT:
	{
	  VM_OptCompiledMethod optCompiledMethod = (VM_OptCompiledMethod)compiledMethod;
	  VM_OptMachineCodeMap mc_map = optCompiledMethod.getMCMap();
	  try {
	    bytecodeIndex = mc_map.getBytecodeIndexForMCOffset(MCOffset);
	    if (bytecodeIndex == -1) {
	      // this can happen we we sample a call 
	      // to a runtimeSerivce routine. 
	      // We aren't setup to inline such methods anyways, 
	      // so skip the sample.
	      if (DEBUG) VM.sysWrite("  *** SKIP SAMPLE "+
				     stackFrameCaller+"@"+compiledMethod+
				     " at MC offset "+MCOffset+
				     " calling "+callee+
				     " due to invalid bytecodeIndex\n");
	      continue; // skip sample.
	    }
	  } catch (java.lang.ArrayIndexOutOfBoundsException e) {
	    VM.sysWrite("  ***ERROR: getBytecodeIndexForMCOffset("+MCOffset
			+") ArrayIndexOutOfBounds!\n");
	    e.printStackTrace(); caller = stackFrameCaller;
	    continue;  // skip sample
	  } catch (OPT_OptimizingCompilerException e) {
	    VM.sysWrite("***Error: SKIP SAMPLE: can't find bytecode index in OPT compiled "+
			stackFrameCaller+"@"+compiledMethod+" at MC offset "+MCOffset+"!\n");
	    continue;  // skip sample
	  }
	  
	  try {
	    caller = mc_map.getMethodForMCOffset(MCOffset);
	  } catch (java.lang.ArrayIndexOutOfBoundsException e) {
	    VM.sysWrite("  ***ERROR: getMethodForMCOffset("
			+MCOffset+") ArrayIndexOutOfBounds!\n");
	    e.printStackTrace(); caller = stackFrameCaller;
	    continue;
	  } catch (OPT_OptimizingCompilerException e) {
	    VM.sysWrite("***Error: SKIP SAMPLE: can't find caller in OPT compiled "+
			stackFrameCaller+"@"+compiledMethod+" at MC offset "
			+MCOffset+"!\n");
	    continue;  // skip sample
	  }

	  if (caller == null) {
	    VM.sysWrite("  ***ERROR: getMethodForMCOffset("+
			MCOffset+") returned null!\n");
	    caller = stackFrameCaller;
	    continue;  // skip sample
	  }
	}
	break;
      }

      // increment the call graph edge, adding it if needed
      callGraph.incrementEdge(caller, bytecodeIndex, callee);
    }

    // If using an offline inline plan, don't recompute anything, and don't 
    // notify the controller.
    if (!VM_Controller.options.USE_OFFLINE_INLINE_PLAN) {
      // force a recomputation of the current state of hot edges
      Vector vectorOfTriples = VM_AdaptiveInlining.recomputeHotEdges();
      
      if(DEBUG) {
	VM.sysWrite("\nNew edges found:\n");
	for (int i=0; i<vectorOfTriples.size(); i++) {
	  VM_CallSiteTriple triple = (VM_CallSiteTriple)
                vectorOfTriples.elementAt(i);
	  VM.sysWrite((i+1)+": "+triple.toString()+"\n");
	}
      }
      
      VM_MethodCountSet HM_data = 
	VM_Controller.methodSamples.collectHotMethods(VM_Controller.options.MAX_OPT_LEVEL,
						      VM_Controller.options.AI_METHOD_HOTNESS_THRESHOLD);
      if (VM.LogAOSEvents) 
	VM_AOSLogging.AIorganizerFoundHotMethods(HM_data.cms.length);
      
      findMethodsToRecompile(vectorOfTriples, HM_data);
    }
    if(DEBUG) callGraph.dump();

    // Clear listener and activate it again.
    edgeListener.reset();
    edgeListener.activate();

    if(DEBUG)VM.sysWrite("VM_AIByEdgeOrganizer.thresholdReached(): exit\n");
  }  

   /*
    * Given a vector of new edges and a set of methods, determine if 
    * a method in the set contains a call site corresponding to an edge 
    * in the vector and that call site has not been inlined.
    *
    * @param vectorOfTriples	new edges that are hot in call graph
    * @param hotMethodSet	methods that are hot and compiled at max opt level.
    *
    */
   private void findMethodsToRecompile(Vector vectorOfTriples,
				       VM_MethodCountSet hotMethodSet) {
     if (DEBUG) VM.sysWrite("\nVM_AIByEdgeOrganizer.findMethodsToRecompile() "
			    + hotMethodSet.cms.length+"\n");

     if (vectorOfTriples.isEmpty() || hotMethodSet.cms.length == 0) {
       if (DEBUG) VM.sysWrite("  return early\n");
       return;
     }

     // Consider each hot max opt level method
     for (int i=0; i<hotMethodSet.cms.length; i++) {
       VM_CompiledMethod hotMethod = hotMethodSet.cms[i];
       int cmid                    = hotMethod.getId();
       double numSamples           = hotMethodSet.counters[i];
       VM_OptMachineCodeMap mcMap  = ((VM_OptCompiledMethod)hotMethod).getMCMap();
       double edgeHotness          = 0.0;

       if (DEBUG) VM.sysWrite(" Process hot method: "+
			      hotMethod.getMethod()+" with "+numSamples+"\n");
       
       if (!hotMethod.getMethod().isInterruptible()) {
	 // This is required because a very small subset of uninterruptible methods
	 // need to have their code in a non-moving heap. 
	 // For now, we use this simple, but conservative test to avoid trouble.
	 if (DEBUG) VM.sysWrite("Not selecting uninterruptible method for recompilation "+hotMethod.getMethod());
	 continue;
       }

       // For each edge, see if the callsite is present, 
       // but the callee is absent in hotMethod.
       for (Enumeration triples = vectorOfTriples.elements(); 
                triples.hasMoreElements(); ) {
	 VM_CallSiteTriple triple = (VM_CallSiteTriple)triples.nextElement();
	 if (!VM_AdaptiveInlining.knownNonInlinedEdge(cmid, triple)) {
	   VM_Method caller = triple.getCaller();
	   int bytecodeIndex= triple.getBytecodeIndex();
	   VM_Method callee = triple.getCallee();
	   if (DEBUG) VM.sysWrite("   Edge candidate "+triple+"\n");

	   try {
	     if (mcMap.callsitePresent(caller, bytecodeIndex)) {
	       if (DEBUG) VM.sysWrite(" FOUND EDGE: "+triple+
				      " that can be inlined into "
				      +hotMethod.getMethod()+"\n");
	       edgeHotness += triple.getWeight();
	       if (VM.LogAOSEvents) 
		 VM_AOSLogging.inliningOpportunityDetected(hotMethod, 
							   numSamples, 
							   triple);
	     }
	   }
	   catch (Throwable e){
	     VM.sysWrite("ERROR in adaptive system! Exception caught!\n");
	     VM.sysWrite(" AI Organizer considering edge "+triple+
			 " to be inlined into "
			   +hotMethod.getMethod()+"\n");
	     e.printStackTrace();
	   }
	 }
       }

       // Notify the controller if we found a candidate edge in hotMethod
       if (edgeHotness > 0.0001) {
	 edgeHotness   /= VM_AdaptiveInlining.getNumYieldPoints();
	 double boost   = 
	   1.0 + (VM_Controller.options.MAX_EXPECTED_AI_BOOST * edgeHotness);
	 VM_AINewHotEdgeEvent event = 
	   new VM_AINewHotEdgeEvent(hotMethod, numSamples, boost);
	 if (!VM_Controller.controllerInputQueue.prioritizedInsert(numSamples, 
								   event)) {
	   if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	 } else {
	   if (VM.LogAOSEvents) {
	     VM_AOSLogging.controllerNotifiedForInlining(hotMethod, 
							 numSamples, 
							 boost);
	   }
	 }
       }
     }
     
     if (DEBUG) 
       VM.sysWrite("\nVM_AIByEdgeOrganizer.findMethodsToRecompile() exit\n\n");
   }

  /**
   * Last opportunity to say something.
   * Dump call graph and edge weights.
   */
  public void report() {
     if (VM_Controller.options.FINAL_REPORT_LEVEL >= 2) {
       VM.sysWrite("\n\nVM_AIByEdgeOrganizer.report()\n");
       VM.sysWrite(" callGraph dump\n");
       callGraph.dump();
     }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer that periodically decays runtime counters
 *
 * @author Michael Hind
 **/
final class VM_DecayOrganizer extends VM_Organizer {

  // Yield point listener: will wake up this organizer periodically
  private VM_YieldCounterListener listener;

  /**
   * @param listener the associated listener
   */
  VM_DecayOrganizer(VM_YieldCounterListener listener) {
    this.listener   = listener;
    listener.setOrganizer(this);
  }

  /**
   * Initialization: install and activate our listener.
   */
  public void initialize() {
    VM_RuntimeMeasurements.installNullListener(listener);
    listener.activate();
  }

  /**
   * Method that is called when the sampling threshold is reached
   * We decay the decayable objects and activate the listener again.
   */
  void thresholdReached() {
    VM_RuntimeMeasurements.decayDecayableObjects();
    listener.activate();
  }  
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer for method listener information. 
 * <p>
 * This organizer is designed to work well with non-decayed 
 * cumulative method samples.  The basic idea is that each time 
 * the sampling threshold is reached we update the accumulated method 
 * sample data with the new data and then notify the controller of all 
 * methods that were sampled in the current window.
 * 
 * @author Dave Grove
 */
final class VM_MethodSampleOrganizer extends VM_Organizer {

  /**
   *  Filter out all opt-compiled methods that were compiled 
   * at this level or higher.
   */
  private int filterOptLevel;

  /**
   *  The listener
   */
  private VM_BasicMethodListener listener;

  /**
   * @param listener         the associated listener
   * @param filterOptLevel   filter out all opt-compiled methods that 
   *                         were compiled at this level or higher
   */
  VM_MethodSampleOrganizer(VM_BasicMethodListener listener, 
			   int filterOptLevel) {
    this.listener         = listener;
    this.filterOptLevel   = filterOptLevel;
    listener.setOrganizer(this);
  }

  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (VM.LogAOSEvents) 
      VM_AOSLogging.methodSampleOrganizerThreadStarted(filterOptLevel);

    // Install and activate my listener
    VM_RuntimeMeasurements.installMethodListener(listener);
    listener.activate();
  }

  /**
   * Method that is called when the sampling threshold is reached
   */
  void thresholdReached() {
    if (VM.LogAOSEvents) VM_AOSLogging.organizerThresholdReached();

    int numSamples = listener.getNumSamples();
    int[] samples = listener.getSamples();

    // (1) Update the global (cumulative) sample data
    VM_Controller.methodSamples.update(samples, numSamples);
    
    // (2) Remove duplicates from samples buffer.
    //     NOTE: This is a dirty trick and may be ill-advised.
    //     Rather than copying the unique samples into a different buffer
    //     we treat samples as if it was a scratch buffer.
    //     NOTE: This is worse case O(numSamples^2) but we expect a 
    //     significant number of duplicates, so it's probably better than
    //     the other obvious alternative (sorting samples).
    int uniqueIdx = 1;
  outer:
    for (int i=1; i<numSamples; i++) {
      int cur = samples[i];
      for (int j=0; j<uniqueIdx; j++) {
	if (cur == samples[j]) continue outer;
      }
      samples[uniqueIdx++] = cur;
    }

    // (3) For all samples in 0...uniqueIdx, if the method represented by
    //     the sample is compiled at an opt level below filterOptLevel
    //     and the total (cumulative) number of samples attributed to the
    //     method is above our absolute minimum, then report it to the
    //     controller. We have an absolute minimum value to avoid
    //     considering methods that haven't been sampled enough times to
    //     give us at least some reason to think that the fact that they
    //     were sampled wasn't just random bad luck.
    //     NOTE: this minimum is since the beginning of time, not
    //           just the current window.
    for (int i=0; i<uniqueIdx; i++) {
      int cmid = samples[i];
      double ns = VM_Controller.methodSamples.getData(cmid);
      if (ns >= VM_Controller.options.MIN_SAMPLES) {
	VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	if (cm != null) {		// not already obsoleted
	  int compilerType = cm.getCompilerType();

	  // Enqueue it unless it's either a trap method or already opt
	  // compiled at filterOptLevel or higher.
	  if (!(compilerType == VM_CompiledMethod.TRAP ||
	        (compilerType == VM_CompiledMethod.OPT && 
	         (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {
	    VM_HotMethodRecompilationEvent event = 
	      new VM_HotMethodRecompilationEvent(cm, ns);
	    if (VM_Controller.controllerInputQueue.prioritizedInsert(ns, event)){
	      if (VM.LogAOSEvents) {
	        VM_AOSLogging.controllerNotifiedForHotness(cm, ns);
	      }
	    } else {
	      if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	    }
	  }
	}
      }
    }
    
    // (4) Get the listener ready to go and activate it for the next 
    //     sampling window.
    listener.reset();
    listener.activate();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An VM_Organizer acts an an intermediary between the low level 
 * online measurements and the controller.  An organizer may perform
 * simple or complex tasks, but it is always simply following the 
 * instructions given by the controller.
 * 
 * @author Matthew Arnold
 * @author Stephen Fink
 */
abstract class VM_Organizer extends VM_Thread {

  /**
   * Called when thread is scheduled.
   */
  public void run() {

    initialize();

    while (true) {
      // sleep until awoken
      synchronized(this) {
        try {
	  wait();
        }
        catch (InterruptedException e) {
	  e.printStackTrace();
        }
      }
      // we've been awoken, process the information
      try {
	thresholdReached();
      }
      catch (Exception e) {
	VM.sysWrite("AOS: WARNING: exception in organizer "+this+"\n");
	e.printStackTrace();

	// Is there a more elegant way to make this exception fatal to
	// the application?
	System.exit(-1);
      }
    } 
  }

  /**
   * Last opportunity to say something.
   */
  public void report() {}

  /**
   * Method that is called when the sampling threshold is reached
   */
  abstract void thresholdReached();

  /**
   * Organizer specific setup.  
   * A good place to install and activate any listeners.
   */
  abstract protected void initialize();

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An organizer for method listener information. 
 * <p>
 * This organizer is designed to work well with non-decayed 
 * cumulative method samples.  The basic idea is that each time 
 * the sampling threshold is reached we update the accumulated method 
 * sample data with the new data and then notify the controller of all 
 * methods that were sampled in the current window.
 * <p>
 * It augments this basic mechanism by tracking estimates of how 
 * many samples are expected for the method in the next sampling window.
 * It then uses the error between the estimated and actual samples 
 * to detect methods that appear to be "ramping up" or "ramping down."  
 * When such methods are detected, the organizer adjusts the number of 
 * samples it reports to the  controller for that method, within bounds 
 * specified by the controller.
 * <p>
 * TODO: Rather than adjusting the num samples reported (ie lying) to
 * the controller, we should add a ramp up/down factor to
 * the hot method event and explictly include it in the model calculation.
 * I'm kludging it for now because it isn't clear if (1) 
 * we can get estimators that will detect the ramp up/down effects accurately
 * enough to be useful and (2) even if we can detect ramp up/down 
 * that adjusting the number of samples will make enough of a difference to
 * overcome the overhead of keeping the history.
 * 
 * @author Dave Grove
 */
final class VM_SlopeDetectingMethodSampleOrganizer extends VM_Organizer {

  private static final boolean DEBUG = false;

  /**
   * Filter out all opt-compiled methods that were compiled 
   * at this level or higher.
   */
  private int filterOptLevel;

  /**
   *  The listener
   */
  private VM_BasicMethodListener listener;

  /**
   * Bounds on adjustment to num samples made by delta from recent history.
   */
  private double adjustmentBounds;
  
  /**
   * mapping from cmid to history array
   */
  private int[] historyMap;

  /**
   * The history arrays.
   * We do not use history[0].
   * For each entry = history[i]:
   *    entry[CMID_IDX]    is the cmid.
   *    entry[EPOCH_IDX]   is the last epoch in which this cmid was sampled.
   *    entry[ENTRY_IDX+k] is the number of samples taken of cmid
   *                       in epoch (k mod numEpochs)
   */
  private int[][] history;
  private static final int FIRST_ENTRY = 1;
  private static final int NOT_MAPPED = 0;
  private static final int CMID_IDX  = 0;
  private static final int EPOCH_IDX = 1;
  private static final int ENTRY_IDX = 2;

  /** idx of next available history entry */
  private int nextEntry = FIRST_ENTRY;

  /**
   * The current epoch number.
   */
  private int curEpoch;

  /**
   * How many epochs should we maintain?
   */
  private int numEpochs;

  /**
   * @param listener         the associated listener
   * @param filterOptLevel   filter out all opt-compiled methods that 
   *                         were compiled at this level or higher
   * @param adjustmentBounds the organizer is allowed to adjust sample
   *                         data by a factor of anywhere from 
   *                         (1-adjustmentBounds) to (1+adjustmentBounds)
   * @param numEpochs        history window size
   */
  VM_SlopeDetectingMethodSampleOrganizer(VM_BasicMethodListener listener, 
					 int filterOptLevel,
					     double adjustmentBounds,
					     int numEpochs) {
    this.listener         = listener;
    this.filterOptLevel   = filterOptLevel;
    this.adjustmentBounds = adjustmentBounds;
    this.numEpochs        = numEpochs;
    this.historyMap       = new int[(int)(VM_CompiledMethods.numCompiledMethods() * 1.25)];
    if (VM.VerifyAssertions) VM.assert(NOT_MAPPED == 0);
    if (VM.VerifyAssertions) VM.assert(NOT_MAPPED != FIRST_ENTRY);
    this.history          = new int[128][];
    listener.setOrganizer(this);
  }


  /**
   * Initialization: set up data structures and sampling objects.
   */
  public void initialize() {
    if (VM.LogAOSEvents) 
      VM_AOSLogging.methodSampleOrganizerThreadStarted(filterOptLevel);

    // Install and activate my listener
    VM_RuntimeMeasurements.installMethodListener(listener);
    listener.activate();
  }


  /**
   * Method that is called when the sampling threshold is reached
   */
  void thresholdReached() {
    if (VM.LogAOSEvents) VM_AOSLogging.organizerThresholdReached();
    prepareForNewEpoch();
    processRawData();
    listener.reset();
    listener.activate();
  }


  // Prepare history data structures to process a new epoch of samples
  private void prepareForNewEpoch() {
    // advance epoch
    if (++curEpoch == numEpochs) {
      curEpoch = 0;
    }
    
    // remove completely defunct entries or clear the data in
    // entry[ENTRY_IDX+curEpoch] to prepare to put new data there.
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      if (entry[EPOCH_IDX] == curEpoch) {
	clearHistory(entry[CMID_IDX]);
      } else {
	entry[ENTRY_IDX+curEpoch] = 0;
      }
    }
  }
  
  
  // Process the raw data from the listener by accumulating samples into
  // the entries for curEpoch, updating the global method sample data,
  // and notifying the controller of interesting methods (reporting their
  // adjusted number of samples -- see TODO in class header comment).
  private void processRawData() {
    int numSamples = listener.getNumSamples();
    int[] samples = listener.getSamples();
    
    for (int i=1; i<numSamples; i++) {
      int cmid = samples[i];
      int[] entry = getEntry(cmid);
      entry[ENTRY_IDX+curEpoch]++;
      entry[EPOCH_IDX] = curEpoch;
    }

    if (DEBUG) dumpHistory();

    for (int i= FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      if (entry[EPOCH_IDX] == curEpoch) {
	int cmid = entry[CMID_IDX];
	int samplesThisTime = entry[ENTRY_IDX+curEpoch];
	double samplesThisTimeDouble = (double)samplesThisTime;

	// (1) update global sampling data
	VM_Controller.methodSamples.update(cmid, samplesThisTimeDouble);
	double totalSamples = VM_Controller.methodSamples.getData(cmid);

	// (2) See if the controller cares about this method. 
	//     If it does, then make a prediction and notify the controller.
	//     The Controller cares unless it's either a trap method or 
	//     already opt compiled at filterOptLevel or higher.
	//     But, we require that a method be sampled at least 3.0 times
	//     before we report it to the controller to avoid reporting truly
	//     cold but randomly sampled once or twice methods.
	if (totalSamples > 3.0) {
	  VM_CompiledMethod cm = VM_CompiledMethods.getCompiledMethod(cmid);
	  if (cm != null) {
	    int compilerType = cm.getCompilerType();
	    if (!(compilerType == VM_CompiledMethod.TRAP ||
		  (compilerType == VM_CompiledMethod.OPT && 
		   (((VM_OptCompiledMethod)cm).getOptLevel() >= filterOptLevel)))) {

	      // (2a) compute prediction
	      int histSamples = -samplesThisTime;
	      for (int j=0; j<numEpochs; j++) {
		histSamples += entry[ENTRY_IDX+j];
	      }
	      double prediction = 
		((double)histSamples) / ((double)(numEpochs-1));
	      double adjFactor;
	      if (prediction > 0.00001) {
		double slope = samplesThisTimeDouble/prediction;
		if (DEBUG) {
		  VM_Method meth = cm.getMethod();
		  VM.sysWrite("For method "+meth+"("+cmid+") with thisTime="+samplesThisTime+
			      " and total="+totalSamples+ " and prediction ="+prediction+
			      " and history ");
		  for (int k=0; k<numEpochs; k++) {
		    VM.sysWrite(entry[ENTRY_IDX+k],false);
		    VM.sysWrite(" ");
		  }
		  VM.sysWrite("\n\tWe compute a slope of "+slope);
		}

		// Bound adjustment factor as previously instructed by controller
		adjFactor = slope;
		if (adjFactor < 1.0) {
		  adjFactor = 1.0 - (0.5 * (1.0 - slope)); // dampen adjustment
		  if (adjFactor < adjustmentBounds) {
		    adjFactor = adjustmentBounds;
		  } 
		} else {
		  adjFactor = 1.0 + (0.5 * (slope - 1.0 )); // dampen adjustment
		  if (adjFactor > adjustmentBounds + 1.0) {
		    adjFactor = 1.0 + adjustmentBounds;
		  }
		}
	      } else {
		adjFactor = 1.0 + (adjustmentBounds / 2.0);
		if (DEBUG) {
		  VM_Method meth = cm.getMethod();
		  VM.sysWrite("No sample history for method "+meth+"("+cmid+")");
		}
	      }

	      // (2b) adjust totalSamples based on how different it is from the prediction
	      totalSamples = adjFactor * totalSamples;
	      if (DEBUG) VM.sysWrite(" leading to an adjustment factor of "+adjFactor+
				     " and adjusted totalSamples="+
				     totalSamples+"\n");

	      // (2c) tell the controller about this method
	      VM_HotMethodRecompilationEvent event = 
		new VM_HotMethodRecompilationEvent(cm, totalSamples);
	      if (VM_Controller.controllerInputQueue.prioritizedInsert(totalSamples, event)){
		if (VM.LogAOSEvents) {
		  VM_AOSLogging.controllerNotifiedForHotness(cm, totalSamples);
		}
	      } else {
		if (VM.LogAOSEvents) VM_AOSLogging.controllerInputQueueFull(event);
	      }
	    }
	  }
	}
      }
    }
  }

  private void dumpHistory() {
    VM.sysWrite("\n######\ncurEpoch = "+curEpoch+
		", numEpochs = "+numEpochs+"\n");
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      int[] entry = history[i];
      VM.sysWrite("cmid = "+entry[CMID_IDX]+", lastEpoch ="+entry[EPOCH_IDX]+
		  "\thistory = ");
      for (int j=0; j<numEpochs; j++) {
	VM.sysWrite(entry[ENTRY_IDX+j],false);
	VM.sysWrite(" ");
      }
      VM.sysWrite("\n");
    }

    VM.sysWrite("Checking history map...");
    for (int i=FIRST_ENTRY; i<nextEntry; i++) {
      if (historyMap[history[i][CMID_IDX]] != i) {
	VM.sysWrite("Mismatch between historyMap and entry (1) "+i);
      }
    }
    for (int i=0; i<historyMap.length; i++) {
      if (historyMap[i] != 0) {
	if (history[historyMap[i]][CMID_IDX] != i) {
	  VM.sysWrite("Mismatch between historyMap and entry (2) "+i);
	}
      }
    }
    VM.sysWrite("...done\n");
  }

  // get the entry[] for a cmid, allocating new entries and
  // growing the backing store as needed.
  private int[] getEntry(int cmid) {
    if (cmid >= historyMap.length) {       // grow historyMap
      int[] tmp = new int[(int)(VM_CompiledMethods.numCompiledMethods() * 1.25)];
      for (int i=0; i< historyMap.length; i++) {
	tmp[i] = historyMap[i];
      }
      historyMap = tmp;
    }
    if (historyMap[cmid] == NOT_MAPPED) {
      int idx = nextEntry++;
      if (idx >= history.length) {         // grow history
	int[][] tmp = new int[history.length*2][];
	for (int i=0; i<history.length; i++) {
	  tmp[i] = history[i];
	}
	history = tmp;
      }
      if (VM.VerifyAssertions) VM.assert(history[idx] == null);
      history[idx] = new int[ENTRY_IDX+numEpochs];
      history[idx][CMID_IDX] = cmid;
      historyMap[cmid] = idx;
      return history[idx];
    } else {
      return history[historyMap[cmid]];
    }
  }

  // remove an entry
  private void clearHistory(int cmid) {
    if (DEBUG) VM.sysWrite("Clearing history for "+cmid+"\n");
    int entryIdx = historyMap[cmid];
    if (entryIdx != NOT_MAPPED) {
      historyMap[cmid] = NOT_MAPPED;
      nextEntry--;
      if (entryIdx<nextEntry) {
	history[entryIdx] = history[nextEntry];
	historyMap[history[entryIdx][CMID_IDX]] = entryIdx;
      }
      history[nextEntry] = null;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

import java.io.*;

/**
 * This class provides logging functionality for the Adaptive Optimization System
 *
 * Right now this is fairly primitive, an evolving number of events are
 * defined and log entries are quite unsophisticated.
 * Some obvious TODO items:
 *  -- compact encoding of log entries
 *  -- some notion of log format versions
 *  -- ...
 *
 * NOTE: All code that writes to the log is synchronized on the PrintStream
 *      object to avoid interspersed messages, which can happen when the
 *      compilation thread and the controller thread try to log a message
 *      "at the same time".
 * 
 * ***When is the log file flushed and closed?
 * ***Do we want to put report() information in the log?
 *
 * The current logging levels are:
 *   0  Do no logging
 *   1  Do minimal logging at startup and VM exit.  
 *      If at all possible, do not log anything during program execution.
 *      This logging level is supposed to produce minimal performance pertubation.
 *   2  Log interesting AOS events and controller actions
 *   3  Exhaustively log pretty much everything that is going on
 *
 * @author Dave Grove
 * @author Michael Hind
 * @modified Peter Sweeney
 */
class VM_AOSLogging {

  /*
   * The output file stream, where all log messages will go
   */
  private static PrintStream log;
  
   /*
    * Record that the AOS logging has been booted.
    * Needed to allow fast exit from reporting to ensure
    * that when no class is specified to be run but "-help" is specified, 
    * don't want null pointer exception to occur!
    */
   private static boolean booted = false;

   /**
    * Return whether AOS logging has booted.
    * @return whether AOS logging has booted
    */
   public static boolean booted() {
     return booted;
   }

  /**
   * Called from VM_ControllerThread.run to initialize the logging subsystem
   */
  public static void boot() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      try {
	log = new PrintStream(new FileOutputStream(VM_Controller.options.LOGFILE_NAME));

	// This statement will force the compilation of println, so it
	// is needed regardless of the particular content of the message!
	synchronized (log) {
	  log.println(VM_Controller.controllerClock +" Logging enabled\n");
	  log.println(VM_Controller.options);
	}
      }
      catch (IOException e) {
	VM.sysWrite("IOException caught in VM_AOSLogging.java while trying to create and start log file.\n");
	VM.sysWrite("Please check for file permission problems\n");
      }
    }
    booted = true;
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 1
  ////////////////////////////////////////////////////////////////

  /**
   * Called from VM_Controller.report to allow a last message to the logging
   *  system
   */
  public static void systemExiting() {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +" System Exiting\n");
      }
    }
  }

  /**
   * Called from VM_RuntimeMeasurements when the argument thread is terminating
   * to allow us to record the time spent in the thread.
   * @param t the thread of interest
   */
  public static void threadExiting(VM_Thread t) {
    if (!booted) return; // fast exit
    try {
      if (VM_Controller.options.LOGGING_LEVEL >= 1) {
	synchronized (log) {
	  log.println(VM_Controller.controllerClock +
		      " ThreadIndex: " + t.getIndex() + " "
		      + t.getClass().getName() + " "
		      + " Time: " 
		      + t.cpuTotalTime
		      + " status("
		      + (  t.isIdleThread ?     "i"         // idle daemon
			   : t.isGCThread   ?     "g"       // gc daemon
			   : t.isDaemon     ?     "d"       // user daemon
			   :                      "" )
		      + (!t.isAlive     ?     "!" : "")     // dead/alive
		      + (t.cpuStartTime > 0 ? "+" : "-")    // running/stopped
		      + ")"
		      );
	}
      }
    } catch (NullPointerException e) {
      // ignore.  A thread exited before the AOS Logging system was
      // initialized.  It can't be interesting.
    }
  }

  /**
   * Call this method when the controller thread initially begins executing
   */
  public static void controllerStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Controller thread started");
      }
    }
  }

  /**
   * Call this method to dump statistics on how often listeners are invoked.
   * @param method method listener info
   * @param context context listener info
   * @param nll null listener info
   */
  public static void listenerStatistics(int method, int context, int nll) {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Listeners called:"+
		    "\n\t method "+method+"\n\t context "+context
		    +"\n\t null "+nll);
      }
    }
  }

  /**
   * Call this method to dump statistics related to decaying
   * @param decayCount the number of decay events
   */
  public static void decayStatistics(int decayCount) {
    if (!booted) return; // fast exit
    if (VM_Controller.options.LOGGING_LEVEL >=  1) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		  +" Decay Organizer Statistics: \n\t"+
		  " Num of Decay events: "+
		  decayCount+"\n");
      }
    }
  }

  /**
   * Call this method when one run of the application is completed
   */
  public static void appRunComplete() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Application completed a run");
      }
    }
  }

  /**
   * Call this method when the controller thread is exiting
   */
  public static void controllerCompleted() {
    if (!booted) return; // fast exit
  
    int awoken = VM_ControllerMemory.getNumAwoken(); 
    int didNothing = VM_ControllerMemory.getNumDidNothing();
    int numMethodsConsidered = VM_ControllerMemory.getNumMethodsConsidered();
    int numMethodsScheduledForRecomp = 
                    VM_ControllerMemory.getNumMethodsScheduledForRecomp(); 
    int numOpt0 = VM_ControllerMemory.getNumOpt0();
    int numOpt1 = VM_ControllerMemory.getNumOpt1();
    int numOpt2 = VM_ControllerMemory.getNumOpt2();
    int numOpt3 = VM_ControllerMemory.getNumOpt3();

    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		  +" Controller thread exiting ... "
		  +"\n  Num times Controller thread is awoken: "+
		  awoken
		  +"\n  Num times did nothing: "+ didNothing +" ("+
		  ((int)((float)didNothing/(float)awoken * 100))
		  +"%)\n  Num methods baseline compiled: "+
		  VM_ControllerMemory.getNumBase()
		  +"\n  Num methods considered for recompilation: "+
		  numMethodsConsidered
		  +"\n  Num methods chosen to recompile: "+ 
		  numMethodsScheduledForRecomp +" ("+
		  ((int) ((float) numMethodsScheduledForRecomp
		                         /  numMethodsConsidered * 100))
		  +"%)\n  Opt Levels Chosen: "
		  +"\n\t Opt Level 0: "+ numOpt0 +" ("+
		  ((int) ((float) numOpt0/numMethodsScheduledForRecomp * 100))

		  +"%)\n\t Opt Level 1: "+ numOpt1 +" ("+
		  ((int) ((float) numOpt1/numMethodsScheduledForRecomp * 100))
		  +"%)\n"

		  +"\t Opt Level 2: "+ numOpt2 +" ("+
		  ((int) ((float) numOpt2/numMethodsScheduledForRecomp * 100))
		  +"%)\n"

		  +"\t Opt Level 3: "+ numOpt3 +" ("+
		  ((int) ((float) numOpt3/numMethodsScheduledForRecomp * 100))
		  +"%)\n\n");

	// Let the controller memory summarize itself to the log file
	VM_ControllerMemory.printFinalMethodStats(log);
      }
    }
  }


  /**
   * Call this method when the compilation thread initially begins executing
   */
  public static void compilationThreadStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation thread started");
      }
    }
  }

  /**
   * Call this method when the compilation thread is exiting
   */
  public static void compilationThreadCompleted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation thread exiting");
      }
    }
  }

  /**
   * Call this method when the organizer thread initially begins executing
   * @param filterOptLevel the opt level that we are filtering
   */
  public static void methodSampleOrganizerThreadStarted(int filterOptLevel) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Method Sample Organizer thread started");
	log.println("  filterOptLevel: "+ filterOptLevel);
      }
    }
  }

  /**
   * Call this method when the organizer thread initially begins executing
   */
  public static void AIByEdgeOrganizerThreadStarted() {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Adaptive Inlining (AI) by Edge Organizer thread started");
      }
    }
  }

  /**
   * This method reports the basic speedup rate for a compiler
   * @param compiler the compiler you are reporting about
   * @param rate the speedup rate
   */
  public static void reportSpeedupRate(int compiler, double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" SpeedupRate for "+ 
		    VM_CompilerDNA.getCompilerString(compiler)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   * This method reports the basic compilation rate for a compiler
   * @param compiler the compiler you are reporting about
   * @param rate the compilation rate (bytecodes per millisecond)
   */
  public static void reportCompilationRate(int compiler, double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compilation Rate (bytecode/msec) for "+ 
		    VM_CompilerDNA.getCompilerString(compiler)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   *  This method reports the benefit ratio from one compiler to the other
   *  @param compiler1 the first compiler
   *  @param compiler2 the second compiler
   *  @param rate the improvement from going from a compiler1-compiled method
   *                   to a compiler2-compiled method
   */
  public static void reportBenefitRatio(int compiler1, 
					int compiler2, 
					double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Benefit Ratio from "+
		    VM_CompilerDNA.getCompilerString(compiler1)
		    +" compiler to "+
		    VM_CompilerDNA.getCompilerString(compiler2)
		    +" compiler: "+ rate);
      }
    }
  }

  /**
   *  This method reports the compile time ratio from one compiler to
   *  the other
   *  @param compiler1 the first compiler
   *  @param compiler2 the second compiler
   *  @param rate the ratio of compiler1 compilation rate to 
   *                compiler2 compilation rate
   */
  public static void reportCompileTimeRatio(int compiler1, 
					    int compiler2, 
					    double rate) {
    if (VM_Controller.options.LOGGING_LEVEL >= 1) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Compile Time Ratio of "+
		    VM_CompilerDNA.getCompilerString(compiler1)
		    +" compiler to "+
		    VM_CompilerDNA.getCompilerString(compiler2)
		    +" compiler: "+ rate);
      }
    }
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 2
  ////////////////////////////////////////////////////////////////

  /**
   * This method logs the scheduling of a recompilation,
   * i.e., it being inserted in the compilation queue.
   * @param plan the OPT_Compilation plan being executed.
   * @param priority a number from 0.0 to 1.0 encoding the plan's priority.
   */
  public static void recompilationScheduled(OPT_CompilationPlan plan, 
					    double priority) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Scheduling level "+
		    plan.options.getOptLevel() +" recompilation of "+
		    plan.method+" (plan has priority "+priority+")");
      }
    }
  }

  /**
   * This method logs when a recompilation could not occur
   * because the queue is full
   * @param plan the OPT_Compilation plan that would have been executed
   */
  public static void recompilationQueueFull(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Level "+ plan.options.getOptLevel()
		    +" recompilation postponed of "+plan.method);
      }
    }
  }

  /**
   * This method logs when the controller could not be notified 
   * of an event because the controllerInputQueue was full.
   * @param event the event the controller would have been told about
   */
  public static void controllerInputQueueFull(Object event) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock + 
		    " Unable to inform controller of "+event+ " due to full input queue");
      }
    }
  }

  /**
   * This method logs the beginning of an adaptively selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationStarted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +" Recompiling (at level "+
		    plan.options.getOptLevel() +") "+ plan.method);
      }
    }
  }

  /**
   * This method logs the successful completion of an adaptively 
   * selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationCompleted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock +"  Recompiled (at level "+
		    plan.options.getOptLevel() +") " +plan.method);
      }
    }
  }

  /**
   * This method logs the abortion of an adaptively selected recompilation
   * @param plan the OPT_Compilation plan being executed.
   */
  public static void recompilationAborted(OPT_CompilationPlan plan) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Failed recompiling (at level "+ 
		    plan.options.getOptLevel()+" "+ plan.method);
      }
    }
  }

  /**
   * this method logs the event when the controller discovers a method that has 
   * been recompiled and the previous version is still regarded as hot, 
   * i.e., still on the stack and signficant.
   */
  public static void oldVersionStillHot(VM_HotMethodEvent hme) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Found a method with an old version still hot "+ hme);
      }
    }
  }

  /**
   * This method logs when the decay organizer runs.
   */
  public static void decayingCounters() {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Decaying clock and decayable objects");
      }
    }
  }

  /**
   * This Method logs when the organizer thread has reached its
   * sampling threshold
   */
  public static void organizerThresholdReached() {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" OrganizerThread reached sample size threshold\n");
      }
    }
  }

  /**
   * This method reports a bulk detection of hot max-opt-level methods to 
   * have their call edges inspected.
   *
   * @param numMethods the total number of max opt level methods found to be hot
   *                     and will have their call edges inspected
   */
  public static void AIorganizerFoundHotMethods(int numMethods) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer found "+numMethods
		    +" hot max-opt-level methods, will inspect their call edges.");
      }
    }
  }

  /**
   * This method logs that the a hot call edge from an max-opt-level
   * method has been identified.
   * 
   * @param hotMethod	method to be recompiled,
   * @param numSamples  number of samples attributed to the method 
   * @param boost 	expected boost factor
   */
  public static void inliningOpportunityDetected(VM_CompiledMethod hotMethod,
						 double numSamples, 
						 VM_CallSiteTriple triple) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer found method "+hotMethod.getMethod()+
		    " with "+numSamples+" samples that has an edge "+
		    triple+" that can be inlined");
      }
    }
  }

  /**
   * This method logs that the controller is notified of a 
   * candidate to be recompiled due to inlining opportunities;
   * i.e., the method has been inserted in the controller queue.
   * @param hotMethod	method to be recompiled,
   * @param numSamples	number of samples attributed to the method
   * @param triple	edge that should be inlined.
   */
  public static void controllerNotifiedForInlining(VM_CompiledMethod hotMethod,
						   double numSamples, 
						   double boost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" AI organizer notified controller that method "+
		    hotMethod.getMethod()+" with "+numSamples+
		    " samples could be recompiled with a boost of "+boost);
      }
    }
  }

  /**
   * This method logs that the controller is notified of a 
   * candidate to be recompiled due to hotness;
   * i.e., the method has been inserted in the controller queue.
   * @param hotMethod	method to be recompiled, and
   * @param numSamples	number of samples attributed to the method
   */
  public static void 
    controllerNotifiedForHotness(VM_CompiledMethod hotMethod,
				 double numSamples) {
    if (VM_Controller.options.LOGGING_LEVEL >= 2) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +" Controller notified that method "+hotMethod.getMethod()+
		    " has "+numSamples+" samples");
      }
    }
  }

  ////////////////////////////////////////////////////////////////
  // Logging level 3
  ////////////////////////////////////////////////////////////////

  /**
   * This method logs a controller cost estimate for doing nothing
   * @param method the method of interest
   * @param optLevel the opt level being estimated, -1 = baseline
   * @param cost  the computed cost for this method and level
   */
  public static void 
    recordControllerEstimateCostDoNothing(VM_Method method, 
					  int optLevel,
					  double cost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 3) {
      synchronized (log) {
	log.print(VM_Controller.controllerClock 
		    +"  Estimated cost of doing nothing (leaving at ");
	if (optLevel == -1) {
	  log.print("baseline");
	}
	else {
	  log.print("O"+ optLevel);
	}
	log.println(") to "+ method +" is "+ cost);
      }
    }
  }

  /**
   * This method logs a controller cost estimate
   * @param method the method of interest
   * @param choiceDesc
   @ @param cost  the computed cost for this method and level
   */
  public static void 
    recordControllerEstimateCostOpt(VM_Method method, 
				    String choiceDesc,
				    double cost) {
    if (VM_Controller.options.LOGGING_LEVEL >= 3) {
      synchronized (log) {
	log.println(VM_Controller.controllerClock 
		    +"  Estimated cost of OPT compiling "+
		    method + " at " + choiceDesc +
		    " is "+ cost);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class extends VM_PriorityQueue to safely
 * support multiple producers/consumers where 
 * the consumers are blocked if no objects are available
 * to consume.
 *
 * @author Dave Grove
 * @author Michael Hind
 */
class VM_BlockingPriorityQueue extends VM_PriorityQueue {

  /**
   * Used to notify consumers when about to wait and when notified
   * Default implementation does nothing, but can be overriden as needed by client.
   */
  public static class CallBack {
    void aboutToWait() {}
    void doneWaiting() {}
  }
  CallBack callback;

  /**
   * @param initialSize the initial number of elements
   * @param _cb the callback object
   */
  VM_BlockingPriorityQueue(int initialSize, CallBack _cb) {
    super(initialSize);
    callback = _cb;
  }

  /**
   * @param initialSize the initial number of elements
   */
  VM_BlockingPriorityQueue(int initialSize) {
    this(initialSize, new CallBack());
  }

  /**
   * Insert the object passed with the priority value passed
   * 
   * Notify any sleeping consumer threads that an object
   * is available for consumption.
   *
   * @param _priority  the priority to 
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized final public boolean insert(double _priority, Object _data) { 
    boolean success = super.insert(_priority, _data);
    if (success) {
      try {
	notifyAll();
      } catch (Exception e) {
	// TODO: should we exit or something more dramatic?
	VM.sysWrite("Exception occurred while notifying that element was inserted!\n");
      }
    }
    return success;
  }

  /**
   * Remove and return the front (minimum) object.  If the queue is currently
   * empty, then block until an object is available to be dequeued.
   * @param callback a VM_BlockingPriorityQueueCallback object. 
   * @return the front (minimum) object.
   */
  synchronized final public Object deleteMin() {
    // While the queue is empty, sleep until notified that an object has been enqueued.
    while (isEmpty()) {
      try {
	callback.aboutToWait();
	wait();
	callback.doneWaiting();
      } catch (InterruptedException e) {
	// TODO: should we exit or something more dramatic?
	VM.sysWrite("Interrupted Exception occurred!\n");
      }
    }

    // When we get to here, we know the queue is non-empty, so dequeue an object and return it.
    return super.deleteMin();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class implements a priority queue using the standard
 * (balanced partially-ordered tree, i.e., "heap") algorithm.  
 * Smaller priority objects are in the front of the queue.
 *
 * @author Michael Hind
 */
class VM_PriorityQueue {

  private static final boolean DEBUG = false;

  /**
   * the queue, we use elements 1..size
   */
  private VM_PriorityQueueNode[] queue; 

  /**
   * the index of the last valid entry
   *  size = queue.length - 1
   */
  private int size;     

  /**
   * the number of elements actually in the queue
   */
  private int numElements; 
  
  /**
   * Constructor
   * @param initialSize the initial number of elements
   */
  VM_PriorityQueue(int initialSize) {
    // We don't use element #0
    int allocSize = initialSize+1;
    queue = new VM_PriorityQueueNode[allocSize];
    
    for (int i=0; i<allocSize; i++) {
      queue[i] = new VM_PriorityQueueNode();
    }

    // We use elements 1..size
    size = initialSize;
    numElements = 0;
  }

  /**
   * Determines number of elements in the queue
   * @return number of elements in the queue
   */
  synchronized final public int numElements() {
    return numElements;
  }

  /**
   * Checks if the queue is empty
   * @return is the queue empty?
   */
  synchronized final boolean isEmpty() {
    return numElements == 0;
  }

  /**
   * Checks if the queue is full
   * @return is the queue full?
   */
  synchronized final boolean isFull() {
    return numElements == size;
  }

  /**
   * Starting at the position passed, swap with parent until heap condition
   * is satisfied, i.e., bubble up
   * @param startingElement the position to start at 
   */
  private void reheapify(int startingElement) {
    int current = startingElement;
    int parent = numElements / 2;
    // keep checking parents that violate the magic condition
    while (parent > 0 && queue[parent].priority < queue[current].priority) {
      //	System.out.println("Parent: "+ parent +", Current: "+ current);
      //	System.out.println("Contents before: "+ this);
      // exchange parrent and current values
      VM_PriorityQueueNode tmp = queue[parent];
      queue[parent] = queue[current];
      queue[current] = tmp;
      
      //	System.out.println("Contents after: "+ this);
      // go up 1 level
      current = parent;
      parent = parent / 2;
    }
  }

  /**
   * Insert the object passed with the priority value passed
   * @param _priority  the priority of the inserted object
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized public boolean insert(double _priority, Object _data) {
    if (isFull()) {
      return false;
    }

    numElements++;
    queue[numElements].data = _data;
    queue[numElements].priority = _priority;
    
    // re-heapify
    reheapify(numElements);
    return true;
  }

  /**
   * Insert the object passed with the priority value passed, but if the queue
   *   is full, a lower priority object is removed to make room for this object.
   *   If no lower priority object is found, we don't insert.
   * @param _priority  the priority to 
   * @param _data the object to insert
   * @return true if the insert succeeded, false if it failed because the queue was full
   */
  synchronized public boolean prioritizedInsert(double _priority, Object _data) {
    if (DEBUG) VM.sysWrite("prioInsert: prio: "+_priority+",size: "+
			   size +", numElements: "+ numElements +")\n");

    // the queue isn't full just use the regular insert
    if (!isFull()) {
      return insert(_priority, _data);
    }

    // search the leaves of the tree and find the lowest priority element
    //  "numElements" is the last element.  The first leaf is the next
    //  node after its parent, i.e,  numElements/2  + 1
    //  We'll go from this value up to numElements.
    int firstChild = numElements/2 + 1;
    int evictee = -1;
    double evicteePriority = _priority;  // start of with the priority of the insertor
    for (int i=firstChild; i<=numElements; i++) {
      if (queue[i].priority < evicteePriority) {
	if (DEBUG) VM.sysWrite("  candidate at entry "+ i 
			       +", prio: "+queue[i].priority +")\n");
	evictee = i;
	evicteePriority = queue[i].priority;
      }
    }

    // Did we find an evictee?
    if (evictee != -1) {
      queue[evictee].data = _data;
      queue[evictee].priority = _priority;
    
      if (DEBUG) VM.sysWrite("  evicting entry "+ evictee +")\n");

      // re-heapify
      reheapify(evictee);
      return true;
    }
    else {
      // didn't find a spot :-(
      return false;
    }
  }

  /**
   * Remove and return the front (minimum) object
   * @return the front (minimum) object or null if the queue is empty.
   */
  synchronized public Object deleteMin() {
    if (isEmpty()) return null;

    Object returnValue = queue[1].data;
    // move the "last" element to the root and reheapify by pushing it down
    queue[1].priority = queue[numElements].priority;
    queue[1].data = queue[numElements].data;
    numElements--;
    
    // reheapify!!!
    int current = 1;
    
    // The children live at 2*current and  2*current+1 
    int child1 = 2 * current;
    while (child1 <= numElements) {
      int child2 = 2 * current + 1;
      
      // find the smaller of the two children
      int smaller;
      if (child2 <= numElements && queue[child2].priority > queue[child1].priority) {
	smaller = child2;
      } else {
	smaller = child1;
      }
      
      if (queue[smaller].priority <= queue[current].priority) {
	break;
      }
      else {
	// exchange parrent and current values
	VM_PriorityQueueNode tmp = queue[smaller];
	queue[smaller] = queue[current];
	queue[current] = tmp;
	
	// go down 1 level
	current = smaller;
	child1 = 2 * current;
      }
    }
    return returnValue;
  }

  /**
   *  Return the priority of front object without removing it
   *  @return the priority of the front object
   */
  synchronized final public double rootValue() {
    if (VM.VerifyAssertions) VM.assert(!isEmpty());

    return queue[1].priority;
  }

  /**
   *  Prints the contents of the queue
   *  @return the queue contents
   */
  synchronized public String toString() {
    StringBuffer sb = new StringBuffer(" --> ");
    sb.append("Dumping Queue with "+ numElements +" elements:\n");
    if (numElements >= 1) {
      sb.append("\t");
    }

    for (int i=1; i<=numElements; i++) {
      sb.append(queue[i].toString());
      if (i<numElements)
	sb.append("\n\t");
    }
    return sb.toString();
  }
}

/**
 * A local class that holds the nodes of the priority tree
 */
class VM_PriorityQueueNode {

  /**
   * the value to compare on, larger is better
   */
  public double priority; 

  /**
   * the associated data 
   */
  public Object data;     

  public String toString() {
    return (new StringBuffer(data +" ... ["+ priority +"]")).toString();
  }

}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Dummy class containing enough references to force java compiler
 * to find every class comprising the vm, so everything gets recompiled
 * by just compiling "Dummy.java".
 *
 * The minimal set has to be discovered by trial and error. Sorry.
 *
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_WriteBarrier;
class Dummy {
  static VM                         a;
  static VM_TableBasedDynamicLinker b;
  static VM_DynamicLinker           c;
  static VM_Runtime                 d;
  static VM_Reflection              e;
  static java.lang.Void             g;
  static java.io.InputStreamReader  h;
  static java.io.StreamTokenizer    i;
  static java.net.PlainSocketImpl   j;
  static java.net.ServerSocket      k;
  static VM_WriteBarrier            l;
  static VM_JNIFunctions            m;
  static VM_JNIStartUp              n;
  static VM_RecompilationManager    o;
  //-#if RVM_WITH_CONCURRENT_GC
  static VM_RCBuffers               p; // not used by opt yet, but referenced in VM_Entrypoints
  static VM_OptRCWriteBarrier       q; // not used by opt yet, but referenced in VM_Entrypoints
  //-#endif
  static VM_MultianewarrayHelper    r;
  static VM_Address                 s;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A virtual machine.
 * Implements VM_Uninterruptible to suppress thread switching in boot() and
 * sysCall() prologues.
 *
 * @author Derek Lieber (project start).
 * @date 21 Nov 1997 
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

public class VM extends VM_Properties implements VM_Constants, 
VM_Uninterruptible { 
  //----------------------------------------------------------------------//
  //                          Initialization.                             //
  //----------------------------------------------------------------------//

  /** 
   * Prepare vm classes for use by boot image writer.
   * @param classPath class path to be used by VM_ClassLoader
   * @param bootCompilerArgs command line arguments for the bootimage compiler
   */ 
  static void initForBootImageWriter(String classPath, 
                                     String[] bootCompilerArgs) 
    throws VM_ResolutionException, VM_PragmaInterruptible {
    writingBootImage = true;
    init(classPath, bootCompilerArgs);
  }

  /**
   * Prepare vm classes for use by tools.
   * @exception VM_ResolutionException
   */
  static void initForTool() throws VM_ResolutionException, VM_PragmaInterruptible  {
    runningTool = true;
    LoadLocalVariableTables = true;  // make sure to load the local table
    init(System.getProperty("java.class.path"), null);
  }

  /**
   * Prepare vm classes for use by tools.
   * @param classpath class path to be used by VM_ClassLoader
   * @exception VM_ResolutionException
   */
  static void initForTool(String classpath) throws VM_ResolutionException, VM_PragmaInterruptible {
    runningTool = true;
    LoadLocalVariableTables = true;  // make sure to load the local table
    init(classpath, null);
  }

  static int verbose = 0;  // Show progress of boot 

  /**
   * Begin vm execution.
   * The following machine registers are set by "C" bootstrap program 
   * before calling this method:
   *    JTOC_POINTER        - required for accessing globals
   *    FRAME_POINTER       - required for accessing locals
   *    THREAD_ID_REGISTER  - required for method prolog (stack overflow check)
   * @exception Exception
   */
  public static void boot() throws Exception, VM_PragmaLogicallyUninterruptible {
    VM.writingBootImage = false;
    VM.runningVM        = true;
    VM.runningAsSubsystem = false;

    if (verbose >= 1) VM.sysWriteln("Booting");

    // Set up the current VM_Processor object.  The bootstrap program
    // has placed a pointer to the current VM_Processor in a special
    // register.
    if (verbose >= 1) VM.sysWriteln("Setting up current VM_Processor");
    VM_ProcessorLocalState.boot();


    // Finish thread initialization that couldn't be done in boot image.
    // The "stackLimit" must be set before any method calls, 
    // because it's accessed by compiler-generated stack overflow checks.
    //
    if (verbose >= 1) VM.sysWriteln("Doing thread initialization");
    VM_Thread currentThread  = VM_Scheduler.threads[VM_Magic.getThreadId() >>> VM_ThinLockConstants.TL_THREAD_ID_SHIFT];
    currentThread.stackLimit = VM_Magic.objectAsAddress(currentThread.stack).add(STACK_SIZE_GUARD);

    VM_Processor.getCurrentProcessor().activeThreadStackLimit = currentThread.stackLimit;

    // get pthread_id from OS and store into vm_processor field
    // 
    if (!BuildForSingleVirtualProcessor)
      VM_Processor.getCurrentProcessor().pthread_id = 
        VM.sysCall0(VM_BootRecord.the_boot_record.sysPthreadSelfIP);

    VM.TraceClassLoading = (VM_BootRecord.the_boot_record.traceClassLoading == 1);   

    // Initialize memory manager's write barrier.
    // This must happen before any putfield or arraystore of object refs
    // because the buffer is accessed by compiler-generated write barrier code.
    //
    if (verbose >= 1) VM.sysWriteln("Setting up write barrier");
    if (VM_Collector.NEEDS_WRITE_BARRIER) {
      VM_Collector.setupProcessor( VM_Processor.getCurrentProcessor() );
    }

    // Initialize memory manager.
    //    This must happen before any uses of "new".
    //
    if (verbose >= 1) VM.sysWriteln("Setting up memory manager");
    VM_Collector.boot(VM_BootRecord.the_boot_record);

    // Reset the options for the baseline compiler to avoid carrying them over from
    // bootimage writing time.
    // 
    VM_BaselineCompiler.initOptions();

    // Create class objects for static synchronized methods in the bootimage.
    // This must happen before any bootimage static synchronized methods 
    // can be invoked.
    if (verbose >= 1) VM.sysWriteln("Creating class objects for static synchronized methods");
    createClassObjects();

    // Fetch arguments from program command line.
    //
    if (verbose >= 1) VM.sysWriteln("Fetching command-line arguments");
    VM_CommandLineArgs.fetchCommandLineArguments();

    // Initialize class loader.
    //
    if (verbose >= 1) VM.sysWriteln("Initializing class loader");
    String vmClasses = VM_CommandLineArgs.getVMClasses();
    VM_ClassLoader.boot(vmClasses);

    //
    // At this point the virtual machine is running as a single thread 
    // that can perform dynamic compilation and linking (by compiler/linker 
    // that's part of boot image).  All that remains is to initialize the 
    // java class libraries, start up the thread subsystem, and launch
    // the user level "main" thread.
    //

    // Initialize statics that couldn't be placed in bootimage, either 
    // because they refer to external state (open files), or because they 
    // appear in fields that are unique to RVM implementation of 
    // standard class library (not part of standard jdk).
    // We discover the latter by observing "host has no field" and 
    // "object not part of bootimage" messages printed out by bootimage 
    // writer.
    //

    if (verbose >= 1) VM.sysWriteln("Running various class initializers");
    //-#if RVM_WITH_GNU_CLASSPATH
    java.lang.ref.Reference.lock = new Object();
    //-#else
    runClassInitializer("java.io.FileDescriptor");
    //-#endif

    runClassInitializer("java.lang.Runtime");
    runClassInitializer("java.lang.System");
    System.boot();
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.io.FileDescriptor");
    //-#endif
    runClassInitializer("java.io.File");
    runClassInitializer("java.lang.Boolean");
    runClassInitializer("java.lang.Byte");
    runClassInitializer("java.lang.Short");
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.lang.Number");
    //-#endif
    runClassInitializer("java.lang.Integer");
    runClassInitializer("java.lang.Long");
    runClassInitializer("java.lang.Float");
    runClassInitializer("java.lang.Double");
    runClassInitializer("java.lang.Character");
    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    runClassInitializer("com.ibm.oti.io.CharacterConverter");
    //-#endif
    runClassInitializer("java.util.Hashtable");
    //-#if RVM_WITH_GNU_CLASSPATH
    runClassInitializer("java.lang.Class");
    runClassInitializer("gnu.java.io.EncodingManager");
    runClassInitializer("java.lang.Thread");
    runClassInitializer("java.lang.ThreadGroup");
    runClassInitializer("java.io.PrintWriter");
    System.makeStandardStreams();
    runClassInitializer("gnu.java.lang.SystemClassLoader");
    if( gnu.java.lang.SystemClassLoader.NO_SUCH_ARCHIVE == null)
      gnu.java.lang.SystemClassLoader.NO_SUCH_ARCHIVE = new Object();
    //-#endif
    runClassInitializer("java.lang.String");
    runClassInitializer("java.lang.ClassLoader");
    runClassInitializer("com.ibm.JikesRVM.librarySupport.ReflectionSupport");
    runClassInitializer("java.lang.Math");
    runClassInitializer("java.lang.RuntimePermission");
    runClassInitializer("java.util.TimeZone");
    runClassInitializer("java.util.Locale");
    runClassInitializer("java.util.Calendar");
    runClassInitializer("java.util.GregorianCalendar");
    runClassInitializer("java.util.ResourceBundle");
    runClassInitializer("java.util.zip.ZipEntry");
    runClassInitializer("java.util.zip.Inflater");
    runClassInitializer("java.util.zip.DeflaterHuffman");
    runClassInitializer("java.util.zip.InflaterDynHeader");
    runClassInitializer("java.util.zip.InflaterHuffmanTree");
    runClassInitializer("java.util.jar.Attributes$Name");

    // Initialize compiler that compiles dynamically loaded classes.
    //
    if (verbose >= 1) VM.sysWriteln("Initializing runtime compiler");
    VM_RuntimeCompiler.boot();


    // Process virtual machine directives.
    //
    if (verbose >= 1) VM.sysWriteln("Processing VM directives");
    String[] applicationArguments = VM_CommandLineArgs.processCommandLineArguments();
    if (applicationArguments.length == 0) {  
      VM.sysWrite("vm: please specify a class to execute\n");
      VM.sysExit(1);
    }

    // Allow Baseline compiler to respond to command line arguments
    // The baseline compiler ignores command line arguments until all are processed
    // otherwise printing may occur because of compilations ahead of processing the
    // method_to_print restriction
    //
    if (verbose >= 1) VM.sysWriteln("Compiler processing rest of boot options");
    VM_BaselineCompiler.postBootOptions();


    // Allow Collector to respond to command line arguments
    //
    if (verbose >= 1) VM.sysWriteln("Collector processing rest of boot options");
    VM_Collector.postBoot();

    //-#if RVM_WITH_GNU_CLASSPATH
    VM_SystemClassLoader.getVMClassLoader().jarCache = null;
    //-#endif

    // Work around class incompatibilities in boot image writer
    // (JDK's java.lang.Thread does not extend VM_Thread) [--IP].
    if (verbose >= 1) VM.sysWriteln("Constructing mainThread");
    Thread      xx         = new MainThread(applicationArguments);
    VM_Address  yy         = VM_Magic.objectAsAddress(xx);
    VM_Thread   mainThread = (VM_Thread)VM_Magic.addressAsObject(yy);

    // record the main thread and the name of the main application class.
    _mainApplicationClassName = applicationArguments[0];
    _mainThread = mainThread;

    VM_Lock.boot();

    // Begin multiprocessing.
    //

    VM_Scheduler.boot(mainThread);
    if (VM.VerifyAssertions) 
      VM.assert(VM.NOT_REACHED);
  }

  private static VM_Class[] classObjects = new VM_Class[0];
  /**
   * Called by the compilers when compiling a static synchronized method
   * during bootimage writing.
   */
  static void deferClassObjectCreation(VM_Class c) throws VM_PragmaInterruptible {
    for (int i=0; i<classObjects.length; i++) {
      if (classObjects[i] == c) return; // already recorded
    }
    VM_Class[] tmp = new VM_Class[classObjects.length+1];
    System.arraycopy(classObjects, 0, tmp, 0, classObjects.length);
    tmp[classObjects.length] = c;
    classObjects = tmp;
  }
  /**
   * Create the java.lang.Class objects needed for 
   * static synchronized methods in the bootimage.
   */
  private static void createClassObjects() throws VM_PragmaInterruptible {
    for (int i=0; i<classObjects.length; i++) {
      classObjects[i].getClassForType();
    }
  }

  /**
   * Run <clinit> method of specified class, if that class appears 
   * in bootimage.
   * @param className
   */
  private static void runClassInitializer(String className) throws VM_PragmaInterruptible {
    VM_Atom  classDescriptor = 
      VM_Atom.findOrCreateAsciiAtom(className.replace('.','/')).descriptorFromClassName();
    VM_Class cls = VM_ClassLoader.findOrCreateType(classDescriptor, VM_SystemClassLoader.getVMClassLoader()).asClass();
    if (cls.isInBootImage()) {
      VM_Method clinit = cls.getClassInitializerMethod();
      clinit.compile();
      VM_Magic.invokeClassInitializer(clinit.getCurrentInstructions());
      cls.setAllFinalStaticJTOCEntries();
    }

  }

  //----------------------------------------------------------------------//
  //                         Execution environment.                       //
  //----------------------------------------------------------------------//

  /**
   * Verify a runtime assertion (die w/traceback if assertion fails).
   * Note: code your assertion checks as 
   * "if (VM.VerifyAssertions) VM.assert(xxx);"
   * @param b the assertion to verify
   */
  public static void assert(boolean b) {
    assert(b, null);
  }

  /**
   * Verify a runtime assertion (die w/message and traceback if 
   * assertion fails).   Note: code your assertion checks as 
   * "if (VM.VerifyAssertions) VM.assert(xxx,yyy);"
   * @param b the assertion to verify
   * @param message the message to print if the assertion is false
   */
  static void assert(boolean b, String message) {
    if (!VM.VerifyAssertions) {
      // somebody forgot to conditionalize their call to assert with
      // "if (VM.VerifyAssertions)"
      _assertionFailure("vm internal error: assert called when !VM.VerifyAssertions");
    }

    if (!b) _assertionFailure(message);
  }

  private static void _assertionFailure(String message) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline {
    if (message == null) message = "vm internal error at:";
    if (VM.runningVM) {
      sysFail(message);
    }
    throw new RuntimeException(message);
  }


  /**
   * Format a 32 bit number as "0x" followed by 8 hex digits.
   * Do this without referencing Integer or Character classes, 
   * in order to avoid dynamic linking.
   * TODO: move this method to VM_Services.
   * @param number
   * @return a String with the hex representation of the integer
   */
  static String intAsHexString(int number) throws VM_PragmaInterruptible {
    char[] buf   = new char[10];
    int    index = 10;
    while (--index > 1) {
      int digit = number & 0x0000000f;
      buf[index] = digit <= 9 ? (char)('0' + digit) : (char)('a' + digit - 10);
      number >>= 4;
    }
    buf[index--] = 'x';
    buf[index]   = '0';
    return new String(buf);
  }

  /**
   * Low level print to console.
   * @param value  what is printed
   */
  public static void sysWrite(VM_Atom value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    value.sysWrite();
  }

  /**
   * Low level print to console.
   * @param value  what is printed
   */
  public static void sysWrite(VM_Member value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    VM.sysWrite(value.getDeclaringClass().getDescriptor());
    VM.sysWrite(".");
    VM.sysWrite(value.getName());
    VM.sysWrite(" ");
    VM.sysWrite(value.getDescriptor());
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   */
  public static void sysWrite(String value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      VM_Processor.getCurrentProcessor().disableThreadSwitching();
      for (int i = 0, n = value.length(); i < n; ++i) {
        sysWrite(value.charAt(i));
      }
      VM_Processor.getCurrentProcessor().enableThreadSwitching();
    } else {
      System.err.print(value);
    }
  }

  /**
    * Low level print to console.
   * @param value	what is printed
   */
  public static void sysWrite(char value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall1(VM_BootRecord.the_boot_record.sysWriteCharIP, value);
    else
      System.err.print(value);
  }


  /**
   * Low level print to console.  Can't pass doubles yet so just print to 2 decimal places.
   * @param value   double to be printed
   *
   */
  public static void sysWrite(double value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int ones = (int) value;
      int hundredths = (int) (100.0 * (value - ones));
      sysWrite(ones, false); 
      sysWrite(".");
      if (hundredths < 10)
        sysWrite("0");
      sysWrite(hundredths, false);
    }
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value	what is printed
   */
  public static void sysWrite(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int mode = (value < -(1<<20) || value > (1<<20)) ? 2 : 0; // hex only or decimal only
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, mode);
    } else {
      System.err.print(value);
    }
  }


  public static void sysWriteField(int fieldWidth, String s) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    sysWrite(s);
    int len = s.length();
    while (fieldWidth > len++) sysWrite(" ");
  }

  /**
   * Low level print to console.
   * @param value	print value and left-fill with enough spaces to print at least fieldWidth characters
   */
  public static void sysWriteField(int fieldWidth, int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    int len = 1, temp = value;
    if (temp < 0) { len++; temp = -temp; }
    while (temp >= 10) { len++; temp /= 10; }
    while (fieldWidth > len++) sysWrite(" ");
    if (runningVM) 
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, 0);
    else 
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value	print value and left-fill with enough spaces to print at least fieldWidth characters
   */
  public static void sysWriteField(int fieldWidth, VM_Atom s) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    int len = s.length();
    while (fieldWidth > len++) sysWrite(" ");
    sysWrite(s);
  }

  /**
   * Low level print to console.
   * @param value	what is printed, as hex only
   */
  public static void sysWriteHex(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, 2 /*just hex*/);
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   * @param hexToo  how to print: true  - print as decimal followed by hex
   *                              false - print as decimal only
   */
  public static void sysWrite(int value, boolean hexToo) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM)
      sysCall2(VM_BootRecord.the_boot_record.sysWriteIP, value, hexToo?1:0);
    else
      System.err.print(value);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   */
  public static void sysWrite(long value) throws VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    sysWrite(value, true);
  }

  /**
   * Low level print to console.
   * @param value   what is printed
   * @param hexToo  how to print: true  - print as decimal followed by hex
   *                              false - print as decimal only
   */
  public static void sysWrite(long value, boolean hexToo) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline /* don't waste code space inlining these --dave */ {
    if (runningVM) {
      int val1, val2;
      val1 = (int)(value>>32);
      val2 = (int)(value & 0xFFFFFFFF);
      sysCall3(VM_BootRecord.the_boot_record.sysWriteLongIP, val1, val2, hexToo?1:0);
    } else
      System.err.print(value);
  }

  /**
   * A group of multi-argument sysWrites with optional newline.
   */
  public static void sysWriteln ()                     throws VM_PragmaNoInline { sysWrite("\n"); }
  public static void sysWrite   (VM_Address addr)      throws VM_PragmaNoInline { sysWriteHex(addr.toInt()); }
  public static void sysWriteln (int i)                throws VM_PragmaNoInline { sysWrite(i);   sysWriteln(); }
  public static void sysWriteln (double d)             throws VM_PragmaNoInline { sysWrite(d);   sysWriteln(); }
  public static void sysWriteln (long l)               throws VM_PragmaNoInline { sysWrite(l);   sysWriteln(); }
  public static void sysWriteln (String s)             throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(); }
  public static void sysWrite   (String s, int i)           throws VM_PragmaNoInline { sysWrite(s);   sysWrite(i); }
  public static void sysWriteln (String s, int i)           throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(i); }
  public static void sysWrite   (String s, double d)        throws VM_PragmaNoInline { sysWrite(s);   sysWrite(d); }
  public static void sysWriteln (String s, double d)        throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(d); }
  public static void sysWrite   (String s, long i)           throws VM_PragmaNoInline { sysWrite(s);   sysWrite(i); }
  public static void sysWriteln (String s, long i)           throws VM_PragmaNoInline { sysWrite(s);   sysWriteln(i); }
  public static void sysWrite   (int i, String s)           throws VM_PragmaNoInline { sysWrite(i);   sysWrite(s); }
  public static void sysWriteln (int i, String s)           throws VM_PragmaNoInline { sysWrite(i);   sysWriteln(s); }
  public static void sysWrite   (String s1, String s2)      throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); }
  public static void sysWriteln (String s1, String s2)      throws VM_PragmaNoInline { sysWrite(s1);  sysWriteln(s2); }
  public static void sysWrite   (String s, VM_Address addr) throws VM_PragmaNoInline { sysWrite(s);   sysWriteHex(addr.toInt()); }
  public static void sysWriteln (String s, VM_Address addr) throws VM_PragmaNoInline { sysWrite(s);   sysWriteHex(addr.toInt()); sysWriteln(); }
  public static void sysWrite   (String s1, String s2, int i)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWrite(i); }
  public static void sysWriteln (String s1, String s2, int i)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWriteln(i); }
  public static void sysWrite   (String s1, int i, String s2)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i);  sysWrite(s2); }
  public static void sysWriteln (String s1, int i, String s2)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i);  sysWriteln(s2); }
  public static void sysWrite   (String s1, String s2, String s3)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWrite(s3); }
  public static void sysWriteln (String s1, String s2, String s3)  throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(s2); sysWriteln(s3); }
  public static void sysWrite   (int i1, String s, int i2)     throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s);  sysWrite(i2); }
  public static void sysWriteln (int i1, String s, int i2)     throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s);  sysWriteln(i2); }
  public static void sysWrite   (int i1, String s1, String s2) throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s1); sysWrite(s2); }
  public static void sysWriteln (int i1, String s1, String s2) throws VM_PragmaNoInline { sysWrite(i1);  sysWrite(s1); sysWriteln(s2); }
  public static void sysWrite   (String s1, int i1, String s2, int i2) throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i1); sysWrite(s2); sysWrite(i2); }
  public static void sysWriteln (String s1, int i1, String s2, int i2) throws VM_PragmaNoInline { sysWrite(s1);  sysWrite(i1); sysWrite(s2); sysWriteln(i2); }

  /**
   * Exit virtual machine due to internal failure of some sort.
   * @param message  error message describing the problem
   */
  public static void sysFail(String message) throws VM_PragmaNoInline {
    // print a traceback and die
    VM_Scheduler.traceback(message);
    VM.shutdown(1);
  }

  /**
   * Exit virtual machine.
   * @param value  value to pass to host o/s
   */
  public static void sysExit(int value) throws VM_PragmaLogicallyUninterruptible, VM_PragmaNoInline {
    // SJF: I don't want this method inlined, since I use it as a
    // breakpoint for the jdp regression test.
    if (runningVM) {
      System.out.flush();
      System.err.flush();
      VM_Callbacks.notifyExit(value);
      VM.shutdown(value);
    } else {
      System.exit(value);
    }
  }

  /**
   * Shut down the virtual machine.
   * Should only be called if the VM is running.
   * @param value  exit value
   */
  public static void shutdown(int value) {
    if (VM.VerifyAssertions) VM.assert(VM.runningVM);
    if (VM.runningAsSubsystem) {
      // Terminate only the system threads that belong to the VM
      VM_Scheduler.processorExit(value);
    } else {
      sysCall1(VM_BootRecord.the_boot_record.sysExitIP, value);
    }
  }

  /**
   * Create a virtual processor (aka "unix kernel thread", "pthread").
   * @param jtoc  register values to use for thread startup
   * @param pr
   * @param ti
   * @param fp
   * @return virtual processor's o/s handle
   */
  static int sysVirtualProcessorCreate(VM_Address jtoc, VM_Address pr, 
                                       int ti, VM_Address fp) {
    return sysCall4(VM_BootRecord.the_boot_record.sysVirtualProcessorCreateIP,
                    jtoc.toInt(), pr.toInt(), ti, fp.toInt());
  }

  /**
   * Bind execution of current virtual processor to specified physical cpu.
   * @param cpuId  physical cpu id (0, 1, 2, ...)
   */
  static void sysVirtualProcessorBind(int cpuId) {
    sysCall1(VM_BootRecord.the_boot_record.sysVirtualProcessorBindIP, cpuId);
  }

  /**
   * Yield execution of current virtual processor back to o/s.
   */
  public static void sysVirtualProcessorYield() {
    //-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
    return;
    //-#else
    sysCall0(VM_BootRecord.the_boot_record.sysVirtualProcessorYieldIP);
    //-#endif
  }

  /**
   * Start interrupt generator for thread timeslicing.
   * The interrupt will be delivered to whatever virtual processor happens 
   * to be running when the timer expires.
   */
  static void sysVirtualProcessorEnableTimeSlicing() {
    sysCall0(VM_BootRecord.the_boot_record.sysVirtualProcessorEnableTimeSlicingIP);
  }

  //-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
  //-#else
  static void sysWaitForVirtualProcessorInitialization() {
    sysCall0(VM_BootRecord.the_boot_record.sysWaitForVirtualProcessorInitializationIP);
  }

  static void sysWaitForMultithreadingStart() {
    sysCall0(VM_BootRecord.the_boot_record.sysWaitForMultithreadingStartIP);
  }

  static void sysInitializeStartupLocks(int howMany) {
    sysCall1(VM_BootRecord.the_boot_record.sysInitializeStartupLocksIP, howMany);
  }
  //-#endif

  //-#if RVM_FOR_POWERPC
  /**
   * Make calls to host operating system services.
   * @param ip address of a function in sys.C 
   * @return integer value returned by function in sys.C
   */
  public static int sysCall0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall0(ip, VM_BootRecord.the_boot_record.sysTOC);
  }

  /**
   * sysCall1
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return integer value returned by function in sys.C
   */
  public static int sysCall1(int ip, int p1) throws VM_PragmaInline {
    return VM_Magic.sysCall1(ip, VM_BootRecord.the_boot_record.sysTOC, p1);
  }

  /**
   * sysCall2
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall2(int ip, int p1, int p2) throws VM_PragmaInline {
    return  VM_Magic.sysCall2(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2);
  }

  /**
   * sysCall3
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall3(int ip, int p1, int p2, int p3) throws VM_PragmaInline {
    return  VM_Magic.sysCall3(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2, p3);
  }

  /**
   * sysCall4
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @param p4
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall4(int ip, int p1, int p2, int p3, int p4) throws VM_PragmaInline {
    return VM_Magic.sysCall4(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2, p3, p4);
  }

  /**
   * sysCall_L_0
   * @param ip  address of a function in sys.C 
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_0(ip, VM_BootRecord.the_boot_record.sysTOC);
  }

  /**
   * sysCall_L_I
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_I(int ip, int p1) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_I(ip, VM_BootRecord.the_boot_record.sysTOC, p1);
  }

  /**
   * sysCallAD
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCallAD(int ip, int p1, double p2) throws VM_PragmaInline {
    return  VM_Magic.sysCallAD(ip, VM_BootRecord.the_boot_record.sysTOC, p1, p2);
  }

  //-#endif
  //-#if RVM_FOR_IA32
  /**
   * sysCall0
   * @param ip  address of a function in sys.C 
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall0(int ip) throws VM_PragmaInline {
    return  VM_Magic.sysCall0(ip);
  }

  /**
   * sysCall1
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall1(int ip, int p1) throws VM_PragmaInline {
    return  VM_Magic.sysCall1(ip, p1);
  }

  /**
   * sysCall2
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall2(int ip, int p1, int p2) throws VM_PragmaInline {
    return  VM_Magic.sysCall2(ip, p1, p2);
  }

  /**
   * sysCall3
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall3(int ip, int p1, int p2, int p3) throws VM_PragmaInline {
    return  VM_Magic.sysCall3(ip, p1, p2, p3);
  }

  /**
   * sysCall4
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @param p3
   * @param p4
   * @return  integer value returned by function in sys.C
   */
  public static int sysCall4(int ip, int p1, int p2, int p3, int p4) throws VM_PragmaInline {
    return VM_Magic.sysCall4(ip, p1, p2, p3, p4);
  }

  /**
   * sysCall_L_0
   * @param ip  address of a function in sys.C 
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_0(int ip) throws VM_PragmaInline {
    return VM_Magic.sysCall_L_0(ip);
  }

  /**
   * sysCall_L_I
   * @param ip  address of a function in sys.C 
   * @param p1
   * @return long value returned by function in sys.C
   */
  public static long sysCall_L_I(int ip, int p1) throws VM_PragmaInline {
    return  VM_Magic.sysCall_L_I(ip, p1);
  }

  /**
   * sysCallAD
   * @param ip  address of a function in sys.C 
   * @param p1
   * @param p2
   * @return  integer value returned by function in sys.C
   */
  public static int sysCallAD(int ip, int p1, double p2) throws VM_PragmaInline {
    return VM_Magic.sysCallAD(ip, p1, p2);
  }

  //-#endif

  //----------------//
  // implementation //
  //----------------//

  /**
   * Create class instances needed for boot image or initialize classes 
   * needed by tools.
   * @param vmClassPath places where vm implemention class reside
   * @param bootCompilerArgs command line arguments to pass along to the 
   *                         boot compiler's init routine.
   */
  private static void init(String vmClassPath, String[] bootCompilerArgs) 
    throws VM_ResolutionException, VM_PragmaInterruptible {
      // create dummy boot record
      //
      VM_BootRecord.the_boot_record = new VM_BootRecord();

      // initialize type subsystem - create type descriptions for java.lang.Object 
      // and the classes whose methods it calls. we do this in an order chosen to 
      // ensure that offset and size information needed by the compiler to 
      // perform "direct" (non-dynamically linked) calls is ready before any 
      // method calls are compiled.
      //
      VM_Statics.init();
      VM_MagicNames.init();
      VM_ClassLoader.init(vmClassPath);
      VM_Class object       = VM_Type.JavaLangObjectType.asClass();
      VM_Class string       = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("Ljava/lang/String;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class stringBuffer = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("Ljava/lang/StringBuffer;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class vm           = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("LVM;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      VM_Class runtime      = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("LVM_Runtime;"), VM_SystemClassLoader.getVMClassLoader()).asClass();

      // initialize JNI environment
      VM_JNIEnvironment.init();

      // load class descriptions
      //
      object.load();
      string.load();
      stringBuffer.load();
      vm.load();
      runtime.load();

      // generate size and offset information needed for compiling methods of java.lang.Object
      //
      object.resolve();
      string.resolve();
      stringBuffer.resolve();
      vm.resolve();
      runtime.resolve();
      // initialize remaining subsystems needed for compilation
      //
      VM_Entrypoints.init();
      VM_OutOfLineMachineCode.init();
      if (writingBootImage) // initialize compiler that builds boot image
        VM_BootImageCompiler.init(bootCompilerArgs);
      VM_Runtime.init();
      VM_Scheduler.init();
      VM_Collector.init();
    }

  /**
   * The following two methods are for use as guards to protect code that 
   * must deal with raw object addresses in a collection-safe manner 
   * (ie. code that holds raw pointers across "gc-sites").
   *
   * Authors of code running while gc is disabled must be certain not to 
   * allocate objects explicitly via "new", or implicitly via methods that, 
   * in turn, call "new" (such as string concatenation expressions that are 
   * translated by the java compiler into String() and StringBuffer() 
   * operations). Furthermore, to prevent deadlocks, code running with gc 
   * disabled must not lock any objects. This means the code must not execute 
   * any bytecodes that require runtime support (eg. via VM_Runtime) 
   * such as:
   *   - calling methods or accessing fields of classes that haven't yet 
   *     been loaded/resolved/instantiated
   *   - calling synchronized methods
   *   - entering synchronized blocks
   *   - allocating objects with "new"
   *   - throwing exceptions 
   *   - executing trap instructions (including stack-growing traps)
   *   - storing into object arrays, except when runtime types of lhs & rhs 
   *     match exactly
   *   - typecasting objects, except when runtime types of lhs & rhs 
   *     match exactly
   *
   * Recommendation: as a debugging aid, VM_Allocator implementations 
   * should test "VM_Thread.disallowAllocationsByThisThread" to verify that 
   * they are never called while gc is disabled.
   */
  public static void disableGC() throws VM_PragmaInline, VM_PragmaInterruptible  { 
    // current (non-gc) thread is going to be holding raw addresses, therefore we must:
    //
    // 1. make sure we have enough stack space to run until gc is re-enabled
    //    (otherwise we might trigger a stack reallocation)
    //
    // 2. force all other threads that need gc to wait until this thread
    //    is done with the raw addresses
    //
    // 3. ensure that this thread doesn't try to allocate any objects
    //    (because an allocation attempt might trigger a collection that
    //    would invalidate the addresses we're holding)
    //

    VM_Thread myThread = VM_Thread.getCurrentThread();

    // 1.
    //
    if (VM_Magic.getFramePointer().sub(STACK_SIZE_GCDISABLED).LT(myThread.stackLimit))
      VM_Thread.resizeCurrentStack(myThread.stack.length + (STACK_SIZE_GCDISABLED >> 2), null);

    // 2.
    //
    VM_Processor.getCurrentProcessor().disableThreadSwitching();

    // 3.
    //
    if (VM.VerifyAssertions) {
      VM.assert(myThread.disallowAllocationsByThisThread == false); // recursion not allowed
      myThread.disallowAllocationsByThisThread = true;
    }
  }

  /**
   * enable GC
   */
  public static void enableGC() throws VM_PragmaInline { 
    if (VM.VerifyAssertions) {
      VM_Thread myThread = VM_Thread.getCurrentThread();
      // recursion not allowed
      VM.assert(myThread.disallowAllocationsByThisThread == true); 
      myThread.disallowAllocationsByThisThread = false;
    }
    VM_Processor.getCurrentProcessor().enableThreadSwitching();
  }

  private static String _mainApplicationClassName;
  private static VM_Thread _mainThread;

  /**
   * getMainMethod
   * @return the main method of the main thread
   */
  public static VM_Method getMainMethod() throws VM_PragmaInterruptible {
    if(VM.VerifyAssertions) VM.assert(_mainThread != null);
    return ((MainThread)_mainThread).getMainMethod();
  } 

  /**
   * Place to set breakpoints (called by compiled code).
   */
  public static void debugBreakpoint() throws VM_PragmaNoInline {
    // the following forces this method to have a prologue.
    // In general, jdp cannot set breakpoints in opt methods that
    // have no prologues.
    VM_Magic.pragmaNoOptCompile();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Flags that specify the configuration of our virtual machine.
 *
 * Note: Final controls require the whole vm to be recompiled and 
 *       rebuilt after their values are changed.
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public abstract class VM_Configuration {

  public static final boolean BuildForPowerPC =
	//-#if RVM_FOR_POWERPC
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean BuildForIA32 =
	//-#if RVM_FOR_IA32
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean LITTLE_ENDIAN = BuildForIA32;

  public static final boolean BuildForAix =
	//-#if RVM_FOR_AIX
	  true;
	//-#else
	  false;
	//-#endif

  public static final boolean BuildForLinux =
	//-#if RVM_FOR_LINUX
	  true;
	//-#else
	  false;
	//-#endif

  // Assertion checking.
  //  false --> no assertion checking at runtime
  //  true  --> execute assertion checks at runtime
  //
  // Note: code your assertion checks as 
  // "if (VM.VerifyAssertions) VM.assert(xxx);"
  //
  public static final boolean VerifyAssertions = 
        //-#if RVM_WITHOUT_ASSERTIONS
          false;
        //-#else
          true;
        //-#endif

  // Verify that Uninterruptible methods actually cannot be interrupted.
  // Disabled for just a little longer since we get too many false positives
  public static final boolean VerifyUnint = false && VerifyAssertions;

  // Ignore supression pragma and print all warning messages.
  public static final boolean ParanoidVerifyUnint = false;

  // Multiprocessor operation.
  //  false --> vm will use multiple processors (requires operatying system that
  //            supports posix pthread, e.g., AIX)
  //  true  --> vm will use just one processor and no
  //            synchronization instructions
  //
  public static final boolean BuildForSingleVirtualProcessor =
	//-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
	  true;
	//-#else
	  false;
	//-#endif

  // Use count of method prologues executed rather than timer interrupts to drive
  // preemptive thread switching.  Non preemptive thread switching is achieved by
  // setting the number of prologues between thread switches to infinity (-1).
  //
  public static final boolean BuildForDeterministicThreadSwitching =
	//-#if RVM_WITH_DETERMINISTIC_THREAD_SWITCHING
	  true;
	//-#else
        //-#if RVM_WITHOUT_PREEMPTIVE_THREAD_SWITCHING 
          true;
        //-#else
          false;
	//-#endif
	//-#endif

  // Normally, a word in memory is used to signal need for a thread switch
  // On PowerPC a control register can be used (and will be set by the interrupt handler).
  // However, this distribution of virtual processors interrupted is very unfair on a multiprocessor.
  // Therefore, the control register is only used for single-virtual-processor builds.
  //
  public static final boolean BuildForThreadSwitchUsingControlRegisterBit = 
	//-#if RVM_FOR_POWERPC
	//-#if RVM_FOR_SINGLE_VIRTUAL_PROCESSOR
          true;
        //-#else
          false;
	//-#endif
	//-#else
          false;
	//-#endif

  // Is this an adaptive build?
  public static final boolean BuildForAdaptiveSystem =
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Dynamic type checking implementations.
  // We have two:
  //  (1) FastDynamicTypeCheck as described in Alpern, Cocchi, & Grove JVM'01
  //  (2) otherwise do an inline type equality check and an inline
  //      cache lookup for the last successful type comparison
  //      falling back to an out of line lookup routine.  This is 
  //      approximately what is done by the IBM product DK (see Ishizaki et al)
  public static final boolean BuildForFastDynamicTypeCheck = true;

  // Interface method invocation.
  // We have five mechanisms:
  //   IMT-based (Alpern, Cocchi, Fink, Grove, and Lieber). 
  //    - embedded directly in the TIB
  //    - indirectly accessed off the TIB
  //    both IMT schemes require BuildForFastDynamicTypeCheck.
  //   ITable-based
  //    - directly indexed (by interface id) iTables. 
  //       requires BuildForFastDynamicTypeCheck.
  //    - searched (at dispatch time); does not require FastDTC
  //   Naive, class object is searched for matching method on every dispatch.
  public static final boolean BuildForIMTInterfaceInvocation = true && 
                                              BuildForFastDynamicTypeCheck;
  public static final boolean BuildForIndirectIMT = true && 
                                              BuildForIMTInterfaceInvocation;
  public static final boolean BuildForEmbeddedIMT = !BuildForIndirectIMT && 
                                              BuildForIMTInterfaceInvocation;
  public static final boolean BuildForITableInterfaceInvocation = true && 
                                              !BuildForIMTInterfaceInvocation;
  public static final boolean DirectlyIndexedITables = false && 
                                              BuildForFastDynamicTypeCheck;

  // Compiler support for garbage collection and stack management.
  //
  public static final boolean BuildForConcurrentGC =
      //-#if RVM_WITH_CONCURRENT_GC
        true;
      //-#else
        false;
      //-#endif

  // Compiler support for real-time garbage collection
  //
  public static final boolean BuildForRealtimeGC =
      //-#if RVM_WITH_REALTIME_GC
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithRedirectSlot =
      //-#if RVM_WITH_REDIRECT_SLOT
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithLazyRedirect =
      //-#if RVM_WITH_LAZY_REDIRECT
        true;
      //-#else
        false;
      //-#endif

  // Brooks-style redirection barrier
  public static final boolean BuildWithEagerRedirect =
      //-#if RVM_WITH_EAGER_REDIRECT
        true;
      //-#else
        false;
      //-#endif

  // Epilogue yieldpoints increase sampling accuracy for adaptive recompilation.
  // In particular, they are key for large, leaf, loop-free methods.
  public static final boolean UseEpilogueYieldPoints =
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Operating system resource monitoring.
  //
  // Getting accurate compilation time information is critical for
  // good adaptive system performance.
  public static final boolean BuildForCpuMonitoring = 
      //-#if RVM_WITH_ADAPTIVE_SYSTEM
        true;
      //-#else
        false;
      //-#endif

  // Adaptive compilation.
  //
  public static final boolean LogAOSEvents =
      //-#if RVM_WITHOUT_AOS_LOG 
        false;
      //-#else
        true;
      //-#endif

  // Use synchronization on PowerPC to access one word fields declared volatile and
  // on all platforms to access two word fields declared volatile.
  //
  // If this control is not set, volatile fields will not be kept in registers but stale
  // values may still be visible in processor caches.
  //
  // Note: for best results the following controls should also be set:
  //     BuildForPrematureClassResolution - so that volatile fields can be recognized as such at compilation time
  //     BuildForLazyCompilation - to avoid sucking in the whole world with premature class resolution.
  //
  public static final boolean BuildForStrongVolatileSemantics = 
      //-#if RVM_WITH_STRONG_VOLATILE_SEMANTICS
        true;
      //-#else
        false;
      //-#endif
  
  // Resolve classes when a compiler encounters a reference to a not yet loaded class.
  // 
  // Note: setting BuildForLazyCompilation will prevent a cascade effect from sucking in too much irrelevant stuff.
  //
  public static final boolean BuildForPrematureClassResolution = 
      //-#if RVM_WITH_PREMATURE_CLASS_RESOLUTION
        true;
      //-#else
        false
        || BuildForStrongVolatileSemantics // TEMP!! remove this clause when dynamic linking for strong volatiles is implemented
        ;
      //-#endif

  // Lazy vs. eager method compilation during class loading.
  //
  public static final boolean BuildForLazyCompilation =
      //-#if RVM_WITHOUT_LAZY_COMPILATION
        false;
      //-#else
        true;
      //-#endif

  // Capture threads that have gone Native (JNI) and not come back.  Issolate them
  // in Native.  Create a new (Native) virtual processor for them.  And create (or revive)
  // new pThreads to run the old virtual processors.
  //
  public static final boolean BuildWithNativeDaemonProcessor = 
	//-#if RVM_WITHOUT_NATIVE_DAEMON_PROCESSOR
	  false;
	//-#else
	  !BuildForSingleVirtualProcessor
	    && !BuildForConcurrentGC;
	//-#endif

  // The following configuration objects are final when disabled, but
  // non-final when enabled.
  
  //-#if RVM_FOR_STRESSGC
  public static boolean ParanoidGCCheck       = true;
  public static boolean ForceFrequentGC       = true;
  //-#else
  public final static boolean ParanoidGCCheck  = false;
  public final static boolean ForceFrequentGC  = false;
  //-#endif

  public final static boolean CompileForGCTracing =
      //-#if RVM_WITH_GCTk_GCTRACE
	true;
      //-#else
        false;
      //-#endif

  //-#if RVM_FOR_IA32
  /**
   * Is ESI dedicated to always hold the processor register?
   */
  public final static boolean dedicatedESI = true;
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Constants describing vm object, stack, and register characteristics.
 * Some of these constants are architecture-specific
 * and some are (at the moment) architecture-neutral.
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public interface VM_Constants
extends   VM_ThinLockConstants,         // architecture-neutral
          VM_TIBLayoutConstants,        // architecture-neutral
          VM_StackframeLayoutConstants, // architecture-neutral
          VM_RegisterConstants,         // architecture-specific
          VM_TrapConstants              // architecture-specific
   {
   // Bit pattern used to represent a null object reference.
   // Note: I don't think this is actually used consistently [--DL]
   // 
   static final int VM_NULL = 0;
   
   // For assertion checking things that should never happen.
   //
   static final boolean NOT_REACHED = false;

   // For assertion checking things that aren't ready yet.
   //
   static final boolean NOT_IMPLEMENTED = false;
  
   static final int BYTES_IN_ADDRESS_LOG = 2;
   static final int BYTES_IN_ADDRESS = 1<<BYTES_IN_ADDRESS_LOG;

   // Reflection uses an integer return from a function which logically
   // returns a triple.  The values are packed in the interger return value
   // by the following masks.
   static final int REFLECTION_GPRS_BITS = 5;
   static final int REFLECTION_GPRS_MASK = (1 << REFLECTION_GPRS_BITS) - 1;
   static final int REFLECTION_FPRS_BITS = 5;
   static final int REFLECTION_FPRS_MASK = (1 << REFLECTION_FPRS_BITS) - 1;

   }
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Flags that control the behavior of our virtual machine.
 * 
 * Typically these are properties that can be set from the command line
 * (and thus are NOT final).  All final properties should be 
 * declared in VM_Configuration
 *
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author David Grove
 */
public class VM_Properties extends VM_Configuration {

  // The VM class hierarchy is used in three ways:
  //    - by boot image writer to create an executable vm image
  //    - by tools that wish use VM classes for generic java programming
  //    - by vm image itself, at execution time
  // The following flags specify which behavior is desired.
  //
  
  /**
   * use classes for boot image generation? (see BootImageWriter)
   */
  public static boolean writingBootImage; 
  /**
   * use classes for generic java programming?
   */
  public static boolean runningTool;      
  /**
   * use classes for running actual VM?
   */
  public static boolean runningVM;        
  /**
   * if true, don't exit from the process
   */
  public static boolean runningAsSubsystem;  

  // Use count of method prologues executed rather than timer interrupts to drive
  // preemptive thread switching.  Non preemptive thread switching is achieved by
  // setting the number of prologues between thread switches to infinity (-1).
  //
  public static int deterministicThreadSwitchInterval =
	//-#if RVM_WITHOUT_PREEMPTIVE_THREAD_SWITCHING 
	  -1;
	//-#else
        //-#if RVM_WITH_DETERMINISTIC_THREAD_SWITCHING
           1000;
        //-#else // the normal case (timer-driven preemptive thread switching)
	  0;
	//-#endif
	//-#endif

  // suppress code gen when system is running on remote interpreter portion of jdp.
  public static boolean runningAsJDPRemoteInterpreter;

  /**
   * The following is set on by -verbose:class command line arg.
   * When true, it generates messages to the sysWrite stream summarizing
   * class loading activities
   */
  public static boolean verboseClassLoading = false;

  // Symbolic info to support debugger.
  //
  public static boolean LoadLocalVariableTables = false;

  /**
   * The following is set on by -X:measureCompilation=true command line arg.
   * When true, it times compilations and generates a report at VM exit.
   */
  public static boolean MeasureCompilation      = false;  

  /**
   * The following is set on by -X:verify=true command line arg.
   * When true, it invokes the bytecode verifier
   */
  public static boolean VerifyBytecode = false;  

  // Runtime subsystem tracing.
  //
  public static final boolean TraceDictionaries       = false;
  public static final boolean TraceStatics            = false;
  public static final boolean TraceDynamicLinking     = false;
  public static final boolean TraceFileSystem         = false;
  public static final boolean TraceThreads            = false;
  public static final boolean TraceStackTrace         = false;
  public static boolean TraceClassLoading             = false;

  // Baseline compiler tracing.
  //
  public static final boolean TraceAssembler         = false; // PPC only

  // Baseline compiler reference map tracing.
  //
  public static final boolean TraceStkMaps                  = false;
  public static final boolean ReferenceMapsStatistics       = false;
  public static final boolean ReferenceMapsBitStatistics    = false;

  // Event logging.
  //
  public static final boolean BuildForEventLogging      = false;
  public static       boolean EventLoggingEnabled       = false;  // TODO!! make this final, see profiler/VM_EventLogger.java
  public static final boolean BuildForNetworkMonitoring = false;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/**
 * Constants exported by the assembler
 * @author Julian Dolby
 */
interface VM_AssemblerConstants {
  static final String [] CONDITION = {
   "O", "NO", "LLT", "LGE", "EQ", "NE", "LLE", "LGT", "S", "NS", "PE", "PO", "LT", "GE", "LE", "GT" 
  };

  static final byte   O = 0x0; // (overflow)
  static final byte  NO = 0x1; // (not overflow)
  static final byte LLT = 0x2; // logically less than (below)
  static final byte LGE = 0x3; // logically greater than or equal (not below) 
  static final byte  EQ = 0x4; // equal (zero)
  static final byte  NE = 0x5; // not equal (not zero)
  static final byte LLE = 0x6; // logically less than or equal (not above)
  static final byte LGT = 0x7; // logically greater than (above)
  static final byte   S = 0x8; // (sign) negative??
  static final byte  NS = 0x9; // (not sign) positive or zero??
  static final byte  PE = 0xA; // (even parity)
  static final byte  PO = 0xB; // (odd parity)
  static final byte  U  = 0xA; // (unordered floating point #s)
  static final byte  NU = 0xB; // (ordered floating point #s)
  static final byte  LT = 0xC; // less than
  static final byte  GE = 0xD; // greater than or equal (not less than)
  static final byte  LE = 0xE; // less than or equal (not greater than)
  static final byte  GT = 0xF; // greater than 

  // scale factors for SIB bytes
  static final short BYTE  = 0;
  static final short SHORT = 1;
  static final short WORD  = 2;
  static final short LONG  = 3;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/** 
 * @author Julian Dolby
 */
class VM_Lister implements VM_Constants {

  private static final int PREFIX_AREA_SIZE = 4;
  private static final int OP_AREA_SIZE     = 9;
  private static final int SOURCE_AREA_SIZE = 16;
  private static final int DEST_AREA_SIZE   = 16;

  VM_Assembler asm;
  boolean lockPrefix = false;

  VM_Lister (VM_Assembler asm) {
    this.asm = asm;
  }

  final void lockPrefix() {
    lockPrefix = true;
  }

  final void OP (int i, String op) {
    i = begin(i, op);
    VM.sysWrite(right("", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void I (int i, String op, int n) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(n) + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
    
  final void R (int i, String op, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RD (int i, String op, byte R0, int d) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RI (int i, String op, byte R0, int n) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RDI (int i, String op, byte R0, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RNI (int i, String op, byte R0, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n) + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RR (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RDR (int i, String op, byte R0, int d, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RDRI (int i, String op, byte R0, int d, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RRD (int i, String op, byte R0, byte R1, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(d) + "[" + GPR_NAMES[R1] + "]", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RNR (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RN (int i, String op, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(" ", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RRN (int i, String op, byte R0, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + GPR_NAMES[R1] + "]", SOURCE_AREA_SIZE));
    end(i);
  }
 
  final void RXD (int i, String op, byte R0, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDI (int i, String op, byte R0, byte X, short s, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFD (int i, String op, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RA (int i, String op, int d) {
    i = begin(i, op);
    VM.sysWrite(right("[" + hex(d) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right("", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDI (int i, String op, byte X, short s, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RAI (int i, String op, int d, int n) {
    i = begin(i, op);
    VM.sysWrite(right("[" + hex(d) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(decimal(n), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRR (int i, String op, byte R0, byte R1, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RNRI (int i, String op, byte R0, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right("[" + GPR_NAMES[R0] + "] ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RRI (int i, String op, byte R0, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE) );
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RRXD (int i, String op, byte R0, byte R1, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R1] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RXDR (int i, String op, byte R0, byte X, short s, int d, byte R1) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDRI (int i, String op, byte R0, byte X, short s, int d, byte R1, int imm) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RXDRR (int i, String op, byte R0, byte X, short s, int d, byte R1, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[R0] + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R1]:GPR_NAMES[R1] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRFD (int i, String op, byte R0, byte X, short s, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RFDR (int i, String op, byte X, short s, int d, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) +  "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDRI (int i, String op, byte X, short s, int d, byte R0, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RFDRR (int i, String op, byte X, short s, int d, byte R0, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + decimal(d) + "+" + GPR_NAMES[X] + "<<" + decimal(s) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RRA (int i, String op, byte R0, int d) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0], DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    end(i);
  }
  
  final void RAR (int i, String op, int d, byte R0) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    end(i);
  }

  final void RARI (int i, String op, int d, byte R0, int imm) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(decimal(imm), SOURCE_AREA_SIZE));
    end(i);
  }

  final void RARR (int i, String op, int d, byte R0, byte R2) {
    i = begin(i, op);
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R0]:GPR_NAMES[R0] + " ", DEST_AREA_SIZE));
    VM.sysWrite(right("[" + hex(d) + "]", SOURCE_AREA_SIZE));
    VM.sysWrite(right(isFP(op)?FPR_NAMES[R2]:GPR_NAMES[R2] + " ", SOURCE_AREA_SIZE));
    end(i);
  }

  private final int begin(int i, String op) {
    if (lockPrefix) i--;
    VM.sysWrite(right(hex(i),6) + "| ");
    if (lockPrefix) {
      VM.sysWrite(right("LOCK", PREFIX_AREA_SIZE) + " ");
    } else {
      VM.sysWrite(right("", PREFIX_AREA_SIZE) + " ");
    }
    VM.sysWrite( left(op, OP_AREA_SIZE));
    return i;
  }

  private final void end(int i) {
    VM.sysWrite(" | ");
    asm.writeLastInstruction(i);
    VM.sysWrite("\n");
    lockPrefix = false;
  }

  private final static boolean isFP(String op) {
    return op.startsWith("F");
  }

  private final static String left (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(0,w);
    for (int i=n; i<w; i++) {
      s = s + " ";
    }
    return s; 
  }

  private final static String left (int i, int w) {
    return left(decimal(i), w); 
  }

  private final static String right (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(n-w);
    for (int i=n; i<w; i++) {
      s = " " + s;
    } 
    return s; 
  }

  private final static String right (int i, int w) {
    return right(decimal(i), w); 
  }

  final static String decimal (int n) {
    if (n==0) return "0";
    String sign = "";
    if (n<0) {
      sign = "-";
      n = -n;
    }
    String result = "";
    while (0<n) {
      int i = n%10;
      n /= 10;
      if (i==0) result = "0" + result;
      else if (i==1) result = "1" + result;
      else if (i==2) result = "2" + result;
      else if (i==3) result = "3" + result;
      else if (i==4) result = "4" + result;
      else if (i==5) result = "5" + result;
      else if (i==6) result = "6" + result;
      else if (i==7) result = "7" + result;
      else if (i==8) result = "8" + result;
      else if (i==9) result = "9" + result;
    }
    return (sign + result);
  }

  private final static String decimal (short s) {
    return decimal((int) s);
  }

  final static String hex (int i) {
    return (hex((short) (i>>16)) + hex((short) i));
  }

  final static String hex (short i) {
    return (hex((byte) (i>>8)) + hex((byte) i));
  }

  final static String hex (byte b) {
    int  i = b & 0xFF;
    byte j = (byte) (i/0x10);
    String s;
         if (j==0x0) s = "0";
    else if (j==0x1) s = "1";
    else if (j==0x2) s = "2";
    else if (j==0x3) s = "3";
    else if (j==0x4) s = "4";
    else if (j==0x5) s = "5";
    else if (j==0x6) s = "6";
    else if (j==0x7) s = "7";
    else if (j==0x8) s = "8";
    else if (j==0x9) s = "9";
    else if (j==0xA) s = "A";
    else if (j==0xB) s = "B";
    else if (j==0xC) s = "C";
    else if (j==0xD) s = "D";
    else if (j==0xE) s = "E";
    else             s = "F";
    j = (byte) (i%0x10);
    String t;
	 if (j==0x0) t = "0";
    else if (j==0x1) t = "1";
    else if (j==0x2) t = "2";
    else if (j==0x3) t = "3";
    else if (j==0x4) t = "4";
    else if (j==0x5) t = "5";
    else if (j==0x6) t = "6";
    else if (j==0x7) t = "7";
    else if (j==0x8) t = "8";
    else if (j==0x9) t = "9";
    else if (j==0xA) t = "A";
    else if (j==0xB) t = "B";
    else if (j==0xC) t = "C";
    else if (j==0xD) t = "D";
    else if (j==0xE) t = "E";
    else             t = "F";
    return s + t;
  }

  final void noteBytecode (int i, String bcode) {
    VM.sysWrite("[" + decimal(i) + "] " + bcode + "\n");
  }

  final void comment (int i, String comment) {
    VM.sysWrite(right(hex(i),6) + "| " + comment + "\n");
  }

  final void comefrom (int i, int j) {
    VM.sysWrite(right(hex(i),6) + "| <<< " + right(hex(j),6) + "\n");
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/** 
 * @author Julian Dolby
 */
class VM_MachineCode {

  /* interface */

  VM_MachineCode (INSTRUCTION[] i, int[] bm) {
    instructions = i;
    bytecodeMap  = bm;
  }

  final INSTRUCTION[] getInstructions () {
    return instructions;
  }

  final int[] getBytecodeMap () {
    return bytecodeMap;
  }

  /* implementation */

  private INSTRUCTION[] instructions;
  private int  [] bytecodeMap;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * @author Bowen Alpern
 * @author Stephen Fink
 * @author Dave Grove
 */
interface VM_RegisterConstants {
  //---------------------------------------------------------------------------------------//
  //               RVM register usage conventions - Intel version.                         //
  //---------------------------------------------------------------------------------------//
    
  static final byte LG_INSTRUCTION_WIDTH = 0;             // log2 of instruction width in bytes
  static final String INSTRUCTION_ARRAY_SIGNATURE = "[B"; // signature for "array of instructions"
    
  // Symbolic values for fixed-point registers.
  // These values are used to assemble instructions and as indices into:
  //    VM_Registers.gprs[]
  //    VM_Registers.fprs[]
  //    VM_GCMapIterator.registerLocations[]
  //    VM_RegisterConstants.GPR_NAMES[]
  //
  static final byte EAX = 0x0;
  static final byte ECX = 0x1;
  static final byte EDX = 0x2;
  static final byte EBX = 0x3;
  static final byte ESP = 0x4;
  static final byte EBP = 0x5;
  static final byte ESI = 0x6;
  static final byte EDI = 0x7;

  // Mnemonics corresponding to the above constants.
  static final String[] GPR_NAMES = {
    "eax", "ecx", "edx", "ebx", "esp", "ebp", "esi", "edi"
  };

  static final byte FP0 = 0x0;
  static final byte FP1 = 0x1;
  static final byte FP2 = 0x2;
  static final byte FP3 = 0x3;
  static final byte FP4 = 0x4;
  static final byte FP5 = 0x5;
  static final byte FP6 = 0x6;
  static final byte FP7 = 0x7;
  
  static final String [] FPR_NAMES = {
    "FP0", "FP1", "FP2", "FP3", "FP4", "FP5", "FP6", "FP7"
  };
    
  // Register sets (``range'' is a misnomer for the alphabet soup of
  // of intel registers)
  //

  // Note: the order here is important.  The opt-compiler allocates
  // the volatile registers in the order they appear here.
  static final byte[]  VOLATILE_GPRS = { EAX, EDX, ECX };
  static final int NUM_VOLATILE_GPRS = VOLATILE_GPRS.length;
    
  // Note: the order here is very important.  The opt-compiler allocates
  // the nonvolatile registers in the reverse of order they appear here.
  // EBX must be last, because it is the only non-volatile that can
  // be used in instructions that are using r8 and we must ensure that
  // opt doesn't skip over another nonvol while looking for an r8 nonvol.
  static final byte[]  NONVOLATILE_GPRS = { EBP, EDI, EBX};
  static final int NUM_NONVOLATILE_GPRS = NONVOLATILE_GPRS.length;
    
  static final byte[]  VOLATILE_FPRS = { FP0, FP1, FP2, FP3, FP4, FP5, FP6, FP7 }; 
  static final int NUM_VOLATILE_FPRS = VOLATILE_FPRS.length;
    
  static final byte[]  NONVOLATILE_FPRS = {  }; 
  static final int NUM_NONVOLATILE_FPRS = NONVOLATILE_FPRS.length;

  /*
   * These constants represent the number of volatile registers used
   * to pass parameters in registers.  They are defined to mean that
   * the first n registers in the corresponding set of volatile
   * registers are used to pass parameters.
   */
  static final int NUM_PARAMETER_GPRS = 2;
  static final int NUM_PARAMETER_FPRS = 4;
  static final int NUM_RETURN_GPRS = 2;
  static final int NUM_RETURN_FPRS = 1;


  // Dedicated registers.
  //
  static final byte STACK_POINTER              = ESP;
  static final byte PROCESSOR_REGISTER         = ESI;
   
  static final byte NUM_GPRS                   =  8;
  static final byte NUM_FPRS                   =  8;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * The machine state comprising a thread's execution context.
 *
 * @author Bowen Alpern
 * @author David Grove
 */
public class VM_Registers implements VM_Constants, VM_Uninterruptible {

  // The following are used both for thread context switching
  // and for software/hardware exception reporting/delivery.
  //
  public int    gprs[]; // general purpose registers
  public double fprs[]; // floating point registers
  public VM_Address ip;     // instruction address register
  public VM_Address fp;     // frame pointer
  
  // set by C hardware exception handler and VM_Runtime.athrow 
  // and reset by each implementation of VM_ExceptionDeliverer.deliverException
  //
  public boolean inuse; // do exception registers currently contain live values?
  
  public VM_Registers() {
    gprs = new int[NUM_GPRS];
    fprs = new double[NUM_FPRS];
  }
  
  /**
   * Return framepointer for the deepest stackframe
   */
  public final VM_Address getInnermostFramePointer() {
    return fp;
  }
  
  /**
   * Return next instruction address for the deepest stackframe
   */
  public final VM_Address getInnermostInstructionAddress() {
    return ip;
  }

  /**
   * update the machine state as if the stackframe were unwound.
   */
  public final void unwindStackFrame() {
    ip = VM_Magic.getReturnAddress(fp);
    fp = VM_Magic.getCallerFramePointer(fp);
  }

  /**
   * set ip & fp. used to control the stack frame at which a scan of
   * the stack during GC will start, for ex., the top java frame for
   * a thread that is blocked in native code during GC.
   */
  public final void setInnermost(VM_Address newip, VM_Address newfp) {
    ip = newip;
    fp = newfp;
  }

  /**
   * set ip and fp values to those of the caller. used just prior to entering
   * sigwait to set fp & ip so that GC will scan the threads stack
   * starting at the frame of the method that called sigwait.
   */
  public final void setInnermost() {
    VM_Address current_fp = VM_Magic.getFramePointer();
    ip = VM_Magic.getReturnAddress(current_fp);
    fp = VM_Magic.getCallerFramePointer(current_fp);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */

//$Id$

/**----------------------------------------------------------------------
 *                   Stackframe layout conventions - Intel version.  
 *-----------------------------------------------------------------------
 *
 * A stack is an array of "slots", declared formally as integers, each slot
 * containing either a primitive (byte, int, float, etc), an object pointer,
 * a machine code pointer (a return address pointer), or a pointer to another
 * slot in the same stack (a frame pointer). The interpretation of a slot's 
 * contents depends on the current value of IP, the machine instruction 
 * address register.
 * Each machine code generator provides maps, for use by the garbage collector,
 * that tell how to interpret the stack slots at "safe points" in the
 * program's execution.
 *
 * Here's a picture of what a stack might look like in memory.
 *
 * Note: this (array) object is drawn upside down compared to other objects
 * because the hardware stack grows from high memory to low memory, but
 * array objects are layed out from low memory to high (header first).
 * <pre>
 *  hi-memory
 *              +---------------+                                            ...
 *              |     IP=0      |                                             .
 *              +---------------+                                             .
 *          +-> |     FP=0      |   <-- "end of vm stack" sentinal            .
 *          |   +---------------+                                             . caller's frame
 *          |   |    cmid=0      |   <-- "invisible method" id                .
 *          |   +---------------+                                          ---.
 *          |   |   parameter0  |  \                                        | .
 *          |   +---------------+   \ parameter area                        | .
 *          |   |   parameter1  |   /  (== caller's operand stack area)     | .
 *   ---    |   +---------------+  /                                        |...
 *    |     |   |   saved IP    |  <-- return address (in caller)           |
 *    |      \  +---------------+                                           |
 *  header FP-> |   saved FP    |  <-- this frame's caller's frame          |
 *    |         +---------------+                                           |
 *    |         |    cmid       |  <-- this frame's compiledmethod id       |
 *    |         +---------------+                                           |
 *    |         |   saved GPRs  |  \                                        |
 *    |         +---------------+   \ nonvolatile register save area        |
 *    |         |   saved FPRS  |   /                                       | frame
 *    |         +---------------+                                           |
 *    |         |   local0      |  \                                        |
 *   body       +---------------+   \_local variables area                  |
 *    |         |   local1      |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |   operand0    |  \                                        |
 *    |         +---------------+   \_operand stack area                    |
 *    |    SP-> |   operand1    |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |     ...       |                                           |
 *   ---        +===============+                                          ---
 *              |     ...       |
 *              +---------------+
 * stackLimit-> |     ...       | \
 *              +---------------+  \_guard region for detecting & processing stack overflow
 *              |     ...       |  /
 *              +---------------+ /
 *              |(object header)|
 *  low-memory  +---------------+
 *
 *
 *
 *  The opt compiler uses a different stackframe layout
 *
 *  hi-memory
 *              +---------------+                                            ...
 *              |     IP=0      |                                             .
 *              +---------------+                                             .
 *          +-> |     FP=0      |   <-- "end of vm stack" sentinal           .
 *          |   +---------------+                                             . caller's frame
 *          |   |    cmid=-1    |   <-- "invisible method" id                .
 *          |   +---------------+                                          ---.
 *          |   |   parameter0  |  \                                        | .
 *          |   +---------------+   \ parameter area                        | .
 *          |   |   parameter1  |   /  (== caller's operand stack area)     | .
 *   ---    |   +---------------+  /                                        |...
 *    |     |   |   saved IP    |  <-- return address (in caller)           |
 *    |      \  +---------------+                                           |
 *  header FP-> |   saved FP    |  <-- this frame's caller's frame          |
 *    |         +---------------+                                           |
 *    |         |    cmid       |  <-- this frame's compiledmethod id       |
 *   ---        +---------------+                                           |
 *    |         |               |                                           |
 *    |         |  Spill Area   |  <-- spills and other method-specific     |
 *    |         |     ...       |      compiler-managed storage             |
 *    |         +---------------+                                           |
 *    |         |   Saved FP    |     only SaveVolatile Frames              |   
 *    |         |    State      |                                           |
 *    |         +---------------+                                           |
 *    |         |  VolGPR[0]    |                                           |
 *    |         |     ...       |     only SaveVolatile Frames              |   
 *    |         |  VolGPR[n]    |                                           |
 *    |         +---------------+                                           | 
 *   body       |  NVolGPR[k]   |  <-- info.getUnsignedNonVolatileOffset()  | frame
 *    |         |     ...       |   k == info.getFirstNonVolatileGPR()      |
 *    |         |  NVolGPR[n]   |                                           |
 *    |         +---------------+                                           |
 *    |         |  NVolFPR[k]   |                                           |
 *    |         |     ...       |   k == info.getFirstNonVolatileFPR()      |
 *    |         |  NVolFPR[n]   |                                           |
 *    |         +---------------+                                           |
 *    |         |   parameter0  |  \                                        |
 *    |         +---------------+   \_parameters to callee frame            |
 *    |    SP-> |   parameter1  |   /                                       |
 *    |         +---------------+  /                                        |
 *    |         |     ...       |                                           |
 *   ---        +===============+                                          ---
 *              |     ...       |
 *              +---------------+
 * stackLimit-> |     ...       | \
 *              +---------------+  \_guard region for detecting & processing stack overflow
 *              |     ...       |  /
 *              +---------------+ /
 *              |(object header)|
 *  low-memory  +---------------+
 *
 * </pre>
 *
 * @author David Grove
 * @author Bowen Alpern
 */
interface VM_StackframeLayoutConstants  {

   static final int STACKFRAME_RETURN_ADDRESS_OFFSET   =  4; // offset of caller's return address from FP
   static final int STACKFRAME_FRAME_POINTER_OFFSET    =  0; // base of this frame
   static final int STACKFRAME_METHOD_ID_OFFSET        = -4; // offset of method id from FP
   static final int STACKFRAME_BODY_OFFSET             = -8; // offset of work area from FP
   static final int STACKFRAME_HEADER_SIZE             = 12; // size of frame header, in bytes
   
   // space to save entire FPU state.  The FPU state is saved only for 'bridge' frames
   static final int FPU_STATE_SIZE       	       = 108;

   static final int STACKFRAME_SENTINAL_FP = -2; // fp value indicating end of stack walkback
   static final int INVISIBLE_METHOD_ID    = -1; // marker for "assembler" frames that have no associated VM_Method

   // Stackframe alignment.
   // Align to 8 byte boundary for good floating point save/restore performance (on powerPC, anyway).
   //
   static final int STACKFRAME_ALIGNMENT = 8;
   static final int STACKFRAME_ALIGNMENT_MASK = STACKFRAME_ALIGNMENT - 1; // roundedUpSize = (size + STACKFRAME_ALIGNMENT_MASK) & ~STACKFRAME_ALIGNMENT_MASK
   
   // Sizes for stacks and subregions thereof.
   // Values are in bytes and must be a multiple of 4 (size of a stack slot).
   //
   static final int STACK_SIZE_GROW      = 8*1024; // how much to grow stack when overflow detected
   static final int STACK_SIZE_GUARD     = 8*1024; // max space needed for stack overflow trap processing
   static final int STACK_SIZE_SYSCALL   = 4*1024; // max space needed for any native code called by vm
   static final int STACK_SIZE_DLOPEN    = 30*1024; // max space needed for dlopen sys call
   static final int STACK_SIZE_GCDISABLED= 4*1024; // max space needed while running with gc disabled
   
   // Complications:
   // - STACK_SIZE_GUARD must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that frames allocated by stack growing code will fit within guard region.
   // - STACK_SIZE_GROW must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that, if stack is grown prior to disabling gc or calling native code,
   //   the new stack will accomodate that code without generating a stack overflow trap.
   // - Values chosen for STACK_SIZE_NATIVE and STACK_SIZE_GCDISABLED are pure guesswork
   //   selected by trial and error.
   
   // Stacks for "normal" threads grow as needed by trapping on guard region.
   // Stacks for "boot" and "collector" threads are fixed in size and cannot grow.
   //
   static final int STACK_SIZE_NORMAL    = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +   8*1024; // initial stack space to allocate for normal    thread (includes guard region)

   static final int STACK_SIZE_BOOT      = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for boot      thread (includes guard region)
   static final int STACK_SIZE_COLLECTOR = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for collector thread (includes guard region)
   static final int STACK_SIZE_MAX       = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED + 244*1024; // upper limit on stack size (includes guard region)
   
   static final int STACK_SIZE_JNINATIVE_GROW = 0; // TODO!!;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *
 * @author Bowen Alpern
 * @author David Grove
 */
interface VM_TrapConstants
   {

       /** 
	*  This base is added to the numeric trap codes in VM_Runtime.java
	* to yield the intel trap number that is given to INT instructions
	*/
       public final static byte RVM_TRAP_BASE = (byte) 0x40;

   }
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frame  built by the Baseline compiler
 * An Instance of this class will iterate through a particular 
 * reference map of a method returning the offsets of any refereces
 * that are part of the input parameters, local variables, and 
 * java stack for the stack frame.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_BaselineGCMapIterator extends VM_GCMapIterator 
  implements VM_BaselineConstants,
	     VM_Uninterruptible {
  private static final boolean TRACE_ALL = false;
  private static final boolean TRACE_DL  = false; // dynamic link frames

  //-------------//
  // Constructor //
  //-------------//

  // 
  // Remember the location array for registers. This array needs to be updated
  // with the location of any saved registers.
  // This information is not used by this iterator but must be updated for the
  // other types of iterators (ones for the quick and opt compiler built frames)
  // The locations are kept as addresses within the stack.
  //
  
  public VM_BaselineGCMapIterator(int registerLocations[]) {
    this.registerLocations = registerLocations; // (in superclass)
    dynamicLink  = new VM_DynamicLink();
  }

  //-----------//
  // Interface //
  //-----------//

  //
  // Set the iterator to scan the map at the machine instruction offset provided.
  // The iterator is positioned to the beginning of the map
  //
  //   method - identifies the method and class
  //   instruction offset - identifies the map to be scanned.
  //   fp  - identifies a specific occurrance of this method and
  //         allows for processing instance specific information
  //         i.e JSR return address values
  //
  //  NOTE: An iterator may be reused to scan a different method and map.
  //
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address fp) {
    currentMethod = compiledMethod.getMethod();
      
    // setup superclass
    //
    framePtr = fp;
      
    // setup stackframe mapping
    //
    maps      = ((VM_BaselineCompiledMethod)compiledMethod).referenceMaps;
    mapId     = maps.locateGCPoint(instructionOffset, currentMethod);
    mapOffset = 0;
    if (mapId < 0) {
      // lock the jsr lock to serialize jsr processing
      VM_ReferenceMaps.jsrLock.lock();
      maps.setupJSRSubroutineMap(framePtr, mapId, compiledMethod);
    }
    if (VM.TraceStkMaps || TRACE_ALL ) {
      VM.sysWrite("VM_BaselineGCMapIterator setupIterator mapId = ");
      VM.sysWrite(mapId);
      VM.sysWrite(".\n");
    }
      
    // setup dynamic bridge mapping
    //
    bridgeTarget                   = null;
    bridgeParameterTypes           = null;
    bridgeParameterMappingRequired = false;
    bridgeRegistersLocationUpdated = false;
    bridgeParameterIndex           = 0;
    bridgeRegisterIndex            = 0;
    bridgeRegisterLocation         = VM_Address.zero();
    bridgeSpilledParamLocation     = VM_Address.zero();
    
    if (currentMethod.getDeclaringClass().isDynamicBridge()) {
      VM_Address        ip                       = VM_Magic.getReturnAddress(fp);
                        fp                       = VM_Magic.getCallerFramePointer(fp);
      int               callingCompiledMethodId  = VM_Magic.getCompiledMethodID(fp);
      VM_CompiledMethod callingCompiledMethod    = VM_CompiledMethods.getCompiledMethod(callingCompiledMethodId);
      int               callingInstructionOffset = ip.diff(VM_Magic.objectAsAddress(callingCompiledMethod.getInstructions()));

      callingCompiledMethod.getDynamicLink(dynamicLink, callingInstructionOffset);
      bridgeTarget                    = dynamicLink.methodRef();
      bridgeParameterTypes            = bridgeTarget.getParameterTypes();
      if (dynamicLink.isInvokedWithImplicitThisParameter()) {
	bridgeParameterInitialIndex     = -1;
	bridgeSpilledParamInitialOffset =  8; // this + return addr
      } else {	
	bridgeParameterInitialIndex     =  0;
	bridgeSpilledParamInitialOffset =  4; // return addr
      }
      bridgeSpilledParamInitialOffset  += (4 * bridgeTarget.getParameterWords());
      if (callingCompiledMethod.getCompilerType() == VM_CompiledMethod.BASELINE) {
	bridgeSpilledParameterMappingRequired = false;
      } else {
	bridgeSpilledParameterMappingRequired = true;
      }
    }
        
    reset();
  }
  
  // Reset iteration to initial state.
  // This allows a map to be scanned multiple times
  //
  public void reset() {
    mapOffset = 0;

    if (bridgeTarget != null) {
      bridgeParameterMappingRequired = true;
      bridgeParameterIndex           = bridgeParameterInitialIndex;
      bridgeRegisterIndex            = 0;
      bridgeRegisterLocation         = framePtr.add(STACKFRAME_FIRST_PARAMETER_OFFSET); // top of frame
      bridgeSpilledParamLocation     = framePtr.add(bridgeSpilledParamInitialOffset);
    }
  }

  // Get location of next reference.
  // A zero return indicates that no more references exist.
  //
  public VM_Address getNextReferenceAddress() {
    if (mapId < 0) {
      mapOffset = maps.getNextJSRRef(mapOffset);
    } else {
      mapOffset = maps.getNextRef(mapOffset, mapId);
    }
    if (VM.TraceStkMaps || TRACE_ALL) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = ");
      VM.sysWriteHex(mapOffset);
      VM.sysWrite(".\n");
      VM.sysWrite("Reference is ");
      VM.sysWriteHex ( VM_Magic.getMemoryWord ( framePtr.add(mapOffset) ) );
      VM.sysWrite(".\n");
      if (mapId < 0) 
	VM.sysWrite("Offset is a JSR return address ie internal pointer.\n");
    }

    if (mapOffset != 0) {
      if (bridgeParameterMappingRequired)
	// TODO  clean this
	return (framePtr.add(mapOffset - BRIDGE_FRAME_EXTRA_SIZE ));
      else
	return (framePtr.add(mapOffset) );
    } else if (bridgeParameterMappingRequired) {
      if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	VM.sysWrite("getNextReferenceAddress: bridgeTarget="); VM.sysWrite(bridgeTarget); VM.sysWrite("\n");
      }         

      if (!bridgeRegistersLocationUpdated) {
	// point registerLocations[] to our callers stackframe
	//
	registerLocations[JTOC] = framePtr.add(JTOC_SAVE_OFFSET).toInt();
	registerLocations[T0]   = framePtr.add(T0_SAVE_OFFSET).toInt();
	registerLocations[T1]   = framePtr.add(T1_SAVE_OFFSET).toInt();
	registerLocations[EBX]  = framePtr.add(EBX_SAVE_OFFSET).toInt();
	
	bridgeRegistersLocationUpdated = true;
      }

      // handle implicit "this" parameter, if any
      //
      if (bridgeParameterIndex == -1) {
	bridgeParameterIndex       += 1;
	bridgeRegisterIndex        += 1;
	bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	
	if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	  VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link GPR this ");
	  VM.sysWrite(bridgeRegisterLocation.add(4));
	  VM.sysWrite(".\n");
	}

	return bridgeRegisterLocation.add(4);
      }
         
      // now the remaining parameters
      //
      while(bridgeParameterIndex < bridgeParameterTypes.length) {
	VM_Type bridgeParameterType = bridgeParameterTypes[bridgeParameterIndex++];
	
	if (bridgeParameterType.isReferenceType()) {
	  bridgeRegisterIndex        += 1;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	  
	  if (bridgeRegisterIndex <= NUM_PARAMETER_GPRS) {
	    if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
	      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link GPR parameter ");
	      VM.sysWrite(bridgeRegisterLocation.add(4));
	      VM.sysWrite(".\n");
	    }
	    return bridgeRegisterLocation.add(4);
	  } else {
	    if (bridgeSpilledParameterMappingRequired) {
	      if (VM.TraceStkMaps || TRACE_ALL || TRACE_DL) {
		VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = dynamic link spilled parameter ");
		VM.sysWrite(bridgeSpilledParamLocation.add(4));
		VM.sysWrite(".\n");
	      }
	      return bridgeSpilledParamLocation.add(4);
	    } else {
	      break;
	    }
	  }
	} else if (bridgeParameterType.isLongType()) {
	  bridgeRegisterIndex        += 2;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(8);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(8);
	} else if (bridgeParameterType.isDoubleType()) {
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(8);
	} else if (bridgeParameterType.isFloatType()) {
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	} else { 
	  // boolean, byte, char, short, int
	  bridgeRegisterIndex        += 1;
	  bridgeRegisterLocation     = bridgeRegisterLocation.sub(4);
	  bridgeSpilledParamLocation = bridgeSpilledParamLocation.sub(4);
	}
      }
    } else {
      // point registerLocations[] to our callers stackframe
      //
      registerLocations[JTOC] = framePtr.add(JTOC_SAVE_OFFSET).toInt();
    }
    
    return VM_Address.zero();
  }

  //
  // Gets the location of the next return address
  // after the current position.
  //  a zero return indicates that no more references exist
  //
  public VM_Address getNextReturnAddressAddress() {
    if (mapId >= 0) {
      if (VM.TraceStkMaps || TRACE_ALL) {
	VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset mapId = ");
	VM.sysWrite(mapId);
	VM.sysWrite(".\n");
      }
      return VM_Address.zero();
    }
    mapOffset = maps.getNextJSRReturnAddr(mapOffset);
    if (VM.TraceStkMaps || TRACE_ALL) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
    }
    return (mapOffset == 0) ? VM_Address.zero() : framePtr.add(mapOffset);
  }

  // cleanup pointers - used with method maps to release data structures
  //    early ... they may be in temporary storage ie storage only used
  //    during garbage collection
  //
  public void cleanupPointers() {
    maps.cleanupPointers();
    maps = null;
    if (mapId < 0)   
      VM_ReferenceMaps.jsrLock.unlock();
    bridgeTarget         = null;
    bridgeParameterTypes = null;
  }

  public int getType() {
    return VM_CompiledMethod.BASELINE;
  }

  // For debugging (used with checkRefMap)
  //
  public int getStackDepth() {
    return maps.getStackDepth(mapId);
  }

  // Iterator state for mapping any stackframe.
  //
  private   int              mapOffset; // current offset in current map
  private   int              mapId;     // id of current map out of all maps
  private   VM_ReferenceMaps maps;      // set of maps for this method

  // Additional iterator state for mapping dynamic bridge stackframes.
  //
  private VM_DynamicLink dynamicLink;                    // place to keep info returned by VM_CompiledMethod.getDynamicLink
  private VM_Method      bridgeTarget;                   // method to be invoked via dynamic bridge (null: current frame is not a dynamic bridge)
  private VM_Method      currentMethod;                  // method for the frame
  private VM_Type[]      bridgeParameterTypes;           // parameter types passed by that method
  private boolean        bridgeParameterMappingRequired; // have all bridge parameters been mapped yet?
  private boolean        bridgeSpilledParameterMappingRequired; // do we need to map spilled params (baseline compiler = no, opt = yes)
  private boolean        bridgeRegistersLocationUpdated; // have the register location been updated
  private int            bridgeParameterInitialIndex;    // first parameter to be mapped (-1 == "this")
  private int            bridgeParameterIndex;           // current parameter being mapped (-1 == "this")
  private int            bridgeRegisterIndex;            // gpr register it lives in
  private VM_Address     bridgeRegisterLocation;         // memory address at which that register was saved
  private VM_Address     bridgeSpilledParamLocation;     // current spilled param location
  private int            bridgeSpilledParamInitialOffset;// starting offset to stack location for param0
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * Registers used by baseline compiler implementation of virtual machine.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
interface VM_BaselineConstants extends VM_Constants {
  
  static final int    WORDSIZE = 4; // bytes
  static final int LG_WORDSIZE = 2; 

  // Dedicated registers.
  //
  static final byte JTOC = EDI;
  static final byte SP   = ESP;
  static final byte PR   = PROCESSOR_REGISTER;
  
  // Volatile (parameter) registers.
  //
  static final byte T0   =  EAX;  // DO NOT CHANGE THIS ASSIGNMENT
  static final byte T1   =  EDX; 
  
  // scratch register
  static final byte S0  =  ECX;

  // Mnemonics corresponding to the above constants.
  // These are some alternate names that can be used in the debugger
  //
  static final String[] RVM_GPR_NAMES =
     {
     "eax", "ecx", "edx", "ebx", "esp", "ebp", "PR", "JT"
     };

  // Constants describing baseline compiler conventions for
  // saving registers in stackframes.
  // 
  static final int STACKFRAME_REG_SAVE_OFFSET	       = STACKFRAME_BODY_OFFSET;
                                        // offset from FP of the saved registers.  
					// Some registers are saved in all baseline
					// frames, and most register as saved in the
					// dynamic bridge frames.
  static final int STACKFRAME_FIRST_PARAMETER_OFFSET  = STACKFRAME_REG_SAVE_OFFSET -8;
  // bridge frames save 3 additional GPRs
  static final int BRIDGE_FRAME_EXTRA_SIZE	       = FPU_STATE_SIZE + 12;

  static final int SAVED_GPRS       = 1; // JTOC is a nonvolatile registers used by baseline compiler
  static final int JTOC_SAVE_OFFSET = STACKFRAME_REG_SAVE_OFFSET;
  static final int EBX_SAVE_OFFSET  = STACKFRAME_REG_SAVE_OFFSET - 4;
  static final int T0_SAVE_OFFSET   = STACKFRAME_FIRST_PARAMETER_OFFSET ;
  static final int T1_SAVE_OFFSET   = STACKFRAME_FIRST_PARAMETER_OFFSET - 4;
  static final int FPU_SAVE_OFFSET  = T1_SAVE_OFFSET - FPU_STATE_SIZE;

}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Handle exception delivery and stack unwinding for methods compiled by 
 * baseline compiler.
 *
 * @author Derek Lieber
 * @date 18 Sep 1998 
 */
class VM_BaselineExceptionDeliverer extends VM_ExceptionDeliverer 
   implements VM_BaselineConstants  {

  /**
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod compiledMethod,
			VM_Address        catchBlockInstructionAddress,
			Throwable         exceptionObject,
			VM_Registers      registers) {
    VM_Address fp     = registers.getInnermostFramePointer();
    VM_Method method = compiledMethod.getMethod();
    VM_Thread myThread = VM_Thread.getCurrentThread();

    // reset sp to "empty expression stack" state
    //
    VM_Address sp = fp.add(VM_Compiler.getEmptyStackOffset(method));
    
    // push exception object as argument to catch block
    //
    sp = sp.sub(4);
    VM_Magic.setMemoryAddress(sp, VM_Magic.objectAsAddress(exceptionObject));
    registers.gprs[SP] = sp.toInt();

    // set address at which to resume executing frame
    registers.ip = catchBlockInstructionAddress;

    // branch to catch block
    //
    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called
    if (VM.VerifyAssertions) VM.assert(registers.inuse == true); 

    registers.inuse = false;

    // 'give back' the portion of the stack we borrowed to run 
    // exception delivery code when invoked for a hardware trap.
    // If this was a straight software trap (athrow) then setting 
    // the stacklimit should be harmless, since the stacklimit should already have exactly
    // the value we are setting it too. 
    if (!myThread.hardwareExceptionRegisters.inuse) {
      myThread.stackLimit = VM_Magic.objectAsAddress(myThread.stack).add(STACK_SIZE_GUARD);
      VM_Processor.getCurrentProcessor().activeThreadStackLimit = myThread.stackLimit;
    }

    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }
   

  /**
   * Unwind a stackframe.
   */
  void unwindStackFrame(VM_CompiledMethod compiledMethod, VM_Registers registers) {
    VM_Method method = compiledMethod.getMethod();
    VM_Address fp     = registers.getInnermostFramePointer();
    if (method.isSynchronized()) { // release the lock, if it is being held
      VM_Address ip = registers.getInnermostInstructionAddress();
      int instr = ip.diff(VM_Magic.objectAsAddress(compiledMethod.getInstructions()));
      int lockOffset = ((VM_BaselineCompiledMethod)compiledMethod).getLockAcquisitionOffset();
      if (instr > lockOffset) { // we actually have the lock, so must unlock it.
	Object lock;
	if (method.isStatic()) {
	  lock = method.getDeclaringClass().getClassForType();
	} else {
	  lock = VM_Magic.addressAsObject(VM_Magic.getMemoryAddress(fp.add(VM_Compiler.getFirstLocalOffset(method))));
	}
	VM_ObjectModel.genericUnlock(lock);
      }
    }
    // Restore nonvolatile registers used by the baseline compiler.
    if (VM.VerifyAssertions) VM.assert(VM_Compiler.SAVED_GPRS == 1);
    registers.gprs[JTOC] = VM_Magic.getMemoryWord(fp.add(VM_Compiler.JTOC_SAVE_OFFSET));
    
    registers.unwindStackFrame();
  }
}


/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * 
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

public class VM_Compiler extends VM_BaselineCompiler implements VM_BaselineConstants {

  private final int parameterWords;
  private int firstLocalOffset;

  /**
   * Create a VM_Compiler object for the compilation of method.
   */
  VM_Compiler(VM_BaselineCompiledMethod cm) {
    super(cm);
    stackHeights = new int[bytecodes.length];
    parameterWords = method.getParameterWords() + (method.isStatic() ? 0 : 1); // add 1 for this pointer
  }

  /**
   * The last true local
   */
  static int getEmptyStackOffset (VM_Method m) {
    return getFirstLocalOffset(m) - (m.getLocalWords()<<LG_WORDSIZE) + WORDSIZE;
  }

  /**
   * This is misnamed.  It should be getFirstParameterOffset.
   * It will not work as a base to access true locals.
   * TODO!! make sure it is not being used incorrectly
   */
  static int getFirstLocalOffset (VM_Method method) {
    if (method.getDeclaringClass().isBridgeFromNative())
      return STACKFRAME_BODY_OFFSET - (VM_JNICompiler.SAVED_GPRS_FOR_JNI << LG_WORDSIZE);
    else
      return STACKFRAME_BODY_OFFSET - (SAVED_GPRS << LG_WORDSIZE);
  }
  

  /*
   * implementation of abstract methods of VM_BaselineCompiler
   */

  /*
   * Misc routines not directly tied to a particular bytecode
   */

  /**
   * Emit the prologue for the method
   */
  protected final void emit_prologue() {
    genPrologue();
  }

  /**
   * Emit code to complete the dynamic linking of a
   * prematurely resolved VM_Type.
   * @param dictionaryId of type to link (if necessary)
   */
  protected final void emit_initializeClassIfNeccessary(int dictionaryId) {
    asm.emitMOV_Reg_Imm (T0, dictionaryId);
    asm.emitPUSH_Reg    (T0);
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.initializeClassIfNecessaryMethod.getOffset());
  }

  /**
   * Emit the code for a threadswitch tests (aka a yieldpoint).
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  protected final void emit_threadSwitchTest(int whereFrom) {
    genThreadSwitchTest(whereFrom);
  }

  /**
   * Emit the code to implement the spcified magic.
   * @param magicMethod desired magic
   */
  protected final void emit_Magic(VM_Method magicMethod) {
    genMagic(magicMethod);
  }


  /*
   * Loading constants
   */


  /**
   * Emit code to load the null constant.
   */
  protected final void emit_aconst_null() {
    asm.emitPUSH_Imm(0);
  }

  /**
   * Emit code to load an int constant.
   * @param val the int constant to load
   */
  protected final void emit_iconst(int val) {
    asm.emitPUSH_Imm(val);
  }

  /**
   * Emit code to load a long constant
   * @param val the lower 32 bits of long constant (upper32 are 0).
   */
  protected final void emit_lconst(int val) {
    asm.emitPUSH_Imm(0);  // high part
    asm.emitPUSH_Imm(val);  //  low part
  }

  /**
   * Emit code to load 0.0f
   */
  protected final void emit_fconst_0() {
    asm.emitPUSH_Imm(0);
  }

  /**
   * Emit code to load 1.0f
   */
  protected final void emit_fconst_1() {
    asm.emitPUSH_Imm(0x3f800000);
  }

  /**
   * Emit code to load 2.0f
   */
  protected final void emit_fconst_2() {
    asm.emitPUSH_Imm(0x40000000);
  }

  /**
   * Emit code to load 0.0d
   */
  protected final void emit_dconst_0() {
    asm.emitPUSH_Imm(0x00000000);
    asm.emitPUSH_Imm(0x00000000);
  }

  /**
   * Emit code to load 1.0d
   */
  protected final void emit_dconst_1() {
    asm.emitPUSH_Imm(0x3ff00000);
    asm.emitPUSH_Imm(0x00000000);
  }

  /**
   * Emit code to load a 32 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc(int offset) {
    asm.emitPUSH_RegDisp(JTOC, offset);   
  }

  /**
   * Emit code to load a 64 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc2(int offset) {
    asm.emitPUSH_RegDisp(JTOC, offset+4); // high 32 bits 
    asm.emitPUSH_RegDisp(JTOC, offset);   // low 32 bits
  }


  /*
   * loading local variables
   */


  /**
   * Emit code to load an int local variable
   * @param index the local index to load
   */
  protected final void emit_iload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP,offset);
  }

  /**
   * Emit code to load a long local variable
   * @param index the local index to load
   */
  protected final void emit_lload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset); // high part
    asm.emitPUSH_RegDisp(ESP, offset); // low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to local a float local variable
   * @param index the local index to load
   */
  protected final void emit_fload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp (ESP, offset);
  }

  /**
   * Emit code to load a double local variable
   * @param index the local index to load
   */
  protected final void emit_dload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset); // high part
    asm.emitPUSH_RegDisp(ESP, offset); // low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to load a reference local variable
   * @param index the local index to load
   */
  protected final void emit_aload(int index) {
    int offset = localOffset(index);
    asm.emitPUSH_RegDisp(ESP, offset);
  }


  /*
   * storing local variables
   */


  /**
   * Emit code to store an int to a local variable
   * @param index the local index to load
   */
  protected final void emit_istore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset); 
  }

  /**
   * Emit code to store a long to a local variable
   * @param index the local index to load
   */
  protected final void emit_lstore(int index) {
    int offset = localOffset(index+1) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp(ESP, offset); // high part
    asm.emitPOP_RegDisp(ESP, offset); //  low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to store a float to a local variable
   * @param index the local index to load
   */
  protected final void emit_fstore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset);
  }

  /**
   * Emit code to store an double  to a local variable
   * @param index the local index to load
   */
  protected final void emit_dstore(int index) {
    int offset = localOffset(index+1) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp(ESP, offset); // high part
    asm.emitPOP_RegDisp(ESP, offset); //  low part (ESP has moved by 4!!)
  }

  /**
   * Emit code to store a reference to a local variable
   * @param index the local index to load
   */
  protected final void emit_astore(int index) {
    int offset = localOffset(index) - 4; // pop computes EA after ESP has moved by 4!
    asm.emitPOP_RegDisp (ESP, offset);
  }


  /*
   * array loads
   */


  /**
   * Emit code to load from an int array
   */
  protected final void emit_iaload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired int array element
  }

  /**
   * Emit code to load from a long array
   */
  protected final void emit_laload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);             // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, WORDSIZE); // load high part of desired long array element
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, 0);        // load low part of desired long array element
  }

  /**
   * Emit code to load from a float array
   */
  protected final void emit_faload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired float array element
  }

  /**
   * Emit code to load from a double array
   */
  protected final void emit_daload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);             // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, WORDSIZE); // load high part of double
    asm.emitPUSH_RegIdx(S0, T0, asm.LONG, 0);        // load low part of double
  }

  /**
   * Emit code to load from a reference array
   */
  protected final void emit_aaload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);       // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);       // S0 is the array ref
    genBoundsCheck(asm, T0, S0);              // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);      // complete popping the 2 args
    asm.emitPUSH_RegIdx(S0, T0, asm.WORD, 0); // push desired object array element
  }

  /**
   * Emit code to load from a byte/boolean array
   */
  protected final void emit_baload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                     // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                     // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // complete popping the 2 args
    asm.emitMOVSX_Reg_RegIdx_Byte(T1, S0, T0, asm.BYTE, 0); // load byte and sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                   // push sign extended byte onto stack
  }

  /**
   * Emit code to load from a char array
   */
  protected final void emit_caload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                      // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                      // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                             // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                     // complete popping the 2 args
    asm.emitMOVZX_Reg_RegIdx_Word(T1, S0, T0, asm.SHORT, 0); // load halfword without sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                    // push char onto stack
  }

  /**
   * Emit code to load from a short array
   */
  protected final void emit_saload() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                      // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 4);                      // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                             // T0 is index, S0 is address of array
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                     // complete popping the 2 args
    asm.emitMOVSX_Reg_RegIdx_Word(T1, S0, T0, asm.SHORT, 0); // load halfword sign extend to a 32 bit word
    asm.emitPUSH_Reg(T1);                                    // push sign extended short onto stack
  }


  /*
   * array stores
   */


  /**
   * Emit code to store to an int array
   */
  protected final void emit_iastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the int value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a long array
   */
  protected final void emit_lastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 8);                     // T0 is the array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 12);                    // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitPOP_Reg(T1);                                    // low part of long value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, 0, T1);        // [S0 + T0<<3 + 0] <- T1 store low part into array i.e.  
    asm.emitPOP_Reg(T1);                                    // high part of long value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, WORDSIZE, T1); // [S0 + T0<<3 + 4] <- T1 store high part into array i.e. 
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // remove index and ref from the stack
  }

  /**
   * Emit code to store to a float array
   */
  protected final void emit_fastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the float value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a double array
   */
  protected final void emit_dastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 8);                     // T0 is the array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 12);                    // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                            // T0 is index, S0 is address of array
    asm.emitPOP_Reg(T1);                                    // low part of double value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, 0, T1);        // [S0 + T0<<3 + 0] <- T1 store low part into array i.e.  
    asm.emitPOP_Reg(T1);                                    // high part of double value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.LONG, WORDSIZE, T1); // [S0 + T0<<3 + 4] <- T1 store high part into array i.e. 
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2);                    // remove index and ref from the stack
  }

  /**
   * Emit code to store to a reference array
   */
  protected final void emit_aastore() {
    asm.emitPUSH_RegDisp(SP, 2<<LG_WORDSIZE);        // duplicate array ref
    asm.emitPUSH_RegDisp(SP, 1<<LG_WORDSIZE);        // duplicate object value
    genParameterRegisterLoad(2);                     // pass 2 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.checkstoreMethod.getOffset()); // checkstore(array ref, value)
    if (VM_Collector.NEEDS_WRITE_BARRIER) 
      VM_Barriers.compileArrayStoreBarrier(asm);
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);              // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);              // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                     // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);              // T1 is the object value
    asm.emitMOV_RegIdx_Reg(S0, T0, asm.WORD, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);             // complete popping the 3 args
  }

  /**
   * Emit code to store to a byte/boolean array
   */
  protected final void emit_bastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the byte value
    asm.emitMOV_RegIdx_Reg_Byte(S0, T0, asm.BYTE, 0, T1); // [S0 + T0<<2] <- T1
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }

  /**
   * Emit code to store to a char array
   */
  protected final void emit_castore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the char value
    asm.emitMOV_RegIdx_Reg_Word(S0, T0, asm.SHORT, 0, T1);// store halfword element into array i.e. [S0 +T0] <- T1 (halfword)
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }

  /**
   * Emit code to store to a short array
   */
  protected final void emit_sastore() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 4);                   // T0 is array index
    asm.emitMOV_Reg_RegDisp(S0, SP, 8);                   // S0 is the array ref
    genBoundsCheck(asm, T0, S0);                          // T0 is index, S0 is address of array
    asm.emitMOV_Reg_RegDisp(T1, SP, 0);                   // T1 is the short value
    asm.emitMOV_RegIdx_Reg_Word(S0, T0, asm.SHORT, 0, T1);// store halfword element into array i.e. [S0 +T0] <- T1 (halfword)
    asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                  // complete popping the 3 args
  }


  /*
   * expression stack manipulation
   */


  /**
   * Emit code to implement the pop bytecode
   */
  protected final void emit_pop() {
    asm.emitPOP_Reg(T0);
  }

  /**
   * Emit code to implement the pop2 bytecode
   */
  protected final void emit_pop2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(T0);
  }

  /**
   * Emit code to implement the dup bytecode
   */
  protected final void emit_dup() {
    asm.emitMOV_Reg_RegInd (T0, SP);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup_x1 bytecode
   */
  protected final void emit_dup_x1() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup_x2 bytecode
   */
  protected final void emit_dup_x2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup2 bytecode
   */
  protected final void emit_dup2() {
    asm.emitMOV_Reg_RegDisp (T0, SP, 4);
    asm.emitMOV_Reg_RegInd (S0, SP);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
  }

  /**
   * Emit code to implement the dup2_x1 bytecode
   */
  protected final void emit_dup2_x1() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the dup2_x2 bytecode
   */
  protected final void emit_dup2_x2() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T1);
    asm.emitPOP_Reg(JTOC);                  // JTOC is scratch register
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(JTOC);
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(S0);
    asm.emitPUSH_Reg(T0);
    // restore JTOC register
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the swap bytecode
   */
  protected final void emit_swap() {
    asm.emitPOP_Reg(T0);
    asm.emitPOP_Reg(S0);
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(S0);
  }


  /*
   * int ALU
   */


  /**
   * Emit code to implement the iadd bytecode
   */
  protected final void emit_iadd() {
    asm.emitPOP_Reg(T0);
    asm.emitADD_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the isub bytecode
   */
  protected final void emit_isub() {
    asm.emitPOP_Reg(T0);
    asm.emitSUB_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the imul bytecode
   */
  protected final void emit_imul() {
    asm.emitPOP_Reg (T0);
    asm.emitIMUL2_Reg_RegInd(T0, SP);
    asm.emitMOV_RegInd_Reg (SP, T0);
  }

  /**
   * Emit code to implement the idiv bytecode
   */
  protected final void emit_idiv() {
    asm.emitMOV_Reg_RegDisp(ECX, SP, 0); // ECX is divisor; NOTE: can't use symbolic registers because of intel hardware requirements
    asm.emitMOV_Reg_RegDisp(EAX, SP, 4); // EAX is dividend
    asm.emitCDQ ();                      // sign extend EAX into EDX
    asm.emitIDIV_Reg_Reg(EAX, ECX);      // compute EAX/ECX - Quotient in EAX, remainder in EDX
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2); // complete popping the 2 values
    asm.emitPUSH_Reg(EAX);               // push result
  }

  /**
   * Emit code to implement the irem bytecode
   */
  protected final void emit_irem() {
    asm.emitMOV_Reg_RegDisp(ECX, SP, 0); // ECX is divisor; NOTE: can't use symbolic registers because of intel hardware requirements
    asm.emitMOV_Reg_RegDisp(EAX, SP, 4); // EAX is dividend
    asm.emitCDQ ();                      // sign extend EAX into EDX
    asm.emitIDIV_Reg_Reg(EAX, ECX);      // compute EAX/ECX - Quotient in EAX, remainder in EDX
    asm.emitADD_Reg_Imm(SP, WORDSIZE*2); // complete popping the 2 values
    asm.emitPUSH_Reg(EDX);               // push remainder
  }

  /**
   * Emit code to implement the ineg bytecode
   */
  protected final void emit_ineg() {
    asm.emitNEG_RegInd(SP); // [SP] <- -[SP]
  }

  /**
   * Emit code to implement the ishl bytecode
   */
  protected final void emit_ishl() {
    asm.emitPOP_Reg(ECX);
    asm.emitSHL_RegInd_Reg(SP, ECX);   
  }

  /**
   * Emit code to implement the ishr bytecode
   */
  protected final void emit_ishr() {
    asm.emitPOP_Reg (ECX);
    asm.emitSAR_RegInd_Reg (SP, ECX);  
  }

  /**
   * Emit code to implement the iushr bytecode
   */
  protected final void emit_iushr() {
    asm.emitPOP_Reg (ECX);
    asm.emitSHR_RegInd_Reg(SP, ECX); 
  }

  /**
   * Emit code to implement the iand bytecode
   */
  protected final void emit_iand() {
    asm.emitPOP_Reg(T0);
    asm.emitAND_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the ior bytecode
   */
  protected final void emit_ior() {
    asm.emitPOP_Reg(T0);
    asm.emitOR_RegInd_Reg (SP, T0);
  }

  /**
   * Emit code to implement the ixor bytecode
   */
  protected final void emit_ixor() {
    asm.emitPOP_Reg(T0);
    asm.emitXOR_RegInd_Reg(SP, T0);
  }

  /**
   * Emit code to implement the iinc bytecode
   * @param index index of local
   * @param val value to increment it by
   */
  protected final void emit_iinc(int index, int val) {
    int offset = localOffset(index);
    asm.emitADD_RegDisp_Imm(ESP, offset, val);
  }


  /*
   * long ALU
   */


  /**
   * Emit code to implement the ladd bytecode
   */
  protected final void emit_ladd() {
    asm.emitPOP_Reg(T0);                 // the low half of one long
    asm.emitPOP_Reg(S0);                 // the high half
    asm.emitADD_RegInd_Reg(SP, T0);          // add low halves
    asm.emitADC_RegDisp_Reg(SP, WORDSIZE, S0);   // add high halves with carry
  }

  /**
   * Emit code to implement the lsub bytecode
   */
  protected final void emit_lsub() {
    asm.emitPOP_Reg(T0);                 // the low half of one long
    asm.emitPOP_Reg(S0);                 // the high half
    asm.emitSUB_RegInd_Reg(SP, T0);          // subtract low halves
    asm.emitSBB_RegDisp_Reg(SP, WORDSIZE, S0);   // subtract high halves with borrow
  }

  /**
   * Emit code to implement the lmul bytecode
   */
  protected final void emit_lmul() {
    // 0: JTOC is used as scratch registers (see 14)
    // 1: load value1.low temp0, i.e., save value1.low
    // 2: eax <- temp0 eax is value1.low
    // 3: edx:eax <- eax * value2.low (product of the two low halves)
    // 4: store eax which is  result.low into place --> value1.low is destroyed
    // 5: temp1 <- edx which is the carry of the product of the low halves
    // aex and edx now free of results
    // 6: aex <- temp0 which is still value1.low
    // 7: pop into aex aex <- value2.low  --> value2.low is sort of destroyed
    // 8: edx:eax <- eax * value1.hi  (value2.low * value1.hi)
    // 9: temp1 += aex
    // 10: pop into eax; eax <- value2.hi -> value2.hi is sort of destroyed
    // 11: edx:eax <- eax * temp0 (value2.hi * value1.low)
    // 12: temp1 += eax  temp1 is now result.hi
    // 13: store result.hi
    // 14: restore JTOC
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);
    if (VM.VerifyAssertions) VM.assert(S0 != EDX);
    asm.emitMOV_Reg_RegDisp (JTOC, SP, 8);          // step 1: JTOC is temp0
    asm.emitMOV_Reg_Reg (EAX, JTOC);            // step 2
    asm.emitMUL_Reg_RegInd(EAX, SP);    // step 3
    asm.emitMOV_RegDisp_Reg (SP, 8, EAX);           // step 4
    asm.emitMOV_Reg_Reg (S0, EDX);              // step 5: S0 is temp1
    asm.emitMOV_Reg_Reg (EAX, JTOC);            // step 6
    asm.emitPOP_Reg (EAX);                  // step 7: SP changed!
    asm.emitIMUL1_Reg_RegDisp(EAX, SP, 8);// step 8
    asm.emitADD_Reg_Reg (S0, EAX);      // step 9
    asm.emitPOP_Reg (EAX);                  // step 10: SP changed!
    asm.emitIMUL1_Reg_Reg(EAX, JTOC);    // step 11
    asm.emitADD_Reg_Reg (S0, EAX);      // step 12
    asm.emitMOV_RegDisp_Reg (SP, 4, S0);            // step 13
    // restore JTOC register
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the ldiv bytecode
   */
  protected final void emit_ldiv() {
    // (1) zero check
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);
    asm.emitOR_Reg_RegDisp(T0, SP, 4);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);
    asm.emitINT_Imm(VM_Runtime.TRAP_DIVIDE_BY_ZERO + RVM_TRAP_BASE);	// trap if divisor is 0
    fr1.resolve(asm);
    // (2) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (3) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    // (4) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysLongDivideIPField.getOffset());
    // (5) pop space for arguments
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (6) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (7) pop expression stack
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (8) push results
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the lrem bytecode
   */
  protected final void emit_lrem() {
    // (1) zero check
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);
    asm.emitOR_Reg_RegDisp(T0, SP, 4);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);
    asm.emitINT_Imm(VM_Runtime.TRAP_DIVIDE_BY_ZERO + RVM_TRAP_BASE);	// trap if divisor is 0
    fr1.resolve(asm);
    // (2) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (3) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+20);
    // (4) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysLongRemainderIPField.getOffset());
    // (5) pop space for arguments
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (6) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (7) pop expression stack
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);
    // (8) push results
    asm.emitPUSH_Reg(T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the lneg bytecode
   */
  protected final void emit_lneg() {
    asm.emitNEG_RegDisp(SP, 4);    // [SP+4] <- -[SP+4] or high <- -high
    asm.emitNEG_RegInd(SP);    // [SP] <- -[SP] or low <- -low
    asm.emitSBB_RegDisp_Imm(SP, 4, 0); // [SP+4] += borrow or high += borrow
  }

  /**
   * Emit code to implement the lshsl bytecode
   */
  protected final void emit_lshl() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: pop high half into T1
    // 4: ECX <- JTOC, copy the shift count
    // 5: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 6: branch to step 12 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 7: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 8: T1 <- T0, or replace the high half with the low half.  This accounts for the 32 bit shift
    // 9: shift T1 left by ECX bits
    // 10: T0 <- 0
    // 11: branch to step 14
    // 12: shift left double from T0 into T1 by ECX bits.  T0 is unaltered
    // 13: shift left T0, the low half, also by ECX bits
    // 14: push high half from T1
    // 15: push the low half from T0
    // 16: restore the JTOC
    asm.emitPOP_Reg (JTOC);                 // original shift amount 6 bits
    asm.emitPOP_Reg (T0);                   // pop low half 
    asm.emitPOP_Reg (T1);                   // pop high half
    asm.emitMOV_Reg_Reg (ECX, JTOC);
    asm.emitAND_Reg_Imm (JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitMOV_Reg_Reg (T1, T0);               // low replaces high
    asm.emitSHL_Reg_Reg (T1, ECX);
    asm.emitXOR_Reg_Reg (T0, T0);
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitSHLD_Reg_Reg_Reg(T1, T0, ECX);          // shift high half (step 12)
    asm.emitSHL_Reg_Reg (T0, ECX);                   // shift low half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 14)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the lshr bytecode
   */
  protected final void emit_lshr() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: pop high half into T1
    // 4: ECX <- JTOC, copy the shift count
    // 5: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 6: branch to step 13 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 7: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 8: T0 <- T1, or replace the low half with the high half.  This accounts for the 32 bit shift
    // 9: shift T0 right arithmetic by ECX bits
    // 10: ECX <- 31
    // 11: shift T1 right arithmetic by ECX=31 bits, thus exending the sigh
    // 12: branch to step 15
    // 13: shift right double from T1 into T0 by ECX bits.  T1 is unaltered
    // 14: shift right arithmetic T1, the high half, also by ECX bits
    // 15: push high half from T1
    // 16: push the low half from T0
    // 17: restore JTOC
    asm.emitPOP_Reg (JTOC);                 // original shift amount 6 bits
    asm.emitPOP_Reg (T0);                   // pop low half 
    asm.emitPOP_Reg (T1);                   // pop high
    asm.emitMOV_Reg_Reg (ECX, JTOC);
    asm.emitAND_Reg_Imm (JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitMOV_Reg_Reg (T0, T1);               // replace low with high
    asm.emitSAR_Reg_Reg (T0, ECX);                   // and shift it
    asm.emitMOV_Reg_Imm (ECX, 31);
    asm.emitSAR_Reg_Reg (T1, ECX);                   // set high half
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitSHRD_Reg_Reg_Reg(T0, T1, ECX);          // shift low half (step 13)
    asm.emitSAR_Reg_Reg (T1, ECX);                   // shift high half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 15)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the lushr bytecode
   */
  protected final void emit_lushr() {
    if (VM.VerifyAssertions) VM.assert (ECX != T0); // ECX is constrained to be the shift count
    if (VM.VerifyAssertions) VM.assert (ECX != T1);
    if (VM.VerifyAssertions) VM.assert (ECX != JTOC);
    // 1: pop shift amount into JTOC (JTOC must be restored at the end)
    // 2: pop low half into T0
    // 3: ECX <- JTOC, copy the shift count
    // 4: JTOC <- JTOC & 32 --> if 0 then shift amount is less than 32
    // 5: branch to step 11 if results is zero
    // the result is not zero --> the shift amount is greater than 32
    // 6: ECX <- ECX XOR JTOC   --> ECX is orginal shift amount minus 32
    // 7: pop high half into T0 replace the low half with the high 
    //        half.  This accounts for the 32 bit shift
    // 8: shift T0 right logical by ECX bits
    // 9: T1 <- 0                        T1 is the high half
    // 10: branch to step 14
    // 11: pop high half into T1
    // 12: shift right double from T1 into T0 by ECX bits.  T1 is unaltered
    // 13: shift right logical T1, the high half, also by ECX bits
    // 14: push high half from T1
    // 15: push the low half from T0
    // 16: restore JTOC
    asm.emitPOP_Reg(JTOC);                // original shift amount 6 bits
    asm.emitPOP_Reg(T0);                  // pop low half 
    asm.emitMOV_Reg_Reg(ECX, JTOC);
    asm.emitAND_Reg_Imm(JTOC, 32);
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);
    asm.emitXOR_Reg_Reg (ECX, JTOC);
    asm.emitPOP_Reg (T0);                   // replace low with high
    asm.emitSHR_Reg_Reg (T0, ECX);      // and shift it (count - 32)
    asm.emitXOR_Reg_Reg (T1, T1);               // high <- 0
    VM_ForwardReference fr2 = asm.forwardJMP();
    fr1.resolve(asm);
    asm.emitPOP_Reg (T1);                   // high half (step 11)
    asm.emitSHRD_Reg_Reg_Reg(T0, T1, ECX);          // shift low half
    asm.emitSHR_Reg_Reg (T1, ECX);                   // shift high half
    fr2.resolve(asm);
    asm.emitPUSH_Reg(T1);                   // push high half (step 14)
    asm.emitPUSH_Reg(T0);                   // push low half
    // restore JTOC
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
  }

  /**
   * Emit code to implement the land bytecode
   */
  protected final void emit_land() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitAND_RegInd_Reg(SP, T0);
    asm.emitAND_RegDisp_Reg(SP, 4, S0);
  }

  /**
   * Emit code to implement the lor bytecode
   */
  protected final void emit_lor() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitOR_RegInd_Reg(SP, T0);
    asm.emitOR_RegDisp_Reg(SP, 4, S0);
  }

  /**
   * Emit code to implement the lxor bytecode
   */
  protected final void emit_lxor() {
    asm.emitPOP_Reg(T0);        // low
    asm.emitPOP_Reg(S0);        // high
    asm.emitXOR_RegInd_Reg(SP, T0);
    asm.emitXOR_RegDisp_Reg(SP, 4, S0);
  }


  /*
   * float ALU
   */


  /**
   * Emit code to implement the fadd bytecode
   */
  protected final void emit_fadd() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFADD_Reg_RegDisp(FP0, SP, WORDSIZE); // FPU reg. stack += value1
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fsub bytecode
   */
  protected final void emit_fsub() {
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1
    asm.emitFSUB_Reg_RegDisp(FP0, SP, 0);        // FPU reg. stack -= value2
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fmul bytecode
   */
  protected final void emit_fmul() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFMUL_Reg_RegDisp(FP0, SP, WORDSIZE); // FPU reg. stack *= value1
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the fdiv bytecode
   */
  protected final void emit_fdiv() {
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1
    asm.emitFDIV_Reg_RegDisp(FP0, SP, 0);        // FPU reg. stack /= value2
    asm.emitPOP_Reg   (T0);           // discard 
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the frem bytecode
   */
  protected final void emit_frem() {
    asm.emitFLD_Reg_RegInd (FP0, SP);        // FPU reg. stack <- value2, or a
    asm.emitFLD_Reg_RegDisp (FP0, SP, WORDSIZE); // FPU reg. stack <- value1, or b
    asm.emitFPREM ();             // FPU reg. stack <- a%b
    asm.emitFSTP_RegDisp_Reg(SP, WORDSIZE, FP0); // POP FPU reg. stack (results) onto java stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);        // POP FPU reg. stack onto java stack
    asm.emitPOP_Reg   (T0);           // shrink the stack (T0 discarded)
  }

  /**
   * Emit code to implement the fneg bytecode
   */
  protected final void emit_fneg() {
    asm.emitFLD_Reg_RegInd (FP0, SP); // FPU reg. stack <- value1
    asm.emitFCHS  ();      // change sign to stop of FPU stack
    asm.emitFSTP_RegInd_Reg(SP, FP0); // POP FPU reg. stack onto stack
  }


  /*
   * double ALU
   */


  /**
   * Emit code to implement the dadd bytecode
   */
  protected final void emit_dadd() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);        // FPU reg. stack <- value2
    asm.emitFADD_Reg_RegDisp_Quad(FP0, SP, 8);        // FPU reg. stack += value1
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);  // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);        // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the dsub bytecode
   */
  protected final void emit_dsub() {
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 8);          // FPU reg. stack <- value1
    asm.emitFSUB_Reg_RegDisp_Quad(FP0, SP, 0);          // FPU reg. stack -= value2
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the dmul bytecode
   */
  protected final void emit_dmul() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);          // FPU reg. stack <- value2
    asm.emitFMUL_Reg_RegDisp_Quad(FP0, SP, 8);          // FPU reg. stack *= value1
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the ddiv bytecode
   */
  protected final void emit_ddiv() {
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 8);          // FPU reg. stack <- value1
    asm.emitFDIV_Reg_RegInd_Quad(FP0, SP);          // FPU reg. stack /= value2
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);          // POP FPU reg. stack onto stack
  }

  /**
   * Emit code to implement the drem bytecode
   */
  protected final void emit_drem() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP);          // FPU reg. stack <- value2, or a
    asm.emitFLD_Reg_RegDisp_Quad (FP0, SP, 2*WORDSIZE); // FPU reg. stack <- value1, or b
    asm.emitFPREM ();               // FPU reg. stack <- a%b
    asm.emitFSTP_RegDisp_Reg_Quad(SP, 2*WORDSIZE, FP0); // POP FPU reg. stack (result) onto java stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);         // POP FPU reg. stack onto java stack
    asm.emitADD_Reg_Imm   (SP, 2*WORDSIZE); // shrink the stack
  }

  /**
   * Emit code to implement the dneg bytecode
   */
  protected final void emit_dneg() {
    asm.emitFLD_Reg_RegInd_Quad (FP0, SP); // FPU reg. stack <- value1
    asm.emitFCHS  ();      // change sign to stop of FPU stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0); // POP FPU reg. stack onto stack
  }


  /*
   * conversion ops
   */


  /**
   * Emit code to implement the i2l bytecode
   */
  protected final void emit_i2l() {
    asm.emitPOP_Reg (EAX);
    asm.emitCDQ ();
    asm.emitPUSH_Reg(EDX);
    asm.emitPUSH_Reg(EAX);
  }

  /**
   * Emit code to implement the i2f bytecode
   */
  protected final void emit_i2f() {
    asm.emitFILD_Reg_RegInd(FP0, SP);
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the i2d bytecode
   */
  protected final void emit_i2d() {
    asm.emitFILD_Reg_RegInd(FP0, SP);
    asm.emitPUSH_Reg(T0);             // grow the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the l2i bytecode
   */
  protected final void emit_l2i() {
    asm.emitPOP_Reg (T0); // low half of the long
    asm.emitPOP_Reg (S0); // high half of the long
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the l2f bytecode
   */
  protected final void emit_l2f() {
    asm.emitFILD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE);                // shrink the stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the l2d bytecode
   */
  protected final void emit_l2d() {
    asm.emitFILD_Reg_RegInd_Quad(FP0, SP);
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the f2i bytecode
   */
  protected final void emit_f2i() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push arg to C function 
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysFloatToIntIPField.getOffset());
    // (4) pop argument;
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the f2l bytecode
   */
  protected final void emit_f2l() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push arg to C function 
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysFloatToLongIPField.getOffset());
    // (4) pop argument;
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 0, T1);
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the f2d bytecode
   */
  protected final void emit_f2d() {
    asm.emitFLD_Reg_RegInd(FP0, SP);
    asm.emitSUB_Reg_Imm(SP, WORDSIZE);                // grow the stack
    asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
  }

  /**
   * Emit code to implement the d2i bytecode
   */
  protected final void emit_d2i() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysDoubleToIntIPField.getOffset());
    // (4) pop arguments
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitPOP_Reg(S0); // shrink stack by 1 word
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the d2l bytecode
   */
  protected final void emit_d2l() {
    // (1) save RVM nonvolatiles
    int numNonVols = NONVOLATILE_GPRS.length;
    for (int i = 0; i<numNonVols; i++) {
      asm.emitPUSH_Reg(NONVOLATILE_GPRS[i]);
    }
    // (2) Push args to C function (reversed)
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    asm.emitPUSH_RegDisp(SP, numNonVols*WORDSIZE+4);
    // (3) invoke C function through bootrecord
    asm.emitMOV_Reg_RegDisp(S0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitCALL_RegDisp(S0, VM_Entrypoints.sysDoubleToLongIPField.getOffset());
    // (4) pop arguments
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(S0);
    // (5) restore RVM nonvolatiles
    for (int i = numNonVols-1; i >=0; i--) {
      asm.emitPOP_Reg(NONVOLATILE_GPRS[i]);
    }
    // (6) put result on expression stack
    asm.emitMOV_RegDisp_Reg(SP, 4, T1);
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);
  }

  /**
   * Emit code to implement the d2f bytecode
   */
  protected final void emit_d2f() {
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE);                // shrink the stack
    asm.emitFSTP_RegInd_Reg(SP, FP0);
  }

  /**
   * Emit code to implement the i2b bytecode
   */
  protected final void emit_i2b() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVSX_Reg_Reg_Byte(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }

  /**
   * Emit code to implement the i2c bytecode
   */
  protected final void emit_i2c() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVZX_Reg_Reg_Word(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }

  /**
   * Emit code to implement the i2s bytecode
   */
  protected final void emit_i2s() {
    asm.emitPOP_Reg   (T0);
    asm.emitMOVSX_Reg_Reg_Word(T0, T0);
    asm.emitPUSH_Reg  (T0);
  }


  /*
   * comparision ops
   */


  /**
   * Emit code to implement the lcmp bytecode
   */
  protected final void emit_lcmp() {
    asm.emitPOP_Reg(T0);        // the low half of value2
    asm.emitPOP_Reg(S0);        // the high half of value2
    asm.emitPOP_Reg(T1);        // the low half of value1
    asm.emitSUB_Reg_Reg(T1, T0);        // subtract the low half of value2 from
                                // low half of value1, result into T1
    asm.emitPOP_Reg(T0);        // the high half of value 1
    //  pop does not alter the carry register
    asm.emitSBB_Reg_Reg(T0, S0);        // subtract the high half of value2 plus
                                // borrow from the high half of value 1,
                                // result in T0
    asm.emitMOV_Reg_Imm(S0, -1);        // load -1 into S0
    VM_ForwardReference fr1 = asm.forwardJcc(asm.LT); // result negative --> branch to end
    asm.emitMOV_Reg_Imm(S0, 0);        // load 0 into S0
    asm.emitOR_Reg_Reg(T0, T1);        // result 0 
    VM_ForwardReference fr2 = asm.forwardJcc(asm.EQ); // result 0 --> branch to end
    asm.emitMOV_Reg_Imm(S0, 1);        // load 1 into S0
    fr1.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);        // push result on stack
  }

  /**
   * Emit code to implement the fcmpl bytecode
   */
  protected final void emit_fcmpl() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp(FP0, SP, WORDSIZE);          // copy value1 into FPU
    asm.emitFLD_Reg_RegInd(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the fcmpg bytecode
   */
  protected final void emit_fcmpg() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp(FP0, SP, WORDSIZE);          // copy value1 into FPU
    asm.emitFLD_Reg_RegInd(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 2*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the dcmpl bytecode
   */
  protected final void emit_dcmpl() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, WORDSIZE*2);        // copy value1 into FPU
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }

  /**
   * Emit code to implement the dcmpg bytecode
   */
  protected final void emit_dcmpg() {
    VM_ForwardReference fr1,fr2,fr3;
    asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, WORDSIZE*2);        // copy value1 into FPU
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);                        // copy value2 into FPU
    asm.emitADD_Reg_Imm(SP, 4*WORDSIZE);                // popping the stack
    if (VM.VerifyAssertions) VM.assert(S0 != EAX);                        // eax is used by FNSTSW
    asm.emitXOR_Reg_Reg(S0, S0);                        // S0 <- 0
    asm.emitFUCOMPP();                        // compare and pop FPU *2
    asm.emitFNSTSW();                     // move FPU flags into (E)AX
    asm.emitSAHF();                       // store AH into flags
    fr1 = asm.forwardJcc(asm.EQ);        // branch if ZF set (eq. or unord.)
    // ZF not set ->  neither equal nor unordered
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr2 = asm.forwardJcc(asm.LLT);        // branch if CF set (val2 < val1)
    asm.emitMOV_Reg_Imm(S0, -1);                        // load -1 into S0
    fr1.resolve(asm);                        // ZF set (equal or unordered)
    fr3 = asm.forwardJcc(asm.LGE);        // branch if CF not set (not unordered)
    asm.emitMOV_Reg_Imm(S0, 1);                        // load 1 into S0
    fr3.resolve(asm);
    fr2.resolve(asm);
    asm.emitPUSH_Reg(S0);                        // push result on stack
  }


  /*
   * branching
   */


  /**
   * Emit code to implement the ifeg bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifeq(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the ifne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifne(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the iflt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_iflt(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.LT, bTarget);
  }

  /**
   * Emit code to implement the ifge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifge(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.GE, bTarget);
  }

  /**
   * Emit code to implement the ifgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifgt(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.GT, bTarget);
  }

  /**
   * Emit code to implement the ifle bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifle(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Imm(T0, 0);
    genCondBranch(asm.LE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpeq(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the if_icmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpne(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the if_icmplt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmplt(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.LT, bTarget);
  }

  /**
   * Emit code to implement the if_icmpge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpge(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.GE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpgt(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.GT, bTarget);
  }

  /**
   * Emit code to implement the if_icmple bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmple(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.LE, bTarget);
  }

  /**
   * Emit code to implement the if_acmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpeq(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the if_acmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpne(int bTarget) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the ifnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnull(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.EQ, bTarget);
  }

  /**
   * Emit code to implement the ifnonnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnonnull(int bTarget) {
    asm.emitPOP_Reg(T0);
    asm.emitTEST_Reg_Reg(T0, T0);
    genCondBranch(asm.NE, bTarget);
  }

  /**
   * Emit code to implement the goto and gotow bytecodes
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_goto(int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    asm.emitJMP_ImmOrLabel(mTarget, bTarget);
  }

  /**
   * Emit code to implement the jsr and jsrw bytecode
   * @param bTarget target bytecode of the jsr
   */
  protected final void emit_jsr(int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    asm.emitCALL_ImmOrLabel(mTarget, bTarget);
  }

  /**
   * Emit code to implement the ret bytecode
   * @param index local variable containing the return address
   */
  protected final void emit_ret(int index) {
    int offset = localOffset(index);
    asm.emitJMP_RegDisp(ESP, offset); 
  }

  /**
   * Emit code to implement the tableswitch bytecode
   * @param defaultval bcIndex of the default target
   * @param low low value of switch
   * @param high high value of switch
   */
  protected final void emit_tableswitch(int defaultval, int low, int high) {
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    int n = high-low+1;                        // n = number of normal cases (0..n-1)
    asm.emitPOP_Reg (T0);                          // T0 is index of desired case
    asm.emitSUB_Reg_Imm(T0, low);                     // relativize T0
    asm.emitCMP_Reg_Imm(T0, n);                       // 0 <= relative index < n
    if (options.EDGE_COUNTERS) {
      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T1, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T1, T1, getEdgeCounterOffset());
      int firstCounter = edgeCounterIdx;
      edgeCounterIdx += (n + 1);

      // Jump around code for default case
      VM_ForwardReference fr = asm.forwardJcc(asm.LLT);
      incEdgeCounter(T1, S0, firstCounter + n);
      asm.emitJMP_ImmOrLabel(mTarget, bTarget);
      fr.resolve(asm);

      // Increment counter for the appropriate case
      incEdgeCounterIdx(T1, S0, T0, firstCounter);
    } else {
      asm.emitJCC_Cond_ImmOrLabel (asm.LGE, mTarget, bTarget);   // if not, goto default case
    }
    asm.emitCALL_Imm(asm.getMachineCodeIndex() + 5 + (n<<LG_WORDSIZE) ); 
    // jump around table, pushing address of 0th delta
    for (int i=0; i<n; i++) {                  // create table of deltas
      int offset = fetch4BytesSigned();
      bTarget = biStart + offset;
      mTarget = bytecodeMap[bTarget];
      // delta i: difference between address of case i and of delta 0
      asm.emitOFFSET_Imm_ImmOrLabel(i, mTarget, bTarget );
    }
    asm.emitPOP_Reg (S0);                          // S0 = address of 0th delta 
    asm.emitADD_Reg_RegIdx (S0, S0, T0, asm.WORD, 0);     // S0 += [S0 + T0<<2]
    asm.emitPUSH_Reg(S0);                          // push computed case address
    asm.emitRET ();                            // goto case
  }
  
  /**
   * Emit code to implement the lookupswitch bytecode.
   * Uses linear search, one could use a binary search tree instead,
   * but this is the baseline compiler, so don't worry about it.
   * 
   * @param defaultval bcIndex of the default target
   * @param npairs number of pairs in the lookup switch
   */
  protected final void emit_lookupswitch(int defaultval, int npairs) {
    if (options.EDGE_COUNTERS) {
      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T1, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T1, T1, getEdgeCounterOffset());
    }

    asm.emitPOP_Reg(T0);
    for (int i=0; i<npairs; i++) {
      int match   = fetch4BytesSigned();
      asm.emitCMP_Reg_Imm(T0, match);
      int offset  = fetch4BytesSigned();
      int bTarget = biStart + offset;
      int mTarget = bytecodeMap[bTarget];
      if (options.EDGE_COUNTERS) {
	// Flip conditions so we can jump over the increment of the taken counter.
	VM_ForwardReference fr = asm.forwardJcc(asm.NE);

	// Increment counter & jump to target
	incEdgeCounter(T1, S0, edgeCounterIdx++);
	asm.emitJMP_ImmOrLabel(mTarget, bTarget);
	fr.resolve(asm);
      } else {
	asm.emitJCC_Cond_ImmOrLabel(asm.EQ, mTarget, bTarget);
      }
    }
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    if (options.EDGE_COUNTERS) {
      incEdgeCounter(T1, S0, edgeCounterIdx++);
    }
    asm.emitJMP_ImmOrLabel(mTarget, bTarget);
  }


  /*
   * returns (from function; NOT ret)
   */


  /**
   * Emit code to implement the ireturn bytecode
   */
  protected final void emit_ireturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T0);
    genEpilogue(4); 
  }

  /**
   * Emit code to implement the lreturn bytecode
   */
  protected final void emit_lreturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T1); // low half
    asm.emitPOP_Reg(T0); // high half
    genEpilogue(8);
  }

  /**
   * Emit code to implement the freturn bytecode
   */
  protected final void emit_freturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitFLD_Reg_RegInd(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE); // pop the stack
    genEpilogue(4);
  }

  /**
   * Emit code to implement the dreturn bytecode
   */
  protected final void emit_dreturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    asm.emitADD_Reg_Imm(SP, WORDSIZE<<1); // pop the stack
    genEpilogue(8);
  }

  /**
   * Emit code to implement the areturn bytecode
   */
  protected final void emit_areturn() {
    if (method.isSynchronized()) genMonitorExit();
    asm.emitPOP_Reg(T0);
    genEpilogue(4); 
  }

  /**
   * Emit code to implement the return bytecode
   */
  protected final void emit_return() {
    if (method.isSynchronized()) genMonitorExit();
    genEpilogue(0); 
  }


  /*
   * field access
   */


  /**
   * Emit code to implement a dynamically linked getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getstatic(VM_Field fieldRef) {
    emitDynamicLinkingSequence(T0, fieldRef); 
    if (fieldRef.getSize() == 4) { 
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, 0);        // get static field
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, WORDSIZE); // get high part
      asm.emitPUSH_RegIdx (JTOC, T0, asm.BYTE, 0);        // get low part
    }
  }

  /**
   * Emit code to implement a getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPUSH_RegDisp(JTOC, fieldOffset);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitPUSH_RegDisp(JTOC, fieldOffset+WORDSIZE); // get high part
      asm.emitPUSH_RegDisp(JTOC, fieldOffset);          // get low part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putstatic(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutstaticBarrier(asm, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, 0);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, 0);        // store low part
      asm.emitPOP_RegIdx(JTOC, T0, asm.BYTE, WORDSIZE); // store high part
    }
  }

  /**
   * Emit code to implement a putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutstaticBarrier(asm, fieldOffset);
    }
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitPOP_RegDisp(JTOC, fieldOffset);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitPOP_RegDisp(JTOC, fieldOffset);          // store low part
      asm.emitPOP_RegDisp(JTOC, fieldOffset+WORDSIZE); // store high part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getfield(VM_Field fieldRef) {
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(S0, SP, 0);              // S0 is object reference
      asm.emitMOV_Reg_RegIdx(S0, S0, T0, asm.BYTE, 0); // S0 is field value
      asm.emitMOV_RegDisp_Reg(SP, 0, S0);              // replace reference with value on stack
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitMOV_Reg_RegDisp(S0, SP, 0);                     // S0 is object reference
      asm.emitMOV_Reg_RegIdx(T1, S0, T0, asm.BYTE, WORDSIZE); // T1 is high part of field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T1);                     // replace reference with value on stack
      asm.emitPUSH_RegIdx(S0, T0, asm.BYTE, 0);               // push the low part of field value
    }
  }

  /**
   * Emit code to implement a getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getfield(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);           // T0 is object reference
      asm.emitMOV_Reg_RegDisp(T0, T0, fieldOffset); // T0 is field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T0);           // replace reference with value on stack
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);                    // T0 is object reference
      asm.emitMOV_Reg_RegDisp(T1, T0, fieldOffset+WORDSIZE); // T1 is high part of field value
      asm.emitMOV_RegDisp_Reg(SP, 0, T1);                    // replace reference with high part of value on stack
      asm.emitPUSH_RegDisp(T0, fieldOffset);                 // push low part of field value
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putfield(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutfieldBarrier(asm, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(T0, fieldRef);
    if (fieldRef.getSize() == 4) {// field is one word
      asm.emitMOV_Reg_RegDisp(T1, SP, 0);               // T1 is the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 4);               // S0 is the object reference
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, 0, T1); // [S0+T0] <- T1
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);              // complete popping the value and reference
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitMOV_Reg_RegDisp(JTOC, SP, 0);                          // JTOC is low part of the value to be stored
      asm.emitMOV_Reg_RegDisp(T1, SP, 4);                            // T1 is high part of the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 8);                            // S0 is the object reference
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, 0, JTOC);            // [S0+T0] <- JTOC
      asm.emitMOV_RegIdx_Reg (S0, T0, asm.BYTE, WORDSIZE, T1);       // [S0+T0+4] <- T1
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                           // complete popping the values and reference
      // restore JTOC
      VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());
    }
  }

  /**
   * Emit code to implement a putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putfield(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutfieldBarrier(asm, fieldRef.getOffset());
    }
    int fieldOffset = fieldRef.getOffset();
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);           // T0 is the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 4);           // S0 is the object reference
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset, T0); // [S0+fieldOffset] <- T0
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);          // complete popping the value and reference
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorLockMethod.getOffset());
      }
      // TODO!! use 8-byte move if possible
      asm.emitMOV_Reg_RegDisp(T0, SP, 0);                    // T0 is low part of the value to be stored
      asm.emitMOV_Reg_RegDisp(T1, SP, 4);                    // T1 is high part of the value to be stored
      asm.emitMOV_Reg_RegDisp(S0, SP, 8);                    // S0 is the object reference
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset, T0);          // store low part
      asm.emitMOV_RegDisp_Reg(S0, fieldOffset+WORDSIZE, T1); // store high part
      if (fieldRef.isVolatile() && VM.BuildForStrongVolatileSemantics) {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	asm.emitPUSH_Reg        (T0);
	VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
	asm.emitCALL_RegDisp    (S0, VM_Entrypoints.processorUnlockMethod.getOffset());
      }
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);                   // complete popping the values and reference
    }
  }


  /*
   * method invocation
   */

  /**
   * Emit code to implement a dynamically linked invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokevirtual(VM_Method methodRef) {
    emitDynamicLinkingSequence(T0, methodRef);
    int methodRefparameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int objectOffset = (methodRefparameterWords << 2) - 4;           // object offset into stack
    asm.emitMOV_Reg_RegDisp (T1, SP, objectOffset);                  // S0 has "this" parameter
    VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
    asm.emitMOV_Reg_RegIdx (S0, S0, T0, asm.BYTE, 0);                // S0 has address of virtual method
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_Reg(S0);                                      // call virtual method
    genResultRegisterUnload(methodRef);                    // push return value, if any
  }

  /**
   * Emit code to implement invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokevirtual(VM_Method methodRef) {
    int methodRefparameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int methodRefOffset = methodRef.getOffset();
    int objectOffset = (methodRefparameterWords << 2) - WORDSIZE; // object offset into stack
    asm.emitMOV_Reg_RegDisp (T1, SP, objectOffset);
    VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_RegDisp(S0, methodRefOffset);
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokespecial
   * @param methodRef the referenced method
   * @param targetRef the method to invoke
   */
  protected final void emit_resolved_invokespecial(VM_Method methodRef, VM_Method target) {
    if (target.isObjectInitializer()) {
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(JTOC, target.getOffset());
      genResultRegisterUnload(target);
    } else {
      if (VM.VerifyAssertions) VM.assert(!target.isStatic());
      // invoke via class's tib slot
      int methodRefOffset = target.getOffset();
      asm.emitMOV_Reg_RegDisp (S0, JTOC, target.getDeclaringClass().getTibOffset());
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, methodRefOffset);
      genResultRegisterUnload(methodRef);
    }
  }

  /**
   * Emit code to implement invokespecial
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokespecial(VM_Method methodRef) {
    emitDynamicLinkingSequence(S0, methodRef);
    genParameterRegisterLoad(methodRef, true);
    asm.emitCALL_RegIdx(JTOC, S0, asm.BYTE, 0);  // call static method
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokestatic(VM_Method methodRef) {
    emitDynamicLinkingSequence(S0, methodRef);
    genParameterRegisterLoad(methodRef, false);          
    asm.emitCALL_RegIdx(JTOC, S0, asm.BYTE, 0); 
    genResultRegisterUnload(methodRef);
  }

  /**
   * Emit code to implement invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokestatic(VM_Method methodRef) {
    int methodOffset = methodRef.getOffset();
    genParameterRegisterLoad(methodRef, false);
    asm.emitCALL_RegDisp(JTOC, methodOffset);
    genResultRegisterUnload(methodRef);
  }


  /**
   * Emit code to implement the invokeinterface bytecode
   * @param methodRef the referenced method
   * @param count number of parameter words (see invokeinterface bytecode)
   */
  protected final void emit_invokeinterface(VM_Method methodRef, int count) {
    // (1) Emit dynamic type checking sequence if required to do so inline.
    if (VM.BuildForIMTInterfaceInvocation || 
	(VM.BuildForITableInterfaceInvocation && VM.DirectlyIndexedITables)) {
      VM_Method resolvedMethodRef = null;
      try {
	resolvedMethodRef = methodRef.resolveInterfaceMethod(false);
      } catch (VM_ResolutionException e) {
	// actually can't be thrown when we pass false for canLoad.
      }
      if (resolvedMethodRef == null) {
	// might be a ghost ref. Call uncommon case typechecking routine to deal with this
	asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                       // "this" object
	asm.emitPUSH_Imm(methodRef.getDictionaryId());                          // dict id of target
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T1);
	asm.emitPUSH_Reg(S0);
	genParameterRegisterLoad(2);                                            // pass 2 parameter word
	asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unresolvedInvokeinterfaceImplementsTestMethod.getOffset());// check that "this" class implements the interface
      } else {
	asm.emitMOV_Reg_RegDisp (T0, JTOC, methodRef.getDeclaringClass().getTibOffset()); // tib of the interface method
	asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                 // "this" object
	asm.emitPUSH_RegDisp(T0, TIB_TYPE_INDEX << 2);                                // type of the interface method
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T1);
	asm.emitPUSH_Reg(S0);
	genParameterRegisterLoad(2);                                          // pass 2 parameter word
	asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.invokeinterfaceImplementsTestMethod.getOffset());// check that "this" class implements the interface
      }
    }

    // (2) Emit interface invocation sequence.
    if (VM.BuildForIMTInterfaceInvocation) {
      int signatureId = VM_ClassLoader.findOrCreateInterfaceMethodSignatureId(methodRef.getName(), methodRef.getDescriptor());
      int offset      = VM_InterfaceInvocation.getIMTOffset(signatureId);
          
      // squirrel away signature ID
      VM_ProcessorLocalState.emitMoveImmToField(asm, 
						VM_Entrypoints.hiddenSignatureIdField.getOffset(),
						signatureId);

      asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                  // "this" object
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
      if (VM.BuildForIndirectIMT) {
	// Load the IMT Base into S0
	asm.emitMOV_Reg_RegDisp(S0, S0, TIB_IMT_TIB_INDEX << 2);
      }
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, offset);                                             // the interface call
    } else if (VM.BuildForITableInterfaceInvocation && 
	       VM.DirectlyIndexedITables && 
	       methodRef.getDeclaringClass().isResolved()) {
      methodRef = methodRef.resolve();
      VM_Class I = methodRef.getDeclaringClass();
      asm.emitMOV_Reg_RegDisp (T1, SP, (count-1) << 2);                                 // "this" object
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T1);
      asm.emitMOV_Reg_RegDisp (S0, S0, TIB_ITABLES_TIB_INDEX << 2);                     // iTables
      asm.emitMOV_Reg_RegDisp (S0, S0, I.getInterfaceId() << 2);                        // iTable
      genParameterRegisterLoad(methodRef, true);
      asm.emitCALL_RegDisp(S0, VM_InterfaceInvocation.getITableIndex(I, methodRef) << 2); // the interface call
    } else {
      VM_Class I = methodRef.getDeclaringClass();
      int itableIndex = -1;
      if (false && VM.BuildForITableInterfaceInvocation) {
	// get the index of the method in the Itable
	if (I.isLoaded()) {
	  itableIndex = VM_InterfaceInvocation.getITableIndex(I, methodRef);
	}
      }
      if (itableIndex == -1) {
	// itable index is not known at compile-time.
	// call "invokeInterface" to resolve object + method id into 
	// method address
	int methodRefId = methodRef.getDictionaryId();
	asm.emitPUSH_RegDisp(SP, (count-1)<<LG_WORDSIZE);  // "this" parameter is obj
	asm.emitPUSH_Imm(methodRefId);                 // id of method to call
	genParameterRegisterLoad(2);               // pass 2 parameter words
	asm.emitCALL_RegDisp(JTOC,  VM_Entrypoints.invokeInterfaceMethod.getOffset()); // invokeinterface(obj, id) returns address to call
	asm.emitMOV_Reg_Reg (S0, T0);                      // S0 has address of method
	genParameterRegisterLoad(methodRef, true);
	asm.emitCALL_Reg(S0);                          // the interface method (its parameters are on stack)
      } else {
	// itable index is known at compile-time.
	// call "findITable" to resolve object + interface id into 
	// itable address
	asm.emitMOV_Reg_RegDisp (T0, SP, (count-1) << 2);             // "this" object
	VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T0);
	asm.emitPUSH_Reg(S0);
	asm.emitPUSH_Imm        (I.getInterfaceId());                // interface id
	genParameterRegisterLoad(2);                                  // pass 2 parameter words
	asm.emitCALL_RegDisp    (JTOC,  VM_Entrypoints.findItableMethod.getOffset()); // findItableOffset(tib, id) returns iTable
	asm.emitMOV_Reg_Reg     (S0, T0);                             // S0 has iTable
	genParameterRegisterLoad(methodRef, true);
	asm.emitCALL_RegDisp    (S0, itableIndex << 2);               // the interface call
      }
    }
    genResultRegisterUnload(methodRef);
  }
 

  /*
   * other object model functions
   */ 


  /**
   * Emit code to allocate a scalar object
   * @param typeRef the VM_Class to instantiate
   */
  protected final void emit_resolved_new(VM_Class typeRef) {
    int instanceSize = typeRef.getInstanceSize();
    int tibOffset = typeRef.getOffset();
    asm.emitPUSH_Imm(instanceSize);            
    asm.emitPUSH_RegDisp (JTOC, tibOffset);       // put tib on stack    
    asm.emitPUSH_Imm(typeRef.hasFinalizer()?1:0); // does the class have a finalizer?
    genParameterRegisterLoad(3);                  // pass 3 parameter words
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.quickNewScalarMethod.getOffset());
    asm.emitPUSH_Reg (T0);
  }

  /**
   * Emit code to dynamically link and allocate a scalar object
   * @param the dictionaryId of the VM_Class to dynamically link & instantiate
   */
  protected final void emit_unresolved_new(int dictionaryId) {
    asm.emitPUSH_Imm(dictionaryId);
    genParameterRegisterLoad(1);           // pass 1 parameter word
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.newScalarMethod.getOffset());
    asm.emitPUSH_Reg (T0);
  }

  /**
   * Emit code to allocate an array
   * @param array the VM_Array to instantiate
   */
  protected final void emit_newarray(VM_Array array) {
    int width      = array.getLogElementSize();
    int tibOffset  = array.getOffset();
    int headerSize = VM_ObjectModel.computeHeaderSize(array);
    // count is already on stack- nothing required
    asm.emitMOV_Reg_RegInd (T0, SP);               // get number of elements
    asm.emitSHL_Reg_Imm (T0, width);              // compute array size
    asm.emitADD_Reg_Imm(T0, headerSize);
    asm.emitPUSH_Reg(T0);      
    asm.emitPUSH_RegDisp(JTOC, tibOffset);        // put tib on stack    
    genParameterRegisterLoad(3);          // pass 3 parameter words
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.quickNewArrayMethod.getOffset());
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to allocate a multi-dimensional array
   * @param typeRef the VM_Array to instantiate
   * @param dimensions the number of dimensions
   * @param dictionaryId, the dictionaryId of typeRef
   */
  protected final void emit_multianewarray(VM_Array typeRef, int dimensions, int dictionaryId) {
    // setup parameters for newarrayarray routine
    asm.emitPUSH_Imm (dimensions);                     // dimension of arays
    asm.emitPUSH_Imm (dictionaryId);                   // type of array elements               
    asm.emitPUSH_Imm ((dimensions + 5)<<LG_WORDSIZE);  // offset to dimensions from FP on entry to newarray 
    // NOTE: 5 extra words- 3 for parameters, 1 for return address on stack, 1 for code technique in VM_Linker
    genParameterRegisterLoad(3);                   // pass 3 parameter words
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.newArrayArrayMethod.getOffset()); 
    for (int i = 0; i < dimensions ; i++) asm.emitPOP_Reg(S0); // clear stack of dimensions (todo use and add immediate to do this)
    asm.emitPUSH_Reg(T0);                              // push array ref on stack
  }

  /**
   * Emit code to implement the arraylength bytecode
   */
  protected final void emit_arraylength() {
    asm.emitMOV_Reg_RegDisp(T0, SP, 0);                   // T0 is array reference
    asm.emitMOV_Reg_RegDisp(T0, T0, VM_ObjectModel.getArrayLengthOffset()); // T0 is array length
    asm.emitMOV_RegDisp_Reg(SP, 0, T0);                   // replace reference with length on stack
  }

  /**
   * Emit code to implement the athrow bytecode
   */
  protected final void emit_athrow() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.athrowMethod.getOffset());
  }

  /**
   * Emit code to implement the checkcast bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this checkcast
   */
  protected final void emit_checkcast(VM_Type typeRef, VM_Method target) {
    asm.emitPUSH_RegInd (SP);                        // duplicate the object ref on the stack
    asm.emitPUSH_Imm(typeRef.getTibOffset());        // JTOC index that identifies klass  
    genParameterRegisterLoad(2);                     // pass 2 parameter words
    asm.emitCALL_RegDisp (JTOC, target.getOffset()); // checkcast(obj, klass-identifier)
  }

  /**
   * Emit code to implement the instanceof bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this instanceof
   */
  protected final void emit_instanceof(VM_Type typeRef, VM_Method target) {
    asm.emitPUSH_Imm(typeRef.getTibOffset());  
    genParameterRegisterLoad(2);          // pass 2 parameter words
    asm.emitCALL_RegDisp(JTOC, target.getOffset());
    asm.emitPUSH_Reg(T0);
  }

  /**
   * Emit code to implement the monitorenter bytecode
   */
  protected final void emit_monitorenter() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.lockMethod.getOffset());
  }

  /**
   * Emit code to implement the monitorexit bytecode
   */
  protected final void emit_monitorexit() {
    genParameterRegisterLoad(1);          // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unlockMethod.getOffset());  
  }


  //----------------//
  // implementation //
  //----------------//
  
  private final void genPrologue () {
    if (shouldPrint) asm.comment("prologue for " + method);
    if (klass.isBridgeFromNative()) {
      // replace the normal prologue with a special prolog
      VM_JNICompiler.generateGlueCodeForJNIMethod (asm, method, compiledMethod.getId());
      // set some constants for the code generation of the rest of the method
      // firstLocalOffset is shifted down because more registers are saved
      firstLocalOffset = STACKFRAME_BODY_OFFSET - (VM_JNICompiler.SAVED_GPRS_FOR_JNI<<LG_WORDSIZE) ;
    } else {
      /* paramaters are on the stack and/or in registers;  There is space
       * on the stack for all the paramaters;  Parameter slots in the
       * stack are such that the first paramater has the higher address,
       * i.e., it pushed below all the other paramaters;  The return
       * address is the topmost entry on the stack.  The frame pointer
       * still addresses the previous frame.
       * The first word of the header, currently addressed by the stack
       * pointer, contains the return address.
       */

      /* establish a new frame:
       * push the caller's frame pointer in the stack, and
       * reset the frame pointer to the current stack top,
       * ie, the frame pointer addresses directly the word
       * that contains the previous frame pointer.
       * The second word of the header contains the frame
       * point of the caller.
       * The third word of the header contains the compiled method id of the called method.
       */
      asm.emitPUSH_RegDisp   (PR, VM_Entrypoints.framePointerField.getOffset());	// store caller's frame pointer
      VM_ProcessorLocalState.emitMoveRegToField(asm, VM_Entrypoints.framePointerField.getOffset(), SP); // establish new frame
      /*
       * NOTE: until the end of the prologue SP holds the framepointer.
       */
      asm.emitMOV_RegDisp_Imm(SP, STACKFRAME_METHOD_ID_OFFSET, compiledMethod.getId());	// 3rd word of header
      
      /*
       * save registers
       */
      asm.emitMOV_RegDisp_Reg (SP, JTOC_SAVE_OFFSET, JTOC);          // save nonvolatile JTOC register
    
      // establish the JTOC register
      VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC, VM_Entrypoints.jtocField.getOffset());

      int savedRegistersSize   = SAVED_GPRS<<LG_WORDSIZE;	// default
      /* handle "dynamic brige" methods:
       * save all registers except FP, SP, PR, S0 (scratch), and
       * JTOC saved above.
       */
      // TODO: (SJF): When I try to reclaim ESI, I may have to save it here?
      if (klass.isDynamicBridge()) {
	savedRegistersSize += 3 << LG_WORDSIZE;
	asm.emitMOV_RegDisp_Reg (SP, T0_SAVE_OFFSET,  T0); 
	asm.emitMOV_RegDisp_Reg (SP, T1_SAVE_OFFSET,  T1); 
	asm.emitMOV_RegDisp_Reg (SP, EBX_SAVE_OFFSET, EBX); 
	asm.emitFNSAVE_RegDisp  (SP, FPU_SAVE_OFFSET);
	savedRegistersSize += FPU_STATE_SIZE;
      } 

      // copy registers to callee's stackframe
      firstLocalOffset         = STACKFRAME_BODY_OFFSET - savedRegistersSize;
      int firstParameterOffset = (parameterWords << LG_WORDSIZE) + WORDSIZE;
      genParameterCopy(firstParameterOffset, firstLocalOffset);

      int emptyStackOffset = firstLocalOffset - (method.getLocalWords() << LG_WORDSIZE) + WORDSIZE;
      asm.emitADD_Reg_Imm (SP, emptyStackOffset);		// set aside room for non parameter locals
      /*
       * generate stacklimit check
       */
      if (isInterruptible) {
	// S0<-limit
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
						  VM_Entrypoints.activeThreadStackLimitField.getOffset());

	asm.emitSUB_Reg_Reg (S0, SP);                                   	// space left
	asm.emitADD_Reg_Imm (S0, method.getOperandWords() << LG_WORDSIZE); 	// space left after this expression stack
	VM_ForwardReference fr = asm.forwardJcc(asm.LT);	// Jmp around trap if OK
	asm.emitINT_Imm ( VM_Runtime.TRAP_STACK_OVERFLOW + RVM_TRAP_BASE );	// trap
	fr.resolve(asm);
      } else {
	// TODO!! make sure stackframe of uninterruptible method doesn't overflow guard page
      }

      if (method.isSynchronized()) genMonitorEnter();

      genThreadSwitchTest(VM_Thread.PROLOGUE);

      asm.emitNOP();                                      // mark end of prologue for JDP
    }
  }
  
  private final void genEpilogue (int bytesPopped) {
    if (klass.isBridgeFromNative()) {
      // pop locals and parameters, get to saved GPR's
      asm.emitADD_Reg_Imm(SP, (this.method.getLocalWords() << LG_WORDSIZE));
      VM_JNICompiler.generateEpilogForJNIMethod(asm, this.method);
    } else if (klass.isDynamicBridge()) {
      // we never return from a DynamicBridge frame
      asm.emitINT_Imm(0xFF);
    } else {
      // normal method
      asm.emitADD_Reg_Imm     (SP, fp2spOffset(0) - bytesPopped);      // SP becomes frame pointer
      asm.emitMOV_Reg_RegDisp (JTOC, SP, JTOC_SAVE_OFFSET);            // restore nonvolatile JTOC register
      asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset()); // discard frame
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE);	 // return to caller- pop parameters from stack
    }
  }
   
  private final void genMonitorEnter () {
    if (method.isStatic()) {
      if (VM.writingBootImage) {
	VM.deferClassObjectCreation(klass);
      } else {
	klass.getClassForType();
      }
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (T0, JTOC, tibOffset);	           // T0 = tib for klass
      asm.emitMOV_Reg_RegInd (T0, T0);		                   // T0 = VM_Class for klass
      asm.emitPUSH_RegDisp(T0, VM_Entrypoints.classForTypeField.getOffset()); // push java.lang.Class object for klass
    } else {
      asm.emitPUSH_RegDisp(ESP, localOffset(0));	                   // push "this" object
    }
    genParameterRegisterLoad(1);			           // pass 1 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.lockMethod.getOffset());  
    lockOffset = asm.getMachineCodeIndex();                       // after this instruction, the method has the monitor
  }
  
  private final void genMonitorExit () {
    if (method.isStatic()) {
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (T0, JTOC, tibOffset);                   // T0 = tib for klass
      asm.emitMOV_Reg_RegInd (T0, T0);                             // T0 = VM_Class for klass
      asm.emitPUSH_RegDisp(T0, VM_Entrypoints.classForTypeField.getOffset()); // push java.lang.Class object for klass
    } else {
      asm.emitPUSH_RegDisp(ESP, localOffset(0));                    // push "this" object
    }
    genParameterRegisterLoad(1); // pass 1 parameter
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.unlockMethod.getOffset());  
  }
  
  private final void genBoundsCheck (VM_Assembler asm, byte indexReg, byte arrayRefReg ) { 
    if (options.ANNOTATIONS &&
	method.queryAnnotationForBytecode(biStart, VM_Method.annotationBoundsCheck)) {
      return;
    }
    asm.emitCMP_RegDisp_Reg(arrayRefReg,
                            VM_ObjectModel.getArrayLengthOffset(), indexReg);  // compare index to array length
    VM_ForwardReference fr = asm.forwardJcc(asm.LGT);                     // Jmp around trap if index is OK
    
    // "pass" index param to C trap handler
    VM_ProcessorLocalState.emitMoveRegToField(asm, 
                                              VM_Entrypoints.arrayIndexTrapParamField.getOffset(),
                                              indexReg);

    asm.emitINT_Imm(VM_Runtime.TRAP_ARRAY_BOUNDS + RVM_TRAP_BASE );	  // trap
    fr.resolve(asm);
  }

  /**
   * Emit a conditional branch on the given condition and bytecode target.
   * The caller has just emitted the instruction sequence to set the condition codes.
   */
  private final void genCondBranch(byte cond, int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    if (options.EDGE_COUNTERS) {
      // Allocate 2 counters, taken and not taken
      int entry = edgeCounterIdx;
      edgeCounterIdx += 2;

      // Load counter array for this method
      asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitMOV_Reg_RegDisp(T0, T0, getEdgeCounterOffset());

      // Flip conditions so we can jump over the increment of the taken counter.
      VM_ForwardReference notTaken = asm.forwardJcc(asm.flipCode(cond));

      // Increment taken counter & jump to target
      incEdgeCounter(T0, T1, entry + VM_EdgeCounts.TAKEN);
      asm.emitJMP_ImmOrLabel(mTarget, bTarget);

      // Increment not taken counter
      notTaken.resolve(asm);
      incEdgeCounter(T0, T1, entry + VM_EdgeCounts.NOT_TAKEN);
    } else {
      asm.emitJCC_Cond_ImmOrLabel(cond, mTarget, bTarget);
    }
  }


  private final void incEdgeCounter(byte counters, byte scratch, int counterIdx) {
    asm.emitMOV_Reg_RegDisp(scratch, counters, counterIdx<<2);
    asm.emitINC_Reg(scratch);
    asm.emitAND_Reg_Imm(scratch, 0x7fffffff); // saturate at max int;
    asm.emitMOV_RegDisp_Reg(counters, counterIdx<<2, scratch);
  }

  private final void incEdgeCounterIdx(byte counters, byte scratch, byte idx, int counterIdx) {
    asm.emitMOV_Reg_RegIdx(scratch, counters, idx, asm.WORD, counterIdx<<2);
    asm.emitINC_Reg(scratch);
    asm.emitAND_Reg_Imm(scratch, 0x7fffffff); // saturate at max int;
    asm.emitMOV_RegIdx_Reg(counters, idx, asm.WORD, counterIdx<<2, scratch);
  }


  /** 
   * Copy a single floating-point double parameter from operand stack into fp register stack.
   * Assumption: method to be called has exactly one parameter.
   * Assumption: parameter type is double.
   * Assumption: parameter is on top of stack.
   * Also, this method is only called before generation of a call
   * to doubleToInt() or doubleToLong()
   */
  private final void genParameterRegisterLoad () {
    if (0 < NUM_PARAMETER_FPRS) {
      asm.emitFLD_Reg_RegInd_Quad(FP0, SP);
    }
  }
   
  /** 
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of a "helper" method call.
   * Assumption: no floating-point parameters.
   * @param params number of parameter words (including "this" if any).
   */
  private final void genParameterRegisterLoad (int params) {
    if (VM.VerifyAssertions) VM.assert(0 < params);
    if (0 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T0, SP, (params-1) << LG_WORDSIZE);
    }
    if (1 < params && 1 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T1, SP, (params-2) << LG_WORDSIZE);
    }
  }
   
  /** 
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of an explicit method call.
   * @param method is the method to be called.
   * @param hasThisParameter is the method virtual?
   */
  private final void genParameterRegisterLoad (VM_Method method, boolean hasThisParam) {
    int max = NUM_PARAMETER_GPRS + NUM_PARAMETER_FPRS;
    if (max == 0) return; // quit looking when all registers are full
    int gpr = 0;  // number of general purpose registers filled
    int fpr = 0;  // number of floating point  registers filled
    byte  T = T0; // next GPR to get a parameter
    int params = method.getParameterWords() + (hasThisParam ? 1 : 0);
    int offset = (params-1) << LG_WORDSIZE; // stack offset of first parameter word
    if (hasThisParam) {
      if (gpr < NUM_PARAMETER_GPRS) {
	asm.emitMOV_Reg_RegDisp(T, SP, offset);
	T = T1; // at most 2 parameters can be passed in general purpose registers
	gpr++;
	max--;
      }
      offset -= WORDSIZE;
    }
    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      if (max == 0) return; // quit looking when all registers are full
      VM_Type t = types[i];
      if (t.isLongType()) {
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_Reg_RegDisp(T, SP, offset); // lo register := hi mem (== hi order word)
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  max--;
	  if (gpr < NUM_PARAMETER_GPRS) {
	    asm.emitMOV_Reg_RegDisp(T, SP, offset - WORDSIZE);  // hi register := lo mem (== lo order word)
	    gpr++;
	    max--;
	  }
	}
	offset -= 2*WORDSIZE;
      } else if (t.isFloatType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  asm.emitFLD_Reg_RegDisp(FP0, SP, offset);
	  fpr++;
	  max--;
	}
	offset -= WORDSIZE;
      } else if (t.isDoubleType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  asm.emitFLD_Reg_RegDisp_Quad(FP0, SP, offset - WORDSIZE);
	  fpr++;
	  max--;
	}
	offset -= 2*WORDSIZE;
      } else { // t is object, int, short, char, byte, or boolean
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_Reg_RegDisp(T, SP, offset);
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  max--;
	}
	offset -= WORDSIZE;
      }
    }
    if (VM.VerifyAssertions) VM.assert(offset == - WORDSIZE);
  }
   
  /** 
   * Store parameters into local space of the callee's stackframe.
   * Taken: srcOffset - offset from frame pointer of first parameter in caller's stackframe.
   *        dstOffset - offset from frame pointer of first local in callee's stackframe
   * Assumption: although some parameters may be passed in registers,
   * space for all parameters is layed out in order on the caller's stackframe.
   */
  private final void genParameterCopy (int srcOffset, int dstOffset) {
    int gpr = 0;  // number of general purpose registers unloaded
    int fpr = 0;  // number of floating point registers unloaded
    byte  T = T0; // next GPR to get a parameter
    if (!method.isStatic()) { // handle "this" parameter
      if (gpr < NUM_PARAMETER_GPRS) {
	asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);
	T = T1; // at most 2 parameters can be passed in general purpose registers
	gpr++;
      } else { // no parameters passed in registers
	asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
      }
      srcOffset -= WORDSIZE;
      dstOffset -= WORDSIZE;
    }
    VM_Type [] types     = method.getParameterTypes();
    int     [] fprOffset = new     int [NUM_PARAMETER_FPRS]; // to handle floating point parameters in registers
    boolean [] is32bit   = new boolean [NUM_PARAMETER_FPRS]; // to handle floating point parameters in registers
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);    // hi mem := lo register (== hi order word)
	  T = T1;                                       // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  if (gpr < NUM_PARAMETER_GPRS) {
	    asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);  // lo mem := hi register (== lo order word)
	    gpr++;
	  } else {
	    asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset); // lo mem from caller's stackframe
	    asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  }
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // hi mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // lo mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else if (t.isFloatType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  fprOffset[fpr] = dstOffset;
	  is32bit[fpr]   = true;
	  fpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else if (t.isDoubleType()) {
        if (fpr < NUM_PARAMETER_FPRS) {
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  fprOffset[fpr] = dstOffset;
	  is32bit[fpr]   = false;
	  fpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // hi mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	  srcOffset -= WORDSIZE;
	  dstOffset -= WORDSIZE;
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);   // lo mem from caller's stackframe
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      } else { // t is object, int, short, char, byte, or boolean
        if (gpr < NUM_PARAMETER_GPRS) {
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, T);
	  T = T1; // at most 2 parameters can be passed in general purpose registers
	  gpr++;
	} else {
	  asm.emitMOV_Reg_RegDisp(S0, SP, srcOffset);
	  asm.emitMOV_RegDisp_Reg(SP, dstOffset, S0);
	}
	srcOffset -= WORDSIZE;
	dstOffset -= WORDSIZE;
      }
    }
    for (int i=fpr-1; 0<=i; i--) { // unload the floating point register stack (backwards)
      if (is32bit[i]) {
	asm.emitFSTP_RegDisp_Reg(SP, fprOffset[i], FP0);
      } else {
	asm.emitFSTP_RegDisp_Reg_Quad(SP, fprOffset[i], FP0);
      }
    }
  }
   
  /** 
   * Push return value of method from register to operand stack.
   */
  private final void genResultRegisterUnload (VM_Method method) {
    VM_Type t = method.getReturnType();
    if (t.isVoidType()) return;
    if (t.isLongType()) {
      asm.emitPUSH_Reg(T0); // high half
      asm.emitPUSH_Reg(T1); // low half
    } else if (t.isFloatType()) {
      asm.emitSUB_Reg_Imm  (SP, 4);
      asm.emitFSTP_RegInd_Reg(SP, FP0);
    } else if (t.isDoubleType()) {
      asm.emitSUB_Reg_Imm  (SP, 8);
      asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
    } else { // t is object, int, short, char, byte, or boolean
      asm.emitPUSH_Reg(T0);
    }
  }
  
  /**
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  private final void genThreadSwitchTest (int whereFrom) {
    if (!isInterruptible) {
      return;
    } else if (VM.BuildForDeterministicThreadSwitching) {
      // decrement the deterministic thread switch count field in the
      // processor object
      VM_ProcessorLocalState.emitDecrementField(asm, 
                                                VM_Entrypoints.deterministicThreadSwitchCountField.getOffset());
      VM_ForwardReference fr1 = asm.forwardJcc(asm.NE);                  // if not, skip
      
      // reset the count.
      VM_ProcessorLocalState.emitMoveImmToField(asm,VM_Entrypoints.deterministicThreadSwitchCountField.getOffset(),
						VM.deterministicThreadSwitchInterval);

      if (whereFrom == VM_Thread.PROLOGUE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromPrologueMethod.getOffset()); 
      } else if (whereFrom == VM_Thread.BACKEDGE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromBackedgeMethod.getOffset()); 
      } else { // EPILOGUE
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromEpilogueMethod.getOffset()); 
      }
      fr1.resolve(asm);
    } else {
      // thread switch requested ??
      VM_ProcessorLocalState.emitCompareFieldWithImm(asm, 
                                                     VM_Entrypoints.threadSwitchRequestedField.getOffset(),
                                                     0);
      VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);                    // if not, skip
      if (whereFrom == VM_Thread.PROLOGUE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromPrologueMethod.getOffset()); 
      } else if (whereFrom == VM_Thread.BACKEDGE) {
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromBackedgeMethod.getOffset()); 
      } else { // EPILOGUE
        asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.threadSwitchFromEpilogueMethod.getOffset()); 
      }
      fr1.resolve(asm);
    }
  }

  private final void genMagic (VM_Method m) {
    VM_Atom methodName = m.getName();

    if (methodName == VM_MagicNames.attempt) {
      // attempt gets called with four arguments
      //   base
      //   offset
      //   oldVal
      //   newVal
      // returns ([base+offset] == oldVal)
      // if ([base+offset] == oldVal) [base+offset] := newVal
      // (operation on memory is atomic)
      asm.emitPOP_Reg (T1);            // newVal
      asm.emitPOP_Reg (EAX);           // oldVal (EAX is implicit arg to LCMPXCNG
      asm.emitPOP_Reg (S0);            // S0 = offset
      asm.emitADD_Reg_RegInd(S0, SP);  // S0 += base
      if (VM.BuildForSingleVirtualProcessor) {
	asm.emitMOV_RegInd_Reg (S0, T1);       // simply a store on uniprocessor (need not be atomic or cmp/xchg)
	asm.emitMOV_RegInd_Imm (SP, 1);        // 'push' true (overwriting base)
      } else {
	asm.emitLockNextInstruction();
	asm.emitCMPXCHG_RegInd_Reg (S0, T1);   // atomic compare-and-exchange
	asm.emitMOV_RegInd_Imm (SP, 0);        // 'push' false (overwriting base)
	VM_ForwardReference fr = asm.forwardJcc(asm.NE); // skip if compare fails
	asm.emitMOV_RegInd_Imm (SP, 1);        // 'push' true (overwriting base)
	fr.resolve(asm);
      }
      return;
    }
    
    if (methodName == VM_MagicNames.invokeMain) {
      // invokeMain gets "called" with two arguments:
      //   String[] mainArgs       // the arguments to the main method
      //   INSTRUCTION[] mainCode  // the code for the main method
      asm.emitPOP_Reg (S0);            // 
      genParameterRegisterLoad(1); // pass 1 parameter word	
      asm.emitCALL_Reg(S0);            // branches to mainCode with mainArgs on the stack
      return;
    }
    
    if (methodName == VM_MagicNames.saveThreadState) {
      int offset = VM_Entrypoints.saveThreadStateInstructionsField.getOffset();
      genParameterRegisterLoad(1); // pass 1 parameter word
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }

    if (methodName == VM_MagicNames.threadSwitch) {
      int offset = VM_Entrypoints.threadSwitchInstructionsField.getOffset();
      genParameterRegisterLoad(2); // pass 2 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }         
         
    if (methodName == VM_MagicNames.restoreHardwareExceptionState) {
      int offset = VM_Entrypoints.restoreHardwareExceptionStateInstructionsField.getOffset();
      genParameterRegisterLoad(1); // pass 1 parameter word
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }

    if (methodName == VM_MagicNames.invokeClassInitializer) {
      asm.emitPOP_Reg (S0);
      asm.emitCALL_Reg(S0); // call address just popped
      return;
    }
    
    /*
     * sysCall0, sysCall1, sysCall2, sysCall3 and sysCall4 return
     * an integer (32 bits).
     *
     *	hi mem
     *	  branch address	<- SP
     *
     * before call to C
     *  hi mem
     *	  branch address
     *	  saved ebx
     *	  saved pr
     *	  saved jtoc		<- SP
     */
    if (methodName == VM_MagicNames.sysCall0) {
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
       asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall1) {
      asm.emitPOP_Reg(S0);	// first and only argument
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(S0);	// push arg on stack
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(S0);	// pop the argument 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall2) {
      // C require its arguments reversed
      asm.emitPOP_Reg(T1);	// second arg
      asm.emitPOP_Reg(S0);	// first arg
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(T1);	// reorder arguments for C 
      asm.emitPUSH_Reg(S0);	// reorder arguments for C
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);	// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);	// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall3) {
      // C require its arguments reversed
      asm.emitMOV_Reg_RegInd(T0, SP);			// load 3rd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 3rd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, ESI);
      asm.emitMOV_RegInd_Reg(SP, JTOC);			// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 3*WORDSIZE); // fourth arg on stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }
    
    if (methodName == VM_MagicNames.sysCall4) {
      // C require its arguments reversed
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 3rd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 3th arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 3*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 3*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, ESI);	
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, JTOC);	// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 4*WORDSIZE); // fifth arg on stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*4);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }
    
    /*
     * sysCall_L_0  returns a long and takes no arguments
     */
    if (methodName == VM_MagicNames.sysCall_L_0) {
      asm.emitMOV_Reg_Reg(T0, SP);
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitCALL_RegInd(T0);	// first arg on stack is address to call
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T1);	// store return value: hi half
      asm.emitPUSH_Reg(T0);	// low half
      return;
    }
    
    /*
     * sysCall_L_I  returns a long and takes an integer argument
     */
    if (methodName == VM_MagicNames.sysCall_L_I) {
      asm.emitPOP_Reg(S0);	// the one integer argument
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- SP
      asm.emitPUSH_Reg(EBX);	// save three nonvolatiles: EBX
      asm.emitPUSH_Reg(ESI);	
      asm.emitPUSH_Reg(JTOC);	// JTOC aka EDI
      asm.emitPUSH_Reg(S0);	// push arg on stack
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitPOP_Reg(S0);	// pop the argument 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T1);	// store return value: hi half
      asm.emitPUSH_Reg(T0);	// low half
      return;
    }
    
    if (methodName == VM_MagicNames.sysCallAD) {  // address, double
      // C require its arguments reversed
      asm.emitMOV_Reg_RegInd(T0, SP);			// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -2*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, WORDSIZE);	// load 2nd arg
      asm.emitMOV_RegDisp_Reg(SP, -1*WORDSIZE, T0);	// store 2nd arg
      asm.emitMOV_Reg_RegDisp(T0, SP, 2*WORDSIZE);	// load 1st arg
      asm.emitMOV_RegDisp_Reg(SP, -3*WORDSIZE, T0);	// store 1st arg
      asm.emitMOV_Reg_Reg(T0, SP);			// T0 <- SP
      asm.emitMOV_RegDisp_Reg(SP, 2*WORDSIZE, EBX);	// save three nonvolatiles: EBX
      asm.emitMOV_RegDisp_Reg(SP, 1*WORDSIZE, ESI);	
      asm.emitMOV_RegInd_Reg(SP, JTOC);			// JTOC aka EDI
      asm.emitADD_Reg_Imm(SP, -3*WORDSIZE);		// grow the stack
      asm.emitCALL_RegDisp(T0, 3*WORDSIZE); // 4th word on orig. stack is address to call
      asm.emitADD_Reg_Imm(SP, WORDSIZE*3);		// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore the three nonvolatiles
      asm.emitPOP_Reg(ESI);
      asm.emitPOP_Reg(EBX);
      asm.emitMOV_RegInd_Reg(SP, T0);			// store return value
      return;
    }

    /*
     * A special version of sysCall2, for invoking sigWait and allowing
     * collection of the frame making the call.  The signature of the
     * magic is shared between powerPC and intel to simplify the caller.
     * The signature is
     * address to "call"
     * toc or undefined for intel
     * address of a lock/barrier which will be passed to sigWait
     * value to store into the lock/barrier, also passed to sigWait.  
     * the VM_Register object of the executing thread.
     *
     * The magic stores the current ip/fp into the VM_Register, to
     * allow collection of this thread from the current frame and below.
     * It then reverses the order of the two parameters on the stack to
     * conform to C calling convention, and finally invokes sigwait.
     *
     * stack:
     *	low memory
     *		ip -- address of sysPthreadSigWait in sys.C
     *		toc --
     *          p1 -- address of lockword
     *		p2 -- value to store in lockword
     *          address of VM_Register object for this thread
     *	high mem
     * This to be invoked from baseline code only.
     */
    if (methodName == VM_MagicNames.sysCallSigWait) {

      int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
      int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
      int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();

      asm.emitMOV_Reg_RegInd(T0, SP);	                // T0 <- context register obj @
      asm.emitLEA_Reg_RegDisp(S0, SP, fp2spOffset(0));  // compute FP
      asm.emitMOV_RegDisp_Reg(T0, fpOffset, S0);	// store fp in context
      asm.emitCALL_Imm (asm.getMachineCodeIndex() + 5);
      asm.emitPOP_Reg(T1);				// T1 <- IP
      asm.emitMOV_RegDisp_Reg(T0, ipOffset, T1);	// store ip in context
      asm.emitMOV_Reg_RegDisp(T0, T0, gprsOffset);	// T0 <- grps array @
      asm.emitMOV_Reg_RegDisp(T1, SP, WORDSIZE);	// second arg
      asm.emitMOV_Reg_RegDisp(S0, SP, 2*WORDSIZE);	// first arg
      asm.emitMOV_Reg_Reg(T0, SP);	// T0 <- [sysPthreadSigWait @]
      asm.emitADD_Reg_Imm(T0, 4*WORDSIZE);
      asm.emitPUSH_Reg(JTOC);	// save JTOC aka EDI
      asm.emitPUSH_Reg(T1);	// reorder arguments for C 
      asm.emitPUSH_Reg(S0);	// reorder arguments for C
      asm.emitCALL_RegInd(T0);	// branch to C code
      asm.emitADD_Reg_Imm(SP, WORDSIZE*2);	// pop the arguments 
      asm.emitPOP_Reg(JTOC);	// restore JTOC
      asm.emitADD_Reg_Imm(SP, WORDSIZE*4);	// pop all but last
      asm.emitMOV_RegInd_Reg(SP, T0);	// overwrite last with return value

      return;
    }
    
    if (methodName == VM_MagicNames.getFramePointer) {
      asm.emitLEA_Reg_RegDisp(S0, SP, fp2spOffset(0));
      asm.emitPUSH_Reg       (S0);
      return;
    }
    
    if (methodName == VM_MagicNames.getCallerFramePointer) {
      asm.emitPOP_Reg(T0);                                       // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_FRAME_POINTER_OFFSET); // Caller FP
      return;
    }

    if (methodName == VM_MagicNames.setCallerFramePointer) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_FRAME_POINTER_OFFSET, T0); // [S0+SFPO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getCompiledMethodID) {
      asm.emitPOP_Reg(T0);                                   // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_METHOD_ID_OFFSET); // Callee CMID
      return;
    }

    if (methodName == VM_MagicNames.setCompiledMethodID) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_METHOD_ID_OFFSET, T0); // [S0+SMIO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getReturnAddress) {
      asm.emitPOP_Reg(T0);                                        // Callee FP
      asm.emitPUSH_RegDisp(T0, STACKFRAME_RETURN_ADDRESS_OFFSET); // Callee return address
      return;
    }
    
    if (methodName == VM_MagicNames.setReturnAddress) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // fp
      asm.emitMOV_RegDisp_Reg(S0, STACKFRAME_RETURN_ADDRESS_OFFSET, T0); // [S0+SRAO] <- T0
      return;
    }

    if (methodName == VM_MagicNames.getTocPointer ||
	methodName == VM_MagicNames.getJTOC ) {
      asm.emitPUSH_Reg(JTOC);
      return;
    }
    
    if (methodName == VM_MagicNames.getThreadId) {
      VM_ProcessorLocalState.emitPushField(asm,VM_Entrypoints.threadIdField.getOffset());
      return;
    }
       
    // set the Thread id register (not really a register)
    if (methodName == VM_MagicNames.setThreadId) {
      VM_ProcessorLocalState.emitPopField(asm,VM_Entrypoints.threadIdField.getOffset()); 
      return;
    }
    
    // get the processor register (PR)
    if (methodName == VM_MagicNames.getProcessorRegister) {
      asm.emitPUSH_Reg(PR);
      return;
    }  

    // set the processor register (PR)
    if (methodName == VM_MagicNames.setProcessorRegister) {
      asm.emitPOP_Reg(PR);
      return;
    }
    
    // Get the value in ESI 
    if (methodName == VM_MagicNames.getESIAsProcessor) {
      asm.emitPUSH_Reg(ESI);
      return;
    }  

    // Set the value in ESI
    if (methodName == VM_MagicNames.setESIAsProcessor) {
      asm.emitPOP_Reg(ESI);
      return;
    }
  
    if (methodName == VM_MagicNames.getIntAtOffset ||
	methodName == VM_MagicNames.getObjectAtOffset ||
	methodName == VM_MagicNames.getObjectArrayAtOffset ||
	methodName == VM_MagicNames.prepare) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 0); // pushes [T0+S0]
      return;
    }
    
    if (methodName == VM_MagicNames.getByteAtOffset) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitMOV_Reg_RegIdx_Byte(T0, T0, S0, asm.BYTE, 0); // load and zero extend byte [T0+S0]
      asm.emitPUSH_Reg (T0);
      return;
    }
    
    if (methodName == VM_MagicNames.setIntAtOffset ||
	methodName == VM_MagicNames.setObjectAtOffset ) {
      asm.emitPOP_Reg(T0);                   // value
      asm.emitPOP_Reg(S0);                   // offset
      asm.emitPOP_Reg(T1);                   // obj ref
      asm.emitMOV_RegIdx_Reg(T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- T0
      return;
    }
    
    if (methodName == VM_MagicNames.setByteAtOffset) {
      asm.emitPOP_Reg(T0);                   // value
      asm.emitPOP_Reg(S0);                   // offset
      asm.emitPOP_Reg(T1);                   // obj ref
      asm.emitMOV_RegIdx_Reg_Byte(T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- (byte) T0
      return;
    }
    
    if (methodName == VM_MagicNames.getLongAtOffset) {
      asm.emitPOP_Reg (T0);                  // object ref
      asm.emitPOP_Reg (S0);                  // offset
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 4); // pushes [T0+S0+4]
      asm.emitPUSH_RegIdx(T0, S0, asm.BYTE, 0); // pushes [T0+S0]
      return;
    }
    
    if (methodName == VM_MagicNames.setLongAtOffset) {
      asm.emitMOV_Reg_RegInd (T0, SP);		// value high
      asm.emitMOV_Reg_RegDisp(S0, SP, +8 );	// offset
      asm.emitMOV_Reg_RegDisp(T1, SP, +12);	// obj ref
      asm.emitMOV_RegIdx_Reg (T1, S0, asm.BYTE, 0, T0); // [T1+S0] <- T0
      asm.emitMOV_Reg_RegDisp(T0, SP, +4 );	// value low
      asm.emitMOV_RegIdx_Reg (T1, S0, asm.BYTE, 4, T0); // [T1+S0+4] <- T0
      asm.emitADD_Reg_Imm    (SP, WORDSIZE * 4); // pop stack locations
      return;
    }
    
    if (methodName == VM_MagicNames.getMemoryWord) {
      asm.emitPOP_Reg(T0);	// address
      asm.emitPUSH_RegInd(T0); // pushes [T0+0]
      return;
    }

    if (methodName == VM_MagicNames.getMemoryAddress) {
      asm.emitPOP_Reg(T0);	// address
      asm.emitPUSH_RegInd(T0); // pushes [T0+0]
      return;
    }
    
    if (methodName == VM_MagicNames.setMemoryWord) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // address
      asm.emitMOV_RegInd_Reg(S0,T0); // [S0+0] <- T0
      return;
    }

    if (methodName == VM_MagicNames.setMemoryAddress) {
      asm.emitPOP_Reg(T0);  // value
      asm.emitPOP_Reg(S0);  // address
      asm.emitMOV_RegInd_Reg(S0,T0); // [S0+0] <- T0
      return;
    }
    
    if (methodName == VM_MagicNames.objectAsAddress         ||
	methodName == VM_MagicNames.addressAsByteArray      ||
	methodName == VM_MagicNames.addressAsIntArray       ||
	methodName == VM_MagicNames.addressAsObject         ||
	methodName == VM_MagicNames.addressAsObjectArray    ||
	methodName == VM_MagicNames.addressAsType           ||
	methodName == VM_MagicNames.objectAsType            ||
	methodName == VM_MagicNames.objectAsShortArray      ||
	methodName == VM_MagicNames.objectAsByteArray       ||
	methodName == VM_MagicNames.objectAsIntArray       ||
	methodName == VM_MagicNames.pragmaNoOptCompile      ||
	methodName == VM_MagicNames.addressAsThread         ||
	methodName == VM_MagicNames.objectAsThread          ||
	methodName == VM_MagicNames.objectAsProcessor       ||
//-#if RVM_WITH_JIKESRVM_MEMORY_MANAGERS
	methodName == VM_MagicNames.addressAsBlockControl   ||
	methodName == VM_MagicNames.addressAsSizeControl    ||
	methodName == VM_MagicNames.addressAsSizeControlArray   ||
//-#if RVM_WITH_CONCURRENT_GC
	methodName == VM_MagicNames.threadAsRCCollectorThread ||
//-#endif
//-#endif
	methodName == VM_MagicNames.threadAsCollectorThread ||
	methodName == VM_MagicNames.addressAsRegisters      ||
	methodName == VM_MagicNames.addressAsStack          ||
	methodName == VM_MagicNames.floatAsIntBits          ||
	methodName == VM_MagicNames.intBitsAsFloat          ||
	methodName == VM_MagicNames.doubleAsLongBits        ||
	methodName == VM_MagicNames.longBitsAsDouble)
      {
	// no-op (a type change, not a representation change)
	return;
      }
    
    // code for      VM_Type VM_Magic.getObjectType(Object object)
    if (methodName == VM_MagicNames.getObjectType) {
      asm.emitPOP_Reg (T0);			          // object ref
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
      asm.emitPUSH_RegDisp(S0, TIB_TYPE_INDEX<<LG_WORDSIZE); // push VM_Type slot of TIB
      return;
    }
    
    if (methodName == VM_MagicNames.getArrayLength) {
      asm.emitPOP_Reg(T0);			// object ref
      asm.emitPUSH_RegDisp(T0, VM_ObjectModel.getArrayLengthOffset()); 
      return;
    }
    
    if (methodName == VM_MagicNames.sync) {  // nothing required on IA32
      return;
    }
    
    if (methodName == VM_MagicNames.isync) { // nothing required on IA32
      return;
    }
    
    // baseline compiled invocation only: all paramaters on the stack
    // hi mem
    //      Code
    //      GPRs
    //      FPRs
    //      Spills
    // low-mem
    if (methodName == VM_MagicNames.invokeMethodReturningVoid) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningInt) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningLong) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0); // high half
      asm.emitPUSH_Reg(T1); // low half
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningFloat) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitSUB_Reg_Imm  (SP, 4);
      asm.emitFSTP_RegInd_Reg(SP, FP0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningDouble) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitSUB_Reg_Imm  (SP, 8);
      asm.emitFSTP_RegInd_Reg_Quad(SP, FP0);
      return;
    }                 

    if (methodName == VM_MagicNames.invokeMethodReturningObject) {
      int offset = VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset();
      genParameterRegisterLoad(4); // pass 4 parameter words
      asm.emitCALL_RegDisp(JTOC, offset);
      asm.emitPUSH_Reg(T0);
      return;
    }                 

    // baseline invocation
    // one paramater, on the stack  -- actual code
    if (methodName == VM_MagicNames.dynamicBridgeTo) {
      if (VM.VerifyAssertions) VM.assert(klass.isDynamicBridge());

      // save the branch address for later
      asm.emitPOP_Reg (S0);		// S0<-code address

      asm.emitADD_Reg_Imm(SP, fp2spOffset(0) - 4); // just popped 4 bytes above.

      // restore FPU state
      asm.emitFRSTOR_RegDisp(SP, FPU_SAVE_OFFSET);

      // restore GPRs
      asm.emitMOV_Reg_RegDisp (T0,  SP, T0_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (T1,  SP, T1_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET); 
      asm.emitMOV_Reg_RegDisp (JTOC,  SP, JTOC_SAVE_OFFSET); 

      // pop frame
      asm.emitPOP_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset()); // FP<-previous FP 

      // branch
      asm.emitJMP_Reg (S0);
      return;
    }
                                                  
    if (methodName == VM_MagicNames.returnToNewStack) {
      // SP gets frame pointer for new stack
      asm.emitPOP_Reg (SP);	

      // restore nonvolatile JTOC register
      asm.emitMOV_Reg_RegDisp (JTOC, SP, JTOC_SAVE_OFFSET);

      // discard current stack frame
      asm.emitPOP_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset());

      // return to caller- pop parameters from stack
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE);	 
      return;
    }

    if (methodName == VM_MagicNames.roundToZero) {
      // Store the FPU Control Word to a JTOC slot
      asm.emitFNSTCW_RegDisp(JTOC, VM_Entrypoints.FPUControlWordField.getOffset());
      // Set the bits in the status word that control round to zero.
      // Note that we use a 32-bit OR, even though we only care about the
      // low-order 16 bits
      asm.emitOR_RegDisp_Imm(JTOC,VM_Entrypoints.FPUControlWordField.getOffset(), 0x00000c00);
      // Now store the result back into the FPU Control Word
      asm.emitFLDCW_RegDisp(JTOC,VM_Entrypoints.FPUControlWordField.getOffset());
      return;
    }
    if (methodName == VM_MagicNames.clearFloatingPointState) {
      // Clear the hardware floating-point state
      asm.emitFNINIT();
      return;
    }
    
    if (methodName == VM_MagicNames.clearThreadSwitchBit) { // nothing to do
      // ignore obsolete magic
      return;
    }

    if (methodName == VM_MagicNames.getTime) {
      VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: getTime in " + method + "\n");
      asm.emitMOV_RegInd_Imm(SP, 0);  // TEMP!! for now, return 0
      return;
    }

    if (methodName == VM_MagicNames.getTimeBase) {
      VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: getTimeBase in " + method + "\n");
      asm.emitMOV_RegInd_Imm(SP, 0);  // TEMP!! for now, return 0
      return;
    }


    if (methodName == VM_MagicNames.addressFromInt ||
	methodName == VM_MagicNames.addressToInt) {
	// no-op
	return;
    }

    if (methodName == VM_MagicNames.addressAdd) {
	asm.emitPOP_Reg(T0);
	asm.emitADD_RegInd_Reg(SP, T0);
	return;
    }

    if (methodName == VM_MagicNames.addressSub ||
	methodName == VM_MagicNames.addressDiff) {
	asm.emitPOP_Reg(T0);
	asm.emitSUB_RegInd_Reg(SP, T0);
	return;
    }

    if (methodName == VM_MagicNames.addressZero) {
	asm.emitPUSH_Imm(0);
	return;
    }

    if (methodName == VM_MagicNames.addressMax) {
	asm.emitPUSH_Imm(-1);
	return;
    }

    if (methodName == VM_MagicNames.addressLT) {
	generateAddrComparison(asm.LT);
	return;
    }
    if (methodName == VM_MagicNames.addressLE) {
	generateAddrComparison(asm.LE);
	return;
    }
    if (methodName == VM_MagicNames.addressGT) {
	generateAddrComparison(asm.GT);
	return;
    }
    if (methodName == VM_MagicNames.addressGE) {
	generateAddrComparison(asm.GE);
	return;
    }
    if (methodName == VM_MagicNames.addressEQ) {
	generateAddrComparison(asm.EQ);
	return;
    }
    if (methodName == VM_MagicNames.addressNE) {
	generateAddrComparison(asm.NE);
	return;
    }
    if (methodName == VM_MagicNames.addressIsZero) {
	asm.emitPUSH_Imm(0);
	generateAddrComparison(asm.EQ);
	return;
    }
    if (methodName == VM_MagicNames.addressIsMax) {
	asm.emitPUSH_Imm(-1);
	generateAddrComparison(asm.EQ);
	return;
    }
						     
    VM.sysWrite("WARNING: VM_Compiler compiling unimplemented magic: " + methodName + " in " + method + "\n");
    asm.emitINT_Imm(0xFF); // trap
    
  }

  private void generateAddrComparison(byte comparator) {
    asm.emitPOP_Reg(S0);
    asm.emitPOP_Reg(T0);
    asm.emitCMP_Reg_Reg(T0, S0);
    asm.emitSET_Cond_Reg_Byte(comparator, T0);
    asm.emitMOVZX_Reg_Reg_Byte(T0, T0);   // Clear upper 3 bytes
    asm.emitPUSH_Reg(T0);
  }

  // Offset of Java local variable (off stack pointer)
  // assuming ESP is still positioned as it was at the 
  // start of the current bytecode (biStart)
  private final int localOffset  (int local) {
    return (stackHeights[biStart] - local)<<LG_WORDSIZE;
  }

  // Translate a FP offset into an SP offset 
  // assuming ESP is still positioned as it was at the 
  // start of the current bytecode (biStart)
  private final int fp2spOffset(int offset) {
    int offsetToFrameHead = (stackHeights[biStart] << LG_WORDSIZE) - firstLocalOffset;
    return offsetToFrameHead + offset;
  }
  
  private void emitDynamicLinkingSequence(byte reg, VM_Field fieldRef) {
    emitDynamicLinkingSequence(reg, fieldRef.getDictionaryId(), 
			       VM_Entrypoints.fieldOffsetsField.getOffset(),
			       VM_Entrypoints.resolveFieldMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(byte reg, VM_Method methodRef) {
    emitDynamicLinkingSequence(reg, methodRef.getDictionaryId(), 
			       VM_Entrypoints.methodOffsetsField.getOffset(),
			       VM_Entrypoints.resolveMethodMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(byte reg, int memberId,
					  int tableOffset,
					  int resolverOffset) {
    int memberOffset = memberId << 2;
    int retryLabel = asm.getMachineCodeIndex();            // branch here after dynamic class loading
    asm.emitMOV_Reg_RegDisp (reg, JTOC, tableOffset);      // reg is offsets table
    asm.emitMOV_Reg_RegDisp (reg, reg, memberOffset);      // reg is offset of member, or 0 if member's class isn't loaded
    asm.emitTEST_Reg_Reg    (reg, reg);                    // reg ?= 0, is field's class loaded?
    VM_ForwardReference fr = asm.forwardJcc(asm.NE);       // if so, skip call instructions
    asm.emitPUSH_Imm(memberId);                            // pass member's dictId
    genParameterRegisterLoad(1);                           // pass 1 parameter word
    asm.emitCALL_RegDisp(JTOC, resolverOffset);            // does class loading as sideffect
    asm.emitJMP_Imm (retryLabel);                          // reload reg with valid value
    fr.resolve(asm);                                       // come from Jcc above.
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generate inline machine instructions for special methods that cannot be implemented
 * in java bytecodes. These instructions are generated whenever we encounter an 
 * "invokestatic" bytecode that calls a method with a signature of 
 * the form "static native VM_Magic.xxx(...)".
 * 23 Jan 1998 Derek Lieber
 *
 * NOTE: when adding a new "methodName" to "generate()", be sure to also consider
 * how it affects the values on the stack and update "checkForActualCall()" accordingly.
 * If no call is actually generated, the map will reflect the status of the 
 * locals (including parameters) at the time of the call but nothing on the 
 * operand stack for the call site will be mapped.
 *
 * @author Janice Shepherd
 * @date 7 Jul 1998 
 */
class VM_MagicCompiler implements VM_BaselineConstants
   {
   //-----------//
   // interface //
   //-----------//

   // Generate inline code sequence for specified method.
   // Taken:    compiler we're generating code with
   //           method whose name indicates semantics of code to be generated
   // Returned: nothing
   //
   static void
   generateInlineCode(VM_Compiler compiler, VM_Method methodToBeCalled)
       {
       //!!TODO
       VM.sysWrite("VM_MagicCompiler.java: no magic for " + methodToBeCalled + "\n");
       if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
       }

     // Indicate if specified VM_Magic method causes a frame to be created on the runtime stack.
     // Taken:   VM_Method of the magic method being called
     // Returned: true if method causes a stackframe to be created
     //
     public static boolean
     checkForActualCall(VM_Method methodToBeCalled)
        {
        VM_Atom methodName = methodToBeCalled.getName();
        return methodName == VM_MagicNames.invokeMain                  ||
               methodName == VM_MagicNames.invokeClassInitializer      ||
               methodName == VM_MagicNames.invokeMethodReturningVoid   ||
               methodName == VM_MagicNames.invokeMethodReturningInt    ||
               methodName == VM_MagicNames.invokeMethodReturningLong   ||
               methodName == VM_MagicNames.invokeMethodReturningFloat  ||
               methodName == VM_MagicNames.invokeMethodReturningDouble ||
               methodName == VM_MagicNames.invokeMethodReturningObject;
       }
   }
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * Helper routine to pull the parameters to multianewarray off the
 * Java expression stack maintained by the baseline compiler and 
 * pass them to VM_Runtime.buildMultiDimensionalArray.
 * 
 * TODO: There is only 1 line of platform dependent code here; refactor?
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Tony Cocchi 
 * @author Derek Lieber
 */
class VM_MultianewarrayHelper {

  /**
   * Allocate something like "new Foo[cnt0][cnt1]...[cntN-1]",
   *                      or "new int[cnt0][cnt1]...[cntN-1]".
   * @param numDimensions number of array dimensions
   * @param dictionaryId  type of array (VM_TypeDictionary id)
   * @param argOffset     position of word *above* `cnt0' argument within caller's frame
   *                      This is used to access the number of elements to 
   *                      be allocated for each dimension.
   * See also: bytecode 0xc5 ("multianewarray") in VM_Compiler
   */
  static Object newArrayArray (int numDimensions, int dictionaryId, int argOffset)
    throws VM_ResolutionException, 
	   NegativeArraySizeException, 
	   OutOfMemoryError {
    // fetch number of elements to be allocated for each array dimension
    //
    int[] numElements = new int[numDimensions];
    VM.disableGC();
    VM_Address argp = VM_Magic.getFramePointer().add(argOffset);
    for (int i = 0; i < numDimensions; ++i) {
	argp = argp.sub(4);
	numElements[i] = VM_Magic.getMemoryWord(argp);
    }
    VM.enableGC();
    
    // validate arguments
    //
    for (int i = 0; i < numDimensions; ++i)
      if (numElements[i] < 0) throw new NegativeArraySizeException();
    
    // create array
    //
    return VM_Runtime.buildMultiDimensionalArray(numElements, 0, VM_TypeDictionary.getValue(dictionaryId).asArray());
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Class called from baseline compiler to generate architecture specific
 * write barrier for generational garbage collectors.  For baseline 
 * compiled methods, the write barrier calls methods of VM_WriteBarrier.
 *
 * @author Steve Blackburn for Jeff Stylos (UMass)
 * @author Stephen Smith
 */
class VM_Barriers implements VM_BaselineConstants {

  static void compileArrayStoreBarrier (VM_Assembler asm) {
    // on entry java stack contains ...|target_array_ref|array_index|ref_to_store|
    // SP -> ref_to_store, SP+8 -> target_ref

    asm.emitPUSH_RegDisp(SP, 8);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 4)
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.arrayStoreWriteBarrierMethod.getOffset());
  }

  static void compilePutfieldBarrier (VM_Assembler asm, int fieldOffset) {
    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref

    asm.emitPUSH_RegDisp(SP, 4);
    asm.emitPUSH_Imm(fieldOffset);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.resolvedPutfieldWriteBarrierMethod.getOffset());
  }

  static void compileUnresolvedPutfieldBarrier (VM_Assembler asm, int fieldID) {
    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref
    
    asm.emitPUSH_RegDisp(SP, 4);
    asm.emitPUSH_Imm(fieldID);
    asm.emitPUSH_RegDisp(SP, 8);  // Push what was originally (SP, 0)
    genParameterRegisterLoad(asm, 3);
    asm.emitCALL_RegDisp (JTOC, VM_Entrypoints.unresolvedPutfieldWriteBarrierMethod.getOffset());
  }

  // currently do not have a "write barrier for putstatic, emit nothing, for now...
  // (the collectors still scan all of statics/jtoc during each GC)
  //
  static void compilePutstaticBarrier (VM_Assembler asm, int fieldOffset) {
  }
  static void compileUnresolvedPutstaticBarrier(VM_Assembler asm, int fieldOffset) {
  }


  /**
   * (Taken from VM_Compiler.java)
   *
   * Copy parameters from operand stack into registers.
   * Assumption: parameters are layed out on the stack in order
   * with SP pointing to the last parameter.
   * Also, this method is called before the generation of a helper method call.
   * Assumption: no floating-point parameters.
   * @param params number of parameter words (including "this" if any).
   */
  private final static void genParameterRegisterLoad (VM_Assembler asm, int params){
    if (VM.VerifyAssertions) VM.assert(0 < params);
    if (0 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T0, SP, (params-1) << LG_WORDSIZE);
    }
    if (1 < params && 1 < NUM_PARAMETER_GPRS) {
      asm.emitMOV_Reg_RegDisp(T1, SP, (params-2) << LG_WORDSIZE);
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Contains architecture-specific helper functions for BURS.
 * 
 * @author Dave Grove
 * @author Stephen Fink
 */
abstract class OPT_BURS_Helpers extends OPT_PhysicalRegisterTools
  implements OPT_Operators, OPT_PhysicalRegisterConstants {
  
  // Generic helper functions.
  // Defined here to allow us to use them in the arch-specific
  // helper functions which are the bulk of this file.

  // returns the given operand as a register
  final OPT_RegisterOperand R(OPT_Operand op) {
    return (OPT_RegisterOperand) op;
  }

  // returns the given operand as an integer constant
  final OPT_IntConstantOperand I(OPT_Operand op) {
    return (OPT_IntConstantOperand) op;
  }
   
  // returns the given operand as a long constant
  final OPT_LongConstantOperand L(OPT_Operand op) {
    return (OPT_LongConstantOperand) op;
  }

  // returns the integer value of the given operand
  final int IV(OPT_Operand op) {
    return I(op).value;
  }

  // Cost functions better suited to grammars with multiple non-termials
  final int ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost) {
    return ADDRESS_EQUAL(store, load, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost, int falseCost) {
    if (Store.getAddress(store).similar(Load.getAddress(load)) &&
	Store.getOffset(store).similar(Load.getOffset(load))) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  final int ARRAY_ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost) {
    return ARRAY_ADDRESS_EQUAL(store, load, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int ARRAY_ADDRESS_EQUAL(OPT_Instruction store, OPT_Instruction load, int trueCost, int falseCost) {
    if (AStore.getArray(store).similar(ALoad.getArray(load)) &&
	AStore.getIndex(store).similar(ALoad.getIndex(load))) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  final int FITS(OPT_Operand op, int numBits, int trueCost) {
    return FITS(op, numBits, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int FITS(OPT_Operand op, int numBits, int trueCost, int falseCost) {
    if(op.isIntConstant() && OPT_Bits.fits(IV(op),numBits)) {
      return trueCost;
    } else {
      return falseCost;
    }
  }

  // can an IV be the scale in a LEA instruction?
  final int LEA_SHIFT(OPT_Operand op, int trueCost) {
    return LEA_SHIFT(op, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int LEA_SHIFT(OPT_Operand op, int trueCost, int falseCost) {
    if (op.isIntConstant()) {
      int val = IV(op);
      if (val >=0 && val <= 3) {
	return trueCost;
      }
    }
    return falseCost;
  }
  final byte LEA_SHIFT(OPT_Operand op) {
    switch (IV(op)) {
    case 0: return B_S;
    case 1: return W_S;
    case 2: return DW_S;
    case 3: return QW_S;
    default:
      throw new OPT_OptimizingCompilerException("bad val for LEA shift "+op);
    }
  }

  final int isFPC_ONE(OPT_Instruction s, int trueCost) {
    return isFPC_ONE(s, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int isFPC_ONE(OPT_Instruction s, int trueCost, int falseCost) {
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      return fc.value == 1.0f ? trueCost : falseCost;
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      return dc.value == 1.0 ? trueCost : falseCost;
    }
  }
  final int isFPC_ZERO(OPT_Instruction s, int trueCost) {
    return isFPC_ZERO(s, trueCost, OPT_BURS_STATE.INFINITE);
  }
  final int isFPC_ZERO(OPT_Instruction s, int trueCost, int falseCost) {
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      return fc.value == 0.0f ? trueCost : falseCost;
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      return dc.value == 0.0 ? trueCost : falseCost;
    }
  }

  // 
  // Begin IA32 specific helper functions.
  // 
  final OPT_IA32ConditionOperand COND(OPT_ConditionOperand op) {
    return new OPT_IA32ConditionOperand(op);
  }

  // word size for memory operands
  static final byte B  = 0x01;  // byte (8 bits)
  static final byte W  = 0x02;  // word (16 bits)
  static final byte DW = 0x04;  // doubleword (32 bits)
  static final byte QW = 0x08;  // quadword (64 bits)

  static final byte B_S  = 0x00;  // byte (8*2^0 bits)
  static final byte W_S  = 0x01;  // word (8*2^116 bits)
  static final byte DW_S = 0x02;  // doubleword (8*2^2 bits)
  static final byte QW_S = 0x03;  // quadword (8*2^3 bits)

  // Get particular physical registers
  OPT_Register getEAX () {
    return getIR().regpool.getPhysicalRegisterSet().getEAX();
  }
  OPT_Register getECX () {
    return getIR().regpool.getPhysicalRegisterSet().getECX();
  }
  OPT_Register getEDX () {
    return getIR().regpool.getPhysicalRegisterSet().getEDX();
  }
  OPT_Register getEBX () {
    return getIR().regpool.getPhysicalRegisterSet().getEBX();
  }
  OPT_Register getESP () {
    return getIR().regpool.getPhysicalRegisterSet().getESP();
  }
  OPT_Register getEBP () {
    return getIR().regpool.getPhysicalRegisterSet().getEBP();
  }
  OPT_Register getESI () {
    return getIR().regpool.getPhysicalRegisterSet().getESI();
  }
  OPT_Register getEDI () {
    return getIR().regpool.getPhysicalRegisterSet().getEDI();
  }
  OPT_Register getFPR (int n) {
    return getIR().regpool.getPhysicalRegisterSet().getFPR(n);
  }

  OPT_Operand myFP0() {
    return new OPT_BURSManagedFPROperand(0);
  }
  OPT_Operand myFP1() {
    return new OPT_BURSManagedFPROperand(1);
  }

  // support to remember an address being computed in a subtree
  private static final class AddrStackElement {
    OPT_RegisterOperand base;
    OPT_RegisterOperand index;
    byte scale;
    int displacement;
    AddrStackElement next;
    AddrStackElement(OPT_RegisterOperand b,
		     OPT_RegisterOperand i,
		     byte s, int d,
		     AddrStackElement n) {
      base = b;
      index = i;
      scale = s;
      displacement = d;
      next = n;
    }
  }
  private AddrStackElement AddrStack;
  final void pushAddress(OPT_RegisterOperand base,
			 OPT_RegisterOperand index,
			 byte scale,
			 int disp) {
    AddrStack = new AddrStackElement(base, index, scale, disp, AddrStack);
  }
  final void augmentAddress(OPT_Operand op) {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to augment");
    if (op.isRegister()) {
      OPT_RegisterOperand rop = op.asRegister();
      if (AddrStack.base == null) {
	AddrStack.base = rop;
      } else if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = rop;
      } else {
	throw new OPT_OptimizingCompilerException("three base registers in address");
      }
    } else {
      int disp = ((OPT_IntConstantOperand)op).value;
      AddrStack.displacement += disp;
    }
  }
  final void combineAddresses() {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to combine");
    AddrStackElement tmp = AddrStack;
    AddrStack = AddrStack.next;
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "only 1 address to combine");
    if (tmp.base != null) {
      if (AddrStack.base == null) {
	AddrStack.base = tmp.base;
      } else if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = tmp.base;
      } else {
	throw new OPT_OptimizingCompilerException("three base registers in address");
      }
    }
    if (tmp.index != null) {
      if (AddrStack.index == null) {
	if (VM.VerifyAssertions) VM.assert(AddrStack.scale == (byte)0);
	AddrStack.index = tmp.index;
	AddrStack.scale = tmp.scale;
      } else if (AddrStack.base == null && tmp.scale == (byte)0) {
	AddrStack.base = tmp.base;
      } else {
	throw new OPT_OptimizingCompilerException("two scaled registers in address");
      }
    }
    AddrStack.displacement += tmp.displacement;
  }
  final OPT_MemoryOperand consumeAddress(byte size, 
					 OPT_LocationOperand loc,
					 OPT_Operand guard) {
    if (VM.VerifyAssertions) VM.assert(AddrStack != null, "No address to consume");
    OPT_MemoryOperand mo = 
      new OPT_MemoryOperand(AddrStack.base, AddrStack.index, AddrStack.scale,
			    AddrStack.displacement, size, loc, guard);
    AddrStack = AddrStack.next;
    return mo;
  }

  // support to remember a memory operand computed in a subtree
  private static final class MOStackElement {
    OPT_MemoryOperand mo;
    MOStackElement next;
    MOStackElement(OPT_MemoryOperand m, 
		   MOStackElement n) {
      mo = m;
      next = n;
    }
  }
  private MOStackElement MOStack;
  final void pushMO(OPT_MemoryOperand mo) {
    MOStack = new MOStackElement(mo, MOStack);
  }
  final OPT_MemoryOperand consumeMO() {
    if (VM.VerifyAssertions) VM.assert(MOStack != null, "No memory operand to consume");
    OPT_MemoryOperand mo = MOStack.mo;
    MOStack = MOStack.next;
    return mo;
  }


  // Construct a memory operand for the effective address of the 
  // load instruction
  final OPT_MemoryOperand MO_L(OPT_Instruction s, byte size) {
    return MO(Load.getAddress(s), Load.getOffset(s), size, 
	      Load.getLocation(s), Load.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // store instruction
  final OPT_MemoryOperand MO_S(OPT_Instruction s, byte size) {
    return MO(Store.getAddress(s), Store.getOffset(s), size, 
	      Store.getLocation(s), Store.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array load instruction
  final OPT_MemoryOperand MO_AL(OPT_Instruction s, byte scale, byte size) {
    return MO_ARRAY(ALoad.getArray(s), ALoad.getIndex(s), scale, size, 
		    ALoad.getLocation(s), ALoad.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array store instruction
  final OPT_MemoryOperand MO_AS(OPT_Instruction s, byte scale, byte size) {
    return MO_ARRAY(AStore.getArray(s), AStore.getIndex(s), scale, size, 
		    AStore.getLocation(s), AStore.getGuard(s));
  }

  // Construct a memory operand for the effective address of the 
  // load instruction 
  final OPT_MemoryOperand MO_L(OPT_Instruction s, byte size, int disp) {
    return MO(Load.getAddress(s), Load.getOffset(s), size, disp,
	      Load.getLocation(s), Load.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // store instruction
  final OPT_MemoryOperand MO_S(OPT_Instruction s, byte size, int disp) {
    return MO(Store.getAddress(s), Store.getOffset(s), size, disp,
	      Store.getLocation(s), Store.getGuard(s));
  }
  // Construct a memory operand for the effective address of the 
  // array load instruction
  final OPT_MemoryOperand MO_AL(OPT_Instruction s, byte scale, byte size, int disp) {
    return MO_ARRAY(ALoad.getArray(s), ALoad.getIndex(s), scale, size, disp,
		    ALoad.getLocation(s), ALoad.getGuard(s));
  }
  // Construct a memory operand for the effective address of the array store instruction
  final OPT_MemoryOperand MO_AS(OPT_Instruction s, byte scale, byte size, int disp) {
    return MO_ARRAY(AStore.getArray(s), AStore.getIndex(s), scale, size, disp,
		    AStore.getLocation(s), AStore.getGuard(s));
  }

  final OPT_MemoryOperand MO(OPT_Operand base, OPT_Operand offset, 
			     byte size, OPT_LocationOperand loc,
			     OPT_Operand guard) {
    if (base instanceof OPT_IntConstantOperand) {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+IV(offset), size, loc, guard);
      } else {
	return MO_BD(offset, IV(base), size, loc, guard);
      }
    } else {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_BD(base, IV(offset), size, loc, guard);
      } else {
	return MO_BI(base, offset, size, loc, guard);
      }
    }
  }

  final OPT_MemoryOperand MO_ARRAY(OPT_Operand base, 
				   OPT_Operand index, 
				   byte scale, byte size, 
				   OPT_LocationOperand loc,
				   OPT_Operand guard) {
    if (index instanceof OPT_IntConstantOperand) {
      return MO_BD(base, IV(index)<<scale, size, loc, guard);
    } else {
      return MO_BIS(base, index, scale, size, loc, guard);
    }
  }


  final OPT_MemoryOperand MO(OPT_Operand base, OPT_Operand offset, 
			     byte size, int disp,
			     OPT_LocationOperand loc,
			     OPT_Operand guard) {
    if (base instanceof OPT_IntConstantOperand) {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+IV(offset)+disp, size, loc, guard);
      } else {
	return MO_BD(offset, IV(base)+disp, size, loc, guard);
      }
    } else {
      if (offset instanceof OPT_IntConstantOperand) {
	return MO_BD(base, IV(offset)+disp, size, loc, guard);
      } else {
	return MO_BID(base, offset, disp, size, loc, guard);
      }
    }
  }

  final OPT_MemoryOperand MO_ARRAY(OPT_Operand base, 
				   OPT_Operand index, 
				   byte scale, byte size, 
				   int disp,
				   OPT_LocationOperand loc,
				   OPT_Operand guard) {
    if (index instanceof OPT_IntConstantOperand) {
      return MO_BD(base, (IV(index)<<scale)+disp, size, loc, guard);
    } else {
      return new OPT_MemoryOperand(R(base), R(index), scale, 
				   disp, size, loc, guard);
    }
  }

 
  final OPT_MemoryOperand MO_B(OPT_Operand base, byte size, 
			       OPT_LocationOperand loc,
			       OPT_Operand guard) {
    return OPT_MemoryOperand.B(R(base), size, loc, guard);
  }

  final OPT_MemoryOperand MO_BI(OPT_Operand base, 
				OPT_Operand index, 
				byte size, OPT_LocationOperand loc,
				OPT_Operand guard) {
    return OPT_MemoryOperand.BI(R(base), R(index), size, loc, guard);
  }

  final OPT_MemoryOperand MO_BD(OPT_Operand base, int disp, 
				byte size, OPT_LocationOperand loc,
				OPT_Operand guard) {
    return OPT_MemoryOperand.BD(R(base), disp, size, loc, guard);
  }

  final OPT_MemoryOperand MO_BID(OPT_Operand base, 
				 OPT_Operand index, 
				 int disp, byte size, 
				 OPT_LocationOperand loc,
				 OPT_Operand guard) {
    return OPT_MemoryOperand.BID(R(base), R(index), disp, size, loc, guard);
  }

  final OPT_MemoryOperand MO_BIS(OPT_Operand base, 
				 OPT_Operand index, 
				 byte scale, byte size, 
				 OPT_LocationOperand loc,
				 OPT_Operand guard) {
    return OPT_MemoryOperand.BIS(R(base), R(index), scale, size, loc, guard);
  }

  final OPT_MemoryOperand MO_D(int disp, 
			       byte size, OPT_LocationOperand loc,
			       OPT_Operand guard) {
    return OPT_MemoryOperand.D(disp, size, loc, guard);
  }



  final OPT_MemoryOperand MO_MC(OPT_Instruction s) {
    OPT_Operand base = Binary.getVal1(s);
    OPT_Operand val = Binary.getVal2(s);
    if (val instanceof OPT_FloatConstantOperand) {
      OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)val;
      int offset = fc.index << 2;
      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
      if (base instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+offset, DW, loc, TG());
      } else {
	return MO_BD(Binary.getVal1(s), offset, DW, loc, TG());
      }
    } else {
      OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)val;
      int offset = dc.index << 2;
      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
      if (base instanceof OPT_IntConstantOperand) {
	return MO_D(IV(base)+offset, QW, loc, TG());
      } else {
	return MO_BD(Binary.getVal1(s), offset, QW, loc, TG());
      }
    }
  }

  final OPT_Operand MO_CONV(OPT_BURS burs, byte size) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    return new OPT_StackLocationOperand(true, offset, size);
  }

  final void STORE_LONG_FOR_CONV(OPT_BURS burs, OPT_Operand op) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    if (op instanceof OPT_RegisterOperand) {
      OPT_RegisterOperand hval = R(op);
      OPT_RegisterOperand lval = R(burs.ir.regpool.getSecondReg(hval.register));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset+4, DW), hval));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset, DW), lval));
    } else {
      OPT_LongConstantOperand val = L(op);
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset+4, DW), I(val.upper32())));
      burs.append(MIR_Move.create(IA32_MOV, new OPT_StackLocationOperand(true, offset, DW), I(val.lower32())));
    }
  }      

  // condition code state
  private OPT_ConditionOperand cc;
  void pushCOND(OPT_ConditionOperand c) {
    if (VM.VerifyAssertions) VM.assert(cc == null);
    cc = c ;
  }
  OPT_ConditionOperand consumeCOND() {
    OPT_ConditionOperand ans = cc;
    if (VM.VerifyAssertions) {
      VM.assert(cc != null);
      cc = null;
    }
    return ans;
  }

  // emit code to load 32 bits form a given jtoc offset
  private OPT_MemoryOperand loadFromJTOC(OPT_BURS burs, int offset) {
    OPT_LocationOperand loc = new OPT_LocationOperand(offset);
    OPT_Operand guard = TG();
    if (burs.ir.options.FIXED_JTOC) {
      return OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(offset).toInt(),
				 (byte)4, loc, guard);
    } else {
      OPT_Operand jtoc = 
	OPT_MemoryOperand.BD(R(burs.ir.regpool.getPhysicalRegisterSet().getPR()),
			     VM_Entrypoints.jtocField.getOffset(), 
			     (byte)4, null, TG());
      OPT_RegisterOperand regOp = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, regOp, jtoc));
      return OPT_MemoryOperand.BD(regOp.copyD2U(), offset, (byte)4, loc, guard);
    }
  }

  /*
   * IA32-specific emit rules that are complex 
   * enough that we didn't want to write them in the LIR2MIR.rules file.
   * However, all expansions in this file are called during BURS and
   * thus are constrained to generate nonbranching code (ie they can't
   * create new basic blocks and/or do branching).
   *
   */

  /**
   * Emit code to get a caught exception object into a register
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void GET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForCaughtException();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, DW);
    burs.append(MIR_Move.mutate(s, IA32_MOV, Nullary.getResult(s), sl));
  }


  /**
   * Emit code to move a value in a register to the stack location
   * where a caught exception object is expected to be.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void SET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager. allocateSpaceForCaughtException();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, DW);
    OPT_RegisterOperand obj = (OPT_RegisterOperand)CacheOp.getRef(s);
    burs.append(MIR_Move.mutate(s, IA32_MOV, sl, obj));
  }


  /**
   * Expansion of INT_2LONG
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param value the second operand
   */
  final void INT_2LONG(OPT_BURS burs, OPT_Instruction s,
		       OPT_RegisterOperand result,
		       OPT_Operand value) {
    OPT_Register hr = result.register;
    OPT_Register lr = burs.ir.regpool.getSecondReg(hr);
    burs.append(MIR_Move.create(IA32_MOV, R(lr), value));
    burs.append(MIR_Move.create(IA32_MOV, R(hr), R(lr)));
    burs.append(MIR_BinaryAcc.create(IA32_SAR, R(hr), I(31)));
  }

  /**
   * Expansion of FLOAT_2INT and DOUBLE_2INT, using the FIST instruction.
   * This expansion does some boolean logic and conditional moves in order
   * to avoid changing the floating-point rounding mode or inserting
   * branches.  Other expansions are possible, and may be better?
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param value the second operand
   */
  final void FPR_2INT(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_MemoryOperand M;

    // Step 1: Get value to be converted into myFP0
    //         and in 'strict' IEEE mode.
    if (value instanceof OPT_MemoryOperand) {
      // value is in memory, all we have to do is load it
      burs.append(MIR_Move.create(IA32_FLD, myFP0(), value));
    } else {
      // sigh.  value is an FP register. Unfortunately,
      // SPECjbb requires some 'strict' FP semantics.  Naturally, we don't
      // normally implement strict semantics, but we try to slide by in
      // order to pass the benchmark.  
      // In order to pass SPECjbb, it turns out we need to enforce 'strict'
      // semantics before doing a particular f2int conversion.  To do this
      // we must have a store/load sequence to cause IEEE rounding.
      if (value instanceof OPT_BURSManagedFPROperand) {
	if (VM.VerifyAssertions) VM.assert(value.similar(myFP0()));
	burs.append(MIR_Move.create(IA32_FSTP, MO_CONV(burs, DW), value));
	burs.append(MIR_Move.create(IA32_FLD, myFP0(), MO_CONV(burs, DW)));
      } else {
	burs.append(MIR_Move.create(IA32_FMOV, MO_CONV(burs, DW), value));
	burs.append(MIR_Move.create(IA32_FLD, myFP0(), MO_CONV(burs, DW)));
      }
    }

    // FP Stack: myFP0 = value 
    burs.append(MIR_Move.create(IA32_FIST, MO_CONV(burs, DW),  myFP0()));
    // MO_CONV now holds myFP0 converted to an integer (round-toward nearest)
    // FP Stack: myFP0 == value

    // isPositive == 1 iff 0.0 < value
    // isNegative == 1 iff 0.0 > value
    OPT_Register one        = burs.ir.regpool.getInteger();
    OPT_Register isPositive = burs.ir.regpool.getInteger();
    OPT_Register isNegative = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(one), I(1)));
    burs.append(MIR_Move.create(IA32_MOV, R(isPositive), I(0)));
    burs.append(MIR_Move.create(IA32_MOV, R(isNegative), I(0)));
    burs.append(MIR_Nullary.create(IA32_FLDZ, myFP0()));
    // FP Stack: myFP0 = 0.0; myFP1 = value 
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    burs.append(MIR_CondMove.create(IA32_CMOV, R(isPositive), R(one),
                                    OPT_IA32ConditionOperand.LLT()));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(isNegative), R(one),
                                    OPT_IA32ConditionOperand.LGT()));

    burs.append(MIR_Move.create(IA32_FILD, myFP0(), MO_CONV(burs, DW)));
    // FP Stack: myFP0 = round(value), myFP1 = value

    // addee      = 1 iff round(x) < x
    // subtractee = 1 iff round(x) > x
    OPT_Register addee      = burs.ir.regpool.getInteger();
    OPT_Register subtractee = burs.ir.regpool.getInteger();
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    burs.append(MIR_Move.create(IA32_MOV, R(addee) , I(0)));
    burs.append(MIR_Move.create(IA32_MOV, R(subtractee) , I(0)));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(addee), R(one),
                                    OPT_IA32ConditionOperand.LLT()));
    burs.append(MIR_CondMove.create(IA32_CMOV, R(subtractee), R(one),
                                    OPT_IA32ConditionOperand.LGT()));
    
    // Now a little tricky part.
    // We will add 1 iff isNegative and x > round(x)
    // We will subtract 1 iff isPositive and x < round(x)
    burs.append(MIR_BinaryAcc.create(IA32_AND, R(addee), R(isNegative)));
    burs.append(MIR_BinaryAcc.create(IA32_AND, R(subtractee), R(isPositive)));
    burs.append(MIR_Move.create(IA32_MOV, result.copy(), MO_CONV(burs, DW)));
    burs.append(MIR_BinaryAcc.create(IA32_ADD, result.copy(), R(addee)));
    burs.append(MIR_BinaryAcc.create(IA32_SUB, result.copy(), R(subtractee)));

    // Acquire the JTOC in a register
    OPT_Register jtoc = null;
    if (!burs.ir.options.FIXED_JTOC) {
      jtoc = burs.ir.regpool.getInteger();
      burs.append(MIR_Move.create(IA32_MOV, 
				  R(jtoc), 
				  MO_BD(R(burs.ir.regpool.getPhysicalRegisterSet().getPR()),
					VM_Entrypoints.jtocField.getOffset(), DW, null, null)));
    }

    // Compare myFP0 with (double)Integer.MAX_VALUE
    if (burs.ir.options.FIXED_JTOC) {
      M = OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(VM_Entrypoints.maxintField.getOffset()).toInt(),
			      QW, null, null);
    } else {
      M = OPT_MemoryOperand.BD(R(jtoc), VM_Entrypoints.maxintField.getOffset(), QW, null, null);
    }
    burs.append(MIR_Move.create(IA32_FLD, myFP0(), M));
    // FP Stack: myFP0 = (double)Integer.MAX_VALUE; myFP1 = value
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    // If MAX_VALUE < value, then result := MAX_INT
    OPT_Register maxInt = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(maxInt), I(Integer.MAX_VALUE)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(maxInt), 
                                    OPT_IA32ConditionOperand.LLT()));
    
    // Compare myFP0 with (double)Integer.MIN_VALUE
    if (burs.ir.options.FIXED_JTOC) {
      M = OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(VM_Entrypoints.minintField.getOffset()).toInt(),
			      QW, null, null);
    } else {
      M = OPT_MemoryOperand.BD(R(jtoc), VM_Entrypoints.minintField.getOffset(), QW, null, null);
    }
    burs.append(MIR_Move.create(IA32_FLD, myFP0(), M));
    // FP Stack: myFP0 = (double)Integer.MIN_VALUE; myFP1 = value
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP1()));
    // FP Stack: myFP0 = value
    // If MIN_VALUE > value, then result := MIN_INT
    OPT_Register minInt = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(minInt), I(Integer.MIN_VALUE)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(minInt), 
                                    OPT_IA32ConditionOperand.LGT()));
    
    // Set condition flags: set PE iff myFP0 is a NaN
    burs.append(MIR_Compare.create(IA32_FCOMIP, myFP0(), myFP0()));
    // FP Stack: back to original level (all BURS managed slots freed)
    // If FP0 was classified as a NaN, then result := 0
    OPT_Register zero = burs.ir.regpool.getInteger();
    burs.append(MIR_Move.create(IA32_MOV, R(zero), I(0)));
    burs.append(MIR_CondMove.create(IA32_CMOV, result.copy(), R(zero),
				    OPT_IA32ConditionOperand.PE()));
    
  }

  /**
   * Emit code to move 64 bits from FPRs to GPRs
   */
  final void FPR2GPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, QW);
    OPT_StackLocationOperand sl1 = new OPT_StackLocationOperand(true, offset+4, DW);
    OPT_StackLocationOperand sl2 = new OPT_StackLocationOperand(true, offset, DW);
    burs.append(MIR_Move.create(IA32_FMOV, sl, Unary.getVal(s)));
    OPT_RegisterOperand i1 = Unary.getResult(s);
    OPT_RegisterOperand i2 = R(burs.ir.regpool.getSecondReg(i1.register));
    burs.append(MIR_Move.create(IA32_MOV, i1, sl1));
    burs.append(MIR_Move.mutate(s, IA32_MOV, i2, sl2));
  }


  /**
   * Emit code to move 64 bits from GPRs to FPRs
   */
  final void GPR2FPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = - burs.ir.stackManager.allocateSpaceForConversion();
    OPT_StackLocationOperand sl = new OPT_StackLocationOperand(true, offset, QW);
    OPT_StackLocationOperand sl1 = new OPT_StackLocationOperand(true, offset+4, DW);
    OPT_StackLocationOperand sl2 = new OPT_StackLocationOperand(true, offset, DW);
    OPT_Operand i1, i2;
    OPT_Operand val = Unary.getVal(s);
    if (val instanceof OPT_RegisterOperand) {
      OPT_RegisterOperand rval = (OPT_RegisterOperand)val;
      i1 = val;
      i2 = R(burs.ir.regpool.getSecondReg(rval.register));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)val;
      i1 = I(rhs.upper32());
      i2 = I(rhs.lower32());
    }      
    burs.append(MIR_Move.create(IA32_MOV, sl1, i1));
    burs.append(MIR_Move.create(IA32_MOV, sl2, i2));
    burs.append(MIR_Move.mutate(s, IA32_FMOV, Unary.getResult(s), sl));
  }

  /**
   * Expansion of ROUND_TO_ZERO.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void ROUND_TO_ZERO(OPT_BURS burs, OPT_Instruction s) {
    // load the JTOC into a register
    OPT_RegisterOperand PR = R(burs.ir.regpool.getPhysicalRegisterSet().
                               getPR());
    OPT_Operand jtoc = OPT_MemoryOperand.BD(PR, VM_Entrypoints.jtocField.getOffset(), 
                                            DW, null, null);
    OPT_RegisterOperand regOp = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Move.create(IA32_MOV, regOp, jtoc));

    // Store the FPU Control Word to a JTOC slot
    OPT_MemoryOperand M = OPT_MemoryOperand.BD
      (regOp.copyRO(), VM_Entrypoints.FPUControlWordField.getOffset(), W, null, null);
    burs.append(MIR_UnaryNoRes.create(IA32_FNSTCW, M));
    // Set the bits in the status word that control round to zero.
    // Note that we use a 32-bit and, even though we only care about the
    // low-order 16 bits
    burs.append(MIR_BinaryAcc.create(IA32_OR, M.copy(), I(0x00000c00)));
    // Now store the result back into the FPU Control Word
    burs.append(MIR_Nullary.mutate(s,IA32_FLDCW, M.copy()));
    return;
  }


  /**
   * Expansion of INT_DIV and INT_REM
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   * @param isDiv true for div, false for rem
   */
  final void INT_DIVIDES(OPT_BURS burs, OPT_Instruction s,
			 OPT_RegisterOperand result,
			 OPT_Operand val1,
			 OPT_Operand val2,
			 boolean isDiv) {
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), val1));
    burs.append(MIR_ConvertDW2QW.create(IA32_CDQ, R(getEDX()), R(getEAX())));
    if (val2 instanceof OPT_IntConstantOperand) {
      OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, temp, val2));
      val2 = temp;
    }
    burs.append(MIR_Divide.mutate(s, IA32_IDIV, R(getEDX()), R(getEAX()), 
				  val2, GuardedBinary.getGuard(s)));
    if (isDiv) {
      burs.append(MIR_Move.create(IA32_MOV, result.copyD2D(), R(getEAX())));
    } else {
      burs.append(MIR_Move.create(IA32_MOV, result.copyD2D(), R(getEDX())));
    }      
  }


  /**
   * Expansion of LONG_ADD_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_ADD(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_ADC, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) {
	burs.append(MIR_BinaryAcc.mutate(s, IA32_ADD, R(lhsReg), I(high)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lowlhsReg), I(low)));
	burs.append(MIR_BinaryAcc.mutate(s, IA32_ADC, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_SUB_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_SUB(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) {
	burs.append(MIR_BinaryAcc.mutate(s, IA32_SUB, R(lhsReg), I(high)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lowlhsReg), I(low)));
	burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_MUL_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_MUL(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    // In general, (a,b) * (c,d) = (l(a imul d)+l(b imul c)+u(b mul d), l(b mul d))
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      OPT_Register tmp = burs.ir.regpool.getInteger();
      burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), R(lowrhsReg)));
      burs.append(MIR_Move.create(IA32_MOV, R(tmp), R(rhsReg)));
      burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
      burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), R(lowlhsReg)));
      burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowrhsReg)));
      burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
      burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();

      // We only have to handle those cases that OPT_Simplifier wouldn't get.  
      // OPT_Simplifier catches 
      // high   low
      //    0     0  (0L)
      //    0     1  (1L)
      //   -1    -1 (-1L)
      // So, the possible cases we need to handle here:
      //   -1     0 
      //   -1     1
      //   -1     *
      //    0    -1
      //    0     *
      //    1    -1
      //    1     0 
      //    1     1
      //    1     *
      //    *    -1
      //    *     0
      //    *     1
      //    *     *
      // (where * is something other than -1,0,1)
      if (high == -1) {
	if (low == 0) {
	  // -1, 0
	  // CLAIM: (x,y) * (-1,0) = (-y,0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // -1, 1
	  // CLAIM: (x,y) * (-1,1) = (x-y,y)
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lhsReg), R(lowlhsReg)));
	} else {
	  // -1, *
	  // CLAIM: (x,y) * (-1, z) = (l(x imul z)-y+u(y mul z)+, l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else if (high == 0) {
	if (low == -1) {
	  // 0, -1
	  // CLAIM: (x,y) * (0,-1) = (u(y mul -1)-x, l(y mul -1))
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(-1)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_SUB, R(getEDX()), R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(getEDX())));
	} else {
	  // 0, *
	  // CLAIM: (x,y) * (0,z) = (l(x imul z)+u(y mul z), l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else if (high == 1) {
	if (low == -1) {
	  // 1, -1
	  // CLAIM: (x,y) * (1,-1) = (-x+y+u(y mul -1), l(y mul -1))
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(-1)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	} else if (low == 0) {
	  // 1, 0 
	  // CLAIM: (x,y) * (1,0) = (y,0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // 1, 1
	  // CLAIM: (x,y) * (1,1)  = (x+y,y)
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	} else {
	  // 1, *
	  // CLAIM: (x,y) * (1,z) = (l(x imul z)+y+u(y mul z), l(y mul z))
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      } else {
	if (low == -1) {
	  // *, -1
	  // CLAIM: (x,y) * (z,-1) = (-x+l(y imul z)+u(y mul -1), l(y mul -1))
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	} else if (low == 0) {
	  // *,  0
	  // CLAIM: (x,y) * (z,0) = (l(y imul z),0)
	  burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
	} else if (low == 1) {
	  // *, 1
	  // CLAIM: (x,y) * (z,1) = (l(y imul z)+x,y)	
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), I(high)));
	  burs.append(MIR_Move.create(IA32_ADD, R(lhsReg), R(tmp)));
	} else {
	  // *, * (sigh, can't do anything interesting...)
	  OPT_Register tmp = burs.ir.regpool.getInteger();
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(lhsReg), I(low)));
	  burs.append(MIR_Move.create(IA32_MOV, R(tmp), I(high)));
	  burs.append(MIR_BinaryAcc.create(IA32_IMUL2, R(tmp), R(lowlhsReg)));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(tmp)));
	  burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), I(low)));
	  burs.append(MIR_Multiply.create(IA32_MUL, R(getEDX()), R(getEAX()), R(lowlhsReg)));
	  burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), R(getEAX())));
	  burs.append(MIR_BinaryAcc.create(IA32_ADD, R(lhsReg), R(getEDX())));
	}
      }
    }
  }


  /**
   * Expansion of LONG_NEG_ACC
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   */
  final void LONG_NEG(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lhsReg)));
    burs.append(MIR_UnaryAcc.create(IA32_NEG, R(lowlhsReg)));
    burs.append(MIR_BinaryAcc.mutate(s, IA32_SBB, R(lhsReg), I(0)));
  }


  /**
   * Expansion of LONG_AND
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_AND(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_AND, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_AND, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x &= 0 ==> x = 0
	burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(0)));
      } else if (low == -1) { // x &= 0xffffffff ==> x = x ==> nop
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_AND, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x &= 0 ==> x = 0
	burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(0)));
      } else if (high == -1) { // x &= 0xffffffff ==> x = x ==> nop
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_AND, R(lhsReg), I(high)));
      }
    }	
  }


  /**
   * Expansion of LONG_OR
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_OR(OPT_BURS burs, OPT_Instruction s,
		     OPT_RegisterOperand result,
		     OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_OR, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_OR, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x |= 0 ==> x = x ==> nop
      } else if (low == -1) { // x |= 0xffffffff ==> x = 0xffffffff
	burs.append(MIR_Move.create(IA32_MOV, R(lowlhsReg), I(-1)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_OR, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x |= 0 ==> x = x ==> nop
      } else if (high == -1) { // x |= 0xffffffff ==> x = 0xffffffff
	burs.append(MIR_Move.create(IA32_MOV, R(lhsReg), I(-1)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_OR, R(lhsReg), I(high)));
      }
    }	
  }


  /**
   * Expansion of LONG_XOR
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   * @param value the second operand
   */
  final void LONG_XOR(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_Operand value) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    if (value instanceof OPT_RegisterOperand) {
      OPT_Register rhsReg = ((OPT_RegisterOperand)value).register;
      OPT_Register lowrhsReg = burs.ir.regpool.getSecondReg(rhsReg);
      burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lowlhsReg), R(lowrhsReg)));
      burs.append(MIR_BinaryAcc.mutate(s, IA32_XOR, R(lhsReg), R(rhsReg)));
    } else {
      OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)value;
      int low = rhs.lower32();
      int high = rhs.upper32();
      if (low == 0) { // x ^= 0 ==> x = x ==> nop
      } else if (low == -1) { // x ^= 0xffffffff ==> x = ~x
	burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lowlhsReg)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lowlhsReg), I(low)));
      }
      if (high == 0) { // x ^= 0 ==> x = x ==> nop
      } else if (high == -1) { // x ^= 0xffffffff ==> x = ~x
	burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lhsReg)));
      } else {
	burs.append(MIR_BinaryAcc.create(IA32_XOR, R(lhsReg), I(high)));
      }
    }
  }


  /**
   * Expansion of LONG_NOT
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param result the result/first operand
   */
  final void LONG_NOT(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result) {
    OPT_Register lhsReg = result.register;
    OPT_Register lowlhsReg = burs.ir.regpool.getSecondReg(lhsReg);
    burs.append(MIR_UnaryAcc.create(IA32_NOT, R(lowlhsReg)));
    burs.append(MIR_UnaryAcc.mutate(s, IA32_NOT, R(lhsReg)));
  }


  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * Moves first value into fp0,
   * accumulates second value into fp0 using op,
   * moves fp0 into result.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_MOV_OP_MOV(OPT_BURS burs, OPT_Instruction s,
			   OPT_Operator op,
			   OPT_Operand result,
			   OPT_Operand val1,
			   OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s, op, D(getFPR(0)), val2));
    burs.append(MIR_Move.create(IA32_FMOV, result, D(getFPR(0))));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * Moves first value into fp0,
   * accumulates second value into fp0 using op.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_MOV_OP(OPT_BURS burs, OPT_Instruction s,
			   OPT_Operator op,
			   OPT_Operand val1,
			   OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s, op, D(getFPR(0)), val2));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * apply op to val1 and val2
   * move val1 to result using movop
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param movop the move op to use
   * @param result the result operand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_OP_MOV(OPT_BURS burs, OPT_Instruction s,
		       OPT_Operator op,
		       OPT_Operator movop,
		       OPT_Operand result,
		       OPT_Operand val1,
		       OPT_Operand val2) {
    burs.append(MIR_BinaryAcc.mutate(s, op, val1, val2));
    burs.append(MIR_Move.create(movop, result, val1.copy()));
  }
  /**
   * Expansion of FP_ADD_ACC, FP_MUL_ACC, 
   * FP_SUB_ACC, and FP_DIV_ACC.
   * apply op to val1 and val2.
   * NOTE: either val1 or val2 must be either FPR0 or ST(0)!
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param op the floating point op to use
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_OP(OPT_BURS burs, OPT_Instruction s,
		   OPT_Operator op,
		   OPT_Operand val1,
		   OPT_Operand val2) {
    burs.append(MIR_BinaryAcc.mutate(s, op, val1, val2));
  }

  /**
   * Expansion of FP_REM 
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param val1 the first operand
   * @param val2 the second operand
   */
  final void FP_REM(OPT_BURS burs, OPT_Instruction s,
		    OPT_Operand val1,
		    OPT_Operand val2) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(1)), val2));
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1));
    burs.append(MIR_BinaryAcc.mutate(s,IA32_FPREM, D(getFPR(0)), D(getFPR(1))));
  }
  /**
   * Expansion of FP_REM
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param val the operand to divide with fp0 to get a remainder
   */
  final void FP_REM(OPT_BURS burs, OPT_Instruction s,
		    OPT_Operand val) {
    burs.append(MIR_Move.create(IA32_FMOV, D(getFPR(1)), val));
    burs.append(MIR_BinaryAcc.mutate(s,IA32_FPREM, D(getFPR(0)), D(getFPR(1))));
  }


  /**
   * Expansion of BOOLEAN_CMP
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result operand
   * @param val1   the first value
   * @param val2   the second value
   * @param cond   the condition operand
   */
  final void BOOLEAN_CMP(OPT_BURS burs, OPT_Instruction s,
			 OPT_Operand res, 
			 OPT_Operand val1,
			 OPT_Operand val2,
			 OPT_ConditionOperand cond) {
    burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    OPT_RegisterOperand temp = burs.ir.regpool.makeTemp(VM_Type.BooleanType);
    burs.append(CPOS(s, MIR_Set.create(IA32_SET$B, temp, COND(cond))));
    burs.append(MIR_Unary.mutate(s, IA32_MOVZX$B, res, temp.copyD2U()));
  }


  /**
   * Expansion of a special case of BOOLEAN_CMP when the 
   * condition registers have already been set by the previous
   * ALU op.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result operand
   * @param cond   the condition operand
   */
  final void BOOLEAN_CMP(OPT_BURS burs, OPT_Instruction s,
			 OPT_Operand res, 
			 OPT_ConditionOperand cond) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTemp(VM_Type.BooleanType);
    burs.append(CPOS(s, MIR_Set.create(IA32_SET$B, temp, COND(cond))));
    burs.append(MIR_Unary.mutate(s, IA32_MOVZX$B, res, temp.copyD2U()));
  }


  /**
   * Generate a compare and branch sequence.
   * Used in the expansion of trees where INT_IFCMP is a root
   * 
   * @param burs and OPT_BURS object
   * @param s the ifcmp instruction 
   * @param val1 the first value operand
   * @param val2 the second value operand
   * @param cond the condition operand
   */
  final void IFCMP(OPT_BURS burs, OPT_Instruction s,
		   OPT_Operand val1, OPT_Operand val2,
		   OPT_ConditionOperand cond) {
    burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    burs.append(MIR_CondBranch.mutate(s, IA32_JCC, COND(cond),
				      IfCmp.getTarget(s), 
				      IfCmp.getBranchProfile(s)));
  }


  /**
   * Generate the compare portion of a conditional move.
   * 
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param val1 the first value to compare
   * @param val2 the second value to compare
   */
  final void CMOV_CMP(OPT_BURS burs, OPT_Instruction s,
		      OPT_Operand val1, OPT_Operand val2) {
    if (val1.isRegister() && val1.asRegister().register.isFloatingPoint()) {
      if (VM.VerifyAssertions) {
        VM.assert(val2.isRegister());
        VM.assert(val2.asRegister().register.isFloatingPoint());
      }
      burs.append(CPOS(s, MIR_Move.create(IA32_FMOV, D(getFPR(0)), val1)));
      burs.append(CPOS(s, MIR_Compare.create(IA32_FCOMI, D(getFPR(0)), val2)));
    } else {
      burs.append(CPOS(s, MIR_Compare.create(IA32_CMP, val1, val2)));
    }
  }

  /**
   * Generate the move portion of a conditional move.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to copy position info from
   * @param result the result of the conditional move
   * @param cond the condition operand
   * @param trueVal the value to move to result if cond is true
   * @param falseVal the value to move to result if cond is not true
   */
  final void CMOV_MOV(OPT_BURS burs, OPT_Instruction s,
		      OPT_RegisterOperand result,
		      OPT_ConditionOperand cond,
		      OPT_Operand trueValue,
		      OPT_Operand falseValue) {
    OPT_Operator movop, cmovop;
    if (result.type.isDoubleType() || result.type.isFloatType()) {
      movop = IA32_FMOV;
      cmovop = IA32_FCMOV;
    } else {
      movop = IA32_MOV;
      cmovop = IA32_CMOV;
    }

    if (result.similar(trueValue)) {
      // in this case, only need a conditional move for the false branch.
      burs.append(MIR_CondMove.mutate(s, cmovop, result,
				      asReg(burs, s, movop, falseValue),
				      COND(cond.flipCode())));
    } else if (result.similar(falseValue)) {
      // in this case, only need a conditional move for the true branch.
      burs.append(MIR_CondMove.mutate(s, cmovop, result, 
				      asReg(burs, s, movop, trueValue),
				      COND(cond)));
    } else {
      // need to handle both possible assignments. Unconditionally
      // assign one value then conditionally assign the other.
      if (falseValue.isRegister()) {
	burs.append(CPOS(s,MIR_Move.create(movop, result, trueValue)));
	burs.append(MIR_CondMove.mutate(s, cmovop, result.copy(), 
					falseValue,
					COND(cond.flipCode())));
      } else {
	burs.append(CPOS(s,MIR_Move.create(movop, result, falseValue)));
	burs.append(MIR_CondMove.mutate(s, cmovop, result.copy(), 
					asReg(burs, s, movop, trueValue),
					COND(cond)));
      }
    }
  }

  // move op into a register operand if it isn't one already.
  private OPT_Operand asReg(OPT_BURS burs, OPT_Instruction s, 
			    OPT_Operator movop, OPT_Operand op) {
    if (op.isRegister()) return op;
    OPT_RegisterOperand tmp = burs.ir.regpool.makeTemp(op);
    burs.append(CPOS(s, MIR_Move.create(movop, tmp, op)));
    return tmp.copy();
  }


  /**
   * Expand a prologue by expanding out longs into pairs of ints
   */
  void PROLOGUE(OPT_BURS burs, OPT_Instruction s) {
    int numFormals = Prologue.getNumberOfFormals(s);
    int numLongs = 0;
    for (int i=0; i<numFormals; i++) {
      if (Prologue.getFormal(s, i).type == VM_Type.LongType) numLongs ++;
    }
    if (numLongs != 0) {
      OPT_Instruction s2 = Prologue.create(IR_PROLOGUE, numFormals+numLongs);
      for (int sidx=0, s2idx=0; sidx<numFormals; sidx++) {
	OPT_RegisterOperand sForm = Prologue.getFormal(s, sidx);
	if (sForm.type == VM_Type.LongType) {
	  sForm.type = VM_Type.IntType;
	  Prologue.setFormal(s2, s2idx++, sForm);
          OPT_Register r2 = burs.ir.regpool.getSecondReg(sForm.register);
	  Prologue.setFormal(s2, s2idx++, R(r2));
          sForm.register.clearType();
          sForm.register.setInteger();
          r2.clearType();
          r2.setInteger();
	} else {
	  Prologue.setFormal(s2, s2idx++, sForm);
	}
      }									     
      burs.append(s2);
    } else {
      burs.append(s);
    }
  }

  /**
   * Expansion of CALL.
   * Expand longs registers into pairs of int registers.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param address the operand containing the target address
   */
  final void CALL(OPT_BURS burs, 
		  OPT_Instruction s,
		  OPT_Operand address) {
    OPT_RegisterPool regpool = burs.ir.regpool;

    // Step 1: Find out how many parameters we're going to have.
    int numParams = Call.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (Call.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be.
    OPT_RegisterOperand result = Call.getResult(s);
    OPT_RegisterOperand result2 = null;
    if (result != null && result.type == VM_Type.LongType) {
      result.type = VM_Type.IntType;
      result2 = R(regpool.getSecondReg(result.register));
    }
    
    // Step 3: Mutate the Call to an MIR_Call.
    // Note MIR_Call and Call have a different number of fixed 
    // arguments, so some amount of copying is required. 
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = Call.getParam(s, i);
    }
    MIR_Call.mutate(s, IA32_CALL, result, result2, 
		    address, Call.getMethod(s),
		    numParams + longParams);
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      if (param instanceof OPT_RegisterOperand) {
	MIR_Call.setParam(s, mirCallIdx++, param);
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
                            L(regpool.getSecondReg(rparam.register)));
        }
      } else if (param instanceof OPT_LongConstantOperand) {
	OPT_LongConstantOperand val = (OPT_LongConstantOperand)param;
	MIR_Call.setParam(s, mirCallIdx++, I(val.upper32()));
	MIR_Call.setParam(s, mirCallIdx++, I(val.lower32()));
      } else {
	MIR_Call.setParam(s, mirCallIdx++, param);
      }
    }

    // emit the call instruction.
    burs.append(s);
  }

  /**
   * Expansion of SYSCALL.
   * Expand longs registers into pairs of int registers.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   * @param address the operand containing the target address
   */
  final void SYSCALL(OPT_BURS burs, OPT_Instruction s, OPT_Operand address) {
    OPT_RegisterPool regpool = burs.ir.regpool;
    burs.ir.setHasSysCall(true);

    // Step 1: Find out how many parameters we're going to have.
    int numParams = CallSpecial.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (CallSpecial.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be.
    OPT_RegisterOperand result = CallSpecial.getResult(s);
    OPT_RegisterOperand result2 = null;
    // NOTE: C callee returns longs little endian!
    if (result != null && result.type == VM_Type.LongType) {
      result.type = VM_Type.IntType;
      result2 = result;
      result = R(regpool.getSecondReg(result.register));
    }
    
    // Step 3: Mutate the CallSpecial to an MIR_Call.
    // Note MIR_Call and CallSpecial have a different number of fixed 
    // arguments, so some amount of copying is required. 
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = CallSpecial.getParam(s, i);
    }
    MIR_Call.mutate(s, IA32_SYSCALL, result, result2, 
		    address, (OPT_MethodOperand)CallSpecial.getMethod(s),
		    numParams + longParams);
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      if (param instanceof OPT_RegisterOperand) {
	// NOTE: longs passed little endian to C callee!
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
                            L(regpool.getSecondReg(rparam.register)));
        }
	MIR_Call.setParam(s, mirCallIdx++, param);
      } else if (param instanceof OPT_LongConstantOperand) {
	long value = ((OPT_LongConstantOperand)param).value; 
	int valueHigh = (int)(value >> 32);
	int valueLow = (int)(value & 0xffffffff);
	// NOTE: longs passed little endian to C callee!
	MIR_Call.setParam(s, mirCallIdx++, I(valueLow));
	MIR_Call.setParam(s, mirCallIdx++, I(valueHigh));
      } else {
	MIR_Call.setParam(s, mirCallIdx++, param);
      }
    }

    // emit the call instruction.
    burs.append(s);
  }

  /**
   * Expansion of LOWTABLESWITCH.  
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void LOWTABLESWITCH(OPT_BURS burs, OPT_Instruction s) {
    // (1) We're changing index from a U to a DU.
    //     Inject a fresh copy instruction to make sure we aren't
    //     going to get into trouble (if someone else was also using index).
    OPT_RegisterOperand newIndex = burs.ir.regpool.makeTempInt(); 
    burs.append(MIR_Move.create(IA32_MOV, newIndex, LowTableSwitch.getIndex(s))); 
    int number = LowTableSwitch.getNumberOfTargets(s);
    OPT_Instruction s2 = CPOS(s,MIR_LowTableSwitch.create(MIR_LOWTABLESWITCH, newIndex, number*2));
    for (int i=0; i<number; i++) {
      MIR_LowTableSwitch.setTarget(s2,i,LowTableSwitch.getTarget(s,i));
      MIR_LowTableSwitch.setBranchProfile(s2,i,LowTableSwitch.getBranchProfile(s,i));
    }
    burs.append(s2);
  }

  /**
   * Expansion of RESOLVE.  Dynamic link point.
   * Build up MIR instructions for Resolve.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void RESOLVE(OPT_BURS burs, 
		     OPT_Instruction s) {
    OPT_Operand target = loadFromJTOC(burs, VM_Entrypoints.optResolveMethod.getOffset());
    burs.append(CPOS(s, MIR_Call.mutate0(s, CALL_SAVE_VOLATILE, 
					 null, null,  target, 
					 OPT_MethodOperand.STATIC(VM_Entrypoints.optResolveMethod))));
  }
  /**
   * Expansion of TRAP_IF, with an int constant as the second value.
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  void TRAP_IF_IMM(OPT_BURS burs, OPT_Instruction s) {
    OPT_RegisterOperand gRes = TrapIf.getGuardResult(s);
    OPT_RegisterOperand v1 =  (OPT_RegisterOperand)TrapIf.getVal1(s);
    OPT_IntConstantOperand v2 = (OPT_IntConstantOperand)TrapIf.getVal2(s);
    OPT_ConditionOperand cond = TrapIf.getCond(s);
    OPT_TrapCodeOperand tc = TrapIf.getTCode(s);

    // A slightly ugly matter, but we need to deal with combining
    // the two pieces of a long register from a LONG_ZERO_CHECK.  
    // A little awkward, but probably the easiest workaround...
    if (tc.getTrapCode() == VM_Runtime.TRAP_DIVIDE_BY_ZERO &&
        v1.type == VM_Type.LongType) {
      OPT_RegisterOperand rr = burs.ir.regpool.makeTempInt();
      burs.append(MIR_Move.create(IA32_MOV, rr, v1.copy()));
      burs.append(MIR_BinaryAcc.create(IA32_OR, rr.copy(), 
				       R(burs.ir.regpool.getSecondReg
					 (v1.register))));
      v1 = rr.copyD2U();
    } 

    // emit the trap instruction
    burs.append(MIR_TrapIf.mutate(s, IA32_TRAPIF, gRes, v1, v2, COND(cond),
                                  tc));
  }


  /**
   * This routine expands an ATTEMPT instruction 
   * into an atomic compare exchange.
   *
   * @param burs     an OPT_BURS object
   * @param result   the register operand that is set to 0/1 as a result of the attempt
   * @param mo       the address at which to attempt the exchange
   * @param oldValue the old value at the address mo
   * @param newValue the new value at the address mo
   */
  void ATTEMPT(OPT_BURS burs, 
	       OPT_RegisterOperand result,
	       OPT_MemoryOperand mo,
	       OPT_Operand oldValue,
	       OPT_Operand newValue) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
    OPT_RegisterOperand temp2 = burs.ir.regpool.makeTemp(result);
    burs.append(MIR_Move.create(IA32_MOV, temp, newValue));
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), oldValue));
    burs.append(MIR_CompareExchange.create(IA32_LOCK_CMPXCHG, R(getEAX()), 
					   mo, (OPT_RegisterOperand)temp.copy())); 
    burs.append(MIR_Set.create(IA32_SET$B, temp2, OPT_IA32ConditionOperand.EQ()));
    // need to zero-extend the result of the set
    burs.append(MIR_Unary.create(IA32_MOVZX$B, result, temp2.copy()));
  }


  /**
   * This routine expands the compound pattern
   * IFCMP(ATTEMPT, ZERO) into an atomic compare/exchange 
   * followed by a branch on success/failure
   * of the attempted atomic compare/exchange.
   *
   * @param burs     an OPT_BURS object
   * @param mo       the address at which to attempt the exchange
   * @param oldValue the old value at the address mo
   * @param newValue the new value at the address mo
   * @param cond     the condition to branch on
   * @param target   the branch target
   * @param bp       the branch profile information
   */
  void ATTEMPT_IFCMP(OPT_BURS burs, 
		     OPT_MemoryOperand mo,
		     OPT_Operand oldValue,
		     OPT_Operand newValue,
		     OPT_ConditionOperand cond,
		     OPT_BranchOperand target,
		     OPT_BranchProfileOperand bp) {
    OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Move.create(IA32_MOV, temp, newValue));
    burs.append(MIR_Move.create(IA32_MOV, R(getEAX()), oldValue));
    burs.append(MIR_CompareExchange.create(IA32_LOCK_CMPXCHG, R(getEAX()), 
					   mo, (OPT_RegisterOperand)temp.copy())); 
    burs.append(MIR_CondBranch.create(IA32_JCC, COND(cond), target, bp));
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Handles the conversion from LIR to MIR of operators whose 
 * expansion requires the introduction of new control flow (new basic blocks).
 *
 * @author Dave Grove
 * @modified Peter Sweeney
 */
abstract class OPT_ComplexLIR2MIRExpansion extends OPT_IRTools {

  /**
   * Converts the given IR to low level IA32 IR.
   *
   * @param ir IR to convert
   */
  public static void convert(OPT_IR ir) {
    OPT_Instruction nextInstr;
    for (OPT_Instruction s = ir.firstInstructionInCodeOrder();
	 s != null; s = nextInstr) {
      switch (s.getOpcode()) {
      case LONG_SHL_ACC_opcode:
	nextInstr = long_shl(s, ir);
	break;
      case LONG_SHR_ACC_opcode:
	nextInstr = long_shr(s, ir);
	break;
      case LONG_USHR_ACC_opcode:
	nextInstr = long_ushr(s, ir);
	break;
      case LONG_IFCMP_opcode:
	{
	  OPT_Operand val2 = IfCmp.getVal2(s);
	  if (val2 instanceof OPT_RegisterOperand) {
	    nextInstr = long_ifcmp(s, ir);
	  } else {
	    nextInstr = long_ifcmp_imm(s, ir);
	  }
	}
	break;
      case LONG_CMP_opcode:
	nextInstr = threeValueLongCmp(s, ir);
	break;
      case FLOAT_IFCMPL_opcode:
      case FLOAT_IFCMPG_opcode:
      case DOUBLE_IFCMPL_opcode:
      case DOUBLE_IFCMPG_opcode:
	nextInstr = fp_ifcmp(s, ir);
	break;
      case FLOAT_CMPL_opcode:
      case FLOAT_CMPG_opcode:
      case DOUBLE_CMPL_opcode:
      case DOUBLE_CMPG_opcode:
	nextInstr = threeValueFPCmp(s, ir);
	break;
      case YIELDPOINT_PROLOGUE_opcode:
      case YIELDPOINT_EPILOGUE_opcode:
      case YIELDPOINT_BACKEDGE_opcode:
	nextInstr = yield_point(s,ir);
	break;
      default:
	nextInstr = s.nextInstructionInCodeOrder();
	break;
      }
    }
    OPT_DefUse.recomputeSpansBasicBlock(ir);
  }


  private static OPT_Instruction long_shl(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(hval), R(lval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SHL, R(hval), I(shift)));
	MIR_Move.mutate(s, IA32_MOV, R(lval), I(0));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHLD, R(hval), R(lval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SHL, R(lval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();

      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(hval), R(lval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(hval), R(ecx)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), I(0)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHLD, R(hval), R(lval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(lval), R(ecx)));
    }
    return nextInstr;
  }


  private static OPT_Instruction long_shr(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SAR, R(lval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SAR, R(hval), I(31));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SAR, R(hval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();
      
      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(lval), R(ecx)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(hval), I(31)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SAR, R(hval), R(ecx)));
    }
    return nextInstr;
  }

  
  private static OPT_Instruction long_ushr(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register hval = BinaryAcc.getResult(s).register;
    OPT_Register lval = ir.regpool.getSecondReg(hval);
    OPT_Operand shiftOp = BinaryAcc.getClearValue(s);
    
    if (shiftOp instanceof OPT_IntConstantOperand) {
      int shift = ((OPT_IntConstantOperand)shiftOp).value;
      shift = shift & 0x3F; // only bottom six bits matter;
      if (shift == 0) {
	s.remove(); // operation is a nop.
      } else if (shift >= 32) {
	s.insertBefore(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
	s.insertBefore(MIR_BinaryAcc.create(IA32_SHR, R(lval), I(shift)));
	MIR_Move.mutate(s, IA32_MOV, R(hval), I(0));
      } else {
	s.insertBefore(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), I(shift)));
	MIR_BinaryAcc.mutate(s, IA32_SHR, R(hval), I(shift));
      }
    } else {
      OPT_RegisterOperand shiftTemp = ir.regpool.makeTempInt();
      OPT_Register shift = shiftTemp.register;
      OPT_Register ecx = ir.regpool.getPhysicalRegisterSet().getECX();

      OPT_BasicBlock sizeTestBB = s.getBasicBlock();
      OPT_BasicBlock nextBB = sizeTestBB.splitNodeAt(s, ir);
      OPT_BasicBlock gt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      OPT_BasicBlock lt32BB = sizeTestBB.createSubBlock(s.bcIndex, ir, 0.5f);
      sizeTestBB.insertOut(gt32BB);
      sizeTestBB.insertOut(lt32BB);
      gt32BB.insertOut(nextBB);
      lt32BB.insertOut(nextBB);
      ir.cfg.linkInCodeOrder(sizeTestBB, gt32BB);
      ir.cfg.linkInCodeOrder(gt32BB, lt32BB);
      ir.cfg.linkInCodeOrder(lt32BB, nextBB);

      s.remove();
      
      // copy the shift value to a temporary so we can destroy it.
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(shift), shiftOp));
      
      // See if the shift is lt or gt 32
      sizeTestBB.appendInstruction(MIR_Move.create(IA32_MOV, R(ecx), R(shift)));
      sizeTestBB.appendInstruction(MIR_BinaryAcc.create(IA32_AND, R(shift), I(32)));
      sizeTestBB.appendInstruction(MIR_CondBranch.create(IA32_JCC, 
							 OPT_IA32ConditionOperand.EQ(),
							 lt32BB.makeJumpTarget(),
							 new OPT_BranchProfileOperand()));
      
      // handle shift gt 32
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_XOR, R(ecx), R(shift)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(lval), R(hval)));
      gt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHR, R(lval), R(ecx)));
      gt32BB.appendInstruction(MIR_Move.create(IA32_MOV, R(hval), I(0)));
      gt32BB.appendInstruction(MIR_Branch.create(IA32_JMP, nextBB.makeJumpTarget()));
      
      // handle shift lt 32
      lt32BB.appendInstruction(MIR_DoubleShift.create(IA32_SHRD, R(lval), R(hval), R(ecx)));
      lt32BB.appendInstruction(MIR_BinaryAcc.create(IA32_SHR, R(hval), R(ecx)));
    }
    return nextInstr;
  }


  private static OPT_Instruction long_ifcmp(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_ConditionOperand cond = IfCmp.getCond(s);
    OPT_Register xh = ((OPT_RegisterOperand)IfCmp.getVal1(s)).register;
    OPT_Register xl = ir.regpool.getSecondReg(xh);
    OPT_RegisterOperand yh = (OPT_RegisterOperand)IfCmp.getClearVal2(s);
    OPT_RegisterOperand yl = R(ir.regpool.getSecondReg(yh.register));
    basic_long_ifcmp(s, ir, cond, xh, xl, yh, yl);
    return nextInstr;
  }


  private static OPT_Instruction long_ifcmp_imm(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_ConditionOperand cond = IfCmp.getCond(s);
    OPT_Register xh = ((OPT_RegisterOperand)IfCmp.getVal1(s)).register;
    OPT_Register xl = ir.regpool.getSecondReg(xh);
    OPT_LongConstantOperand rhs = (OPT_LongConstantOperand)IfCmp.getVal2(s);
    int low = rhs.lower32();
    int high = rhs.upper32();
    OPT_IntConstantOperand yh = I(high);
    OPT_IntConstantOperand yl = I(low);
    
    if (cond.isEQUAL() || cond.isNOT_EQUAL()) {
      // tricky... ((xh^yh)|(xl^yl) == 0) <==> (lhll == rhrl)!!
      OPT_Register th = ir.regpool.getInteger();
      OPT_Register tl = ir.regpool.getInteger();
      if (high == 0) {
	if (low == 0) { // 0,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // 0,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(tl), R(xh)));
	} else { // 0,*
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(tl), R(xh)));
	}
      } else if (high == -1) {
	if (low == 0) { // -1,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // -1,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	} else { // -1,*
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(th)));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	}
      } else { 
	if (low == 0) { // *,0
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(xl)));
	} else if (low == -1) { // *,-1
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_UnaryAcc.create(IA32_NOT, R(tl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	} else { // neither high nor low is special
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(th), R(xh)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(th), yh));
	  s.insertBefore(MIR_Move.create(IA32_MOV, R(tl), R(xl)));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, R(tl), yl));
	  s.insertBefore(MIR_BinaryAcc.create(IA32_OR, R(th), R(tl)));
	}
      }
      MIR_CondBranch.mutate(s, IA32_JCC, 
			    new OPT_IA32ConditionOperand(cond),
			    IfCmp.getTarget(s),
			    IfCmp.getBranchProfile(s));
      return nextInstr;
    } else {
      // pick up a few special cases where the sign of xh is sufficient
      if (rhs.value == 0L) {
	if (cond.isLESS()) {
	  // xh < 0 implies true
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.LT(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	} else if (cond.isGREATER_EQUAL()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.GE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	}
      } else if (rhs.value == -1L) {
	if (cond.isLESS_EQUAL()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(-1)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.LE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	} else if (cond.isGREATER()) {
	  s.insertBefore(MIR_Compare.create(IA32_CMP, R(xh), I(0)));
	  MIR_CondBranch.mutate(s, IA32_JCC,
				OPT_IA32ConditionOperand.GE(),
				IfCmp.getTarget(s),
				IfCmp.getBranchProfile(s));
	  return nextInstr;
	}
      }

      basic_long_ifcmp(s, ir, cond, xh, xl, yh, yl);
      return nextInstr;
    }
  }


  private static void basic_long_ifcmp(OPT_Instruction s, OPT_IR ir, 
				       OPT_ConditionOperand cond, 
				       OPT_Register xh, 
				       OPT_Register xl, 
				       OPT_Operand yh, 
				       OPT_Operand yl) {
    if (cond.isEQUAL() || cond.isNOT_EQUAL()) {
      OPT_RegisterOperand th = ir.regpool.makeTempInt();
      OPT_RegisterOperand tl = ir.regpool.makeTempInt();
      // tricky... ((xh^yh)|(xl^yl) == 0) <==> (lhll == rhrl)!!
      s.insertBefore(MIR_Move.create(IA32_MOV, th, R(xh)));
      s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, th.copyD2D(), yh));
      s.insertBefore(MIR_Move.create(IA32_MOV, tl, R(xl)));
      s.insertBefore(MIR_BinaryAcc.create(IA32_XOR, tl.copyD2D(), yl));
      s.insertBefore(MIR_BinaryAcc.create(IA32_OR, th.copyD2D(), tl.copyD2U()));
      MIR_CondBranch.mutate(s, IA32_JCC, 
			    new OPT_IA32ConditionOperand(cond),
			    IfCmp.getTarget(s),
			    IfCmp.getBranchProfile(s));
    } else {
      // Do the naive thing and generate multiple compare/branch implementation.
      OPT_IA32ConditionOperand cond1;
      OPT_IA32ConditionOperand cond2;
      OPT_IA32ConditionOperand cond3;
      if (cond.isLESS()) {
	cond1 = OPT_IA32ConditionOperand.LT();
	cond2 = OPT_IA32ConditionOperand.GT();
	cond3 = OPT_IA32ConditionOperand.LLT();
      } else if (cond.isGREATER()) {
	cond1 = OPT_IA32ConditionOperand.GT();
	cond2 = OPT_IA32ConditionOperand.LT();
	cond3 = OPT_IA32ConditionOperand.LGT();
      } else if (cond.isLESS_EQUAL()) {
	cond1 = OPT_IA32ConditionOperand.LT();
	cond2 = OPT_IA32ConditionOperand.GT();
	cond3 = OPT_IA32ConditionOperand.LLE();
      } else if (cond.isGREATER_EQUAL()) {
	cond1 = OPT_IA32ConditionOperand.GT();
	cond2 = OPT_IA32ConditionOperand.LT();
	cond3 = OPT_IA32ConditionOperand.LGE();
      } else {
	// I don't think we use the unsigned compares for longs,
	// so defer actually implementing them until we find a test case. --dave
	cond1 = cond2 = cond3 = null;
	OPT_OptimizingCompilerException.TODO();
      }

      OPT_BasicBlock myBlock = s.getBasicBlock();
      OPT_BasicBlock test2Block = myBlock.createSubBlock(s.bcIndex, ir, 0.25f);
      OPT_BasicBlock falseBlock = myBlock.splitNodeAt(s, ir);
      OPT_BasicBlock trueBlock = IfCmp.getTarget(s).target.getBasicBlock();
      
      falseBlock.recomputeNormalOut(ir);
      myBlock.insertOut(test2Block);
      myBlock.insertOut(falseBlock);
      myBlock.insertOut(trueBlock);
      test2Block.insertOut(falseBlock);
      test2Block.insertOut(trueBlock);
      ir.cfg.linkInCodeOrder(myBlock, test2Block);
      ir.cfg.linkInCodeOrder(test2Block, falseBlock);
      
      s.remove();
      
      myBlock.appendInstruction(MIR_Compare.create(IA32_CMP, R(xh), yh));
      myBlock.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						       cond1, trueBlock.makeJumpTarget(), new OPT_BranchProfileOperand(),
						       cond2, falseBlock.makeJumpTarget(), new OPT_BranchProfileOperand()));
      test2Block.appendInstruction(MIR_Compare.create(IA32_CMP, R(xl), yl));
      test2Block.appendInstruction(MIR_CondBranch.create(IA32_JCC, cond3, trueBlock.makeJumpTarget(), new OPT_BranchProfileOperand()));
    }
  }


  // the fcmoi/fcmoip was generated by burs
  // we do the rest of the expansion here because in some
  // cases we must remove a trailing goto, and we 
  // can't do that in burs!
  private static OPT_Instruction fp_ifcmp(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Operator op = s.operator();
    OPT_BranchOperand testFailed;
    OPT_BasicBlock bb = s.getBasicBlock();
    OPT_Instruction lastInstr = bb.lastRealInstruction();
    if (lastInstr.operator() == IA32_JMP) {
      // We're in trouble if there is another instruction between s and lastInstr!
      if (VM.VerifyAssertions) VM.assert(s.nextInstructionInCodeOrder() == lastInstr);
      // Set testFailed to target of GOTO
      testFailed = MIR_Branch.getTarget(lastInstr);
      nextInstr = lastInstr.nextInstructionInCodeOrder();
      lastInstr.remove();
    } else {
      // Set testFailed to label of next (fallthrough basic block)
      testFailed = bb.nextBasicBlockInCodeOrder().makeJumpTarget();
    }

    OPT_ConditionOperand c = IfCmp.getCond(s);
    OPT_BranchOperand target = IfCmp.getTarget(s);
    boolean UeqL = (op == DOUBLE_IFCMPL) || (op == FLOAT_IFCMPL);
    boolean UeqG = (op == DOUBLE_IFCMPG) || (op == FLOAT_IFCMPG);
    OPT_BranchOperand unorderedTarget;
    if (c.value == OPT_ConditionOperand.EQUAL || 
	(UeqL && (c.value == OPT_ConditionOperand.GREATER || 
		  c.value == OPT_ConditionOperand.GREATER_EQUAL)) || 
	(UeqG && (c.value == OPT_ConditionOperand.LESS || 
		  c.value == OPT_ConditionOperand.LESS_EQUAL))) {
      unorderedTarget = (OPT_BranchOperand)testFailed.copy();
    } else {
      unorderedTarget = (OPT_BranchOperand)target.copy();
    }
    
    // IMPORTANT: FCOMI only sets 3 of the 6 bits in EFLAGS, so
    // we can't just translate the condition operand as if it 
    // were an integer compare.
    // FCMOI sets ZF, PF, and CF as follows: 
    // Compare Results      ZF     PF      CF
    // left > right          0      0       0
    // left < right          0      0       1
    // left == right         1      0       0
    // UNORDERED             1      1       1
    if (c.isEQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2, 
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.EQ(),  // ZF == 1
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, testFailed));
    } else if (c.isNOT_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.EQ(),  // ZF == 1
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else if (c.isLESS()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LLT(), // CF == 1
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, testFailed));
    } else if (c.isGREATER()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LGT(), // ZF == 0 and CF == 0
					    target,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP,testFailed));
    } else if (c.isLESS_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LGT(), // ZF == 0 and CF == 0
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else if (c.isGREATER_EQUAL()) {
      s.insertBefore(MIR_CondBranch2.create(IA32_JCC2,
					    OPT_IA32ConditionOperand.PE(),  // PF == 1
					    unorderedTarget,
					    new OPT_BranchProfileOperand(),
					    OPT_IA32ConditionOperand.LLT(), // CF == 1
					    testFailed,
					    new OPT_BranchProfileOperand()));
      s.insertBefore(MIR_Branch.create(IA32_JMP, target));
    } else {
      throw new OPT_OptimizingCompilerException("Unexpected fp compare operation" + c.toString());
    }
    s.remove();
    return nextInstr;
  }


  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static OPT_Instruction threeValueFPCmp (OPT_Instruction s, OPT_IR ir) {
    // IMPORTANT: FCOMI only sets 3 of the 6 bits in EFLAGS, so 
    // we can't quite just translate the condition operand as if it 
    // were an integer compare.
    // FCMOI sets ZF, PF, and CF as follows: 
    // Compare Results      ZF     PF      CF
    // left > right          0      0       0
    // left < right          0      0       1
    // left == right         1      0       0
    // UNORDERED             1      1       1
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand two = (OPT_RegisterOperand) Binary.getClearVal2(s);
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s.remove();
    OPT_BasicBlock oneBlock = BB1.createSubBlock(0, ir, .33f);
    OPT_BasicBlock zeroBlock = BB1.createSubBlock(0, ir, .34f);
    OPT_BasicBlock minusOneBlock = BB1.createSubBlock(0, ir, .33f);
    OPT_BasicBlock unorderedBlock = oneBlock;
    if ((s.operator == DOUBLE_CMPL) || (s.operator == FLOAT_CMPL)) {
      unorderedBlock = minusOneBlock;
    }

    OPT_Register FP0 = ir.regpool.getPhysicalRegisterSet().getFPR(0);
    BB1.appendInstruction(MIR_Move.create(IA32_FMOV, R(FP0), one));
    BB1.appendInstruction(MIR_Compare.create(IA32_FCOMI, R(FP0), two));
    BB1.appendInstruction(MIR_CondBranch2.create(IA32_JCC2,
						 OPT_IA32ConditionOperand.PE(),  // PF == 1	
						 unorderedBlock.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.01f),
						 OPT_IA32ConditionOperand.EQ(),  // ZF == 1
						 zeroBlock.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.33f)));
    BB1.appendInstruction(MIR_CondBranch.create(IA32_JCC,
						OPT_IA32ConditionOperand.LLT(), // CF == 1
						minusOneBlock.makeJumpTarget(),
						new OPT_BranchProfileOperand(0.33f)));
    
    oneBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(1)));
    oneBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));

    zeroBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(0)));
    zeroBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));

    minusOneBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(-1)));
    minusOneBlock.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));


    // fix CFG
    BB1.insertOut(oneBlock);
    BB1.insertOut(zeroBlock);
    BB1.insertOut(minusOneBlock);
    oneBlock.insertOut(BB6);
    zeroBlock.insertOut(BB6);
    minusOneBlock.insertOut(BB6);

    ir.cfg.linkInCodeOrder(BB1, oneBlock);
    ir.cfg.linkInCodeOrder(oneBlock, zeroBlock);
    ir.cfg.linkInCodeOrder(zeroBlock, minusOneBlock);
    ir.cfg.linkInCodeOrder(minusOneBlock, BB6);
    return nextInstr;
  }

  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static OPT_Instruction threeValueLongCmp (OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand lone = L(ir.regpool.getSecondReg(one.register));
    OPT_Operand two = Binary.getClearVal2(s);
    OPT_Operand ltwo;
    if (two instanceof OPT_RegisterOperand) {
      ltwo = L(ir.regpool.getSecondReg(((OPT_RegisterOperand)two).register));
    } else {
      OPT_LongConstantOperand tmp = (OPT_LongConstantOperand)two;
      two = I(tmp.upper32());
      ltwo = I(tmp.lower32());
    }
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s = s.remove();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB4 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB5 = BB1.createSubBlock(0, ir);
    BB1.appendInstruction(MIR_Compare.create(IA32_CMP, one, two));
    BB1.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						 OPT_IA32ConditionOperand.LT(),
						 BB4.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f),
						 OPT_IA32ConditionOperand.GT(),
						 BB5.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f)));
    BB2.appendInstruction(MIR_Compare.create(IA32_CMP, lone, ltwo));
    BB2.appendInstruction(MIR_CondBranch2.create(IA32_JCC2, 
						 OPT_IA32ConditionOperand.LLT(),
						 BB4.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f),
						 OPT_IA32ConditionOperand.LGT(),
						 BB5.makeJumpTarget(),
						 new OPT_BranchProfileOperand(0.49f)));
    BB3.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(0)));
    BB3.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));
    BB4.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(-1)));
    BB4.appendInstruction(MIR_Branch.create(IA32_JMP, BB6.makeJumpTarget()));
    BB5.appendInstruction(MIR_Move.create(IA32_MOV, R(res), I(1)));
    // fix CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB4);
    BB1.insertOut(BB5);
    BB2.insertOut(BB3);
    BB2.insertOut(BB4);
    BB2.insertOut(BB5);
    BB3.insertOut(BB6);
    BB4.insertOut(BB6);
    BB5.insertOut(BB6);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);
    ir.cfg.linkInCodeOrder(BB4, BB5);
    ir.cfg.linkInCodeOrder(BB5, BB6);
    return nextInstr;
  }


  /*
   * This routine expands a yield_point instruction.
   * Split the yield point's basic block just after the yield point instruction.
   * Create a new yield point basic block that jumps to the thread switch code
   *   and then jumps to the split basic block.
   * Before the yield point, test if thread switch flag is on.
   *  Mutate yield point to a conditional jump if true to yield point 
   *  basic block. 
   * If options.FIXED_JTOC, then we can delay the yieldpoint expansion
   * until final mir expansion, since we can expand it without impacting
   * register allocation.
   */
  private static OPT_Instruction yield_point(OPT_Instruction s, OPT_IR ir) {
    OPT_Instruction nextInstr = s.nextInstructionInCodeOrder();
    if (ir.options.FIXED_JTOC) return nextInstr; // defer expansion until later

    s.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(0)));
    
    // get the correct method to be called for a thread switch
    VM_Method meth = null;
    if (s.getOpcode() == YIELDPOINT_PROLOGUE_opcode) {
      meth = VM_Entrypoints.optThreadSwitchFromPrologueMethod;
    } else if (s.getOpcode() == YIELDPOINT_EPILOGUE_opcode) {
      meth = VM_Entrypoints.optThreadSwitchFromEpilogueMethod;
    } else { 
      meth = VM_Entrypoints.optThreadSwitchFromBackedgeMethod;
    }

    // split the basic block after the yieldpoint
    OPT_BasicBlock thisBlock = s.getBasicBlock();
    OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(s,ir);
    
    // create a basic block at the end of the IR to hold the yieldpoint   
    OPT_BasicBlock yieldpoint = thisBlock.createSubBlock(s.bcIndex, ir, .00001f);
    thisBlock.insertOut(yieldpoint);
    yieldpoint.insertOut(nextBlock);
    ir.cfg.addLastInCodeOrder(yieldpoint);
    
    int offset = meth.getOffset();
    OPT_Operand jtoc = 
      OPT_MemoryOperand.BD(R(ir.regpool.getPhysicalRegisterSet().getPR()),
			   VM_Entrypoints.jtocField.getOffset(), 
			   (byte)4, null, TG());
    OPT_RegisterOperand regOp = ir.regpool.makeTempInt();
    yieldpoint.appendInstruction(MIR_Move.create(IA32_MOV, regOp, jtoc));
    OPT_Operand target =
      OPT_MemoryOperand.BD(regOp.copyD2U(), offset, (byte)4, 
			   new OPT_LocationOperand(offset), TG());
    
    // call thread switch
    OPT_Instruction call = 
      MIR_Call.create0(CALL_SAVE_VOLATILE, null, null, target, 
		       OPT_MethodOperand.STATIC(meth));
    call.markAsNonPEI();
    call.copyPosition(s);
    yieldpoint.appendInstruction(call);
    yieldpoint.appendInstruction(MIR_Branch.create(IA32_JMP,
						   nextBlock.makeJumpTarget())); 
    
    // Check to see if threadSwitch requested
    OPT_Register PR = ir.regpool.getPhysicalRegisterSet().getPR();
    int tsr = VM_Entrypoints.threadSwitchRequestedField.getOffset();
    OPT_MemoryOperand M = OPT_MemoryOperand.BD(R(PR),tsr,(byte)4,null,null);
    OPT_Instruction compare = MIR_Compare.create(IA32_CMP, M, I(0));
    s.insertBefore(compare);
    MIR_CondBranch.mutate(s, IA32_JCC, OPT_IA32ConditionOperand.NE(),
			  yieldpoint.makeJumpTarget(),
			  OPT_BranchProfileOperand.unlikely());
    return nextInstr;
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * <ul>
 * <li> Convert instructions with 3-operand binary ALU operators to use 
 *      2-operand ALU operators. 
 * <li> Convert instructions with 2-operand unary ALU operators to use 
 *      1-operand ALU operators.
 * </ul>
 * 
 * <pre>
 * In the most general case, we must do the following:
 *
 *  op r100 = r200, r300      =====>    move r100 = r200
 *                                      op   r100 <-- r300
 *
 * but there are several easy cases where we can avoid the move
 * 
 *  op r100 = r100, r300      =====>    op   r100 <-- r300
 *                                          
 *  op r100 = r200, r100      =====>    op   r100 <-- r200
 *  (if op is commutative) 
 *
 * but, we must be careful in this case. If r100 spans a basic block,
 * then we are better doing the following (since it will break the
 * BURS expression tree _after_ op).
 *
 *  op r100 = r200, r100      =====>    move rtemp = r200
 *  (if op is non commutative)          op   rtemp <-- r100
 *                                      move r100  = rtemp 
 *
 * We also keep our eyes open for the special (somewhat common) case 
 * of one of the uses being the last use of a temporary.  When this happens
 * we can sometimes avoid inserting a move at all. When this happens, we 
 * rewrite:
 * 
 *  op r100 = r200, r300     =====>    op r200 <-- r300
 * and replace all uses of r100 with r200. 
 *
 * We aren't doing a full live analysis, but the following conditions
 * covers the cases where it is critical to get this right:
 *  (a) r100 is ssa
 *  (b) r100 does not span a basic block
 *  (c) r200 does not span a basic block
 *  (d) this instruction is the last use of r200
 * These conditions are designed to be cheap to verify and 
 * cover those cases where it is advantegous from BURS's perspective to
 * coalesce the registers to avoid the move instruction.
 * 
 * If we are in the following very similar case:
 *  op r100 = r200, r300     =====>      op r200 <-- r300
 *                                           move r100 = r200
 *  (1) r200 does not span a basic block
 *  (2) this instruction is the last use of r200
 * then we want the move instruction here (but after op), because 
 * merging registers r100 and r200 would force BURS to break its 
 * exprssion trep _before_ op since r200 would now span a basic block 
 * (since r100 spans a basic block).
 * We depend on the register allocator to later coalesce r100 and r200,
 * since they are not simultaneously live.
 * Ditto (5) and (6) on r300 if op is commutative and r200 doesn't work out.
 * 
 * </pre>
 * @author Dave Grove
 */
final class OPT_ConvertALUOperators extends OPT_CompilerPhase 
  implements OPT_Operators {

  private static final boolean OPTIMIZE = true;

  final String getName() { return "ConvertALUOps"; }
  final OPT_CompilerPhase newExecution(OPT_IR ir) { return this; }
  final boolean printingEnabled (OPT_Options options, boolean before) {
    return false;
  }

  final void perform(OPT_IR ir) { 
    // Calling OPT_Simplifier.simplify ensures that the instruction is 
    // in normalized form. This reduces the number of cases we have to 
    // worry about (and does last minute constant folding on the off 
    // chance we've missed an opportunity...)
    // BURS assumes that this has been done, so we must do it even if
    // OPTIMIZE is false.
    for (OPT_InstructionEnumeration instrs = ir.forwardInstrEnumerator();
	 instrs.hasMoreElements();) {
      OPT_Instruction s = instrs.next(); 
      OPT_Simplifier.simplify(s);
    }

    if (OPTIMIZE) {
      // Compute simple ssa, u/d chains, spansBasicBlock
      // to catch some additional cases where we don't have to insert moves
      OPT_DefUse.computeDU(ir);
      OPT_DefUse.recomputeSSA(ir);
      OPT_DefUse.recomputeSpansBasicBlock(ir);
      for (OPT_Register reg = ir.regpool.getFirstRegister(); 
	   reg != null; 
	   reg = reg.getNext()) {
	markDead(reg);
      }
    }

    // Reverse pass over instructions supports simple live analysis.
    for (OPT_Instruction next, s = ir.lastInstructionInCodeOrder(); 
	 s != null; 
	 s = next) {
      next = s.prevInstructionInCodeOrder();
      
      switch(s.getOpcode()) {
      case BOOLEAN_NOT_opcode: unary(s, BOOLEAN_NOT_ACC, ir); break;

      case INT_ADD_opcode: commutative(s, INT_ADD_ACC, ir); break;
      case INT_SUB_opcode: noncommutative(s, INT_SUB_ACC, ir); break;
      case INT_MUL_opcode: commutative(s, INT_MUL_ACC, ir); break;
      case INT_SHL_opcode: noncommutative(s, INT_SHL_ACC, ir); break;
      case INT_SHR_opcode: noncommutative(s, INT_SHR_ACC, ir); break;
      case INT_USHR_opcode: noncommutative(s, INT_USHR_ACC, ir); break;
      case INT_AND_opcode: commutative(s, INT_AND_ACC, ir); break;
      case INT_OR_opcode: commutative(s, INT_OR_ACC, ir); break;
      case INT_XOR_opcode: commutative(s, INT_XOR_ACC, ir); break;
      case INT_NEG_opcode: unary(s, INT_NEG_ACC, ir); break;
      case INT_NOT_opcode: unary(s, INT_NOT_ACC, ir); break;

      case LONG_ADD_opcode: commutative(s, LONG_ADD_ACC, ir); break;
      case LONG_SUB_opcode: noncommutative(s, LONG_SUB_ACC, ir); break;
      case LONG_MUL_opcode: commutative(s, LONG_MUL_ACC, ir); break;
      case LONG_SHL_opcode: noncommutative(s, LONG_SHL_ACC, ir); break;
      case LONG_SHR_opcode: noncommutative(s, LONG_SHR_ACC, ir); break;
      case LONG_USHR_opcode: noncommutative(s, LONG_USHR_ACC, ir); break;
      case LONG_AND_opcode: commutative(s, LONG_AND_ACC, ir); break;
      case LONG_OR_opcode: commutative(s, LONG_OR_ACC, ir); break;
      case LONG_XOR_opcode: commutative(s, LONG_XOR_ACC, ir); break;
      case LONG_NEG_opcode: unary(s, LONG_NEG_ACC, ir); break;
      case LONG_NOT_opcode: unary(s, LONG_NOT_ACC, ir); break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case FLOAT_ADD_opcode: s.operator = FP_ADD; break;
      case DOUBLE_ADD_opcode: s.operator = FP_ADD; break;
      case FLOAT_SUB_opcode: s.operator = FP_SUB; break;
      case DOUBLE_SUB_opcode: s.operator = FP_SUB; break;
      case FLOAT_MUL_opcode: s.operator = FP_MUL; break;
      case DOUBLE_MUL_opcode: s.operator = FP_MUL; break;
      case FLOAT_DIV_opcode: s.operator = FP_DIV; break;
      case DOUBLE_DIV_opcode: s.operator = FP_DIV; break; 
      case FLOAT_REM_opcode: s.operator = FP_REM; break;
      case DOUBLE_REM_opcode: s.operator = FP_REM; break;
      case FLOAT_NEG_opcode: s.operator = FP_NEG; break;
      case DOUBLE_NEG_opcode: s.operator = FP_NEG; break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case INT_COND_MOVE_opcode: s.operator = CMOV; break;
      case REF_COND_MOVE_opcode: s.operator = CMOV; break;
      case FLOAT_COND_MOVE_opcode: s.operator = CMOV; break;
      case DOUBLE_COND_MOVE_opcode: s.operator = CMOV; break;
      case LONG_COND_MOVE_opcode: OPT_OptimizingCompilerException.TODO(); break;
      case GUARD_COND_MOVE_opcode: OPT_OptimizingCompilerException.TODO(); break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case INT_2FLOAT_opcode: s.operator = INT_2FP; break;
      case INT_2DOUBLE_opcode: s.operator = INT_2FP; break;
      case LONG_2FLOAT_opcode: s.operator = LONG_2FP; break;
      case LONG_2DOUBLE_opcode: s.operator = LONG_2FP; break;

      // BURS doesn't really care, so consolidate to reduce rule space
      case REF_LOAD_opcode: s.operator = INT_LOAD; break;
      case REF_STORE_opcode: s.operator = INT_STORE; break;
      case REF_ALOAD_opcode: s.operator = INT_ALOAD; break;
      case REF_ASTORE_opcode: s.operator = INT_ASTORE; break;
      case REF_MOVE_opcode: s.operator = INT_MOVE; break;
      case REF_IFCMP_opcode: s.operator = INT_IFCMP; break;
      }

      if (OPTIMIZE) {
	// update liveness 
	for (OPT_OperandEnumeration defs = s.getPureDefs();
	     defs.hasMoreElements();) {
	  OPT_Operand op = defs.next();
	  if (op.isRegister()) {
	    markDead(op.asRegister().register);
	  }
	}
	for (OPT_OperandEnumeration uses = s.getUses(); // includes def/uses
	     uses.hasMoreElements();) {
	  OPT_Operand op = uses.next();
	  if (op.isRegister()) {
	    markLive(op.asRegister().register);
	  }
	}
      }
    }
  }

  private void commutative(OPT_Instruction s, OPT_Operator opCode, OPT_IR ir) {
    OPT_RegisterOperand result = Binary.getClearResult(s);
    OPT_Operand op1 = Binary.getClearVal1(s);
    OPT_Operand op2 = Binary.getClearVal2(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op2);
      return;
    }
    if (result.similar(op2)) {
      OPT_DefUse.removeUse(op2.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op1);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
      if (op2.isRegister()) {
	OPT_RegisterOperand rop2 = op2.asRegister();
	if (!rop2.register.spansBasicBlock() && isDead(rop2.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop2);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop2);
	    OPT_DefUse.mergeRegisters(ir, rop2.register, result.register);
	    rop2.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop2, op1);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop2);
	    OPT_DefUse.recordDefUse(rop2);
	    BinaryAcc.mutate(s, opCode, rop2, op1);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop2.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh, need some kind of move instruction
    OPT_Instruction move =   
      Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
    OPT_DefUse.updateDUForNewInstruction(move);
    s.insertBefore(move);
    OPT_DefUse.removeDef(result);
    OPT_DefUse.recordDefUse(result);
    if (op1.isRegister()) {
      OPT_DefUse.removeUse(op1.asRegister());
    }
    BinaryAcc.mutate(s, opCode, result, op2);
  }    

  private void noncommutative(OPT_Instruction s, OPT_Operator opCode, 
			      OPT_IR ir) {
    OPT_RegisterOperand result = Binary.getClearResult(s);
    OPT_Operand op1 = Binary.getClearVal1(s);
    OPT_Operand op2 = Binary.getClearVal2(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      BinaryAcc.mutate(s, opCode, result, op2);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    BinaryAcc.mutate(s, opCode, rop1, op2);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh need some move instructions after all.
    if (result.similar(op2)) {
      OPT_RegisterOperand tmp = ir.gc.temps.makeTemp(op1);
      OPT_Instruction move = 
	Move.create(getMoveOp(tmp.type), tmp.copyRO(), op1.copy());
      s.insertBefore(move);
      OPT_DefUse.updateDUForNewInstruction(move);
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(tmp);
      if (op1.isRegister()) {
	OPT_DefUse.removeUse(op1.asRegister());
      }
      BinaryAcc.mutate(s, opCode, tmp, op2);
      move = Move.create(getMoveOp(tmp.type), result.copyRO(), tmp.copyRO());
      s.insertAfter(move);
      OPT_DefUse.updateDUForNewInstruction(move);
    } else {
      OPT_Instruction move =   
	Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
      OPT_DefUse.updateDUForNewInstruction(move);
      s.insertBefore(move);
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      if (op1.isRegister()) {
	OPT_DefUse.removeUse(op1.asRegister());
      }
      BinaryAcc.mutate(s, opCode, result, op2);
    }
  }

  private void unary(OPT_Instruction s, OPT_Operator opCode, OPT_IR ir) {
    OPT_RegisterOperand result = Unary.getClearResult(s);
    OPT_Operand op1 = Unary.getClearVal(s);

    // Handle the easy cases of avoiding useless moves.
    if (result.similar(op1)) {
      OPT_DefUse.removeUse(op1.asRegister());
      OPT_DefUse.removeDef(result);
      OPT_DefUse.recordDefUse(result);
      UnaryAcc.mutate(s, opCode, result);
      return;
    }

    // attempt to detect additional cases using simple liveness and DU info
    if (OPTIMIZE) {
      if (op1.isRegister()) {
	OPT_RegisterOperand rop1 = op1.asRegister();
	if (!rop1.register.spansBasicBlock() && isDead(rop1.register)) {
	  if (result.register.isSSA() && !result.register.spansBasicBlock()) {
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.recordDefUse(rop1);
	    OPT_DefUse.mergeRegisters(ir, rop1.register, result.register);
	    rop1.register.putSSA(false);
	    UnaryAcc.mutate(s, opCode, rop1);
	    return;
	  } else {
	    OPT_DefUse.removeDef(result);
	    OPT_DefUse.removeUse(rop1);
	    OPT_DefUse.recordDefUse(rop1);
	    UnaryAcc.mutate(s, opCode, rop1);
	    OPT_Instruction move =   
	      Move.create(getMoveOp(result.type), result, rop1.copy());
	    OPT_DefUse.updateDUForNewInstruction(move);
	    s.insertAfter(move);
	    return;
	  }
	}
      }
    }

    // Sigh, need the move instruction before op.
    OPT_Instruction move =   
      Move.create(getMoveOp(result.type), result.copyRO(), op1.copy());
    OPT_DefUse.updateDUForNewInstruction(move);
    s.insertBefore(move);
    OPT_DefUse.removeDef(result);
    OPT_DefUse.recordDefUse(result);
    if (op1.isRegister()) {
      OPT_DefUse.removeUse(op1.asRegister());
    }
    UnaryAcc.mutate(s, opCode, result);
  }

  private static OPT_Operator getMoveOp(VM_Type t) {
    OPT_Operator op = OPT_IRTools.getMoveOp(t);
    if (op == REF_MOVE) { 
      return INT_MOVE;
    } else {
      return op;
    }
  }

  // Use the scratch field of the register to record 
  // dead/live for local live analysis.
  private static void markDead(OPT_Register r) {
    r.scratch = 0;
  }
  private static void markLive(OPT_Register r) {
    r.scratch = 1;
  }
  private static boolean isDead(OPT_Register r) {
    return r.scratch == 0;
  }
  private static boolean isLive(OPT_Register r) {
    return r.scratch == 1;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Normalize the use of constants in the LIR
 * to match the patterns supported in LIR2MIR.rules
 *
 * @author Dave Grove
 */
abstract class OPT_NormalizeConstants implements OPT_Operators {

  /**
   * Only thing we do for IA32 is to restrict the usage of 
   * String, Float, and Double constants.  The rules are prepared 
   * to deal with everything else.
   * 
   * @param ir IR to normalize
   */
  static void perform(OPT_IR ir) { 
    for (OPT_Instruction s = ir.firstInstructionInCodeOrder(); 
	 s != null; 
	 s = s.nextInstructionInCodeOrder()) {

      // Get 'large' constants into a form the the BURS rules are 
      // prepared to deal with.
      // Constants can't appear as defs, so only scan the uses.
      //
      int numUses = s.getNumberOfUses();
      if (numUses > 0) {
        int numDefs = s.getNumberOfDefs();
        for (int idx = numDefs; idx < numUses + numDefs; idx++) {
          OPT_Operand use = s.getOperand(idx);
          if (use != null) {
            if (use instanceof OPT_StringConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.JavaLangStringType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_StringConstantOperand sc = (OPT_StringConstantOperand)use;
              int offset = sc.index << 2;
              if (offset == 0)
                throw new OPT_OptimizingCompilerException("String constant w/o valid JTOC offset");
              OPT_LocationOperand loc = new OPT_LocationOperand(offset);
	      s.insertBefore(Load.create(INT_LOAD, rop, jtoc, new OPT_IntConstantOperand(offset), loc));
	      s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_DoubleConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.DoubleType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)use.copy();
              if (dc.index == 0) {
                dc.index = VM_Statics.findOrCreateDoubleLiteral(VM_Magic.doubleAsLongBits(dc.value));
              }
	      s.insertBefore(Binary.create(MATERIALIZE_FP_CONSTANT, rop, jtoc, dc));
              s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_FloatConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.FloatType);
	      OPT_Operand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)use.copy();
              if (fc.index  == 0) {
                fc.index = VM_Statics.findOrCreateFloatLiteral(VM_Magic.floatAsIntBits(fc.value));
              }
	      s.insertBefore(Binary.create(MATERIALIZE_FP_CONSTANT, rop, jtoc, fc));
              s.putOperand(idx, rop.copyD2U());
	    } else if (use instanceof OPT_NullConstantOperand) {
	      s.putOperand(idx, new OPT_IntConstantOperand(0));
	    }
	  }
        }
      }
    }
  }

  /**
   * IA32 supports 32 bit int immediates, so nothing to do.
   */
  static OPT_Operand asImmediateOrReg (OPT_Operand addr, 
				       OPT_Instruction s, 
				       OPT_IR ir) {
    return addr;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.*;
import java.util.*;
import java.lang.reflect.*;

/**
 *  <P> Generates the assembler that is used by the optimizing compiler,
 * using a combination of the tables describing the low-level
 * instruction formats and operators used by the opt compiler, and the
 * interface of the low-level assembler that understands how to
 * generate IA32 opcodes given specific operands.  Essentially, the
 * opt assembler becomes a rather large piece of impedence-matching
 * code that decodes the OPT_Instructions and OPT_Operators understood
 * by the opt compiler to determine what is the appropriate IA32
 * machine code to emit.  </P>
 *
 *  <P> In order for this to work, both the optimizing compiler tables and
 * the VM_Assembler must use stylized formats.  On the optimizing
 * com[piler side, the major stylization is that the low-level
 * operators that represent assembly code must correspond directly to
 * the official IA32 assembler pneumonics; i.e. since there is an ADD
 * assembler pneumonic in the Intel assembly specification, there must
 * be a correponding IA32_ADD operator in the opt compiler tables.
 * The stylization of the VM_Assembler side is more thoroughgoing, and
 * the reader is referred to the VM_Assembler header comments for a
 * definition. </P>
 *
 *  <P> Given these stylizations, GenerateAssembler reads the set of
 * assembler pneumonics supported by the VM_Assembler using reflection
 * to examinme its stylized method signitures.  GenerateAssembler also
 * reads the set of IA32 operators that the opt compiler defines,
 * using the helper classes OPT_InstructionFormatTable and
 * OPT_OperatorFormatTable.  It then, for each operator, generates a
 * handler method to call the appropriate VM_Assembler emit method
 * given an OPT_Instruction.  The VM_Assembler will have a family of
 * emit methods named for each opcode, each such emit method takes a 
 * specific set of operand addressing modes and sizes.  The handler
 * methods that the GenerateAssembler emits examine the operands to an
 * OPT_Instruction, and determine which VM_Assembler method to call
 * for the operand addressing modes and sizes that it finds.
 * GenerateAssembler also generates a top-level dispatch method that
 * examines the operator and calls the appropriate handler. </P>
 *
 *  <P> GenerateAssembler generates the opt assembler as part of the
 * normal build process; this poses a slight problem in that it needs
 * to examine the VM_Assembler via reflection to generate the
 * OPT_Assembler, but that is not possible until the VM sources
 * (including, of course, the OPT_Assembler) have been compiled.  The
 * current hack to get around this is to compile the VM_Assembler in
 * advance, and read the resulting class file.  This utilizies some
 * supporting files to make the VM_Assembler compile in isolation.
 * This is the purpose of the .fake files in the optimizing compiler's
 * assembler directory. </P>
 *
 * @see OPT_InstructionFormatTables
 * @see OPT_OperatorFormatTables
 * @see OPT_AssemblerBase
 * @see OPT_Instruction
 * @see OPT_Assembler
 * @see VM_Assembler
 *
 * @author Julian Dolby 
 */
public class GenerateAssembler {

    /** Global flag controlling printing of debugging information */
    static final boolean DEBUG = false;

    /** Global reference to the assembler being generated */
    static FileWriter out;

    /**
     * Write a single string to the assembler source file.
     * @param String s  The string to be written
     */
    private static void emit(String s) {
	try {
	    out.write(s, 0, s.length());
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit(-1);
	}
    }

    /**
     * Write tabification to the assembler source file.  This is used 
     * to make the generates source more readable by identing it.
     * @param int level  The level of indentation to generate
     */
    private static void emitTab(int level) {
	for(int i = 0; i < level; i++) emit("  ");
    }

    /**
     *  Global reference to the OPT_InstructionFormatTables class that 
     * contains descriptions of each optimizing compiler instruction
     * format that sis visible to the assembler (i.e. the MIR_* 
     * instruction formats.
     *
     * @see OPT_InstructionFormatTables
     */
    private static Class formats;

    /**
     *  Load the instruction format table, and throw up if that is
     * not possible.
     */
    static {
	try {
	    formats = Class.forName("OPT_InstructionFormatTables");
	} catch (ClassNotFoundException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }

    /**
     *  Global reference to the opcode argument table for the current
     * opcode being processed.  This table is null unless some of the
     * operands in the OPT_Instruction are to ignored when generating
     * code for the opcode.  Ignoring arguments is an ad-hock special
     * case that is controlled by the global opcodeArgTable.
     */
    static int[] currentOpcodeArgTable;

    /**
     *  Global reference to the table of symbolic names of the arguments
     * to the current MIR_ instruction format.  This information is read
     * from the OPT_InstructionFormatTables
     */
    static String[] currentOpcodeSymbolicNames;

    /**
     *  The current IA32 opcode being processed.  This is the name of
     * IA32 instruction.  Typically, it is the name of the opt compiler
     * IA32_* opcode as well, but there are exceptions in that multiple
     * IA32_* opcodes can map to the same IA32 instruction
     */
    static String currentOpcode;

    /**
     *  The instruction format for the IA32_* opt compiler opcode(s)
     * being processed. 
     *
     */
    static String currentFormat;

    /**
     *  Global table mapping opt compiler IA32_* opcodes to arrays
     * listing the set of OPT_Instruction operands that are to be used
     * as arguments to the IA32 architecture instruction.  This is used
     * when an instruction has extra operands that are not used in
     * assembly (e.g. CALL) has mappings only for such instructions.
     */
    static Hashtable opcodeArgTables;

    /**
     *  Initialize the opcodeArgTables table
     */
    static {
	opcodeArgTables = new Hashtable();
	opcodeArgTables.put("CALL", new int[]{2});
	opcodeArgTables.put("INT", new int[]{1});
	opcodeArgTables.put("CDQ", new int[]{0});
	opcodeArgTables.put("DIV", new int[]{1,2});
	opcodeArgTables.put("IDIV", new int[]{1,2});
	opcodeArgTables.put("MUL", new int[]{1,2});
	opcodeArgTables.put("IMUL1", new int[]{1,2});
	opcodeArgTables.put("DIV", new int[]{1,2});
	opcodeArgTables.put("IDIV", new int[]{1,2});
	opcodeArgTables.put("SET", new int[]{1,0});
	opcodeArgTables.put("CMPXCHG", new int[]{1,2});
	opcodeArgTables.put("FCMOV", new int[]{2,0,1});
	opcodeArgTables.put("CMOV", new int[]{2,0,1});
    }

    /**
     *  Set the current opcode.  This sets four global fields:
     * the currentOpcode, the currentOpcodeArgTable, the currentFormat
     * and the currentOpcodeSymbolicNames.
     *
     * @param opcode  The IA32 architecture opcode to make the current opcode
     */
    static void setCurrentOpcode(String opcode) {
	try {
	    currentOpcode = opcode;
	    currentOpcodeArgTable = (int[]) opcodeArgTables.get( opcode );
	    currentFormat = OPT_OperatorFormatTables.getFormat( opcode );
	    Field f = formats.getDeclaredField(currentFormat+"ParameterNames");
	    currentOpcodeSymbolicNames = (String[]) f.get( null );
	} catch (Throwable e) {
	    System.err.println("Cannot handle VM_Assembler opcode " + opcode);
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }

    /**
     * Constant representing immediate arguments to VM_Assembler calls
     */
    static final int Immediate = 0;
    /**
     * Constant representing register arguments to VM_Assembler calls.
     * This covers the cases when a register is encoded into the mod/rm
     * byte; the VM_Assembler handles the detais of generating either 
     * the reg bits of the mod/rm byte or encoding a register as mod 11.
     */
    static final int Register = 1;
    /**
     * Constant representing condition arguments to VM_Assembler calls.
     * Such operands are not arguments to the ultimate IA32 machine 
     * code instruction, but they are used to calculate the opcode that
     * is generated.
     */
    static final int Condition = 2;
    /**
     * Constant representing arguments to VM_Assembler calls that use the
     * scaled-index-base (SIB) addressing mode in the special way that uses
     * neither a base not an index to generate an absolute address
     */
    static final int Absolute = 3;
    /**
     * Constant representing IA32 memory operands that use register-
     * displacement addressing mode (usually mod bits 01 and 10) arguments 
     * to VM_Assembler calls.  The VM_Assembler takes care of choosing the
     * right mode for the size of the displacement, so this one mode
     * covers two of the four addressing modes the IA32 has.  The
     * VM_Assembler also handles the special cases in which this mode
     * requires weird SIB bytes.
     */
    static final int RegisterDisplacement = 4;
    /**
     * Constant representing arguments to VM_Assembler calls that use the
     * scaled-index-base (SIB) addressing mode in the special way that does
     * not use a base register.  The OPT_Assembler simply assumes it has
     * an [index < < scale + disp] addressing mode, and the VM_Assembler takes
     * care of generating the special mod/rm that causes the base register
     * to be ignored.
     */
    static final int RegisterOffset = 5;
    /**
     * Constant representing scaled-index-base (SIB) mode arguments to 
     * VM_Assembler calls.
     */
    static final int RegisterIndexed = 6;
    /**
     * Constant representing register-indirect arguments to VM_Assembler 
     * calls.  This mode handles what is (usually) mod 00 in the mod/rm
     * byte.
     */
    static final int RegisterIndirect = 7;
    /**
     * Constant representing labels used as branch targets.  While code
     * is being generated, the machine code offset for a forward branch
     * cannot, in general, be computed as the target code has not been
     * generated yet.  The OPT_Assembler uses synthetic code offsets,
     * based upon the order of OPT_Instructions in the code being 
     * compiled, to communicate forward branch targets to the 
     * VM_Assembler.  These synthetic offsets are passed to the
     * VM_Assembler where it expected Label arguments.
     */
    static final int Label = 8;
    /**
     * Constant representing arguments to VM_Assembler calls in which
     * it may be either a backward branch target (resolved to an
     * immediate being the exact branch displacement) or a forward
     * branch (which will be a synthetic Label).
     */
    static final int LabelOrImmediate = 9;

    /**
     * How many different sizes of instruction operand are there, not
     * counting the standard double word.
     */
    static final int SIZES = 3;
    /**
     * Constant representing instructions that operate upon bytes
     */
    static final int Byte = 10;
    /**
     * Constant representing instructions that operate upon words (16 bits)
     */
    static final int Word = 11;
    /**
     * Constant representing instructions that operate upon quad words (64 bits)
     */
    static final int Quad = 12;

    /**
     *  This array denotes all possible encodings in a VM_Assembler emitter
     * function.  It includes all possible operand types and all possible
     * instruction sizes.  For all of the constants corresponding to a 
     * possible operand type or instruction size, the corresponding entry
     * is this table holds the string that the VM_Assembler uses to denote
     * that operand type or instruction size.
     *
     * This table is used when parsing a VM_Assembler emitter name to create 
     * a descriptor that denotes the operand size and types of the given
     * emitter in terms of the constants.
     *
     * This table is also used when generating the OPT_Assembler emitter
     * functions to allow the generator to pick which queries to use to
     * dispatch an OPT_Instruction to the appropriate VM_Assembler emitter.
     */
    static final String[] encoding = 
    {"Imm",		// encoding[Immediate]
     "Reg",		// encoding[Register]
     "Cond",		// encoding[Condition]
     "Abs",		// encoding[Absolute]
     "RegDisp",		// encoding[RegisterDisplacement]
     "RegOff",		// encoding[RegisterOffset]
     "RegIdx",		// encoding[RegisterIndexed]
     "RegInd",		// encoding[RegisterIndirect]
     "Label",		// encoding[Label]
     "ImmOrLabel",	// encoding[LabelOrImmediate]
     "Byte",
     "Word",
     "Quad"};

    /**
     * For a given string representing a valid operand encoding for the 
     * VM_Assembler, return the corresponding OPT_Assembler constant.  This
     * function only looks for encodings of operand types, and will not
     * accept strings that correspond to size encodings.
     *
     * @param str A valid VM_Assembler encoding of operand type
     * @return The OPT_Assembler constant corresponding to str, or -1 if none
     *
     */
    private static int getEncoding(String str) {
	for(int i = 0; i < encoding.length - SIZES; i++)
	    if (encoding[i].equals(str))
		return i;

	return -1;
    }

    /**
     * For a given string representing a valid size encoding for the 
     * VM_Assembler, return the corresponding OPT_Assembler constant.  This
     * function only looks for encodings of sizes, and will not accept 
     * strings that correspond to operand types.
     *
     * @param str A valid VM_Assembler encoding of operand size
     * @return The OPT_Assembler constant corresponding to str, or -1 if none
     *
     */
    private static int getSize(String str) {
	for(int i = encoding.length - SIZES; i < encoding.length; i++)
	    if (encoding[i].equals(str))
		return i;

	return -1;
    }

    /**
     * For a given operand number, return a string which is a valid Java
     * expression for reading that operand out of the current instruction.
     * This function uses the currentOpcodSymbolicNames table to determine
     * the appropriate accessor (e.g. getValue if the current name is Value),
     * and it uses the currentOpcodeArgTable (in cases where it has an
     * entry for the kind of instruction being processed) to determine which
     * operand in OPT_Instruction corresponds to operand sought.
     *
     * @param op  The operand number sought.
     * @return A Java expression for adcessing the requested operand.
     */
    private static String getOperand(int op) {
	try {
	    if (currentOpcodeArgTable == null)
		return currentFormat + ".get" + currentOpcodeSymbolicNames[op] + "(inst)";
	    else
		return currentFormat + ".get" + currentOpcodeSymbolicNames[currentOpcodeArgTable[op]] + "(inst)";
	} catch (ArrayIndexOutOfBoundsException e) {
	    System.err.println(currentOpcode + ": cannot access operand " + op  + ":");
	    for(int i = 0; i < currentOpcodeSymbolicNames.length; i++)
		System.err.println( currentOpcodeSymbolicNames[i] );
	    System.exit( -1 );
	    return null;
	}
    }

    /**
     * Given an operand number and an encoding, generate a test to
     * determine whether the given operand matches the encoding.  That
     * is, generate code to the OPT_Assembler that examines a given operand
     * of the current OPT_Instruction, and determines whether it is of
     * the type encoded by the given encoding.  This is used to generate the
     * if statements of the dispatch functions for each opt compiler opcode.
     *
     * @param argNumber The argument to examine
     * @param argEncoding The encoding for which to check 
     */
    private static void emitTest(int argNumber, int argEncoding) {   
	if (argEncoding < encoding.length - SIZES)
	    emit("is" + encoding[argEncoding] + "(" + getOperand(argNumber) + ")");
	else
	    emit("is" + encoding[argEncoding] + "(inst)");
    }

    /**
     * Generate code to verify that a given operand matches a given encoding.
     * Since the IA32 architecture is not exactly orthogonal (please note
     * the charitable understatement), there are cases when the opt assembler
     * can determine the VM_Assembler emitter to call without looking at
     * all (or, in some cases, any) of the arguments of the OPT_Instruction.
     * An example is the ENTER instruction that only takes one immediate
     * parameter, so the opt assembler could simply call that VM_Assembler
     * emiiter without checking that argument is really an immediate. In 
     * such cases, the opt assembler generates guarded tests that verify 
     * that OPT_Instruction operand actually matches the required encoding.
     * This function emits such tests to the assembler being generated.
     *
     * @param argNumber The argument to examine
     * @param argEncoding The encoding for which to check
     * @param level current level for generating pretty, tabified output
     */
    private static void emitVerify(int argNumber, int argEncoding, int level) {   
	emitTab(level);
	emit("if (VM.VerifyAssertions && !");
	emitTest(argNumber, argEncoding);
	emit(") VM.assert(false, inst.toString());\n");
    }

    /**
     * Generate code to fetch all the arguments needed for a given operand
     * number and encoding.  The different argument encodings of the
     * VM_Assembler need different arguments to be passed to the emitter
     * function.  For instance, a register-displacement mode operand
     * needs to be given a base register and an immediate displacement.
     * This function generates the appropriate arguments given the
     * operand number and encoding; that is, it generates reads of the
     * appropriate OPT_Instruction argument and fetches of the appropriate
     * pieces of information from the operand.
     * 
     * @param argNumber The argument being generated.
     * @param argEcoding The encoding to use.
     */
    private static void emitArgs(int argNumber, int argEncoding) {
	String op = getOperand(argNumber);
	if (argEncoding == LabelOrImmediate)
	    emit("getImm(" + op + "), getLabel(" + op + ")");
    	else if (argEncoding == RegisterDisplacement)
	    emit("getBase(" + op + "), getDisp(" + op + ")");
	else if (argEncoding == Absolute)
	    emit("getDisp(" + op + ")");
	else if (argEncoding == RegisterOffset)
	    emit("getIndex(" + op + "), getScale(" + op + 
		 "), getDisp(" + op + ")");
	else if (argEncoding == RegisterIndexed)
	    emit("getBase(" + op + "), getIndex(" + op + 
		 "), getScale(" + op + "), getDisp(" + op + ")");
	else if (argEncoding == RegisterIndirect)
	    emit("getBase(" + op + ")");
	else 
	    emit("get" + encoding[argEncoding] + "(" + op + ")");
    }

    /**
     *  This exception class is used to indicate that GenerateAssembler
     * found an emit* method in the vM_Assembler that it does not 
     * understand. To generate the OPT_Assembler for a given 
     * IA32 OPT_Operator, GenerateAssembler looks at all of the emit* 
     * methods for the corresponding IA32 opcode in the VM_Assembler.  It 
     * parses each name to determine what kinds of operands it expects and
     * what size operands it uses; this requires the emit* methods to
     * have stylized names (see the header comment of VM_Assembler for 
     * details).  If an emit* method name does not have the stylized 
     * format required, GenerateAssembler will throw a BadEmitMethod
     * exception and abort.
     */
    static class BadEmitMethod extends RuntimeException {

	/**
	 *  Create a BadEmitMethod exception indicating that 
	 * GenerateAssembler cannot understand the code portion
	 * of the method name methodName.
	 *
	 * @param methodName The method name causing trouble
	 * @param code The portion of methodName that does not parse
	 */
	BadEmitMethod(String methodName, String code) {
	    super("cannot interpret method " + methodName + "(" + code + ")");
	}

    }

    /**
     *  An EmitterDescriptor represents a single emit method from the
     * VM_Assembler: it explicitly represents the types of operands the
     * method expects, their number, and the size of the data it uses.
     * When GenerateAssembler encounters an emit* method from the 
     * VM_Assembler, it creates an EmitterDescriptor for it.  Based upon 
     * the stlyized form the method name is required to have, the
     * EmitterDexcriptor represents information about its arguments. This 
     * information is stored in terms of the GenerateAssembler constants 
     * that represent operand type and size.
     * <P>
     * The EmitterDescriptor class encapsulates the logic for parsing the 
     * stylized emit* method names that the VM_Assembler has, and turning
     * them into the explicit representation that GenerateAssembler uses.  
     * If parsing a name fails, a {@link GenerateAssembler.BadEmitMethod} 
     * runtime exception is thrown and assembler generation is aborted.
     * <P>
     * <HR>
     * <EM>See the descriptions of the GenerateAssembler constants:</EM>
     * <DL>
     * <DT> <EM>Operand types</EM>
     * <DI> 
     *  <UL>
     *   <LI> {@link #Immediate}
     *   <LI> {@link #Label}
     *   <LI> {@link #LabelOrImmediate}
     *   <LI> {@link #Absolute}
     *   <LI> {@link #Register}
     *   <LI> {@link #RegisterIndirect}
     *   <LI> {@link #RegisterOffset}
     *   <LI> {@link #RegisterIndexed}
     *  </UL>
     * <DT> <EM>Data size</EM>
     *  <UL>
     *   <LI> {@link #Byte}
     *   <LI> {@link #Word}
     *   <LI> {@link #Quad}
     *  </UL>
     * </DL>
     */
    static class EmitterDescriptor {
	private int size;
	private int count;
	private final int args[];

	/**
	 * Create an EmitterDescriptor for the given methodName.  This 
	 * conmstructor creates a descriptor that represents explicitly 
	 * the types and size of the operands of the given emit* method.
	 * This constructor encapsulate the logic to parse the given
	 * method name into the appropriate explicit representation.
	 */
	EmitterDescriptor(String methodName) {
	    StringTokenizer toks = new StringTokenizer(methodName, "_");
	    toks.nextElement(); // first element is emitXXX;
	    args = new int[ toks.countTokens() ];
	    this.size = 0;
	    this.count = 0;
	    for(int i = 0; i < args.length; i++) {
		String cs = toks.nextToken();
		int code = getEncoding(cs);
		int size = GenerateAssembler.getSize(cs);

		if (DEBUG) {
		    System.err.println(methodName + "[" + i + "] is " + code + "," + size + " for " + cs);
		}

		if (code != -1)
		    args[count++] = code;
		else if (size != -1)
		    this.size = size;
		else
		    throw new BadEmitMethod( methodName, cs );
	    }
	}

	/**
	 *  This method checks whether the emit* method represented by
	 * this EmitterDescriptor expects the argument type represented
	 * by enc as its argument'th operand.  If enc is an operand type
	 * encoding, this method checks wether the given argument is of
	 * the appropriate type.  If enc is an operand size encoding,
	 * the argument parameter is ignored, and this method checks
	 * whether the emit* method represented operates upon data of
	 * the desired size.
	 * <P>
	 * <EM>See the descriptions of the GenerateAssembler constants:</EM>
	 * <DL>
	 * <DT> <EM>Operand types</EM>
	 * <DI> 
	 *  <UL>
	 *   <LI> {@link #Immediate}
	 *   <LI> {@link #Label}
	 *   <LI> {@link #LabelOrImmediate}
	 *   <LI> {@link #Absolute}
	 *   <LI> {@link #Register}
	 *   <LI> {@link #RegisterIndirect}
	 *   <LI> {@link #RegisterOffset}
	 *   <LI> {@link #RegisterIndexed}
	 *  </UL>
	 * <DT> <EM>Data size</EM>
	 *  <UL>
	 *   <LI> {@link #Byte}
	 *   <LI> {@link #Word}
	 *   <LI> {@link #Quad}
	 *  </UL>
	 * </DL>
	 * <P>
	 * @param argument The operand number examined 
	 * @param enc The argument type queried, as encoded as one of
	 *    the operand type constants used throughout 
	 *    GenerateAssembler.
	 *
	 * @return True if this method expects an argument type encoded
	 *    by enc as its argument'th operand, and false otherwise.
	 */
	boolean argMatchesEncoding(int argument, int enc) {
	    if (enc < encoding.length - SIZES)
		return (count > argument) && args[argument] == enc;
	    else
		return size == enc;
	}

	/**
	 * Access the array that stores the encodings of the arguments
	 * to the emit method represented by this EmitterDescriptor.
	 *
	 * @return the array of argument encodings
	 */
	int[] getArgs() { return args; }

	/**
	 * Access the data size operated upon by emit method represented 
	 * by this EmitterDescriptor.
	 *
	 * @return data size for this descriptor
	 */
	int getSize() { return size; }

	/**
	 * Access the number of operands operated upon by emit method 
	 * represented by this EmitterDescriptor.
	 *
	 * @return number of operands for this descriptor
	 */
	int getCount() { return count; }

	public String toString() {
	    StringBuffer s = new StringBuffer();
	    s.append ("ed:");
	    for(int i = 0; i < count; i++)
		s.append(" " + encoding[args[i]]);
	    if (size != 0) s.append(" (" + encoding[size] + ")");
	    return s.toString();
	}
    }

    /**
     *  An EmitterSet represents a set of emit methods from the
     * VM_Assembler for the same IA32 assembler opcode.  These sets
     * are used when generating the do<opcode> method for a given IA32
     * opcde: first an EmitterSet of all the VM_Assembler emit methods
     * for that opcode is built, and then the do method is recursively
     * generated by emitting operand type and size tests that
     * partition the set of emitters into two smaller sets.  This
     * continues until the set is a singleton
     */
    static class EmitterSet {

	/**
	 *  The VM_Assembler emit methods that this set represents.
	 * This is a set of EmitterDescriptor objects.
	 */
	private final Set emitters = new HashSet();

	/**
	 * Print this EmitterSet readably.
	 * @return a string describing this EmitterSet
	 */
	public String toString() {
	    StringBuffer s = new StringBuffer();
	    s.append("Emitter Set of:\n");
	    Iterator i = emitters.iterator();
	    while (i.hasNext()) 
		s.append(i.next().toString() + "\n");
	    
	    s.append("-------------\n");
	    return s.toString();
	}

	/**
	 *  Test whethe rthis EmitterSet as exactly one element.
	 * @return true if this EmitterSet as exactly one element.
	 */
	boolean isSingleton() {
	    return  (emitters.size() == 1);
	}

	/**
	 *  Insert an EmitterDescriptor into this set
	 * @param ed the EmitterDescriptor to insert
	 */
	void add(EmitterDescriptor ed) {
	    emitters.add( ed );
	}

	/**
	 *  Count how many of the emit represented by this set match a
	 * given operand type and size encoding.  This method is used
	 * (via getEncodingSplit) while recursively partitioning a
	 * given EmitterSet to determine how evenly (or even whether)
	 * a given operand type and size splits this set.
	 *
	 * @see #getEncodingSplit
	 *
	 * @param n the operand being examined
	 * @param code the operand type or size code being considered
	 * @return the number of emit methods of which the specified
	 *         operand type matches the specified one.  */
	private int countEncoding(int n, int code) {
	    Iterator i = emitters.iterator();
	    int count = 0;
	    while (i.hasNext())
		if (((EmitterDescriptor)i.next()).argMatchesEncoding(n, code))
		    count++;
	    return count;
	}

	/**
	 *  Return the difference between the number of emit methods
	 * in this set that match a given operand type and size for a
	 * given operand, and the number of those that do not. This
	 * method is used while recursively partitioning a given
	 * EmitterSet to determine how evenly (or even whether) a
	 * given operand type and size splits this set.
	 *
	 * @param n the operand being examined
	 * @param code the operand type or size code being considered
	 * @return the different between matching and non-matching
	 *         emit method in this set.  */
	private int getEncodingSplit(int n, int code) {
	    int count = countEncoding(n, code);
	    return Math.abs( (emitters.size() - count) - count );
	}

	/**
	 * This class is used just to communicate the two results of
	 * searching for the best split for a given set: the chosen
	 * operand type or size, and the chosen operand nummber.  This
	 * class is basically to avoid writing the slew of required
	 * type casts that a generic pair would need given Java's
	 * primitive type system.
	 *
	 * @see #makeSplit
	 * @see #split
	 */
	static class SplitRecord {
	    /**
	     * The operand number to be split.
	     */
	    int argument;

	    /**
	     * The operand type or size test on which to split.
	     */
	    int test;

	    /**
	     * Make s split record to communicate the results of
	     * searching for the best operand to split.
	     *
	     * argument The operand number to be split.
	     * test The operand type or size test on which to split.
	     */
	    SplitRecord(int argument, int test) {
		this.argument = argument;
		this.test = test;
	    }
	}

	/**
	 * This method uses a SplitRecord as the criertion to
	 * partition the given EmitterSet into two subsets.
	 *
	 * @param split the plit record dicatating how to split
	 */
	private EmitterSet[] makeSplit(SplitRecord split) {
	    int arg = split.argument;
	    int test = split.test;
	    EmitterSet yes = new EmitterSet();
	    EmitterSet no = new EmitterSet();
	    Iterator i = emitters.iterator();
	    while (i.hasNext()) {
		EmitterDescriptor ed = (EmitterDescriptor) i.next();
		if (ed.argMatchesEncoding(arg, test))
		    yes.add( ed );
		else
		    no.add( ed );
	    }

	    return new EmitterSet[]{yes, no};
	}

	/**
	 *  Find the best operand type or size and operand number to
	 * partition this EmitterSet.  This method searches across all
	 * possible ways of splitting this set--all possible operand
	 * types and sizes, and all possible operands--to determine
	 * which one splits the set most evenly.  
	 *
	 * @return a SplitRecord representing the most-even split
	 */
	SplitRecord split() {
	    int splitArg = -1;
	    int splitTest = -1;
	    int splitDiff = 1000;
	    for(int arg = 0; arg < 4; arg++) {
		for (int test = 0; test < encoding.length; test++) {
		    int c = getEncodingSplit(arg, test);
		    if (c == 0)
			return new SplitRecord(arg, test);
		    else if (c < splitDiff) {
			splitArg = arg;
			splitTest = test;
			splitDiff = c;
		    }
		}
	    }

	    return new SplitRecord(splitArg, splitTest);
	}

	/**
	 *  Emit the Java code to call a particular emit method for a
	 * particular opcode.  This method takes representations of
	 * the opcode and operands of a given emit method, and
	 * generates the appropriate Java source code to call it.  It
	 * synthesizes the encoded emit method name, and uses emitArgs
	 * to pass all the required arguments.
	 *
	 * @see #emitArgs
	 *
	 * @param opcode the IA32 opcode of the emit method
	 * @param args the encoding of each operand to the emit method
	 * @param count the number of operands
	 * @param level the level of tabbing for pretty output
	 */
	private void emitEmitCall(String opcode, int[] args, int count, int level, int size) {
	    emitTab(level);
	    emit("emit" + opcode);
	    for(int i = 0; i < count; i++)
		emit("_" + encoding[args[i]]);
	    if (size != 0) emit("_" + encoding[size]);

	    if (count == 0)
		emit("();\n");
	    else {
		emit("(");
		for(int i = 0; i < count; i++) {
		    emit("\n");
		    emitTab(level+1);
		    emitArgs(i, args[i]);
		    if (i == count-1)
			emit(");\n");
		    else
			emit(",");
		}
	    }
	}

	/**
	 *  Write the Java code required for error checking and
	 * calling the emit method represented by a singleton
	 * EmitterSet.  A singleton EmiiterSet will typically be the
	 * result of a series of splits of bigger sets, where the
	 * splits represent emitted queries of operand types and
	 * sizes.  (See emitSet) However, there may be cases when some
	 * operand has only one possible options, so the splitting
	 * will not have generated any tests for it.  In this case, we
	 * will emit assertions that guarantee the operand is of the
	 * expected type.  Note that the answers to queries alrrready
	 * performed by splitting are known to be fine, so no
	 * additional error checking is needed for cases they cover.
	 *
	 * @see #emitSet
	 *
	 * @param opcode the IA32 opcode to generate
	 * @param testsPerformed the set of queries already performed
	 *        by splitting.  
	 * @param level level of indentation for prett printing */
	private void emitSingleton(String opcode, boolean[][] testsPerformed, int level) {
	    EmitterDescriptor ed = 
		(EmitterDescriptor) emitters.iterator().next();

	    int[] args = ed.getArgs();
	    int count = ed.getCount();
	    for(int i = 0; i < count; i++) 
		if (! testsPerformed[i][args[i]])
		    emitVerify(i, args[i], level);

	    int size = ed.getSize();
	    if (size != 0) {
		boolean needed = true;

		for(int i = 0; i < count; i++) 
		    if (testsPerformed[i][size])
			needed = false;
		    
		if (needed)
		    emitVerify(0, size, level);

		if (size == Byte)
		    for(int i = 0; i < count; i++) 
			if (args[i] == Register)
			    if (currentOpcode.indexOf("MOVZX") == -1 &&
				currentOpcode.indexOf("MOVSX") == -1)
			    {
				emitTab(level);
				emit("if (VM.VerifyAssertions && !(");
				emitArgs(i, Register);
				emit(" < 4)) VM.assert(false, inst.toString());\n");
			    }
		
	    }

	    emitEmitCall(opcode, args, count, level, ed.getSize());
	}

	/**
	 *  Emit Java code for deciding which emit method in the given
	 * set applies to an OPT_Instruction, and then calling the
	 * apprpriate method.  The method essentially works by
	 * recursively parititioning the given set into two smaller
	 * pieces until it finds a set with only one element.  On each
	 * partition, this method generates code for the appropriate
	 * operand type or size query, and then calls itself
	 * recursively on the two sets resulting from the partition.
	 *
	 * This method uses split to determine what test to apply, and
	 * emitSingleton when it encounteres a singleton set.
	 *
	 * Note that the testsPerformed parameter is not needed to do
	 * the recursive splitting; this is passed to emitSingleton to
	 * help it generate appropriate error checking for operands.
	 *
	 * @see #split
	 * @see #emitSingleton
	 *
	 * @param opcode the IA32 opcode being generated
	 * @param testsPerformed the set of tests already performed
	 * @param level the indentation level for pretty printing
	 *
	 */
	private void emitSet(String opcode, boolean[][] testsPerformed, int level) {
	    if (emitters.isEmpty()) {
		// do nothing
	    } else if (isSingleton())
		emitSingleton(opcode, testsPerformed, level);
	    else {
		SplitRecord rec = split();

		if (DEBUG) {
		    for(int i = 0; i < level; i++) System.err.print("  ");
		    System.err.println("split of " + opcode + "[" + rec.argument + "] for " + encoding[rec.test]);
		}

		if (testsPerformed[rec.argument][rec.test] == true) {
		    System.err.println("repeated split of " + opcode + "[" + rec.argument + "] for " + encoding[rec.test]);
		    System.err.println( this );
		    System.exit( -1 );
		}

		testsPerformed[rec.argument][rec.test] = true;
		EmitterSet[] splits = makeSplit(rec);
		emitTab(level);	emit("if (");
		emitTest( rec.argument, rec.test );
		emit(") {\n");
		splits[0].emitSet(opcode, testsPerformed, level+1);
		emit("\n"); emitTab(level); emit("} else {\n");
		splits[1].emitSet(opcode, testsPerformed, level+1);
		emitTab(level); emit("}\n");
		testsPerformed[rec.argument][rec.test] = false;
	    }
	}
    }

    /**
     * the Class object of the VM_Assembler.  This is used for
     * reflective inquiries about emit methods.
     *
     * @see #main
     */
    static Class lowLevelAsm;

    /**
     * Computes the set of emit methods in the VM_Assembler for a
     * given IA32 opcode.
     *
     * @param emitters the set of all emit methods
     * @param opcode the opcode being examined
     */
    private static EmitterSet 
	buildSetForOpcode(Method[] emitters, String opcode)
    {
	EmitterSet s = new EmitterSet();
	for(int i = 0; i < emitters.length; i++) {
	    Method m = emitters[i];
	    if (m.getName().startsWith("emit" + opcode + "_") 
		                    ||
		m.getName().equals("emit" + opcode))
	    {
		s.add(new EmitterDescriptor(m.getName()));
	    }
	}

	return s;
    }

    /**
     * the set of IA32 opcodes to ignore.  Some opcode are not used by
     * the opt compiler (NOP is a good example) but may be present in
     * the VM_Assembler if other compilers use them.  We keep an
     * explicit list of such opcodes to ignore.
     */
    private static Set excludedOpcodes;

    /**
     *  Initialize the set of opcodes to ignore 
     *
     * @see #excludedOpcodes
     */
    static {
	excludedOpcodes = new HashSet();
	excludedOpcodes.add("FSAVE");
	excludedOpcodes.add("FNSTSW");
	excludedOpcodes.add("FUCOMPP");
	excludedOpcodes.add("SAHF");
	excludedOpcodes.add("NOP");
	excludedOpcodes.add("RDTSC");
	excludedOpcodes.add("ENTER");
    }

    /**
     * Compute the set of all IA32 opcodes that have emit methods in
     * the VM_Assembler.  This method uses the stylized form of all
     * emit method names in the VM_Assembler to extract the opcode of
     * each one.  It returns a set of all such distinct names, as a
     * set of Strings.
     *
     * @param emitters the set of all emit methods in the VM_Assembler
     * @return the set of all opcodes handled by the VM_Assembler
     */
    private static Set getOpcodes(Method[] emitters) {
	Set s = new HashSet();
	for(int i = 0; i < emitters.length; i++) {
	    String name = emitters[i].getName();
	    if (DEBUG) System.out.println(name);
	    if (name.startsWith("emit")) {
		int posOf_ = name.indexOf('_');
		if (posOf_ != -1) {
		    String opcode = name.substring(4, posOf_);
		    if (! excludedOpcodes.contains(opcode)) s.add( opcode );
		} else {
		    String opcode = name.substring(4);
		    // make sure it is an opcode
		    if (opcode.equals(opcode.toUpperCase(Locale.getDefault())))
			if (! excludedOpcodes.contains(opcode))
			    s.add( opcode );
		}
	    }
	}

	return s;
    }

    /**
     * returns a list of all IA32_ opt compiler operators that do not
     * correspond to real IA32 opcodes handled by the assembler.
     * These are all supposed to have been removed by the time the
     * assembler is called, so the assembler actually seeing such an
     * opcode is an internal compiler error.  This set is used during
     * generating of error checking code.
     *
     * @param emittedOpcodes the set of IA32 opcodes the assembler
     * understands. 
     * @return the set of IA32 opt operators that the assembler does
     * not understand.
     */
    private static Set getErrorOpcodes(Set emittedOpcodes) {
	Iterator e = OPT_OperatorFormatTables.getOpcodes();
	Set errorOpcodes = new HashSet();
	while (e.hasNext()) {
	    String opcode = (String) e.next();
	    if (! emittedOpcodes.contains(opcode))
		errorOpcodes.add( opcode );
	}

	return errorOpcodes;
    }

    /**
     * Given an IA32 opcode, return the set of opt compiler IA32_
     * operators that translate to it.  There is, by and large, a
     * one-to-one mapping in each each IA332_ opt operator represents
     * an IA32 opcde, so this method might seem useless.  However,
     * there are some special cases, notably for operand size.  In
     * this case, an opt operator of the form ADD$B would mean use the
     * ADD IA32 opcode with a byte operand size.  
     */
    private static Set getMatchingOperators(String lowLevelOpcode) {
	Iterator e = OPT_OperatorFormatTables.getOpcodes();
	Set matchingOperators = new HashSet();
	while (e.hasNext()) {
	    String o = (String) e.next();
	    if (o.equals(lowLevelOpcode) || o.startsWith(lowLevelOpcode+"$"))
		matchingOperators.add( o );
	}

	return matchingOperators;
    }

    /**
     * Generate an assembler for the opt compiler
     */
    public static void main(String[] args) {
	try {
	    out = new FileWriter(System.getProperty("generateToDir") + "/OPT_Assembler.java");
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}

	try {
	    lowLevelAsm = Class.forName("VM_Assembler");
	} catch (ClassNotFoundException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}

	emit("import instructionFormats.*;\n\n");
	emit("\n\n");

	emit("/**\n");
	emit(" *  This class is the automatically-generated assembler for\n");
	emit(" * the optimizing compiler.  It consists of methods that\n");
	emit(" * understand the possible operand combinations of each\n");
	emit(" * instruction type, and how to translate those operands to\n");
	emit(" * calls to the VM_Assember low-level emit method\n");
	emit(" *\n");
	emit(" * @see GenerateAssembler\n");
	emit(" *\n");
	emit(" * @author Julian Dolby\n");
	emit(" * @author {@link GenerateAssembler}\n");
	emit(" */\n");
	emit("class OPT_Assembler extends OPT_AssemblerBase {\n\n");

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  This class requires no special construction;\n");
	emitTab(1);emit(" * this constructor simply invokes the\n");
	emitTab(1);emit(" * constructor for VM_Assembler\n");
	emitTab(1);emit(" *\n");
	emitTab(1);emit(" * @see VM_Assembler\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("OPT_Assembler(int bcSize, boolean print) {\n");
	emitTab(2);   emit("super(bcSize, print);\n");
	emitTab(1); emit("}");
	emit("\n\n");

	Method[] emitters = lowLevelAsm.getDeclaredMethods();
	Set opcodes = getOpcodes(emitters);

	Iterator i = opcodes.iterator();
	while (i.hasNext()) {
	    String opcode = (String) i.next();
	    setCurrentOpcode( opcode );
	    emitTab(1);emit("/**\n");
	    emitTab(1);emit(" *  Emit the given instruction, assuming that\n");
	    emitTab(1);emit(" * it is a " + currentFormat + " instruction\n");
	    emitTab(1);emit(" * and has a " + currentOpcode + " operator\n");
	    emitTab(1);emit(" *\n");
	    emitTab(1);emit(" * @param inst the instruction to assemble\n");
	    emitTab(1);emit(" */\n");
	    emitTab(1);
	    emit("private void do" + opcode + "(OPT_Instruction inst) {\n");
	    EmitterSet emitter = buildSetForOpcode(emitters, opcode);
	    boolean[][] tp = new boolean[4][ encoding.length ];
	    emitter.emitSet(opcode, tp, 2);
	    emitTab(1);
	    emit("}\n\n");
	}

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  The number of instructions emitted so far\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("private int instructionCount = 0;\n\n");

	emitTab(1);emit("/**\n");
	emitTab(1);emit(" *  Assemble the given instruction\n");
	emitTab(1);emit(" *\n");
	emitTab(1);emit(" * @param inst the instruction to assemble\n");
	emitTab(1);emit(" */\n");
	emitTab(1); emit("void doInst(OPT_Instruction inst) {\n");
	emitTab(2);    emit("resolveForwardReferences(++instructionCount);\n");
	emitTab(2);    emit("switch (inst.getOpcode()) {\n");

	Set emittedOpcodes = new HashSet();

	i = opcodes.iterator();
	while (i.hasNext()) {
	    String opcode = (String) i.next();
	    Iterator operators = getMatchingOperators( opcode ).iterator();
	    while ( operators.hasNext() ) {
		Object operator = operators.next();
		emitTab(3); 
		emittedOpcodes.add( operator );
		emit("case IA32_" + operator + "_opcode:\n");
	    }
	    emitTab(4);    emit("do" + opcode + "(inst);\n");
	    emitTab(4);    emit("break;\n");
	}

	// Kludge for IA32_LOCK which needs to call emitLockNextInstruction
	emittedOpcodes.add("LOCK");
	emitTab(3);    emit("case IA32_LOCK_opcode:\n");
	emitTab(4);    emit("emitLockNextInstruction();\n");
	emitTab(4);    emit("break;\n");

	// Kludge for PATCH_POINT 
	emittedOpcodes.add("LOCK");
	emitTab(3);    emit("case IG_PATCH_POINT_opcode:\n");
	emitTab(4);    emit("emitPatchPoint();\n");
	emitTab(4);    emit("break;\n");
	
	Set errorOpcodes = getErrorOpcodes( emittedOpcodes );
	if (! errorOpcodes.isEmpty()) {
	    i = errorOpcodes.iterator();
	    while (i.hasNext()) {
		emitTab(3); 
		emit("case IA32_" + i.next() + "_opcode:\n");
	    }
	    emitTab(4); emit("throw new OPT_OptimizingCompilerException(inst + \" has unimplemented IA32 opcode (check excludedOpcodes)\");\n");
	}
	
	emitTab(2);    emit("}\n");
	emitTab(2);    emit("inst.setmcOffset( mi );\n");
	emitTab(1); emit("}\n\n");
	
	emit("\n}\n");

	try {
	    out.close();
	} catch (IOException e) {
	    e.printStackTrace();
	    System.exit( -1 );
	}
    }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 *  This class provides support functionality used by the generated
 * OPT_Assembler; it handles basic impedance-matching functionality
 * such as determining which addressing mode is suitable for a given
 * OPT_IA32MemoryOperand.  This class also provides some boilerplate
 * methods that do not depend on how instructions sould actually be
 * assembled, like the top-level generateCode driver.  This class is
 * not meant to be used in isolation, but rather to provide support
 * from the OPT_Assembler.
 *
 * @author Julian Dolby
 */
abstract class OPT_AssemblerBase 
    extends VM_Assembler 
    implements OPT_Operators, VM_Constants, OPT_PhysicalRegisterConstants
{
    /**
     *  This class requires no particular construction behavior; this
     * constructor simply calls super.
     *
     * @see VM_Assembler
     */
    OPT_AssemblerBase(int bytecodeSize, boolean shouldPrint) {
	super(bytecodeSize, shouldPrint);
    }

    /**
     *  Is the given operand an immediate?  In the IA32 assembly, one
     * cannot specify floating-point constants, so the possible
     * immediates we may see are OPT_IntegerConstants and
     * OPT_TrapConstants (a trap constant really is an integer), and
     * jump targets for which the exact offset is known.
     *
     * @see #getImm
     *
     * @param op the operand being queried
     * @return true if op represents an immediate
     */
    static boolean isImm(OPT_Operand op) {
	return 
	    (op instanceof OPT_IntConstantOperand)
	                   ||
	    (op instanceof OPT_TrapCodeOperand)
	                   ||
	    (op instanceof OPT_BranchOperand 
                           &&
	     op.asBranch().target.getmcOffset() >= 0);
    }

    /**
     *  Return the IA32 ISA encoding of the immediate value
     * represented by the the given operand.  This method assumes the
     * operand is an immediate and will likely throw a
     * ClassCastException if this not the case.  It treats
     * OPT_BranchOperands somewhat differently than isImm does: in
     * case a branch target is not resolved, it simply returns a wrong
     * answer and trusts the caller to ignore it. This behavior
     * simplifies life when generating code for ImmOrLabel operands.
     *
     * @see #isImm
     *
     * @param op the operand being queried
     * @return the immediate value represented by the operand
     */
    static int getImm(OPT_Operand op) {
	if (op instanceof OPT_BranchOperand) {
	    // used by ImmOrLabel stuff
	    return op.asBranch().target.getmcOffset();
	}
	else if (op instanceof OPT_TrapCodeOperand) 
	    return ((OPT_TrapCodeOperand)op).getTrapCode() 
		                  +
		   VM_TrapConstants.RVM_TRAP_BASE;
	else
	    return op.asIntConstant().value;
    }

    /**
     *  Is the given operand a register operand?
     *
     * @see #getReg
     *
     * @param op the operand being queried
     * @return true if op is an OPT_RegisterOperand
     */
    static boolean isReg(OPT_Operand op) {
	return (op instanceof OPT_RegisterOperand);
    }

    /**
     *  Return the machine-level register number corresponding to a
     * given OPT_Register.  The optimizing compiler has its own notion
     * of register numbers, which is not the same as the numbers used
     * by the IA32 ISA.  This method takes an optimizing compiler
     * register and translates it into the appropriate machine-level
     * encoding.  This method is not applied directly to operands, but
     * rather to register objects.
     *
     * @see #getReg
     * @see #getBase
     * @see #getIndex
     *
     * @param reg the register being queried
     * @return the 3 bit machine-level encoding of reg
     */
    static private byte getMachineRegister(OPT_Register reg) {
	int type = OPT_PhysicalRegisterSet.getPhysicalRegisterType(reg);
	if (type == INT_REG) 
	    return (byte) (reg.number - FIRST_INT);
	else if (type == DOUBLE_REG)
	    return (byte) (reg.number - FIRST_DOUBLE);
	else
	    throw new OPT_OptimizingCompilerException("unexpected register type " + type);
    }
	
    /**
     *  Given a register operand, return the 3 bit IA32 ISA encoding
     * of that register.  This function translates an optimizing
     * compiler register operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a register operand, and will blow up if it is not;
     * use isReg to check operands passed to this method.
     *
     * @see #isReg 
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of op
     */
    static byte getReg(OPT_Operand op) {
	return getMachineRegister( op.asRegister().register );
    }

    /**
     *  Given a memory operand, return the 3 bit IA32 ISA encoding
     * of its base regsiter.  This function translates the optimizing
     * compiler register operand representing the base of the given
     * memory operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a memory operand, and will blow up if it is not;
     * one should confirm an operand really has a base register before
     * invoking this method on it.
     *
     * @see #isRegDisp
     * @see #isRegIdx
     * @see #isRegInd
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of the base register of op
     */
    static byte getBase(OPT_Operand op) {
	return getMachineRegister( ((OPT_MemoryOperand)op).base.register );
    }

    /**
     *  Given a memory operand, return the 3 bit IA32 ISA encoding
     * of its index regsiter.  This function translates the optimizing
     * compiler register operand representing the index of the given
     * memory operand into the 3 bit IA32 ISA encoding that
     * can be passed to the VM_Assembler.  This function assumes its
     * operand is a memory operand, and will blow up if it is not;
     * one should confirm an operand really has an index register before
     * invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the 3 bit IA32 ISA encoding of the index register of op
     */
    static byte getIndex(OPT_Operand op) {
	return getMachineRegister(((OPT_MemoryOperand)op).index.register);
    }

    /**
     *  Given a memory operand, return the 2 bit IA32 ISA encoding
     * of its scale, suitable for passing to the VM_Assembler to mask
     * into a SIB byte.  This function assumes its operand is a memory
     * operand, and will blow up if it is not; one should confirm an
     * operand really has a scale before invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the IA32 ISA encoding of the scale of op
     */
    static short getScale(OPT_Operand op) {
	return ((OPT_MemoryOperand)op).scale;
    }

    /**
     *  Given a memory operand, return the 2 bit IA32 ISA encoding
     * of its scale, suitable for passing to the VM_Assembler to mask
     * into a SIB byte.  This function assumes its operand is a memory
     * operand, and will blow up if it is not; one should confirm an
     * operand really has a scale before invoking this method on it.
     *
     * @see #isRegIdx
     * @see #isRegOff
     *
     * @param op the register operand being queried
     * @return the IA32 ISA encoding of the scale of op
     */
    static int getDisp(OPT_Operand op) {
	return ((OPT_MemoryOperand)op).disp;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-displacement mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-displacement mode.  That is, does it have a non-zero
     * displacement and a base register, but no scale and no index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-displacement mode
     */
    static boolean isRegDisp(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base != null) &&
		(mop.index == null) &&
		(mop.disp != 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * absolute mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * absolute address mode.  That is, does it have a non-zero
     * displacement, but no scale, no scale and no index register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as absolute mode
     */
    static boolean isAbs(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base == null) &&
		(mop.index == null) &&
		(mop.disp != 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-indirect mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-displacement mode.  That is, does it have a base
     * register, but no displacement, no scale and no index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-indirect mode
     */
    static boolean isRegInd(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base != null) &&
		(mop.index == null) &&
		(mop.disp == 0) &&
		(mop.scale == 0);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * register-offset mode addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * register-offset mode.  That is, does it have a non-zero
     * displacement, a scale parameter and an index register, but no
     * base register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as register-offset mode
     */
    static boolean isRegOff(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) {
	    OPT_MemoryOperand mop = (OPT_MemoryOperand) op;
	    return (mop.base == null) &&
		(mop.index != null);
	} else
	    return false;
    }

    /**
     *  Determine if a given operand is a memory operand representing
     * the full glory of scaled-index-base addressing.  This method takes an
     * arbitrary operand, checks whether it is a memory operand, and,
     * if it is, checks whether it should be assembled as IA32
     * SIB mode.  That is, does it have a non-zero
     * displacement, a scale parameter, a base register and an index
     * register?
     *
     * @param op the operand being queried
     * @return true if op should be assembled as SIB mode
     */
    static boolean isRegIdx(OPT_Operand op) {
	if (op instanceof OPT_MemoryOperand) 
	    return !(isAbs(op) || isRegInd(op) || isRegDisp(op) || isRegOff(op));
	else
	    return false;
    }

    /**
     *  Return the condition bits of a given optimizing compiler
     * condition operand.  This method returns the IA32 ISA bits
     * representing a given condition operand, suitable for passing to
     * the VM_Assembler to encode into the opcode of a SET, Jcc or
     * CMOV instruction.  This being IA32, there are of course
     * exceptions in the binary encoding of conditions (see FCMOV),
     * but the VM_Assembler handles that.  This function assumes its
     * argument is an OPT_IA32ConditionOperand, and will blow up if it
     * is not.
     *
     * @param op the operand being queried
     * @return the bits that (usually) represent the given condition
     * in the IA32 ISA */
    static byte getCond(OPT_Operand op) {
	return ((OPT_IA32ConditionOperand)op).value;
    }

    /**
     *  Is the given operand an IA32 condition operand?
     *
     * @param op the operand being queried
     * @return true if op is an IA32 condition operand
     */
    static boolean isCond(OPT_Operand op) {
	return (op instanceof OPT_IA32ConditionOperand);
    }

    /**
     *  Return the label representing the target of the given branch
     * operand.  These labels are used to represent branch targets
     * that have not yet been assembled, and so cannot be given
     * concrete machine code offsets.  All instructions are nunbered
     * just prior to assembly, and these numbers are used as labels.
     * This method also returns 0 (not a valid label) for int
     * constants to simplify generation of branches (the branch
     * generation code will ignore this invalid label; it is used to
     * prevent type exceptions).  This method assumes its operand is a
     * branch operand (or an int) and will blow up if it is not.
     *
     * @param op the branch operand being queried
     * @return the label representing the branch target
     */
    static int getLabel(OPT_Operand op) {
	if (op instanceof OPT_IntConstantOperand)
	    // used by ImmOrLabel stuff
	    return 0;
	else {
	    if (op.asBranch().target.getmcOffset() < 0)
		return - op.asBranch().target.getmcOffset();
	    else
		return -1;
	}
    }

    /**
     *  Is the given operand a branch target that requires a label?
     *
     * @see #getLabel
     *
     * @param op the operand being queried
     * @return true if it represents a branch requiring a label target
     */
    static boolean isLabel(OPT_Operand op) {
	return (op instanceof OPT_BranchOperand
		                &&
		op.asBranch().target.getmcOffset() < 0);
    }
    
    /**
     *  Is the given operand a branch target?
     *
     * @see #getLabel
     * @see #isLabel
     *
     * @param op the operand being queried
     * @return true if it represents a branch target
     */
    static boolean isImmOrLabel(OPT_Operand op) {
	return (isImm(op) || isLabel(op));
    }

    /**
     *  Does the given instruction operate upon byte-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a byte.  This does not work for the
     * size-converting moves (MOVSX and MOVZX), and those instructions
     * use the operator convention that $b on the end of the operator
     * name means operate upon byte data.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon byte data
     */
    static boolean isByte(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$b") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 1);
	}

	return false;
    }

    /**
     *  Does the given instruction operate upon word-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a word.  This does not work for the
     * size-converting moves (MOVSX and MOVZX), and those instructions
     * use the operator convention that $w on the end of the operator
     * name means operate upon word data.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon word data
     */
    static boolean isWord(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$w") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 2);
	}

	return false;
    }

    /**
     *  Does the given instruction operate upon quad-sized data?  The
     * opt compiler does not represent the size of register data, so
     * this method typically looks at the memory operand, if any, and
     * checks whether that is a byte.  This method also recognizes 
     * the operator convention that $q on the end of the operator
     * name means operate upon quad data; no operator currently uses
     * this convention.
     *
     * @param inst the instruction being queried
     * @return true if inst operates upon quad data
     */
    static boolean isQuad(OPT_Instruction inst) {
	if (inst.operator.toString().indexOf("$q") != -1)
	    return true;

	for(int i = 0; i < inst.getNumberOfOperands(); i++) {
	    OPT_Operand op = inst.getOperand(i);
	    if (op instanceof OPT_MemoryOperand)
		return (((OPT_MemoryOperand)op).size == 8);
	}

	return false;
    }

  /**
   * Debugging support (return a printable representation of the machine code).
   *
   * @param instr, an integer to be interpreted as a PowerPC instruction
   * @param offset the mcoffset (in bytes) of the instruction
   *
   */
  public static String disasm(int instr, int offset) {
    OPT_OptimizingCompilerException.TODO("OPT_Assembler: disassembler");
    return null;
  }

  /**
   * generate machine code into ir.machinecode
   * @param ir the IR to generate
   * @return   the number of machinecode instructions generated
   */
  public static final int generateCode(OPT_IR ir, boolean shouldPrint) {
      int count = 0;

      for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	   p != null; p = p.nextInstructionInCodeOrder()) 
      {
	  p.setmcOffset( - ++count);
      }
      
      OPT_Assembler asm = new OPT_Assembler(count, shouldPrint);

      for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	   p != null; p = p.nextInstructionInCodeOrder()) 
      {
	  asm.doInst(p);
      }
      
    ir.MIRInfo.machinecode = asm.getMachineCodes();

    return ir.MIRInfo.machinecode.length;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Final acts of MIR expansion for the IA32 architecture.
 * Things that are expanded here (immediately before final assembly)
 * should only be those sequences that cannot be expanded earlier
 * due to difficulty in keeping optimizations from interfering with them.
 *
 * One job of this phase is to handle the expansion of the remains of
 * table switch.  The code looks like a mess (which it is), but there
 * is little choice for relocatable IA32 code that does this.  And the
 * details of this code are shared with the baseline compiler and
 * dependent in detail on the VM_Assembler (see {@link
 * VM_Assembler#emitOFFSET_Imm_ImmOrLabel}).  If you want to mess with
 * it, you will probably need to mess with them as well.
 *
 * @author Dave Grove
 * @author Stephen Fink
 * @author Julian Dolby
 * @modified Peter Sweeney 
 */
class OPT_FinalMIRExpansion extends OPT_IRTools {

  /**
   * @param ir the IR to expand
   * @return return value is garbage for IA32
   */
  public final static int expand(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (OPT_Instruction next, p = ir.firstInstructionInCodeOrder();
         p != null;
         p = next) {
      next = p.nextInstructionInCodeOrder();
      p.setmcOffset(-1);
      p.scratchObject = null; 

      switch (p.getOpcode()) {
      case MIR_LOWTABLESWITCH_opcode:
	{
	  // split the basic block after the MIR_LOWTABLESWITCH
	  OPT_BasicBlock thisBlock = p.getBasicBlock();
	  OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(p,ir);
	  nextBlock.firstInstruction().setmcOffset(-1); 

	  // place offset data table after call so that call pushes
	  // the base address of this table onto the stack
	  int NumTargets = MIR_LowTableSwitch.getNumberOfTargets(p);
	  for (int i = 0; i < NumTargets; i++) {
	    thisBlock.appendInstruction(MIR_CaseLabel.create
					(IA32_OFFSET, I(i), 
					 MIR_LowTableSwitch.getClearTarget(p, i)));
	  }
	  // calculate address to which to jump, and store it
	  // on the top of the stack
	  OPT_Register regS = MIR_LowTableSwitch.getIndex(p).register;
	  nextBlock.appendInstruction(MIR_BinaryAcc.create(IA32_SHL, R(regS), I(2)));
	  nextBlock.appendInstruction(MIR_BinaryAcc.create
				      (IA32_ADD, R(regS), 
				       OPT_MemoryOperand.I(R(phys.getESP()),
							   (byte)4,null,null)));
	  nextBlock.appendInstruction(MIR_Move.create(IA32_MOV, R(regS), 
						      OPT_MemoryOperand.I
						      (R(regS),(byte)4,null,
						       null)));
	  nextBlock.appendInstruction(MIR_BinaryAcc.create
				      (IA32_ADD, 
				       OPT_MemoryOperand.I(R(phys.getESP()),
							   (byte)4,null,null),
				       R(regS))); 
	  // ``return'' to mangled return address
	  nextBlock.appendInstruction(MIR_Return.create(IA32_RET, I(0), null, null));

	  // CALL next block to push pc of next ``instruction'' onto stack
	  MIR_Call.mutate0(p, IA32_CALL, null, null, nextBlock.makeJumpTarget(), null);
	}
	break;
	  
      case IA32_TEST_opcode: 
	// don't bother telling rest of compiler that memory operand
	// must be first; we can just commute it here.
	if (MIR_Test.getVal2(p).isMemory()) {
	  OPT_Operand tmp = MIR_Test.getClearVal1(p);
	  MIR_Test.setVal1(p, MIR_Test.getClearVal2(p));
	  MIR_Test.setVal2(p, tmp);
	}
	break;

      case NULL_CHECK_opcode:
	{
	  // mutate this into a TRAPIF, and then fall through to the the
	  // TRAP_IF case. 
	  OPT_Operand ref = NullCheck.getRef(p);
	  MIR_TrapIf.mutate(p,IA32_TRAPIF,null,ref.copy(),I(0),
			    OPT_IA32ConditionOperand.EQ(),
			    OPT_TrapCodeOperand.NullPtr());
	} 
	// There is no break statement here on purpose!
      case IA32_TRAPIF_opcode: 
	{
	  // split the basic block right before the IA32_TRAPIF
	  OPT_BasicBlock thisBlock = p.getBasicBlock();
	  OPT_BasicBlock trap = thisBlock.createSubBlock(p.bcIndex,ir,0f);
	  OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(p,ir);
	  OPT_TrapCodeOperand tc = MIR_TrapIf.getClearTrapCode(p);
	  p.remove();
	  nextBlock.firstInstruction().setmcOffset(-1); 

	  // add code to thisBlock to conditionally jump to trap
	  OPT_Instruction cmp = MIR_Compare.create(IA32_CMP, 
						   MIR_TrapIf.getVal1(p), 
						   MIR_TrapIf.getVal2(p));
	  if (p.isMarkedAsPEI()) {
	    // The trap if was explictly marked, which means that it has 
	    // a memory operand into which we've folded a null check.
	    // Actually need a GC map for both the compare and the INT.
	    cmp.markAsPEI();
	    cmp.copyPosition(p);
	    ir.MIRInfo.gcIRMap.insertTwin(p, cmp);
	  }
	  thisBlock.appendInstruction(cmp);
	  thisBlock.appendInstruction(MIR_CondBranch.create
				      (IA32_JCC, MIR_TrapIf.getCond(p), 
				       trap.makeJumpTarget(), null));

	  // add block at end to hold trap instruction, and 
	  // insert trap sequence
	  ir.cfg.addLastInCodeOrder(trap);
	  if (tc.isArrayBounds()) {
	    // attempt to store index expression in processor object for 
	    // C trap handler
	    OPT_Operand index = MIR_TrapIf.getVal2(p);
	    if (!(index instanceof OPT_RegisterOperand ||
		  index instanceof OPT_IntConstantOperand)) {
	      index = I(0xdeadbeef); // index was spilled, and 
	      // we can't get it back here.
	    }
	    OPT_MemoryOperand mo = 
	      OPT_MemoryOperand.BD(R(phys.getPR()),
				   VM_Entrypoints.arrayIndexTrapParamField.getOffset(),
				   (byte)4, 
				   null, 
				   null);
	    trap.appendInstruction(MIR_Move.create(IA32_MOV, mo, 
						   index.copy()));
	  }
	  // NOTE: must make p the trap instruction: it is the GC point!
	  // IMPORTANT: must also inform the GCMap that the instruction has 
	  // been moved!!!
	  trap.appendInstruction(MIR_Trap.mutate(p, IA32_INT, null, tc));
	  ir.MIRInfo.gcIRMap.moveToEnd(p);

	  if (tc.isStackOverflow()) {
	    // only stackoverflow traps resume at next instruction.
	    trap.appendInstruction(MIR_Branch.create
				   (IA32_JMP, nextBlock.makeJumpTarget()));
	  }
	}
	break;

      case IA32_FMOV_ENDING_LIVE_RANGE_opcode:
	OPT_Operand result = MIR_Move.getResult(p);
	OPT_Operand value = MIR_Move.getValue(p);
	if (result.isRegister() && value.isRegister()) {
	  if (result.similar(value)) {
	    // eliminate useless move
	    p.remove(); 
	  } else {
	    int i = phys.getFPRIndex(result.asRegister().register);   
	    int j = phys.getFPRIndex(value.asRegister().register);   
	    if (i == 0) {
	      MIR_XChng.mutate(p, IA32_FXCH, result, value);
	    } else if (j == 0) {
	      MIR_XChng.mutate(p, IA32_FXCH, value, result);
	    } else {
	      expandFmov(p,phys);
	    }
	  }
	} else {
	  expandFmov(p,phys);
	}
	break;

      case DUMMY_DEF_opcode:
      case DUMMY_USE_opcode:
      case REQUIRE_ESP_opcode:
      case ADVISE_ESP_opcode:
	p.remove();
	break;

      case IA32_FMOV_opcode:
	expandFmov(p,phys);
	break;

      case IA32_FCLEAR_opcode:
	expandFClear(p,ir);
	break;

      case IA32_JCC2_opcode:
	p.insertBefore(MIR_CondBranch.create
		       (IA32_JCC, MIR_CondBranch2.getCond1(p), 
			MIR_CondBranch2.getTarget1(p), 
			MIR_CondBranch2.getBranchProfile1(p)));
	MIR_CondBranch.mutate(p, IA32_JCC, MIR_CondBranch2.getCond2(p),
			      MIR_CondBranch2.getTarget2(p),
			      MIR_CondBranch2.getBranchProfile2(p));
	break;

      case CALL_SAVE_VOLATILE_opcode:
	p.operator=IA32_CALL;
	break;

      case IA32_LOCK_CMPXCHG_opcode:
	p.insertBefore(MIR_Empty.create(IA32_LOCK));
	p.operator=IA32_CMPXCHG;
	break;

      case YIELDPOINT_PROLOGUE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromPrologueMethod);
	break;

      case YIELDPOINT_EPILOGUE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromEpilogueMethod);
	break;

      case YIELDPOINT_BACKEDGE_opcode:
	expandYieldpoint(p, ir, VM_Entrypoints.optThreadSwitchFromBackedgeMethod);
	break;

      case IR_ENDPROLOGUE_opcode:
	// Remember where the end of prologue is for jdp
	p.remove();
	ir.MIRInfo.instAfterPrologue = next;
	break;

      }
    }
    return 0;
  }

  /**
   * expand an FCLEAR pseudo-insruction using FFREEs.
   *
   * @param s the instruction to expand
   * @param phys controlling physical register set
   */
  private static void expandFClear(OPT_Instruction s, OPT_IR ir) {
    int nSave = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
    int fpStackHeight = ir.MIRInfo.fpStackHeight;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (int i=nSave; i<fpStackHeight; i++) {
      OPT_Register f = phys.getFPR(i);
      s.insertBefore(MIR_UnaryAcc.create(IA32_FFREE,D(f)));
    }

    // Remove the FCLEAR.
    s.remove();
  }

  /**
   * expand an FMOV pseudo-insruction.
   *
   * @param s the instruction to expand
   * @param phys controlling physical register set
   */
  private static void expandFmov(OPT_Instruction s, 
                                 OPT_PhysicalRegisterSet phys) {
    OPT_Operand result = MIR_Move.getResult(s); 
    OPT_Operand value = MIR_Move.getValue(s); 

    if (result.isRegister() && value.isRegister()) {
      if (result.similar(value)) {
        // eliminate useless move
        s.remove(); 
      } else { 
        int i = phys.getFPRIndex(result.asRegister().register);   
        int j = phys.getFPRIndex(value.asRegister().register);   
        if (j == 0) {
          // We have FMOV Fi, F0
          // Expand as:
          //        FST F(i)  (copy F0 to F(i))
          MIR_Move.mutate(s,IA32_FST,D(phys.getFPR(i)),D(phys.getFPR(0)));
        } else {
          // We have FMOV Fi, Fj
          // Expand as:
          //        FLD Fj  (push Fj on FP stack).
          //        FSTP F(i+1)  (copy F0 to F(i+1) and then pop register stack)
          s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));

          MIR_Move.mutate(s,IA32_FSTP,D(phys.getFPR(i+1)),D(phys.getFPR(0)));
        }

      }
    } else if (value instanceof OPT_MemoryOperand) {
      if (result instanceof OPT_MemoryOperand) {
        // We have FMOV M1, M2
        // Expand as:
        //        FLD M1   (push M1 on FP stack).
        //        FSTP M2  (copy F0 to M2 and pop register stack)
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,result,D(phys.getFPR(0)));
      } else {
        // We have FMOV Fi, M
        // Expand as:
        //        FLD M    (push M on FP stack).
        //        FSTP F(i+1)  (copy F0 to F(i+1) and pop register stack)
        if (VM.VerifyAssertions) VM.assert(result.isRegister());
        int i = phys.getFPRIndex(result.asRegister().register);   
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,D(phys.getFPR(i+1)),D(phys.getFPR(0)));
      }
    } else {
      // We have FMOV M, Fi
      if (VM.VerifyAssertions) VM.assert(value.isRegister());
      if (VM.VerifyAssertions) 
        VM.assert(result instanceof OPT_MemoryOperand);
      int i = phys.getFPRIndex(value.asRegister().register);   
      if (i!=0) {
        // Expand as:
        //        FLD Fi    (push Fi on FP stack).
        //        FSTP M    (store F0 in M and pop register stack);
        s.insertBefore(MIR_Move.create(IA32_FLD,D(phys.getFPR(0)),value));
        MIR_Move.mutate(s,IA32_FSTP,result,D(phys.getFPR(0)));
      } else {
        // Expand as:
        //        FST M    (store F0 in M);
        MIR_Move.mutate(s,IA32_FST,result,value);
      }
    }
  }

  private static void expandYieldpoint(OPT_Instruction s,
				       OPT_IR ir,
				       VM_Method meth) {
    if (VM.VerifyAssertions) VM.assert(ir.options.FIXED_JTOC);

    // split the basic block after the yieldpoint, create a new
    // block at the end of the IR to hold the yieldpoint,
    // remove the yieldpoint (to prepare to out it in the new block at the end)
    OPT_BasicBlock thisBlock = s.getBasicBlock();
    OPT_BasicBlock nextBlock = thisBlock.splitNodeWithLinksAt(s,ir);
    OPT_BasicBlock yieldpoint = thisBlock.createSubBlock(s.bcIndex, ir, 0);
    thisBlock.insertOut(yieldpoint);
    yieldpoint.insertOut(nextBlock);
    ir.cfg.addLastInCodeOrder(yieldpoint);
    s.remove();
    
    // change thread switch instruction into call to thread switch routine
    // NOTE: must make s the call instruction: it is the GC point!
    //       must also inform the GCMap that s has been moved!!!
    int offset = meth.getOffset();
    OPT_LocationOperand loc = new OPT_LocationOperand(offset);
    OPT_Operand guard = TG();
    OPT_Operand target = 
      OPT_MemoryOperand.D(VM_Magic.getTocPointer().add(offset).toInt(), (byte)4, loc, guard);
    MIR_Call.mutate0(s, CALL_SAVE_VOLATILE, null, null, target, 
		     OPT_MethodOperand.STATIC(meth));
    yieldpoint.appendInstruction(s);
    ir.MIRInfo.gcIRMap.moveToEnd(s);

    yieldpoint.appendInstruction(MIR_Branch.create(IA32_JMP,
						   nextBlock.makeJumpTarget())); 
    
    // Check to see if threadSwitch requested
    OPT_Register PR = ir.regpool.getPhysicalRegisterSet().getPR();
    int tsr = VM_Entrypoints.threadSwitchRequestedField.getOffset();
    OPT_MemoryOperand M = OPT_MemoryOperand.BD(R(PR),tsr,(byte)4,null,null);
    thisBlock.appendInstruction(MIR_Compare.create(IA32_CMP, M, I(0)));
    thisBlock.appendInstruction(MIR_CondBranch.create(IA32_JCC, OPT_IA32ConditionOperand.NE(),
						      yieldpoint.makeJumpTarget(),
						      OPT_BranchProfileOperand.never()));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An FPR register that BURS is managing.
 * Created by a fld, and then eventually
 * deallocated with some popping alu/store.
 *
 * @author Dave Grove
 */
public final class OPT_BURSManagedFPROperand extends OPT_Operand {
  int regNum;

  OPT_BURSManagedFPROperand(int r) {
    regNum = r;
  }

  /**
   * Returns a copy of the current operand.
   */
  OPT_Operand copy() { 
    return new OPT_BURSManagedFPROperand(regNum);
  }

  /**
   * Returns if this operand is the 'same' as another operand.
   *
   * @param op other operand
   */
  boolean similar(OPT_Operand op) {
    return (op instanceof OPT_BURSManagedFPROperand) && 
      ((OPT_BURSManagedFPROperand)op).regNum == regNum;
  }

  // Returns the string representation of this operand.
  public String toString() {
    return "ST("+regNum+")";
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$
/**
 * An IA32 condition operand
 *
 * @author Dave Grove
 */
public final class OPT_IA32ConditionOperand extends OPT_Operand 
  implements VM_AssemblerConstants {
  
  /**
   * Value of this operand (one of the ConditionCode constants operands 
   * defined in VM_AssemblerConstants)
   */
  byte value;

  /**
   * Returns a copy of the current operand.
   */
  OPT_Operand copy() { 
    return new OPT_IA32ConditionOperand(value);
  }

  /**
   * Returns if this operand is the 'same' as another operand.
   *
   * @param op other operand
   */
  boolean similar(OPT_Operand op) {
    return (op instanceof OPT_IA32ConditionOperand) && 
      ((OPT_IA32ConditionOperand)op).value == value;
  }

  /**
   * flip the direction of the condition (return this, mutated to flip value)
   */
  OPT_IA32ConditionOperand flipCode() { 
    switch (value) {
    case O:   value =  NO; break;
    case NO:  value =   O; break;
    case LLT: value = LGE; break;
    case LGE: value = LLT; break;
    case EQ:  value =  NE; break;
    case NE:  value =  EQ; break;
    case LLE: value = LGT; break;
    case LGT: value = LLE; break;
    case S:   value =  NS; break;
    case NS:  value =   S; break;
    case PE:  value =  PO; break;
    case PO:  value =  PE; break;
    case LT:  value =  GE; break;
    case GE:  value =  LT; break;
    case LE:  value =  GT; break;
    case GT:  value =  LE; break;
    default:
      OPT_OptimizingCompilerException.UNREACHABLE();
    }
    return this;
  }

  /**
   * change the condition when operands are flipped 
   * (return this mutated to change value)
   */
  OPT_IA32ConditionOperand flipOperands() {
    switch (value) {
    case LLT: value = LGT; break;
    case LGE: value = LLE; break;
    case LLE: value = LGE; break;
    case LGT: value = LLT; break;
    case LT:  value =  GT; break;
    case GE:  value =  LE; break;
    case LE:  value =  GE; break;
    case GT:  value =  LT; break;
    default:
      OPT_OptimizingCompilerException.TODO();
    }
    return this;
  }      

  /**
   * Construct the IA32 Condition Operand that corresponds to the 
   * argument ConditionOperand
   */
  OPT_IA32ConditionOperand(OPT_ConditionOperand c) {
    translate(c);
  }

  static OPT_IA32ConditionOperand EQ() {
    return new OPT_IA32ConditionOperand(EQ);
  }
  static OPT_IA32ConditionOperand NE() {
    return new OPT_IA32ConditionOperand(NE);
  }
  static OPT_IA32ConditionOperand LT() {
    return new OPT_IA32ConditionOperand(LT);
  }
  static OPT_IA32ConditionOperand LE() {
    return new OPT_IA32ConditionOperand(LE);
  }
  static OPT_IA32ConditionOperand GT() {
    return new OPT_IA32ConditionOperand(GT);
  }
  static OPT_IA32ConditionOperand GE() {
    return new OPT_IA32ConditionOperand(GE);
  }
  static OPT_IA32ConditionOperand O() {
    return new OPT_IA32ConditionOperand(O);
  }
  static OPT_IA32ConditionOperand NO() {
    return new OPT_IA32ConditionOperand(NO);
  }
  static OPT_IA32ConditionOperand LGT() {
    return new OPT_IA32ConditionOperand(LGT);
  }
  static OPT_IA32ConditionOperand LLT() {
    return new OPT_IA32ConditionOperand(LLT);
  }
  static OPT_IA32ConditionOperand LGE() {
    return new OPT_IA32ConditionOperand(LGE);
  }
  static OPT_IA32ConditionOperand LLE() {
    return new OPT_IA32ConditionOperand(LLE);
  }
  static OPT_IA32ConditionOperand PE() {
    return new OPT_IA32ConditionOperand(PE);
  }
  static OPT_IA32ConditionOperand PO() {
    return new OPT_IA32ConditionOperand(PO);
  }

  private OPT_IA32ConditionOperand(byte c) {
    value = c;
  }

  // translate from OPT_ConditionOperand: used during LIR => MIR translation
  void translate(OPT_ConditionOperand c) {
     switch(c.value) {
     case OPT_ConditionOperand.EQUAL:         value =  EQ; break;
     case OPT_ConditionOperand.NOT_EQUAL:     value =  NE; break;
     case OPT_ConditionOperand.LESS:          value =  LT; break;
     case OPT_ConditionOperand.LESS_EQUAL:    value =  LE; break;
     case OPT_ConditionOperand.GREATER:       value =  GT; break;
     case OPT_ConditionOperand.GREATER_EQUAL: value =  GE; break;
     case OPT_ConditionOperand.OVERFLOW:      value =   O; break;
     case OPT_ConditionOperand.NOT_OVERFLOW:  value =  NO; break;
     case OPT_ConditionOperand.HIGHER:        value = LGT; break;
     case OPT_ConditionOperand.LOWER:         value = LLT; break;
     case OPT_ConditionOperand.HIGHER_EQUAL:  value = LGE; break;
     case OPT_ConditionOperand.LOWER_EQUAL:   value = LLE; break;
     default:
       OPT_OptimizingCompilerException.TODO();
     }
  }

  // Returns the string representation of this operand.
  public String toString() {
    return CONDITION[value];
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Pool of symbolic registers.
 * Intel specific implementation where JTOC is stored in the processor object
 * and accessed through the processor register.  
 * 
 * @see OPT_Register
 * 
 * @author Peter Sweeney
 * @author Stephen Fink
 */
class OPT_RegisterPool extends OPT_GenericRegisterPool implements OPT_Operators {

  /**
   * Initializes a new register pool for the method meth.
   * 
   * @param meth the VM_Method of the outermost method
   */
  OPT_RegisterPool(VM_Method meth) {
    super(meth);
  }

  /**
   * Inject an instruction to load the JTOC from
   * the processor register and return an OPT_RegisterOperand
   * that contains the result of said load.
   * 
   * @param  ir  the containing IR
   * @param s    the instruction to insert the load operand before
   * @return     a register operand that holds the JTOC
   */ 
  public OPT_Operand makeJTOCOp(OPT_IR ir, OPT_Instruction s) {
    if (ir.options.FIXED_JTOC) {
      VM_Address jtoc = VM_Magic.getTocPointer();
      return new OPT_IntConstantOperand(jtoc.toInt());
    } else {
      OPT_RegisterOperand res = ir.regpool.makeTemp
	(OPT_ClassLoaderProxy.IntArrayType);
      s.insertBefore(Unary.create(GET_JTOC, res, 
                                  OPT_IRTools.
                                  R(ir.regpool.getPhysicalRegisterSet().
                                    getPR())));
      return res.copyD2U();
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This class implements the machine-specific magics for the opt compiler.
 *
 * @see OPT_GenerateMagic for the machine-independent magics
 * 
 * @author Dave Grove
 */
class OPT_GenerateMachineSpecificMagic implements OPT_Operators, VM_Constants {

  /**
   * "Semantic inlining" of methods of the VM_Magic class.
   * Based on the methodName, generate a sequence of opt instructions
   * that implement the magic, updating the stack as neccessary
   *
   * @param bc2ir the bc2ir object generating the ir containing this magic
   * @param gc == bc2ir.gc
   * @param meth the VM_Method that is the magic method
   */
  static void generateMagic(OPT_BC2IR bc2ir, 
			    OPT_GenerationContext gc, 
			    VM_Method meth) 
    throws OPT_MagicNotImplementedException {

    VM_Atom methodName = meth.getName();
    OPT_PhysicalRegisterSet phys = gc.temps.getPhysicalRegisterSet();

    if (methodName == VM_MagicNames.getESIAsProcessor) {
      OPT_RegisterOperand rop = gc.temps.makePROp();
      bc2ir.markGuardlessNonNull(rop);
      bc2ir.push(rop);
    } else if (methodName == VM_MagicNames.setESIAsProcessor) {
      OPT_Operand val = bc2ir.popRef();
      if (val instanceof OPT_RegisterOperand) {
	bc2ir.appendInstruction(Move.create(REF_MOVE, 
					    gc.temps.makePROp(), 
					    val));
      } else {
	String msg = " Unexpected operand VM_Magic.setProcessorRegister";
	throw OPT_MagicNotImplementedException.UNEXPECTED(msg);
      }
    }else if (methodName == VM_MagicNames.getFramePointer) {
      gc.allocFrame = true;
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      VM_Field f = VM_Entrypoints.processorFPField;
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      bc2ir.appendInstruction(GetField.create(GETFIELD, val, pr.copy(), 
					      new OPT_LocationOperand(f), 
					      new OPT_TrueGuardOperand()));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.getJTOC || 
	       methodName == VM_MagicNames.getTocPointer) {
      VM_Type t = (methodName == VM_MagicNames.getJTOC ? OPT_ClassLoaderProxy.IntArrayType : VM_Type.AddressType);
      OPT_RegisterOperand val = gc.temps.makeTemp(t);
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      if (gc.options.FIXED_JTOC) {
        VM_Address jtoc = VM_Magic.getTocPointer();
        OPT_IntConstantOperand I = new OPT_IntConstantOperand(jtoc.toInt());
        bc2ir.appendInstruction(Move.create(REF_MOVE, val, I));
      } else {
        bc2ir.appendInstruction(Unary.create(GET_JTOC, val, pr.copy()));
      }
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.isync) {
      // nothing required on Intel
    } else if (methodName == VM_MagicNames.sync) {
      // nothing required on Intel
    } else if (methodName == VM_MagicNames.getThreadId) {
      OPT_RegisterOperand val = gc.temps.makeTempInt();
      OPT_RegisterOperand pr = null;
      if (VM.dedicatedESI) {
        pr = OPT_IRTools.R(phys.getESI());
      } else {
        pr = gc.temps.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
        bc2ir.appendInstruction(Nullary.create(GET_CURRENT_PROCESSOR,pr)); 
      }
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, pr.copy(),
					  new
                                          OPT_IntConstantOperand(VM_Entrypoints.threadIdField.getOffset()),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.getCallerFramePointer) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCallerFramePointer) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getCompiledMethodID) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTempInt();
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCompiledMethodID) {
      OPT_Operand val = bc2ir.popInt();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getReturnAddress) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_RETURN_ADDRESS_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setReturnAddress) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_RETURN_ADDRESS_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.sysCall0) {
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, null));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_0) {
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, null));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_I) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, null, p1));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall1) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, null, p1));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall2) {
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, null, 
                                                  p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCallAD) {
      OPT_Operand p2 = bc2ir.popDouble();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, null,
                                                  p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall3) {
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create3(SYSCALL, op0, ip, null,
						  p1, p2, p3));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall4) {
      OPT_Operand p4 = bc2ir.popInt();
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create4(SYSCALL, op0, ip, null,
						  p1, p2, p3, p4));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.roundToZero) {
      bc2ir.appendInstruction(Empty.create(ROUND_TO_ZERO));
    } else if (methodName == VM_MagicNames.clearFloatingPointState) {
      bc2ir.appendInstruction(Empty.create(CLEAR_FLOATING_POINT_STATE));
    } else {
      // Distinguish between magics that we know we don't implement
      // (and never plan to implement) and those (usually new ones) 
      // that we want to be warned that we don't implement.
      String msg = " Magic method not implemented: " + meth;
      if (methodName == VM_MagicNames.returnToNewStack || 
	  methodName == VM_MagicNames.pragmaNoOptCompile) {
	throw OPT_MagicNotImplementedException.EXPECTED(msg);
      } else {
	throw OPT_MagicNotImplementedException.UNEXPECTED(msg);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import  java.util.Vector;

/**
 * This class specifies the order in which OPT_CompilerPhases are
 * executed in the target-specific backend of the optimzing compiler.
 * The methods LIR2MIR, MIROptimizations, and MIR2MC each specify the
 * elements that make up the main compilation stages.
 *
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind */
class OPT_MIROptimizationPlanner extends OPT_OptimizationPlanner {

  /**
   * Initialize the "master plan" for the IA32 backend of the opt compiler.
   */
  static void intializeMasterPlan(Vector temp) {
    LIR2MIR(temp);
    MIROptimizations(temp);
    MIR2MC(temp);
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert LIR to IA32 MIR.
   *
   * @param p the plan under construction
   */
  private static void LIR2MIR(Vector p) {
    composeComponents(p, "Convert LIR to MIR", new Object[] {
      // Split very large basic blocks into smaller ones.
      new OPT_SplitBasicBlock(), 
      // Optional printing of final LIR
      new OPT_IRPrinter("Final LIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_FINAL_LIR;
        }
      }, 
      // Change operations that split live ranges to moves
      new OPT_MutateSplits(),
      // Instruction Selection
      new OPT_ConvertLIRtoMIR(), 
      // For now, always print the Initial MIR
      new OPT_IRPrinter("Initial MIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_MIR;
        }
      }
    });
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed on IA32 MIR.
   *
   * @param p the plan under construction
   */
  private static void MIROptimizations(Vector p) {
    // NullCheck combining and validation operand removal.
    addComponent(p, new OPT_NullCheckCombining());

    // Register Allocation
    composeComponents(p, "Register Mapping", new Object[] {
      new OPT_MIRSplitRanges(),
      // MANDATORY: Expand calling convention
      new OPT_ExpandCallingConvention(),
      // MANDATORY: Insert defs/uses due to floating-point stack
      new OPT_ExpandFPRStackConvention(),
      // MANDATORY: Perform Live analysis and create GC maps
      new OPT_LiveAnalysis(true, false),
      // MANDATORY: Perform register allocation
      new OPT_RegisterAllocator(),
      // MANDATORY: Add prologue and epilogue
      new OPT_PrologueEpilogueCreator(),
    });
    // Peephole branch optimizations
    addComponent(p, new OPT_MIRBranchOptimizations(1));
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert IA32 MIR into
   * ready-to-execute machinecode (and associated mapping tables).
   *
   * @param p the plan under construction
   */
  private static void MIR2MC(Vector p) {
    // MANDATORY: Final assembly
    addComponent(p, new OPT_ConvertMIRtoMC());
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Enumeration;

/**
 * This class contains IA32 calling conventions
 * The two public methods are:
 *  (1) expandCallingConventions(OPT_IR) which is called by the 
 *  register allocator immediately before allocation to make manifest the 
 *  use of registers by the calling convention.
 *  (2) expandSysCall(OPT_Instruction, OPT_IR) which is called to expand 
 *  a SYSCALL HIR instruction into the appropriate sequence of 
 *  LIR instructions.
 *
 * TODO: Much of this code could still be factored out as
 * architecture-independent.
 *
 * @author Dave Grove
 * @author Stephen Fink
 */
final class OPT_CallingConvention extends OPT_IRTools
  implements OPT_Operators,
	     OPT_PhysicalRegisterConstants {

  /**
   * Size of a word, in bytes
   */
  private static final int WORDSIZE = 4;

  /**
   * Expand calling conventions to make physical registers explicit in the
   * IR when required for calls, returns, and the prologue.
   */
  public static void expandCallingConventions(OPT_IR ir)  {
    // expand each call and return instruction
    for (OPT_Instruction inst = ir.firstInstructionInCodeOrder(); 
         inst != null; inst = inst.nextInstructionInCodeOrder()) {
      if (inst.isCall()) {
        callExpand(inst, ir);
      } else if (inst.isReturn()) {
        returnExpand(inst, ir);
      }
    }

    // expand the prologue instruction
    expandPrologue(ir);
  }

  /**
   * Expand the calling convention for a particular call instruction
   */
  private static void callExpand(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    boolean isSysCall = call.operator() == IA32_SYSCALL;

    // 0. Handle the parameters
    int parameterBytes = isSysCall ? expandParametersToSysCall(call,ir) : 
      expandParametersToCall(call,ir);

    // 1. Clear the floating-point stack if dirty.
    if (call.operator != CALL_SAVE_VOLATILE) {
      int FPRRegisterParams= countFPRParams(call);
      FPRRegisterParams = Math.min(FPRRegisterParams, 
				   phys.getNumberOfFPRParams());
      call.insertBefore(MIR_UnaryNoRes.create(IA32_FCLEAR,
					      I(FPRRegisterParams)));
    }
    
    // 2. Move the return value into a register
    expandResultOfCall(call,ir);
    
    // 3. If this is an interface invocation, set up the hidden parameter
    //    in the processor object to hold the interface signature id.
    if (VM.BuildForIMTInterfaceInvocation) {
      if (MIR_Call.hasMethod(call)) {
	OPT_MethodOperand mo = MIR_Call.getMethod(call);
        if (mo.isInterface()) {
          int signatureId = VM_ClassLoader.
            findOrCreateInterfaceMethodSignatureId(mo.method.getName(), 
                                                   mo.method.getDescriptor());
          OPT_MemoryOperand M = OPT_MemoryOperand.BD
            (R(phys.getPR()), VM_Entrypoints.hiddenSignatureIdField.getOffset(), 
             (byte)WORDSIZE, null, null);
          call.insertBefore(MIR_Move.create(IA32_MOV,M,I(signatureId)));
        }
      }
    }

    // 4. ESP must be parameterBytes before call, will be at either parameterBytes
    //    or 0 afterwards depending on whether or it is an RVM method or a sysCall.
    call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes)));
    call.insertAfter(MIR_UnaryNoRes.create(ADVISE_ESP, I(isSysCall?parameterBytes:0)));
  }

  /**
   * Expand the calling convention for a particular return instruction
   */
  private static void returnExpand(OPT_Instruction ret, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    if (MIR_Return.hasVal(ret)) {
      OPT_Operand symb1 = MIR_Return.getClearVal(ret);
      MIR_Return.setVal(ret, null);
      VM_Type type = symb1.getType();
      if (type.isFloatType() || type.isDoubleType()) {
        OPT_Register r = phys.getReturnFPR();
        OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
	ret.insertBefore(MIR_Move.create(IA32_FMOV, rOp, symb1));
	MIR_Return.setVal(ret, rOp.copyD2U());
      } else {
        OPT_Register r = phys.getFirstReturnGPR();
        OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
	ret.insertBefore(MIR_Move.create(IA32_MOV, rOp, symb1));
	MIR_Return.setVal(ret, rOp.copyD2U());
      }
    }

    if (MIR_Return.hasVal2(ret)) {
      OPT_Operand symb2 = MIR_Return.getClearVal2(ret);
      MIR_Return.setVal2(ret,null);
      VM_Type type = symb2.getType();
      OPT_Register r = phys.getSecondReturnGPR();
      OPT_RegisterOperand rOp= new OPT_RegisterOperand(r, type);
      ret.insertBefore(MIR_Move.create(IA32_MOV, rOp, symb2));
      MIR_Return.setVal2(ret, rOp.copyD2U());
    }

    // Clear the floating-point stack if dirty.
    int nSave=0;
    if (MIR_Return.hasVal(ret)) {
      OPT_Operand symb1 = MIR_Return.getClearVal(ret);
      VM_Type type = symb1.getType();
      if (type.isFloatType() || type.isDoubleType()) {
	nSave=1;
      }
    }
    ret.insertBefore(MIR_UnaryNoRes.create(IA32_FCLEAR,I(nSave)));

    // Set the first 'Val' in the return instruction to hold an integer
    // constant which is the number of words to pop from the stack while 
    // returning from this method.
    MIR_Return.setPopBytes(ret, I(ir.incomingParameterBytes()));
  }

  /**
   * Explicitly copy the result of a call instruction from the result
   * register to the appropriate symbolic register,
   * as defined by the calling convention.
   */
  private static void expandResultOfCall(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // copy the first result parameter
    if (MIR_Call.hasResult(call)) {
      OPT_RegisterOperand result1 = MIR_Call.getClearResult(call);
      MIR_Call.setResult(call,null);
      if (result1.type.isFloatType() || result1.type.isDoubleType()) {
        OPT_Register r = phys.getReturnFPR();
        OPT_RegisterOperand physical = 
	  new OPT_RegisterOperand(r, result1.type);
        OPT_Instruction tmp = MIR_Move.create(IA32_FMOV, result1, physical);
        call.insertAfter(tmp);
        MIR_Call.setResult(call, null);
      } else {
        // first GPR result register
        OPT_Register r = phys.getFirstReturnGPR();
        OPT_RegisterOperand physical = 
	  new OPT_RegisterOperand(r, result1.type);
        OPT_Instruction tmp = MIR_Move.create(IA32_MOV, result1, physical);
        call.insertAfter(tmp);
        MIR_Call.setResult(call, null);
      }
    }

    // copy the second result parameter
    if (MIR_Call.hasResult2(call)) {
      OPT_RegisterOperand result2 = MIR_Call.getClearResult2(call);
      MIR_Call.setResult2(call,null);
      // second GPR result register
      OPT_Register r = phys.getSecondReturnGPR();
      OPT_RegisterOperand physical = 
	new OPT_RegisterOperand(r, result2.type);
      OPT_Instruction tmp = MIR_Move.create(IA32_MOV, result2, physical);
      call.insertAfter(tmp);
      MIR_Call.setResult2(call, null);
    }
  }


  /**
   * Explicitly copy parameters to a call into the appropriate physical
   * registers as defined by the calling convention.
   *
   * Note: Assumes that ESP points to the word before the slot where the
   * first parameter should be stored.
   */
  private static int expandParametersToCall(OPT_Instruction call, OPT_IR ir) {
    int nGPRParams = 0;
    int nFPRParams = 0;

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    // count the number FPR parameters in a pre-pass
    int FPRRegisterParams= countFPRParams(call);
    FPRRegisterParams = Math.min(FPRRegisterParams, 
                                 phys.getNumberOfFPRParams());

    // offset, in bytes, from the SP, for the next parameter slot on the
    // stack
    int parameterBytes = 0;
    OPT_Register ESP = phys.getESP();

    // Require ESP to be at bottom of frame before a call,
    call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(0)));

    // walk over each parameter
    // must count then before we start nulling them out!
    int numParams = MIR_Call.getNumberOfParams(call); 
    int nParamsInRegisters = 0;
    for (int i = 0; i < numParams;  i++) {
      OPT_Operand param = MIR_Call.getClearParam(call,i);
      MIR_Call.setParam(call,i,null);
      VM_Type paramType = param.getType();
      if (paramType.isFloatType() || paramType.isDoubleType()) {
	nFPRParams++;
	int size = paramType.isFloatType() ? 4 : 8;
	parameterBytes -= size;
	if (nFPRParams > phys.getNumberOfFPRParams()) {
	  // pass the FP parameter on the stack
	  OPT_Operand M =
	    new OPT_StackLocationOperand(false, parameterBytes, size);
	  call.insertBefore(MIR_Move.create(IA32_FMOV, M, param));
	} else {
	  // Pass the parameter in a register.
	  // Note that if k FPRs are passed in registers, 
	  // the 1st goes in F(k-1),
	  // the 2nd goes in F(k-2), etc...
	  OPT_Register phy = 
	    phys.getFPRParam(FPRRegisterParams - nFPRParams);
	  OPT_RegisterOperand real = new OPT_RegisterOperand(phy, paramType);
	  call.insertBefore(MIR_Move.create(IA32_FMOV, real, param));
	  // Record that the call now has a use of the real register.
	  MIR_Call.setParam(call,nParamsInRegisters++,real.copy());
	}
      } else {
	nGPRParams++;
	parameterBytes -= 4;
	if (nGPRParams > phys.getNumberOfGPRParams()) {
	  // Too many parameters to pass in registers.  Write the
	  // parameter into the appropriate stack frame location.
	  call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes + 4)));
	  call.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, param));
	} else {
	  // Pass the parameter in a register.
	  OPT_Register phy = phys.getGPRParam(nGPRParams-1);
	  OPT_RegisterOperand real = new OPT_RegisterOperand(phy, paramType);
	  call.insertBefore(MIR_Move.create(IA32_MOV, real, param));
	  // Record that the call now has a use of the real register.
	  MIR_Call.setParam(call,nParamsInRegisters++,real.copy());       
	}
      }
    }
    return parameterBytes;
  }

  /**
   * Save and restore all nonvolatile registers around a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * Side effect: changes the operator of the call instruction to
   * IA32_CALL.
   *
   * @param call the sys call
   */
  static void saveNonvolatilesAroundSysCall(OPT_Instruction call, OPT_IR ir) {
    saveNonvolatilesBeforeSysCall(call, ir); 
    restoreNonvolatilesAfterSysCall(call, ir);
    call.operator = IA32_CALL;
  }

  /**
   * Save all nonvolatile registers before a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * @param call the sys call
   */
  static void saveNonvolatilesBeforeSysCall(OPT_Instruction call, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    OPT_Instruction result = null;

    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    // get the offset into the stack frame of where to stash the first
    // nonvolatile for this case.
    int location = sm.getOffsetForSysCall();
    
    // save each non-volatile
    for (Enumeration e = phys.enumerateNonvolatileGPRs();
         e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      OPT_Operand M = 
	new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
      call.insertBefore(MIR_Move.create(IA32_MOV, M, R(r)));
      location += WORDSIZE;
    }
    
    // save the processor register
    OPT_Register PR = phys.getPR();
    OPT_Operand M = 
      new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
    call.insertBefore(MIR_Move.create(IA32_MOV, M, R(PR)));
  }
  /**
   * Restore all nonvolatile registers after a syscall.  
   * We do this in case the sys call does not respect our
   * register conventions.
   *
   * We save/restore all nonvolatiles and the PR, whether
   * or not this routine uses them.  This may be a tad inefficient, but if
   * you're making a system call, you probably don't care.
   *
   * @param call the sys call
   */
  static void restoreNonvolatilesAfterSysCall(OPT_Instruction call,
                                              OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    
    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    // get the offset into the stack frame of where to stash the first
    // nonvolatile for this case.
    int location = sm.getOffsetForSysCall();
    
    // restore each non-volatile
    for (Enumeration e = phys.enumerateNonvolatileGPRs();
         e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      OPT_Operand M = 
	new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
      call.insertAfter(MIR_Move.create(IA32_MOV, R(r), M));
      location += WORDSIZE;
    }
    
    // restore the processor register
    OPT_Register PR = phys.getPR();
    OPT_Operand M = 
      new OPT_StackLocationOperand(true, -location, (byte)WORDSIZE);
    call.insertAfter(MIR_Move.create(IA32_MOV, R(PR), M));
  }

  /**
   * Explicitly copy parameters to a system call into the appropriate physical
   * registers as defined by the calling convention.  Note that for a system
   * call (ie., a call to C), the order of parameters on the stack is
   * <em> reversed </em> compared to the normal RVM calling convention
   *
   * TODO: much of this code is exactly the same as in expandParametersToCall().
   *       factor out the common code.
   *
   * Note: Assumes that ESP points to the word before the slot where the
   * first parameter should be stored.
   */
  private static int expandParametersToSysCall(OPT_Instruction call, OPT_IR ir){
    int nGPRParams = 0;
    int nFPRParams = 0;
    int parameterBytes = 0;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    OPT_Register ESP = phys.getESP();
    // count the number FPR parameters in a pre-pass
    int FPRRegisterParams= countFPRParams(call);
    FPRRegisterParams = Math.min(FPRRegisterParams, phys.getNumberOfFPRParams());
    
    // walk over the parameters in reverse order
    // NOTE: All params to syscall are passed on the stack!
    int numParams = MIR_Call.getNumberOfParams(call); 
    for (int i = numParams-1; i >=0;  i--) {
      OPT_Operand param = MIR_Call.getClearParam(call,i);
      MIR_Call.setParam(call,i,null);
      VM_Type paramType = param.getType();
      if (paramType.isFloatType() || paramType.isDoubleType()) {
	nFPRParams++;
	int size = paramType.isFloatType() ? 4 : 8;
	parameterBytes -= size;
	OPT_Operand M = 
	  new OPT_StackLocationOperand(false, parameterBytes, size);
	call.insertBefore(MIR_Move.create(IA32_FMOV, M, param));
      } else {
	nGPRParams++;
	parameterBytes -= 4;
	call.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(parameterBytes + 4)));
	call.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, param));
      }
    }
    return parameterBytes;
  }

  /**
   * We have to save/restore the non-volatile registers around syscalls,
   * to protect ourselves from malicious C compilers and Linux kernels.
   * 
   * Although the register allocator is not yet ready to insert these
   * spills, allocate space on the stack in preparation.
   *
   * For now, we naively save/restore all nonvolatiles.
   */
  public static void allocateSpaceForSysCall(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_StackManager sm = (OPT_StackManager)ir.stackManager;
    
    // add one to account for the processor register.  
    int nToSave = phys.getNumberOfNonvolatileGPRs() + 1;

    sm.allocateSpaceForSysCall(nToSave);
  }

  /**
   * Calling convention to implement calls to native (C) routines 
   * using the Linux linkage conventions.
   */
  public static void expandSysCall(OPT_Instruction s, OPT_IR ir) {
    
    // Determine the address of the method to call.
    OPT_RegisterOperand ip = null;
    if (CallSpecial.getMethod(s) != null) {
      OPT_SysMethodOperand sysM = 
	(OPT_SysMethodOperand)CallSpecial.getClearMethod(s);
      OPT_RegisterOperand t1 = 
	OPT_ConvertToLowLevelIR.getStatic(s, ir, VM_Entrypoints.the_boot_recordField);
      ip = OPT_ConvertToLowLevelIR.getField(s, ir, t1, sysM.ip);
    } else {
      ip = (OPT_RegisterOperand)CallSpecial.getClearAddress(s);
    }

    // Allocate space to save non-volatiles.
    allocateSpaceForSysCall(ir);
    
    // Make sure we allocate enough space for the parameters to this call.
    int numberParams = CallSpecial.getNumberOfParams(s);
    int parameterWords = 0;
    for (int i = 0; i < numberParams; i++) {
      parameterWords++;
      OPT_Operand op = CallSpecial.getParam(s, i);
      if (op instanceof OPT_RegisterOperand) {
        OPT_RegisterOperand reg = (OPT_RegisterOperand)op;
        if ((reg.type == VM_Type.LongType) || (reg.type == VM_Type.DoubleType))
          parameterWords++;
      } else if ((op instanceof OPT_LongConstantOperand) || 
                 (op instanceof OPT_DoubleConstantOperand)) {
        parameterWords++;
      }
    }
    // allocate space for each parameter, plus one word on the stack to
    // hold the address of the callee.
    ir.stackManager.allocateParameterSpace((1 + parameterWords)*4);
                                                   
    // Convert to a SYSCALL instruction with a null method operand.
    CallSpecial.mutate0(s, SYSCALL, CallSpecial.getClearResult(s), ip, null);
  }

  /**
   * Count the number of FPR parameters in a call instruction.
   */
  private static int countFPRParams(OPT_Instruction call) {
    int result = 0;
    // walk over the parameters 
    int numParams = MIR_Call.getNumberOfParams(call); 
    for (int i = 0; i <numParams;  i++) {
      OPT_Operand param = MIR_Call.getParam(call,i);
      if (param.isRegister()) {
        OPT_RegisterOperand symb = (OPT_RegisterOperand)param;
        if (symb.type.isFloatType() || symb.type.isDoubleType()) {
          result++;
        }
      }
    }
    return result;
  }
  /**
   * Count the number of FPR parameters in a prologue instruction.
   */
  private static int countFPRParamsInPrologue(OPT_Instruction p) {
    int result = 0;
    // walk over the parameters 
    for (OPT_OperandEnumeration e = p.getDefs(); e.hasMoreElements(); ) {
      OPT_Operand param = (OPT_Operand)e.nextElement();
      if (param.isRegister()) {
        OPT_RegisterOperand symb = (OPT_RegisterOperand)param;
        if (symb.type.isFloatType() || symb.type.isDoubleType()) {
          result++;
        }
      }
    }
    return result;
  }

  /**
   * Expand the prologue instruction.
   */
  private static void expandPrologue(OPT_IR ir) {
    if (ir.options.SIMPLE_OPT) {
      // set up register lists for dead code elimination.
      OPT_DefUse.computeDU(ir);
    }
    
    OPT_Instruction p = ir.firstInstructionInCodeOrder().
      nextInstructionInCodeOrder();
    if (VM.VerifyAssertions) VM.assert(p.operator == IR_PROLOGUE);
    OPT_Instruction start = p.nextInstructionInCodeOrder();
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    int gprIndex = 0; 
    int fprIndex = 0; 
    int paramByteOffset = ir.incomingParameterBytes() + 8;

    // count the number of FPR params in a pre-pass
    int FPRRegisterParams= countFPRParamsInPrologue(p);
    FPRRegisterParams = Math.min(FPRRegisterParams, phys.getNumberOfFPRParams());
    ir.MIRInfo.fpStackHeight = Math.max(ir.MIRInfo.fpStackHeight, FPRRegisterParams);

    // deal with each parameter
    for (OPT_OperandEnumeration e = p.getDefs(); e.hasMoreElements(); ) {
      OPT_RegisterOperand symbOp = (OPT_RegisterOperand)e.nextElement();
      VM_Type rType = symbOp.type;
      if (rType.isFloatType() || rType.isDoubleType()) {
	int size = rType.isFloatType() ? 4 : 8;
	paramByteOffset -= size;
        // if optimizing, only define the register if it has uses
        if (!ir.options.SIMPLE_OPT || symbOp.register.useList != null) {
          if (fprIndex < phys.getNumberOfFPRParams()) {
            // insert a MOVE symbolic register = parameter
            // Note that if k FPRs are passed in registers, 
            // the 1st goes in F(k-1),
            // the 2nd goes in F(k-2), etc...
            OPT_Register param = 
	      phys.getFPRParam(FPRRegisterParams - fprIndex - 1);
            start.insertBefore(MIR_Move.create(IA32_FMOV,symbOp.copyRO(),
					       D(param)));
          } else {
	    OPT_Operand M = 
	      new OPT_StackLocationOperand(true, paramByteOffset, size);
	    start.insertBefore(MIR_Move.create(IA32_FMOV, symbOp.copyRO(), M));
          }
        }
        fprIndex++;
      } else {
        // if optimizing, only define the register if it has uses
	paramByteOffset -= 4;
        if (!ir.options.SIMPLE_OPT || symbOp.register.useList != null) {
          // t is object, 1/2 of a long, int, short, char, byte, or boolean
          if (gprIndex < phys.getNumberOfGPRParams()) {
	    // to give the register allocator more freedom, we
	    // insert two move instructions to get the physical into
	    // the symbolic.  First a move from the physical to a fresh temp 
	    // before start and second a move from the temp to the
	    // 'real' parameter symbolic after start.
	    OPT_RegisterOperand tmp = ir.gc.temps.makeTemp(rType);
            OPT_Register param = phys.getGPRParam(gprIndex);
	    OPT_RegisterOperand pOp = new OPT_RegisterOperand(param, rType);
            start.insertBefore(OPT_PhysicalRegisterTools.makeMoveInstruction(tmp,pOp));
	    OPT_Instruction m2 = OPT_PhysicalRegisterTools.makeMoveInstruction(symbOp.copyRO(),tmp.copyD2U());
	    start.insertBefore(m2);
	    start = m2;
          } else {
	    OPT_Operand M = 
	      new OPT_StackLocationOperand(true, paramByteOffset, 4);
	    start.insertBefore(MIR_Move.create(IA32_MOV, symbOp.copyRO(), M));
          }
        }
        gprIndex++;
      }
    }

    if (VM.VerifyAssertions) VM.assert(paramByteOffset == 8, "pb = "+paramByteOffset);
    
    // Now that we've made the calling convention explicit in the prologue,
    // set IR_PROLOGUE to have no defs.
    p.replace(Prologue.create(IR_PROLOGUE, 0));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$ 

import java.util.Enumeration;
import instructionFormats.*;

/**
 * At the beginning of each basic block, the register allocator expects
 * all floating-point stack locations to be available, and named
 * FPi, 0 < i < 7
 *
 * <p>However, BURS may consume FP stack locations by inserting instructions
 * that push or pop the floating-point stack.  This phase inserts dummy
 * definitions and uses to indicate when symbolic FP registers are not
 * available for register allocation since BURS has consumed a stack slot.
 *
 * For example,
 * <pre>
 *    FLD t1
 *    ...
 *    FSTP M, t1
 * </pre>
 *
 * will be modified by this phase to indicate that FP6 is not available
 * for allocation in the interval:
 *
 * <pre>
 *   DUMMY_DEF FP6
 *   FLD t1
 *   .....
 *   FSTP M, t1
 *   DUMMY_USE FP6
 * </pre>
 *
 * <p> Additionally, by convention, we will always clear the
 * floating-point stack when delivering an exception.  To model this, we
 * insert dummy defs and uses for each floating-point register at the
 * beginning of each catch block.
 *
 * @author Stephen Fink
 */

final class OPT_ExpandFPRStackConvention extends OPT_CompilerPhase
implements OPT_Operators{

  // The number of FPRs available for allocation.
  // Normally 7: we reserve one for final MIR expansion.
  int NUM_ALLOCATABLE_FPR = 7;

  boolean printingEnabled (OPT_Options options, boolean before) {
    return  options.PRINT_CALLING_CONVENTIONS && !before;
  }

  final boolean shouldPerform(OPT_Options options) { 
    return true; 
  }

  final String getName() { 
    return "Expand Calling Convention"; 
  }

  /**
   * Insert the needed dummy defs and uses.
   */
  final void perform(OPT_IR ir)  {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    for (Enumeration b = ir.getBasicBlocks(); b.hasMoreElements(); ) {
      OPT_BasicBlock bb = (OPT_BasicBlock)b.nextElement();
      
      if (bb instanceof OPT_ExceptionHandlerBasicBlock) {
        // clear all floating-point state at the entry to a catch block
        for (int i=0; i<NUM_ALLOCATABLE_FPR; i++) {
          OPT_Register fpr = phys.getFPR(i);
          bb.prependInstruction(MIR_UnaryNoRes.create(DUMMY_USE,
                                                      OPT_IRTools.D(fpr)));
          bb.prependInstruction(MIR_Nullary.create(DUMMY_DEF,
                                                   OPT_IRTools.D(fpr)));
        }
      }
      
      // The following holds the floating point stack offset from its
      // 'normal' position.
      int fpStackOffset = 0;

      for (Enumeration inst = bb.forwardInstrEnumerator(); 
           inst.hasMoreElements();) {
        OPT_Instruction s = (OPT_Instruction)inst.nextElement();
        if (s.operator().isFpPop()) {
          // A pop instruction 'ends' a dummy live range.
          OPT_Register fpr = phys.getFPR(NUM_ALLOCATABLE_FPR-fpStackOffset);
          s.insertAfter(MIR_UnaryNoRes.create(DUMMY_USE,OPT_IRTools.D(fpr)));
          fpStackOffset--;
        } else if (s.operator().isFpPush()) {
          fpStackOffset++;
          OPT_Register fpr = phys.getFPR(NUM_ALLOCATABLE_FPR-fpStackOffset);
          s.insertBefore(MIR_Nullary.create(DUMMY_DEF,OPT_IRTools.D(fpr)));
        }
        if (VM.VerifyAssertions) VM.assert(fpStackOffset >= 0);
        if (VM.VerifyAssertions) VM.assert(fpStackOffset <
                                           NUM_ALLOCATABLE_FPR);
      }
    }
  } 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import instructionFormats.*;

/**
 * This class splits live ranges for certain special cases to ensure
 * correctness during IA32 register allocation.
 *
 * @author Stephen Fink
 */
class OPT_MIRSplitRanges extends OPT_CompilerPhase 
implements OPT_Operators {

  /**
   * Should this phase be performed?
   * @param options controlling compiler options
   * @return true or false
   */
  final boolean shouldPerform (OPT_Options options) {
    return true;
  }

  /**
   * Return the name of this phase
   * @return "Live Range Splitting"
   */
  final String getName () {
    return "MIR Range Splitting"; 
  }

  public boolean printingEnabled(OPT_Options options, boolean before) {
    return false;
  } 

  /**
   * The main method.
   * 
   * We split live ranges for registers around PEIs which have catch
   * blocks.  Suppose we have a
   * PEI s which uses a symbolic register r1.  We must ensure that after
   * register allocation, r1 is NOT assigned to a scratch location in s,
   * since this would mess up code in the catch block tghat uses r1.
   *
   * So, instead, we introduce a new temporary r2 which holds the value of
   * r1.  The live range for r2 spans only the instruction s.  Later, we
   * will ensure that r2 is never spilled.
   * 
   * TODO: This could be implemented more efficiently.
   *
   * @param ir the governing IR
   */
  final public void perform (OPT_IR ir) {

    java.util.HashMap newMap = new java.util.HashMap(5);

    for (Enumeration be = ir.getBasicBlocks(); be.hasMoreElements(); ) {
      OPT_BasicBlock bb = (OPT_BasicBlock)be.nextElement();
      for (OPT_InstructionEnumeration ie  = bb.forwardInstrEnumerator(); 
           ie.hasMoreElements(); ) {
        OPT_Instruction s = ie.next();

        // clear the cache of register assignments
        newMap.clear();

        // Split live ranges at PEIs and a few special cases to 
        // make sure we can pin values that must be in registers.
        // NOTE: Any operator that is an IA32 special case that must have
        //       a particular operand in a register must be mentioned both
        //       here and in OPT_RegisterRestrictions!
        if (s.isPEI() && s.operator != IR_PROLOGUE) {
          if (bb.hasApplicableExceptionalOut(s) ||
              !OPT_RegisterRestrictions.SCRATCH_IN_PEI) {
            splitAllLiveRanges(s, newMap, ir, false);
          }
        }

        // handle special cases for IA32
        //  (1) Some operands must be in registers
        switch (s.getOpcode()) {
          case MIR_LOWTABLESWITCH_opcode:
            {
              OPT_RegisterOperand rOp = MIR_LowTableSwitch.getIndex(s);
              OPT_RegisterOperand temp = findOrCreateTemp(rOp, newMap, ir);
              // NOTE: Index as marked as a DU because LowTableSwitch is 
              //       going to destroy the value in the register.
              //       By construction (see ConvertToLowLevelIR), no one will
              //       every read the value computed by a LowTableSwitch.
              //       Therefore, don't insert a move instruction after the
              //       LowTableSwitch (which would cause IR verification 
              //       problems anyways, since LowTableSwitch is a branch).
              insertMoveBefore(temp, rOp.copyRO(), s); // move r into 'temp' before s
              rOp.register = temp.register;
            }
            break;
        }
      }
    }
  }

  /**
   * Split the live ranges of all register operands of an instruction
   * @param s      the instruction to process
   * @param map a mapping from symbolics to temporaries
   * @param ir  the containing IR
   * @param rootOnly only consider root operands?
   */
  private static void splitAllLiveRanges(OPT_Instruction s, 
                                         java.util.HashMap newMap,
                                         OPT_IR ir,
					 boolean rootOnly) {
    // walk over each USE
    for (OPT_OperandEnumeration u = rootOnly?s.getRootUses():s.getUses(); 
	 u.hasMoreElements(); ) {
      OPT_Operand use = u.next();
      if (use.isRegister()) {
	OPT_RegisterOperand rUse = use.asRegister();
	OPT_RegisterOperand temp = findOrCreateTemp(rUse, newMap, ir);
	// move 'use' into 'temp' before s
	insertMoveBefore(temp, rUse.copyRO(), s);
      }
    }
    // walk over each DEF (by defintion defs == root defs)
    for (OPT_OperandEnumeration d = s.getDefs(); d.hasMoreElements(); ) {
      OPT_Operand def = d.next();
      if (def.isRegister()) {
	OPT_RegisterOperand rDef = def.asRegister();
	OPT_RegisterOperand temp = findOrCreateTemp(rDef ,newMap, ir);
	// move 'temp' into 'r' after s
	insertMoveAfter(rDef.copyRO(), temp, s);
      }
    }
    // Now go back and replace the registers.
    for (OPT_OperandEnumeration ops = rootOnly?s.getRootOperands():s.getOperands(); 
	 ops.hasMoreElements(); ) {
      OPT_Operand op = ops.next();
      if (op.isRegister()) {
	OPT_RegisterOperand rOp = op.asRegister();
	OPT_Register r = rOp.register;
	OPT_Register newR = (OPT_Register)newMap.get(r); 
	if (newR != null) {
	  rOp.register = newR;
	}
      }
    }
  }

  /**
   * Find or create a temporary register to cache a symbolic register.
   *
   * @param r the symbolic register
   * @param map a mapping from symbolics to temporaries
   * @param ir the governing IR
   */
  private static OPT_RegisterOperand findOrCreateTemp(OPT_RegisterOperand rOp,
						      java.util.HashMap map,
						      OPT_IR ir) {
    OPT_Register tReg = (OPT_Register)map.get(rOp.register);
    if (tReg == null) {
      OPT_RegisterOperand tOp = ir.regpool.makeTemp(rOp.type);
      map.put(rOp.register, tOp.register);
      return tOp;
    } else {
      return new OPT_RegisterOperand(tReg, rOp.type);
    }
  }

  /**
   * Insert an instruction to move r1 into r2 before instruction s
   */
  private static void insertMoveBefore(OPT_RegisterOperand r2, 
				       OPT_RegisterOperand r1,
                                       OPT_Instruction s) {
    OPT_Instruction m = OPT_PhysicalRegisterTools.makeMoveInstruction(r2,r1);
    s.insertBefore(m);
  }
  /**
   * Insert an instruction to move r1 into r2 after instruction s
   */
  private static void insertMoveAfter(OPT_RegisterOperand r2, 
				      OPT_RegisterOperand r1,
				      OPT_Instruction s) {
    OPT_Instruction m = OPT_PhysicalRegisterTools.makeMoveInstruction(r2,r1);
    s.insertAfter(m);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class provides utilities to record defs and uses of physical
 * registers by IR operators.
 *
 * @author Stephen Fink
 * @author Dave Grove
 */
class OPT_PhysicalDefUse {

  // constants used to encode defs/uses of physical registers
  final static int mask             = 0x0000;  // empty mask
  final static int maskAF           = 0x0001;
  final static int maskCF           = 0x0002;
  final static int maskOF           = 0x0004;
  final static int maskPF           = 0x0008;
  final static int maskSF           = 0x0010;
  final static int maskZF           = 0x0020;
  final static int maskC0           = 0x0040;
  final static int maskC1           = 0x0080;
  final static int maskC2           = 0x0100;
  final static int maskC3           = 0x0200;
  final static int maskPR           = 0x0400;
  // Meta mask for the enumeration.
  private final static int maskHIGH = 0x0400;
  private final static int maskALL  = 0x07FF;
  
  final static int maskCF_OF = maskCF | maskOF;
  final static int maskCF_PF_ZF = maskCF | maskPF | maskZF;
  final static int maskCF_OF_PF_SF_ZF = maskCF | maskOF | maskPF | maskSF | 
                                        maskZF;
  final static int maskAF_OF_PF_SF_ZF = maskAF | maskOF | maskPF | maskSF | 
                                        maskZF;
  final static int maskAF_CF_OF_PF_SF_ZF = maskAF | maskCF | maskOF |
                                           maskPF | maskSF | maskZF;
  final static int maskC0_C1_C2_C3 = maskC0 | maskC1 | maskC2 | maskC3;
  final static int maskcallDefs = maskAF_CF_OF_PF_SF_ZF;
  final static int maskcallUses = mask;
  final static int maskIEEEMagicUses = mask;
  final static int maskTSPUses = mask;
  final static int maskTSPDefs = maskAF_CF_OF_PF_SF_ZF | maskPR;


  /**
   * @return whether or not an OPT_Operator uses the EFLAGS
   */
  static boolean usesEFLAGS(OPT_Operator op) {
    return (op.implicitUses & maskAF_CF_OF_PF_SF_ZF) != 0;
  }
			      
  /**
   * @return whether or not an OPT_Operator uses the EFLAGS
   */
  static boolean definesEFLAGS(OPT_Operator op) {
    return (op.implicitDefs & maskAF_CF_OF_PF_SF_ZF) != 0;
  }
			      

  /**
   * @return a string representation of the physical registers encoded by
   * an integer
   */
  static String getString(int code) {
    if (code == mask) return "";
    if (code == maskAF_CF_OF_PF_SF_ZF) return " AF CF OF PF SF ZF";
    // Not a common case, construct it...
    String s = "";
    if ((code & maskAF) != 0) s += " AF";
    if ((code & maskCF) != 0) s += " CF";
    if ((code & maskOF) != 0) s += " OF";
    if ((code & maskPF) != 0) s += " PF";
    if ((code & maskZF) != 0) s += " ZF";
    if ((code & maskC0) != 0) s += " CO";
    if ((code & maskC1) != 0) s += " C1";
    if ((code & maskC2) != 0) s += " C2";
    if ((code & maskC3) != 0) s += " C3";
    if ((code & maskPR) != 0) s += " PR";
    return s;
  }

  /**
   * @param code an integer that encodes a set of physical registers
   * @param ir the governing IR
   * @return an enumeration of the physical registers embodied by a code
   */
  static PDUEnumeration enumerate(int code, OPT_IR ir) {
    return new PDUEnumeration(code,ir);
  }

  /**
   * @param ir the governing IR
   * @return an enumeration of all physical registers that code be 
   *         implicitly defed/used
   */
  static PDUEnumeration enumerateAllImplicitDefUses(OPT_IR ir) {
    return new PDUEnumeration(maskALL,ir);
  }

  /**
   * A class to enumerate physical registers based on a code.
   */
  static final class PDUEnumeration implements Enumeration {
    private int code;
    private int curMask;
    private OPT_PhysicalRegisterSet phys;
    
    PDUEnumeration(int c, OPT_IR ir) {
      phys = ir.regpool.getPhysicalRegisterSet();
      code = c;
      curMask = maskHIGH;
    }

    public boolean hasMoreElements() {
      return code != 0;
    }

    public Object nextElement() {
      while (true) {
	int curBit = code & curMask;
	code -= curBit;
	curMask = curMask >> 1;
	if (curBit != 0) return getReg(curBit, phys);
      }
    }

    // artifically make static to enable scalar replacement of 
    // enumeration object without requiring this method to be inlined.
    private static OPT_Register getReg(int m, OPT_PhysicalRegisterSet phys) {
      switch(m) {
      case maskAF: return phys.getAF();
      case maskCF: return phys.getCF();
      case maskOF: return phys.getOF();
      case maskPF: return phys.getPF();
      case maskSF: return phys.getSF();
      case maskZF: return phys.getZF();
      case maskC0: return phys.getC0();
      case maskC1: return phys.getC1();
      case maskC2: return phys.getC2();
      case maskC3: return phys.getC3();
      case maskPR: return phys.getPR();
      }
      OPT_OptimizingCompilerException.UNREACHABLE();
      return null; // placate jikes.
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class holds constants that describe IA32 physical register set.
 *
 * @author Stephen Fink
 */
interface OPT_PhysicalRegisterConstants extends VM_RegisterConstants {

  // Types of values stored in physical registers; 
  // These affect instruction selection for accessing
  // the data
  static final byte INT_VALUE= 0;
  static final byte DOUBLE_VALUE = 1;
  static final byte FLOAT_VALUE = 2;
  static final byte CONDITION_VALUE = 3;
  
  // There are different types of hardware registers, so we define
  // the following register classes:
  // NOTE: they must be in consecutive ordering
  // TODO: Kill this?
  static final byte INT_REG = 0;
  static final byte DOUBLE_REG = 1;
  static final byte SPECIAL_REG = 2;
  static final byte NUMBER_TYPE = 3;

  // Derived constants for use by the register pool.
  // In the register pool, the physical registers are assigned integers
  // based on these constants.
  static final int FIRST_INT = 0;
  static final int FIRST_DOUBLE = NUM_GPRS;
  static final int FIRST_SPECIAL = NUM_GPRS + NUM_FPRS;

  // special intel registers or register sub-fields.
  static final int NUM_SPECIALS = 10;
  static final int AF = FIRST_SPECIAL + 0;      // AF bit of EFLAGS
  static final int CF = FIRST_SPECIAL + 1;      // CF bit of EFLAGS
  static final int OF = FIRST_SPECIAL + 2;      // OF bit of EFLAGS
  static final int PF = FIRST_SPECIAL + 3;      // PF bit of EFLAGS
  static final int SF = FIRST_SPECIAL + 4;      // SF bit of EFLAGS
  static final int ZF = FIRST_SPECIAL + 5;      // ZF bit of EFLAGS
  static final int C0 = FIRST_SPECIAL + 6;      // FP status bit
  static final int C1 = FIRST_SPECIAL + 7;      // FP status bit
  static final int C2 = FIRST_SPECIAL + 8;      // FP status bit
  static final int C3 = FIRST_SPECIAL + 9;      // FP status bit
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class represents a set of OPT_Registers corresponding to the
 * IA32 register set.
 *
 * @author Stephen Fink
 */
final class OPT_PhysicalRegisterSet extends OPT_GenericPhysicalRegisterSet
implements VM_RegisterConstants, OPT_PhysicalRegisterConstants {

  /**
   * This array holds a pool of objects representing physical registers
   */
  private OPT_Register[] reg = new OPT_Register[getSize()];

  /**
   * Cache the set of volatile registers for efficiency
   */
  private OPT_BitSet volatileSet;

  /**
   * Cache the set of floating-point registers for efficiency
   */
  private OPT_BitSet fpSet;

  /**
   * Return the total number of physical registers.
   */
  static final int getSize() {
    return NUM_GPRS + NUM_FPRS + NUM_SPECIALS;
  }

  /**
   * Return the total number of physical registers.
   */
  final int getNumberOfPhysicalRegisters() {
    return getSize();
  }

  /**
   * Return the total number of nonvolatile GPRs.
   */
  static final int getNumberOfNonvolatileGPRs() {
    return NUM_NONVOLATILE_GPRS;
  }

  /**
   * Return the total number of GPRs that may hold parameters.
   */
  static final int getNumberOfGPRParams() {
    return NUM_PARAMETER_GPRS;
  }

  /**
   * Return the total number of FPRs that may hold parameters.
   */
  static final int getNumberOfFPRParams() {
    return NUM_PARAMETER_FPRS;
  }


  /**
   * Return the (zero-based indexed) nth GPR that may hold a parameter.
   */
  final OPT_Register getGPRParam(int n) {
    if (VM.VerifyAssertions) VM.assert(n < 2);
    if (n==0) {
      return getEAX();
    } else {
      return getEDX();
    }
  }

  /**
   * Return the (zero-based indexed) nth FPR that may hold a parameter.
   */
  final OPT_Register getFPRParam(int n) {
    return getFPR(VOLATILE_FPRS[n]);
  }

  /**
   * Return the (zero-based indexed) nth GPR that may hold a return value.
   */
  OPT_Register getReturnGPR(int n) {
    if (VM.VerifyAssertions) VM.assert(n < 2);
    if (n==0) {
      return getEAX();
    } else {
      return getEDX();
    }
  }

  /**
   * Constructor: set up a pool of physical registers.
   */
  OPT_PhysicalRegisterSet() {

    // 1. Create all the physical registers in the pool.
    for (int i = 0; i < reg.length ; i++) {
      OPT_Register r = new OPT_Register(i);
      r.setPhysical();
      reg[i] = r;
    }

    // 2. Set the 'integer' attribute on each GPR
    for (int i = FIRST_INT; i < FIRST_DOUBLE; i++) {
      reg[i].setInteger();
    }

    // 3. Set the 'double' attribute on each FPR
    for (int i = FIRST_DOUBLE; i < FIRST_SPECIAL; i++) {
      reg[i].setDouble();
    }

    // 4. set up the volatile GPRs
    for (Enumeration e = enumerateVolatileGPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setVolatile();
    }

    // 5. set up the non-volatile GPRs
    for (Enumeration e = enumerateNonvolatileGPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setNonVolatile();
    }

    // 6. set properties on some special registers
    reg[AF].setSpansBasicBlock();
    reg[CF].setSpansBasicBlock();
    reg[OF].setSpansBasicBlock();
    reg[PF].setSpansBasicBlock();
    reg[SF].setSpansBasicBlock();
    reg[ZF].setSpansBasicBlock();
    reg[C0].setSpansBasicBlock();
    reg[C1].setSpansBasicBlock();
    reg[C2].setSpansBasicBlock();
    reg[C3].setSpansBasicBlock();
    reg[PROCESSOR_REGISTER].setSpansBasicBlock();

    // 7. set up the volatile FPRs
    for (Enumeration e = enumerateVolatileFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setVolatile();
    }

    // 8. set up the non-volatile FPRs
    for (Enumeration e = enumerateNonvolatileFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      r.setNonVolatile();
    }

    // 9. Cache the volatile registers for efficiency
    volatileSet = new OPT_BitSet(this);
    for (Enumeration e = enumerateVolatiles(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      volatileSet.add(r);
    }

    // 10. Cache the FPRs for efficiency
    fpSet = new OPT_BitSet(this);
    for (Enumeration e = enumerateFPRs(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      fpSet.add(r);
    }

    // Note no registers are excluded from live analysis (as is done for PPC)

  }

  /**
   * Is a particular register subject to allocation?
   */
  boolean isAllocatable(OPT_Register r) {
    return (r.number < FIRST_SPECIAL && r != getPR() && r != getESP());
  }


  /**
   * @return the processor register
   */
  OPT_Register getPR() {
    return getGPR(PROCESSOR_REGISTER);
  }

  /**
   * @return the frame pointer register
   */
  OPT_Register getFP() {
    throw new OPT_OptimizingCompilerException("Framepointer is not a register on IA32");
  }

  /**
   * @return the EAX register
   */
  OPT_Register getEAX() {
    return getGPR(EAX);
  }

  /**
   * @return the ECX register
   */
  OPT_Register getECX() {
    return getGPR(ECX);
  }

  /**
   * @return the EDX register
   */
  OPT_Register getEDX() {
    return getGPR(EDX);
  }

  /**
   * @return the EBX register
   */
  OPT_Register getEBX() {
    return getGPR(EBX);
  }

  /**
   * @return the ESP register
   */
  OPT_Register getESP() {
    return getGPR(ESP);
  }

  /**
   * @return the EBP register
   */
  OPT_Register getEBP() {
    return getGPR(EBP);
  }

  /**
   * @return the ESI register
   */
  OPT_Register getESI() {
    return getGPR(ESI);
  }

  /**
   * @return the EDI register
   */
  OPT_Register getEDI() {
    return getGPR(EDI);
  }


  /**
   * @return a register representing the AF bit of the EFLAGS register.
   */
  OPT_Register getAF() {
    return reg[AF];
  }

  /**
   * @return a register representing the CF bit of the EFLAGS register.
   */
  OPT_Register getCF() {
    return reg[CF];
  }

  /**
   * @return a register representing the OF bit of the EFLAGS register.
   */
  OPT_Register getOF() {
    return reg[OF];
  }

  /**
   * @return a register representing the PF bit of the EFLAGS register.
   */
  OPT_Register getPF() {
    return reg[PF];
  }

  /**
   * @return a register representing the SF bit of the EFLAGS register.
   */
  OPT_Register getSF() {
    return reg[SF];
  }

  /**
   * @return a register representing the ZF bit of the EFLAGS register.
   */
  OPT_Register getZF() {
    return reg[ZF];
  }

  /**
   * @return a register representing the C0 floating-point status bit
   */
  OPT_Register getC0() {
    return reg[C0];
  }

  /**
   * @return a register representing the C1 floating-point status bit
   */
  OPT_Register getC1() {
    return reg[C1];
  }

  /**
   * @return a register representing the C2 floating-point status bit
   */
  OPT_Register getC2() {
    return reg[C2];
  }

  /**
   * @return a register representing the C3 floating-point status bit
   */
  OPT_Register getC3() {
    return reg[C3];
  }

  /**
   * @return the nth physical GPR 
   */
  OPT_Register getGPR(int n) {
    return reg[FIRST_INT+n];
  }

  /**
   * @return the index into the GPR set corresponding to a given register.
   *
   * PRECONDITION: r is a physical GPR
   */
  static int getGPRIndex(OPT_Register r) {
    return r.number - FIRST_INT;
  }

  /**
   * @return the first GPR register used to hold a return value
   */
  OPT_Register getFirstReturnGPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_GPRS > 0);
    return getEAX();
  }

  /**
   * @return the second GPR register used to hold a return value
   */
  OPT_Register getSecondReturnGPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_GPRS > 1);
    return getEDX();
  }

  /**
   * @return the FPR register used to hold a return value
   */
  OPT_Register getReturnFPR() {
    if (VM.VerifyAssertions) VM.assert(NUM_RETURN_FPRS == 1);
    return getFPR(0);
  }

  /**
   * @return the nth physical FPR 
   */
  OPT_Register getFPR(int n) {
    return reg[FIRST_DOUBLE + n];
  }

  /**
   * @return the index into the GPR set corresponding to a given register.
   *
   * PRECONDITION: r is a physical GPR
   */
  static int getFPRIndex(OPT_Register r) {
    return r.number - FIRST_DOUBLE;
  }

  /**
   * @return the nth physical register in the pool. 
   */
  OPT_Register get(int n) {
    return reg[n];
  }

  /**
   * Given a symbolic register, return a code that gives the physical
   * register type to hold the value of the symbolic register.
   * @param r a symbolic register
   * @return one of INT_REG, DOUBLE_REG 
   */
  static final int getPhysicalRegisterType(OPT_Register r) {
    if (r.isInteger() || r.isLong()) {
      return INT_REG;
    } else if (r.isFloatingPoint()) {
      return DOUBLE_REG;
    } else {
      throw new OPT_OptimizingCompilerException("getPhysicalRegisterType "
                                                + " unexpected " + r);
    }
  }

  /**
   * Register names for each class. used in printing the IR
   */
  private static final String registerName[] = new String[getSize()];
  static {
    String regName[] = registerName;
    for (int i = 0; i < NUM_GPRS; i++)
      regName[i + FIRST_INT] = GPR_NAMES[i];
    for (int i = 0; i < NUM_FPRS; i++)
      regName[i + FIRST_DOUBLE] = FPR_NAMES[i];
    regName[PROCESSOR_REGISTER] = "PR";
    regName[AF] = "AF";
    regName[CF] = "CF";
    regName[OF] = "OF";
    regName[PF] = "PF";
    regName[SF] = "SF";
    regName[ZF] = "ZF";
  }

  /**
   * Get the register name for a register with a particular number in the
   * pool
   */
  static String getName(int number) {
    return registerName[number];
  }
  /**
   * Get the spill size for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  static int getSpillSize(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }
  /**
   * Get the required spill alignment for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG,  SPECIAL_REG
   */
  static int getSpillAlignment(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }

  /**
   * Enumerate all the physical registers in this set.
   */
  Enumeration enumerateAll() {
    return new RangeEnumeration(0,getSize()-1);
  }

  /**
   * Enumerate all the GPRs in this set.
   */
  Enumeration enumerateGPRs() {
    return new RangeEnumeration(FIRST_INT,FIRST_DOUBLE-1);
  }

  /**
   * Enumerate all the GPRs in this set.
   */
  Enumeration enumerateFPRs() {
    return new RangeEnumeration(FIRST_DOUBLE,FIRST_SPECIAL-1);
  }

  /**
   * Enumerate all the volatile GPRs in this set.
   */
  Enumeration enumerateVolatileGPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_VOLATILE_GPRS ];
    for(int i = 0; i < NUM_VOLATILE_GPRS; i++)
      r[i] = getGPR(VOLATILE_GPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /**
   * Enumerate all the nonvolatile GPRs in this set.
   */
  Enumeration enumerateNonvolatileGPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_NONVOLATILE_GPRS ];
    for(int i = 0; i < NUM_NONVOLATILE_GPRS; i++)
      r[i] = getGPR(NONVOLATILE_GPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }
  /**
   * Enumerate all the volatile FPRs in this set.
   */
  Enumeration enumerateVolatileFPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_VOLATILE_FPRS ];
    for(int i = 0; i < NUM_VOLATILE_FPRS; i++)
      r[i] = getFPR(VOLATILE_FPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /**
   * Enumerate all the nonvolatile FPRs in this set.
   */
  Enumeration enumerateNonvolatileFPRs() {
    OPT_Register r[] = new OPT_Register[ NUM_NONVOLATILE_FPRS ];
    for(int i = 0; i < NUM_NONVOLATILE_FPRS; i++)
      r[i] = getFPR(NONVOLATILE_FPRS[i]);
    return new PhysicalRegisterEnumeration(r);
  }

  /** 
   * Enumerate the volatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateVolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateVolatileGPRs();
      case DOUBLE_REG:
        return enumerateVolatileFPRs();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException("Unsupported volatile type");
    }
  }

  /**
   * Enumerate all the volatile physical registers
   */
  Enumeration enumerateVolatiles() {
    Enumeration e1 = enumerateVolatileGPRs();
    Enumeration e2 = enumerateVolatileFPRs();
    return new OPT_CompoundEnumerator(e1, e2);
  }

  /**
   * @return the set of volatile physical registers
   */
  OPT_BitSet getVolatiles() {
    return volatileSet;
  }

  /**
   * @return the set of FPR physical registers
   */
  OPT_BitSet getFPRs() {
    return fpSet;
  }

  /** 
   * Enumerate the nonvolatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateNonvolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateNonvolatileGPRs();
      case DOUBLE_REG:
        return enumerateNonvolatileFPRs();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException
          ("Unsupported non-volatile type");
    }
  }
  /** 
   * Enumerate the nonvolatile physical registers of a given class,
   * backwards
   * @param regClass one of INT_REG, DOUBLE_REG, SPECIAL_REG
   */
  Enumeration enumerateNonvolatilesBackwards(int regClass) {
    return new OPT_ReverseEnumerator(enumerateNonvolatiles(regClass));
  }


  /**
   * An enumerator for use by the physical register utilities.
   */
  class PhysicalRegisterEnumeration implements Enumeration {
    private int start;
    private int end;
    private int index;
    private OPT_Register r[];
    PhysicalRegisterEnumeration(OPT_Register[] r) {
      this.r = r;
      this.index = 0;
    }
    public Object nextElement() {
      return r[index++];
    }
    public boolean hasMoreElements() {
      return (index < r.length);
    }
  }
  /**
   * An enumerator for use by the physical register utilities.
   */
  class RangeEnumeration implements Enumeration {
    private int start;
    private int end;
    private int index;
    private int exclude = -1; // an index in the register range to exclude
    RangeEnumeration(int start, int end) {
      this.start = start;
      this.end = end;
      this.index = start;
    }
    RangeEnumeration(int start, int end, int exclude) {
      this.start = start;
      this.end = end;
      this.exclude = exclude;
      this.index = start;
    }
    public Object nextElement() {
      if (index == exclude) index++;
      return reg[index++];
    }
    public boolean hasMoreElements() {
      if (index == exclude) index++;
      return (index <= end);
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This abstract class provides a set of useful methods for
 * manipulating physical registers for an IR.
 *
 * @author Jong-Deok Choi
 * @author Dave Grove
 * @author Mauricio Serrano
 * @author John Whaley
 * @author Stephen Fink
 */
abstract class OPT_PhysicalRegisterTools extends
OPT_GenericPhysicalRegisterTools{

  /**
   * Return the governing IR.
   */
  abstract OPT_IR getIR();

  /**
   * Create an MIR instruction to move rhs into lhs
   */
  static OPT_Instruction makeMoveInstruction(OPT_RegisterOperand lhs, 
                                             OPT_RegisterOperand rhs) {
    if (rhs.register.isInteger() || rhs.register.isLong()) {
      if (VM.VerifyAssertions) 
	VM.assert(lhs.register.isInteger() || lhs.register.isLong());
      return MIR_Move.create(IA32_MOV, lhs, rhs);
    } else if (rhs.register.isDouble() || rhs.register.isFloat()) {
      if (VM.VerifyAssertions) 
	VM.assert(lhs.register.isDouble() || lhs.register.isFloat());
      return MIR_Move.create(IA32_FMOV, lhs, rhs);
    } else {
      OPT_OptimizingCompilerException.TODO("OPT_PhysicalRegisterTools.makeMoveInstruction");
      return null;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import instructionFormats.*;

/**
 * @author Stephen Fink
 */
final class OPT_RegisterPreferences extends OPT_GenericRegisterPreferences
implements OPT_Operators {

  /**
   * Set up register preferences based on instructions in an IR.
   */
  void initialize(OPT_IR ir) {

    for (Enumeration e = ir.forwardInstrEnumerator(); 
         e.hasMoreElements();) {
      OPT_Instruction s = (OPT_Instruction)e.nextElement();
      switch (s.operator.opcode) {
        case IA32_MOV_opcode:
          // add affinities produced by MOVE instructions
          OPT_Operand result = MIR_Move.getResult(s);
          OPT_Operand value = MIR_Move.getValue(s);
          if (result.isRegister() && value.isRegister()) {
            OPT_Register r1 = result.asRegister().register;
            OPT_Register r2 = value.asRegister().register;
            addAffinity(1,r1,r2);
          }
          break;
        default:
          break;
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Iterator;
import instructionFormats.*;
import java.util.ArrayList;
import java.util.Enumeration;

/**
 * An instance of this class encapsulates restrictions on register
 * assignment.
 * 
 * @author Stephen Fink
 */
final class OPT_RegisterRestrictions extends OPT_GenericRegisterRestrictions implements OPT_Operators, OPT_PhysicalRegisterConstants {

  /**
   * Allow scratch registers in PEIs?
   */
  final static boolean SCRATCH_IN_PEI = true;

  /**
   * Default Constructor
   */
  OPT_RegisterRestrictions(OPT_PhysicalRegisterSet phys) {
    super(phys);
  }

  /**
   * Add architecture-specific register restrictions for a basic block.
   * Override as needed.
   *
   * @param bb the basic block 
   * @param symbolics the live intervals for symbolic registers on this
   * block
   */
  void addArchRestrictions(OPT_BasicBlock bb, ArrayList symbolics) {
    // If there are any registers used in catch blocks, we want to ensure
    // that these registers are not used or evicted from scratch registers
    // at a relevant PEI, so that the assumptions of register homes in the
    // catch block remain valid.  For now, we do this by forcing any
    // register used in such a PEI as not spilled.  TODO: relax this
    // restriction for better code.
    for (OPT_InstructionEnumeration ie = bb.forwardInstrEnumerator();
         ie.hasMoreElements(); ) {
      OPT_Instruction s = ie.next();
      if (s.isPEI() && s.operator != IR_PROLOGUE) {
        if (bb.hasApplicableExceptionalOut(s) || !SCRATCH_IN_PEI) {
          for (Enumeration e = s.getOperands(); e.hasMoreElements(); ) {
            OPT_Operand op = (OPT_Operand)e.nextElement();
            if (op != null && op.isRegister()) {
              noteMustNotSpill(op.asRegister().register);
              handle8BitRestrictions(s);
            }
          }
        }
      }

      // handle special cases 
      switch (s.getOpcode()) {
        case MIR_LOWTABLESWITCH_opcode:
          {
            OPT_RegisterOperand op = MIR_LowTableSwitch.getIndex(s);
            noteMustNotSpill(op.register);
          }
          break;
        case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
          {
            OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
            if (MIR_Unary.getVal(s).isRegister()) {
              OPT_RegisterOperand val = MIR_Unary.getVal(s).asRegister();
              restrictTo8Bits(val.register);
            }
          }
          break;
        case IA32_SET$B_opcode:
          { 
            if (MIR_Set.getResult(s).isRegister()) {
              OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
              restrictTo8Bits(op.register);
            }
          }
          break;

        default:
          handle8BitRestrictions(s);
          break;
      }
    }
    for (OPT_InstructionEnumeration ie = bb.forwardInstrEnumerator();
         ie.hasMoreElements(); ) {
      OPT_Instruction s = ie.next();
      if (s.operator == IA32_FNINIT) {
        // No floating point register survives across an FNINIT
        for (Iterator sym = symbolics.iterator(); sym.hasNext(); ) {
          OPT_LiveIntervalElement symb = (OPT_LiveIntervalElement) sym.next();
          if (symb.getRegister().isFloatingPoint()) {
            if (contains(symb,s.scratch)) {
              addRestrictions(symb.getRegister(),phys.getFPRs());
            }
          }
        }
      } else if (s.operator == IA32_FCLEAR) {
        // Only some FPRs survive across an FCLEAR
        for (Iterator sym = symbolics.iterator(); sym.hasNext(); ) {
          OPT_LiveIntervalElement symb = (OPT_LiveIntervalElement) sym.next();
          if (symb.getRegister().isFloatingPoint()) {
            if (contains(symb,s.scratch)) {
              int nSave = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
              for (int i = nSave; i < NUM_FPRS; i++) {
                addRestriction(symb.getRegister(), phys.getFPR(i));
              }
            }
          }
        }
      }
    }
  }

  /**
   * Does instruction s contain an 8-bit memory operand?
   */
  final boolean has8BitMemoryOperand(OPT_Instruction s) {
    for (OPT_OperandEnumeration me = s.getMemoryOperands(); 
         me.hasMoreElements(); ) {
      OPT_MemoryOperand mop = (OPT_MemoryOperand)me.next();
      if (mop.size == 1) {
        return true;
      }
    }
    return false;
  }
  /**
   * Ensure that if an operand has an 8 bit memory operand that
   * all of its register operands are in 8 bit registers.
   * @param s the instruction to restrict
   */
  final void handle8BitRestrictions(OPT_Instruction s) {
    for (OPT_OperandEnumeration me = s.getMemoryOperands(); 
         me.hasMoreElements(); ) {
      OPT_MemoryOperand mop = (OPT_MemoryOperand)me.next();
      if (mop.size == 1) {
        for (OPT_OperandEnumeration e2 = s.getRootOperands(); 
             e2.hasMoreElements(); ) {
          OPT_Operand rootOp = e2.next();
          if (rootOp.isRegister()) {
            restrictTo8Bits(rootOp.asRegister().register);
          }
        }
      }
    }
  }


  /**
   * Ensure that a particular register is only assigned to AL, BL, CL, or
   * DL, since these are the only 8-bit registers we normally address.
   */
  final void restrictTo8Bits(OPT_Register r) {
    OPT_Register ESP = phys.getESP();        
    OPT_Register EBP = phys.getEBP();        
    OPT_Register ESI = phys.getESI();        
    OPT_Register EDI = phys.getEDI();        
    addRestriction(r,ESP);
    addRestriction(r,EBP);
    addRestriction(r,ESI);
    addRestriction(r,EDI);
  }

  /**
   * Given symbolic register r that appears in instruction s, does the
   * architecture demand that r be assigned to a physical register in s?
   */
  static boolean mustBeInRegister(OPT_Register r, OPT_Instruction s) {
    switch (s.getOpcode()) {
      case IA32_SHRD_opcode: case IA32_SHLD_opcode:
        {
          OPT_RegisterOperand op = MIR_DoubleShift.getSource(s);
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_FCOMI_opcode: case IA32_FCOMIP_opcode:
        {
          OPT_Operand op = MIR_Compare.getVal2(s);
          if (!(op instanceof OPT_BURSManagedFPROperand)) {
            if (op.asRegister().register == r) return true;
          }
        }
        break;
      case IA32_IMUL2_opcode:
        { 
          OPT_RegisterOperand op = MIR_BinaryAcc.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case MIR_LOWTABLESWITCH_opcode:
        {
          OPT_RegisterOperand op = MIR_LowTableSwitch.getIndex(s);
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_CMOV_opcode: case IA32_FCMOV_opcode:
        {
          OPT_RegisterOperand op = MIR_CondMove.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
        {
          OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_MOVZX$W_opcode: case IA32_MOVSX$W_opcode:
        { 
          OPT_RegisterOperand op = MIR_Unary.getResult(s).asRegister();
          if (op.asRegister().register == r) return true;
        }
        break;
      case IA32_SET$B_opcode:
        { 
          if (MIR_Set.getResult(s).isRegister()) {
            OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
            if (op.asRegister().register == r) return true;
          }
        }
        break;
      case IA32_TEST_opcode:
        {
          // at least 1 of the two operands must be in a register
          if (!MIR_Test.getVal2(s).isConstant()) {
            if (MIR_Test.getVal1(s).isRegister()) {
              if (MIR_Test.getVal1(s).asRegister().register == r) return true;
            } else if (MIR_Test.getVal2(s).isRegister()) {
              if (MIR_Test.getVal2(s).asRegister().register == r) return true;
            }
          }
        }
        break;

      default:
        break;
    }
    return false;
  }

  /**
   * Can physical register r hold an 8-bit value?
   */
  private boolean okFor8(OPT_Register r) {
    OPT_Register ESP = phys.getESP();        
    OPT_Register EBP = phys.getEBP();        
    OPT_Register ESI = phys.getESI();        
    OPT_Register EDI = phys.getEDI();        
    return (r!=ESP && r!=EBP && r!=ESI && r!=EDI);
  }

  /**
   * Is it forbidden to assign symbolic register symb to physical register r
   * in instruction s?
   */
  boolean isForbidden(OPT_Register symb, OPT_Register r,
                             OPT_Instruction s) {

    // Look at 8-bit restrictions.
    switch (s.operator.opcode) {
      case IA32_MOVZX$B_opcode: case IA32_MOVSX$B_opcode:
        {
          if (MIR_Unary.getVal(s).isRegister()) {
            OPT_RegisterOperand val = MIR_Unary.getVal(s).asRegister();
            if (val.register == symb) {
              return !okFor8(r);
            }
          }
        }
        break;
      case IA32_SET$B_opcode:
        { 
          if (MIR_Set.getResult(s).isRegister()) {
            OPT_RegisterOperand op = MIR_Set.getResult(s).asRegister();
            if (op.asRegister().register == symb) {
              return !okFor8(r);
            }
          }
        }
        break;
     }

    if (has8BitMemoryOperand(s)) {
      return !okFor8(r);
    }

    // Otherwise, it's OK.
    return false;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
///$Id$

import instructionFormats.*;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.HashSet;
import java.util.HashMap;

/**
 * Class to manage the allocation of the "compiler-specific" portion of 
 * the stackframe.  This class holds only the architecture-specific
 * functions.
 * <p>
 *
 * @author Stephen Fink
 * @author Dave Grove
 * @author Mauricio J. Serrano
 * @author Julian Dolby
 */
final class OPT_StackManager extends OPT_GenericStackManager
implements OPT_Operators {


  /**
   * A frame offset for 108 bytes of stack space to store the 
   * floating point state in the SaveVolatile protocol.
   */
  private int fsaveLocation;

  /**
   * We allow the stack pointer to float from its normal position at the
   * bottom of the frame.  This field holds the 'current' offset of the
   * SP.
   */
  private int ESPOffset = 0;

  /**
   * Should we allow the stack pointer to float in order to avoid scratch
   * registers in move instructions.  Note: as of Feb. 02, we think this
   * is a bad idea.
   */
  private static boolean FLOAT_ESP = false;

  /**
   * Return the size of the fixed portion of the stack.
   * (in other words, the difference between the framepointer and
   * the stackpointer after the prologue of the method completes).
   * @return size in bytes of the fixed portion of the stackframe
   */
  final int getFrameFixedSize() {
    return frameSize-WORDSIZE;
  }

  /**
   * Return the size of a type of value, in bytes.
   * NOTE: For the purpose of register allocation, a FLOAT_VALUE is 64 bits!
   *
   * @param type one of INT_VALUE, FLOAT_VALUE, or DOUBLE_VALUE
   */
  private static byte getSizeOfType(byte type) {
    switch(type) {
      case INT_VALUE:
        return (byte)(WORDSIZE);
      case FLOAT_VALUE: case DOUBLE_VALUE:
        return (byte)(2 * WORDSIZE);
      default:
        OPT_OptimizingCompilerException.TODO("getSizeOfValue: unsupported");
        return 0;
    }
  }

  /**
   * Return the move operator for a type of value.
   *
   * @param type one of INT_VALUE, FLOAT_VALUE, or DOUBLE_VALUE
   */
  private static OPT_Operator getMoveOperator(byte type) {
    switch(type) {
      case INT_VALUE:
        return IA32_MOV;
      case DOUBLE_VALUE:
      case FLOAT_VALUE:
        return IA32_FMOV;
      default:
        OPT_OptimizingCompilerException.TODO("getMoveOperator: unsupported");
        return null;
    }
  }

  /**
   * Allocate a new spill location and grow the
   * frame size to reflect the new layout.
   *
   * @param type the type to spill
   * @return the spill location
   */
  final int allocateNewSpillLocation(int type) {

    // increment by the spill size
    spillPointer += OPT_PhysicalRegisterSet.getSpillSize(type);

    if (spillPointer + WORDSIZE > frameSize) {
      frameSize = spillPointer + WORDSIZE;
    }
    return spillPointer;
  }


  /**
   * Insert a spill of a physical register before instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location, as an offset from the frame
   * pointer
   */
  final void insertSpillBefore(OPT_Instruction s, OPT_Register r,
                               byte type, int location) {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operator move = getMoveOperator(type);
    byte size = getSizeOfType(type);
    OPT_RegisterOperand rOp;
    switch(type) {
      case FLOAT_VALUE: rOp = F(r); break;
      case DOUBLE_VALUE: rOp = D(r); break;
      default: rOp = R(r); break;
    }
    OPT_StackLocationOperand spill = 
      new OPT_StackLocationOperand(true, -location, size);
    s.insertBefore(MIR_Move.create(move, spill, rOp));
  }

  /**
   * Insert a load of a physical register from a spill location before 
   * instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location
   */
  final void insertUnspillBefore(OPT_Instruction s, OPT_Register r, 
                                 byte type, int location) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operator move = getMoveOperator(type);
    byte size = getSizeOfType(type);
    OPT_RegisterOperand rOp;
    switch(type) {
      case FLOAT_VALUE: rOp = F(r); break;
      case DOUBLE_VALUE: rOp = D(r); break;
      default: rOp = R(r); break;
    }
    OPT_StackLocationOperand spill = 
      new OPT_StackLocationOperand(true, -location, size);
    s.insertBefore(MIR_Move.create(move, rOp, spill ));
  }

  /**
   * Compute the number of stack words needed to hold nonvolatile
   * registers.
   *
   * Side effects: 
   * <ul>
   * <li> updates the VM_OptCompiler structure 
   * <li> updates the <code>frameSize</code> field of this object
   * <li> updates the <code>frameRequired</code> field of this object
   * </ul>
   */
  void computeNonVolatileArea() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    if (ir.compiledMethod.isSaveVolatile()) {
      // Record that we use every nonvolatile GPR
      int numGprNv = phys.getNumberOfNonvolatileGPRs();
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);

      // set the frame size
      frameSize += numGprNv * WORDSIZE;
      frameSize = align(frameSize, STACKFRAME_ALIGNMENT);

      // TODO!!
      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)0);

      // Record that we need a stack frame.
      setFrameRequired();

      // Grab 108 bytes (same as 27 4-byte spills) in the stack
      // frame, as a place to store the floating-point state with FSAVE
      for (int i=0; i<27; i++) {
        fsaveLocation = allocateNewSpillLocation(INT_REG);
      }

      // Map each volatile register to a spill location.
      int i = 0;
      for (Enumeration e = phys.enumerateVolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        saveVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }

      // Map each non-volatile register to a spill location.
      i=0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        nonVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }

      // Set the offset to find non-volatiles.
      int gprOffset = getNonvolatileGPROffset(0);
      ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);

    } else {
      // Count the number of nonvolatiles used. 
      int numGprNv = 0;
      int i = 0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs();
           e.hasMoreElements(); ) {
        OPT_Register r = (OPT_Register)e.nextElement();
        if (r.isTouched() ) {
          // Note that as a side effect, the following call bumps up the
          // frame size.
          nonVolatileGPRLocation[i++] = allocateNewSpillLocation(INT_REG);
          numGprNv++;
        }
      }
      // Update the VM_OptCompiledMethod object.
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);
      if (numGprNv > 0) {
        int gprOffset = getNonvolatileGPROffset(0);
        ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);
        // record that we need a stack frame
        setFrameRequired();
      } else {
        ir.compiledMethod.setUnsignedNonVolatileOffset(0);
      }

      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)0);

    }
  }



  /**
   * Clean up some junk that's left in the IR after register allocation,
   * and add epilogue code.
   */ 
  void cleanUpAndInsertEpilogue() {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    OPT_Instruction inst = ir.firstInstructionInCodeOrder().getNext();
    for (; inst != null; inst = inst.nextInstructionInCodeOrder()) {
      switch (inst.getOpcode()) {
        case IA32_MOV_opcode:
          // remove frivolous moves
          OPT_Operand result = MIR_Move.getResult(inst);
          OPT_Operand val = MIR_Move.getValue(inst);
          if (result.similar(val)) {
            inst = inst.remove();
          }
          break;
        case IA32_FMOV_opcode:
          // remove frivolous moves
          result = MIR_Move.getResult(inst);
          val = MIR_Move.getValue(inst);
          if (result.similar(val)) {
            inst = inst.remove();
          }
          break;
        case IA32_RET_opcode:
          if (frameIsRequired()) {
            insertEpilogue(inst);
          }
        default:
          break;
      }
    }
    // now that the frame size is fixed, fix up the spill location code
    rewriteStackLocations();
  }

  /**
   * Insert an explicit stack overflow check in the prologue <em>after</em>
   * buying the stack frame.
   * SIDE EFFECT: mutates the plg into a trap instruction.  We need to
   * mutate so that the trap instruction is in the GC map data structures.
   *
   * @param plg the prologue instruction
   */
  private void insertNormalStackOverflowCheck(OPT_Instruction plg) {
    if (!ir.method.isInterruptible()) {
      plg.remove();
      return;
    }

    if (ir.compiledMethod.isSaveVolatile()) {
      return;
    }

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register PR = phys.getPR();
    OPT_Register ESP = phys.getESP();
    OPT_MemoryOperand M = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.activeThreadStackLimitField.getOffset(), 
			   (byte)WORDSIZE, null, null);

    //    Trap if ESP <= active Thread Stack Limit
    MIR_TrapIf.mutate(plg,IA32_TRAPIF,null,R(ESP),M,
                      OPT_IA32ConditionOperand.LE(),
                      OPT_TrapCodeOperand.StackOverflow());
  }

  /**
   * Insert an explicit stack overflow check in the prologue <em>before</em>
   * buying the stack frame.
   * SIDE EFFECT: mutates the plg into a trap instruction.  We need to
   * mutate so that the trap instruction is in the GC map data structures.
   *
   * @param plg the prologue instruction
   */
  private void insertBigFrameStackOverflowCheck(OPT_Instruction plg) {
    if (!ir.method.isInterruptible()) {
      plg.remove();
      return;
    }

    if (ir.compiledMethod.isSaveVolatile()) {
      return;
    }

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register PR = phys.getPR();
    OPT_Register ESP = phys.getESP();
    OPT_Register ECX = phys.getECX();

    //    ECX := active Thread Stack Limit
    OPT_MemoryOperand M = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.activeThreadStackLimitField.getOffset(), 
			   (byte)WORDSIZE, null, null);
    plg.insertBefore(MIR_Move.create(IA32_MOV, R(ECX), M));

    //    ECX += frame Size
    int frameSize = getFrameFixedSize();
    plg.insertBefore(MIR_BinaryAcc.create(IA32_ADD, R(ECX), I(frameSize)));
    //    Trap if ESP <= ECX
    MIR_TrapIf.mutate(plg,IA32_TRAPIF,null,R(ESP),R(ECX),
                      OPT_IA32ConditionOperand.LE(),
                      OPT_TrapCodeOperand.StackOverflow());
  }

  /**
   * Insert the prologue for a normal method.  
   *
   * Assume we are inserting the prologue for method B called from method
   * A.  
   *    <ul>
   *    <li> Perform a stack overflow check.
   *    <li> Store a back pointer to A's frame
   *    <li> Store B's compiled method id
   *    <li> Adjust frame pointer to point to B's frame
   *    <li> Save any used non-volatile registers
   *    </ul>
   */
  void insertNormalPrologue() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register ESP = phys.getESP(); 
    OPT_Register PR = phys.getPR();
    OPT_MemoryOperand fpHome = 
      OPT_MemoryOperand.BD(R(PR),
			   VM_Entrypoints.framePointerField.getOffset(),
			   (byte)WORDSIZE, null, null);

    // inst is the instruction immediately after the IR_PROLOGUE
    // instruction
    OPT_Instruction inst = ir.firstInstructionInCodeOrder().getNext().getNext();
    OPT_Instruction plg = inst.getPrev();

    int frameFixedSize = getFrameFixedSize();
    ir.compiledMethod.setFrameFixedSize(frameFixedSize);

    // I. Buy a stackframe (including overflow check)
    // NOTE: We play a little game here.  If the frame we are buying is
    //       very small (less than 256) then we can be sloppy with the 
    //       stackoverflow check and actually allocate the frame in the guard
    //       region.  We'll notice when this frame calls someone and take the
    //       stackoverflow in the callee. We can't do this if the frame is too big, 
    //       because growing the stack in the callee and/or handling a hardware trap 
    //       in this frame will require most of the guard region to complete.
    //       See libjvm.C.
    if (frameFixedSize >= 256) {
      // 1. Insert Stack overflow check.  
      insertBigFrameStackOverflowCheck(plg);

      // 2. Save caller's frame pointer
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, fpHome));

      // 3. Set my frame pointer to current value of stackpointer
      inst.insertBefore(MIR_Move.create(IA32_MOV, fpHome.copy(), R(ESP)));

      // 4. Store my compiled method id
      int cmid = ir.compiledMethod.getId();
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, I(cmid)));
    } else {
      // 1. Save caller's frame pointer
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, fpHome));

      // 2. Set my frame pointer to current value of stackpointer
      inst.insertBefore(MIR_Move.create(IA32_MOV, fpHome.copy(), R(ESP)));

      // 3. Store my compiled method id
      int cmid = ir.compiledMethod.getId();
      inst.insertBefore(MIR_UnaryNoRes.create(IA32_PUSH, I(cmid)));

      // 4. Insert Stack overflow check.  
      insertNormalStackOverflowCheck(plg);
    }

    // II. Save any used volatile and non-volatile registers
    if (ir.compiledMethod.isSaveVolatile())  {
      saveVolatiles(inst);
      saveFloatingPointState(inst);
    }
    saveNonVolatiles(inst);
    inst.insertBefore(Empty.create(IR_ENDPROLOGUE));
  }

  /**
   * Insert code into the prologue to save any used non-volatile
   * registers.  
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    // Save each non-volatile GPR used by this method. 
    int n = nNonvolatileGPRS - 1;
    for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
         e.hasMoreElements() && n >= 0 ; n--) {
      OPT_Register nv = (OPT_Register)e.nextElement();
      int offset = getNonvolatileGPROffset(n);
      OPT_Operand M = new OPT_StackLocationOperand(true, -offset, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, M, R(nv)));
    }
  }

  /**
   * Insert code before a return instruction to restore the nonvolatile 
   * registers.
   *
   * @param inst the return instruction
   */
  private void restoreNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    int n = nNonvolatileGPRS - 1;
    for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
         e.hasMoreElements() && n >= 0 ; n--) {
      OPT_Register nv = (OPT_Register)e.nextElement();
      int offset = getNonvolatileGPROffset(n);
      OPT_Operand M = new OPT_StackLocationOperand(true, -offset, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, R(nv), M));
    }
  }

  /**
   * Insert code into the prologue to save the floating point state.
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveFloatingPointState(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = new OPT_StackLocationOperand(true, -fsaveLocation, 4);
    inst.insertBefore(MIR_FSave.create(IA32_FNSAVE, M));
  }

  /**
   * Insert code into the epilogue to restore the floating point state.
   *
   * @param inst the return instruction after the epilogue.  
   */
  private void restoreFloatingPointState(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = new OPT_StackLocationOperand(true, -fsaveLocation, 4);
    inst.insertBefore(MIR_FSave.create(IA32_FRSTOR, M));
  }

  /**
   * Insert code into the prologue to save all volatile
   * registers.  
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // Save each GPR. 
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      OPT_Operand M = new OPT_StackLocationOperand(true, -location, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, M, R(r)));
    }
  }
  /**
   * Insert code before a return instruction to restore the volatile 
   * and volatile registers.
   *
   * @param inst the return instruction
   */
  private void restoreVolatileRegisters(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // Restore every GPR
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs(); 
         e.hasMoreElements(); i++){
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      OPT_Operand M = new OPT_StackLocationOperand(true, -location, 4);
      inst.insertBefore(MIR_Move.create(IA32_MOV, R(r), M));
    }
  }
  /**
   * Insert the epilogue before a particular return instruction.
   *
   * @param ret the return instruction.
   */
  private void insertEpilogue(OPT_Instruction ret) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet(); 
    OPT_Register ESP = phys.getESP(); 
    OPT_Register PR = phys.getPR();

    // 1. Restore any saved registers
    if (ir.compiledMethod.isSaveVolatile())  {
      restoreVolatileRegisters(ret);
      restoreFloatingPointState(ret);
    }
    restoreNonVolatiles(ret);

    // 2. Restore caller's stackpointer and framepointer
    int frameSize = getFrameFixedSize();
    ret.insertBefore(MIR_UnaryNoRes.create(REQUIRE_ESP, I(frameSize)));
    OPT_MemoryOperand fpHome = 
      OPT_MemoryOperand.BD(R(PR), VM_Entrypoints.framePointerField.getOffset(),
			   (byte)WORDSIZE, null, null);
    ret.insertBefore(MIR_Nullary.create(IA32_POP, fpHome));
  }

  /**
   * In instruction s, replace all appearances of a symbolic register 
   * operand with uses of the appropriate spill location, as cached by the
   * register allocator.
   *
   * @param s the instruction to mutate.
   * @param symb the symbolic register operand to replace
   */
  void replaceOperandWithSpillLocation(OPT_Instruction s, 
                                               OPT_RegisterOperand symb) {

    // Get the spill location previously assigned to the symbolic
    // register.
    int location = OPT_RegisterAllocatorState.getSpill(symb.register);

    // Create a memory operand M representing the spill location.
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Operand M = null;
    int type = phys.getPhysicalRegisterType(symb.register);
    int size = phys.getSpillSize(type);

    M = new OPT_StackLocationOperand(true, -location, (byte)size);

    // replace the register operand with the memory operand
    s.replaceOperand(symb,M);
  }

  /**
   * Does a memory operand hold a symbolic register?
   */
  private boolean hasSymbolicRegister(OPT_MemoryOperand M) {
    if (M.base != null && !M.base.register.isPhysical()) return true;
    if (M.index != null && !M.index.register.isPhysical()) return true;
    return false;
  }

  /**
   * Is s a MOVE instruction that can be generated without resorting to
   * scratch registers?
   */
  private boolean isScratchFreeMove(OPT_Instruction s) {
    if (s.operator() != IA32_MOV) return false;

    // if we don't allow ESP to float, we will always use scratch
    // registers in these move instructions.
    if (!FLOAT_ESP) return false;

    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand value = MIR_Move.getValue(s);

    // We need scratch registers for spilled registers that appear in
    // memory operands.
    if (result.isMemory()) {
      OPT_MemoryOperand M = result.asMemory();
      if (hasSymbolicRegister(M)) return false;
      // We will perform this transformation by changing the MOV to a PUSH
      // or POP.  Note that IA32 cannot PUSH/POP 8-bit quantities, so
      // disable the transformation for that case.  Also, (TODO), our
      // assembler does not emit the prefix to allow 16-bit push/pops, so
      // disable these too.  What's left?  32-bit only.
      if (M.size != 4) return false;
    }
    if (value.isMemory()) {
      OPT_MemoryOperand M = value.asMemory();
      if (hasSymbolicRegister(M)) return false;
      // We will perform this transformation by changing the MOV to a PUSH
      // or POP.  Note that IA32 cannot PUSH/POP 8-bit quantities, so
      // disable the transformation for that case.  Also, (TODO), our
      // assembler does not emit the prefix to allow 16-bit push/pops, so
      // disable these too.  What's left?  32-bit only.
      if (M.size != 4) return false;
    }
    // If we get here, all is kosher.
    return true;
  }

  /**
   * Given symbolic register r in instruction s, do we need to ensure that
   * r is in a scratch register is s (as opposed to a memory operand)
   */
  boolean needScratch(OPT_Register r, OPT_Instruction s) {
    // We never need a scratch register for a floating point value in an
    // FMOV instruction.
    if (r.isFloatingPoint() && s.operator==IA32_FMOV) return false;

    // Some MOVEs never need scratch registers
    if (isScratchFreeMove(s)) return false;

    // If s already has a memory operand, it is illegal to introduce
    // another.
    if (s.hasMemoryOperand()) return true;

    // Check the architecture restrictions.
    if (getRestrictions().mustBeInRegister(r,s)) return true;
    
    // Otherwise, everything is OK.
    return false;
  }

  /**
   * Before instruction s, insert code to adjust ESP so that it lies at a
   * particular offset from its usual location.
   */
  private void moveESPBefore(OPT_Instruction s, int desiredOffset) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet(); 
    OPT_Register ESP = phys.getESP(); 
    int delta = desiredOffset - ESPOffset;
    if (delta != 0) {
      if (canModifyEFLAGS(s)) {
	s.insertBefore(MIR_BinaryAcc.create(IA32_ADD, R(ESP), I(delta)));
      } else {
	OPT_MemoryOperand M = 
	  OPT_MemoryOperand.BD(R(ESP),delta, (byte)4, null, null); 
	s.insertBefore(MIR_Lea.create(IA32_LEA, R(ESP), M));
      }
      ESPOffset = desiredOffset;
    }
  }
  private boolean canModifyEFLAGS(OPT_Instruction s) {
    if (OPT_PhysicalDefUse.usesEFLAGS(s.operator()))
      return false;
    if (OPT_PhysicalDefUse.definesEFLAGS(s.operator()))
      return true;
    if (s.operator == BBEND) return true;
    return canModifyEFLAGS(s.nextInstructionInCodeOrder());
  }

  /**
   * Attempt to rewrite a move instruction to a NOP.
   *
   * @return true iff the transformation applies
   */
  private boolean mutateMoveToNop(OPT_Instruction s) {
    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand val = MIR_Move.getValue(s);
    if (result.isStackLocation() && val.isStackLocation()) {
      if (result.similar(val)) {
        Empty.mutate(s,NOP);
        return true;
      }
    }
    return false;
  }

  /**
   * Rewrite a move instruction if it has 2 memory operands.
   * One of the 2 memory operands must be a stack location operand.  Move
   * the SP to the appropriate location and use a push or pop instruction.
   */
  private void rewriteMoveInstruction(OPT_Instruction s) {
    // first attempt to mutate the move into a noop
    if (mutateMoveToNop(s)) return;

    OPT_Register ESP = ir.regpool.getPhysicalRegisterSet().getESP();
    OPT_Operand result = MIR_Move.getResult(s);
    OPT_Operand val = MIR_Move.getValue(s);
    if (result instanceof OPT_StackLocationOperand) {
      if (val instanceof OPT_MemoryOperand || 
          val instanceof OPT_StackLocationOperand) {
        int offset = ((OPT_StackLocationOperand)result).getOffset();
        byte size = ((OPT_StackLocationOperand)result).getSize();
        offset = FPOffset2SPOffset(offset) + size;
        moveESPBefore(s,offset);
        MIR_UnaryNoRes.mutate(s,IA32_PUSH,val);
      }
    } else {
      if (result instanceof OPT_MemoryOperand) {
        if (val instanceof OPT_StackLocationOperand) {
          int offset = ((OPT_StackLocationOperand)val).getOffset();
          offset = FPOffset2SPOffset(offset);
          moveESPBefore(s,offset);
          MIR_Nullary.mutate(s,IA32_POP,result);
	}
      }
    }
  }

  /**
   * Walk through the IR.  For each OPT_StackLocationOperand, replace the
   * operand with the appropriate OPT_MemoryOperand.
   */
  private void rewriteStackLocations() {
    // ESP is initially 4 bytes above where the framepointer is going to be.
    ESPOffset = getFrameFixedSize() + 4;
    OPT_Register ESP = ir.regpool.getPhysicalRegisterSet().getESP();

    boolean seenReturn = false;
    for (OPT_InstructionEnumeration e = ir.forwardInstrEnumerator(); 
	 e.hasMoreElements();) {
      OPT_Instruction s = e.next();
      
      if (s.isReturn()) {
	seenReturn = true;
	continue;
      }

      if (s.isBranch()) {
	// restore ESP to home location at end of basic block.
	moveESPBefore(s, 0);
	continue;
      }
	
      if (s.operator() == BBEND) {
	if (seenReturn) {
	  // at a return ESP will be at FrameFixedSize, 
	  seenReturn = false;
	  ESPOffset = 0;
	} else {
	  moveESPBefore(s, 0);
	}
	continue;
      }

      if (s.operator() == ADVISE_ESP) {
        ESPOffset = MIR_UnaryNoRes.getVal(s).asIntConstant().value;
	continue;
      }

      if (s.operator() == REQUIRE_ESP) {
	// ESP is required to be at the given offset from the bottom of the frame
	moveESPBefore(s, MIR_UnaryNoRes.getVal(s).asIntConstant().value);
	continue;
      }

      if (s.operator() == YIELDPOINT_PROLOGUE ||
	  s.operator() == YIELDPOINT_BACKEDGE ||
	  s.operator() == YIELDPOINT_EPILOGUE) {
	moveESPBefore(s, 0);
	continue;
      }

      if (s.operator() == IA32_MOV) {
        rewriteMoveInstruction(s);
      }

      // pop computes the effective address of its operand after ESP
      // is incremented.  Therefore update ESPOffset before rewriting 
      // stacklocation and memory operands.
      if (s.operator() == IA32_POP) {
	ESPOffset  += 4; 
      }	

      for (OPT_OperandEnumeration ops = s.getOperands(); ops.hasMoreElements(); ) {
        OPT_Operand op = ops.next();
        if (op instanceof OPT_StackLocationOperand) {
	  OPT_StackLocationOperand sop = (OPT_StackLocationOperand)op;
          int offset = sop.getOffset();
	  if (sop.isFromTop()) {
	    offset = FPOffset2SPOffset(offset);
	  }
	  offset -= ESPOffset;
          byte size = sop.getSize();
          OPT_MemoryOperand M = 
	    OPT_MemoryOperand.BD(R(ESP),offset,
				 size, null, null); 
          s.replaceOperand(op, M);
        } else if (op instanceof OPT_MemoryOperand) {
	  OPT_MemoryOperand M = op.asMemory();
	  if ((M.base != null && M.base.register == ESP) ||
	      (M.index != null && M.index.register == ESP)) {
	    M.disp -= ESPOffset;
	  }
	}
      }

      // push computes the effective address of its operand after ESP
      // is decremented.  Therefore update ESPOffset after rewriting 
      // stacklocation and memory operands.
      if (s.operator() == IA32_PUSH) {
	ESPOffset -= 4;
      }
    }
  }

  /**
   * @param fpOffset offset in bytes from the top of the stack frame
   * @return offset in bytes from the stack pointer.
   *
   * PRECONDITION: The final frameSize is calculated before calling this
   * routine.
   */
  private int FPOffset2SPOffset(int fpOffset) {
    // Note that SP = FP - frameSize + WORDSIZE;  
    // So, FP + fpOffset = SP + frameSize - WORDSIZE
    // + fpOffset
    return frameSize + fpOffset - WORDSIZE;
  }
  /**
   * Walk over the currently available scratch registers. 
   *
   * <p>For any scratch register r which is def'ed by instruction s, 
   * spill r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which is used by instruction s, 
   * restore r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which has current contents symb, and 
   * symb is spilled to location M, and s defs M: the old value of symb is
   * dead.  Mark this.
   *
   * <p>Invalidate any scratch register assignments that are illegal in s.
   */
  void restoreScratchRegistersBefore(OPT_Instruction s) {
    for (Iterator i = scratchInUse.iterator(); i.hasNext(); ) {
      ScratchRegister scratch = (ScratchRegister)i.next();

      if (scratch.currentContents == null) continue;
      if (verboseDebug) {
        System.out.println("RESTORE: consider " + scratch);
      }
      boolean removed = false;
      boolean unloaded = false;
      if (definedIn(scratch.scratch,s) 
          || (s.isCall() && s.operator != CALL_SAVE_VOLATILE 
              && scratch.scratch.isVolatile()) 
          || (s.operator == IA32_FNINIT && scratch.scratch.isFloatingPoint())
          || (s.operator == IA32_FCLEAR && scratch.scratch.isFloatingPoint())) {
        // s defines the scratch register, so save its contents before they
        // are killed.
        if (verboseDebug) {
          System.out.println("RESTORE : unload because defined " + scratch);
        }
        unloadScratchRegisterBefore(s,scratch);

        // update mapping information
        if (verboseDebug) System.out.println("RSRB: End scratch interval " + 
                                             scratch.scratch + " " + s);
        scratchMap.endScratchInterval(scratch.scratch,s);
        OPT_Register scratchContents = scratch.currentContents;
        if (scratchContents != null) {
          if (verboseDebug) System.out.println("RSRB: End symbolic interval " + 
                                               scratch.currentContents + " " 
                                               + s);
          scratchMap.endSymbolicInterval(scratch.currentContents,s);
        } 

        i.remove();
        removed = true;
        unloaded = true;
      }

      if (usedIn(scratch.scratch,s) ||
          !isLegal(scratch.currentContents,scratch.scratch,s) ||
          (s.operator == IA32_FCLEAR && scratch.scratch.isFloatingPoint())) {
        // first spill the currents contents of the scratch register to 
        // memory 
        if (!unloaded) {
          if (verboseDebug) {
            System.out.println("RESTORE : unload because used " + scratch);
          }
          unloadScratchRegisterBefore(s,scratch);

          // update mapping information
          if (verboseDebug) System.out.println("RSRB2: End scratch interval " + 
                                               scratch.scratch + " " + s);
          scratchMap.endScratchInterval(scratch.scratch,s);
          OPT_Register scratchContents = scratch.currentContents;
          if (scratchContents != null) {
            if (verboseDebug) System.out.println("RSRB2: End symbolic interval " + 
                                                 scratch.currentContents + " " 
                                                 + s);
            scratchMap.endSymbolicInterval(scratch.currentContents,s);
          } 

        }
        // s or some future instruction uses the scratch register, 
	// so restore the correct contents.
        if (verboseDebug) {
          System.out.println("RESTORE : reload because used " + scratch);
        }
        reloadScratchRegisterBefore(s,scratch);

        if (!removed) {
          i.remove();
          removed=true;
        }
      }
    }
  }
  /**
   * Initialize some architecture-specific state needed for register
   * allocation.
   */
  void initForArch(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    // We reserve the last (bottom) slot in the FPR stack as a scratch register.
    // This allows us to do one push/pop sequence in order to use the
    // top of the stack as a scratch location
    phys.getFPR(7).reserveRegister();
  }

  /**
   * Is a particular instruction a system call?
   */
  boolean isSysCall(OPT_Instruction s) {
    return s.operator == IA32_SYSCALL;
  } 
} 
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Handle exception delivery and stack unwinding for methods 
 *  compiled by optimizing Compiler 
 *
 * @author Dave Grove
 */
final class VM_OptExceptionDeliverer extends VM_ExceptionDeliverer 
  implements VM_Constants {

  private static final boolean TRACE = false;
  
  /** 
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod compiledMethod,
			VM_Address catchBlockInstructionAddress,
			Throwable exceptionObject,
			VM_Registers registers)  {
    VM_OptCompiledMethod optMethod = (VM_OptCompiledMethod)compiledMethod;
    VM_Address fp = registers.getInnermostFramePointer();
    VM_Thread myThread = VM_Thread.getCurrentThread();
    
    if (TRACE) {
      VM.sysWrite("Frame size of ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite(" is ");
      VM.sysWrite(optMethod.getFrameFixedSize());
      VM.sysWrite("\n");
    }

    // reset sp to "empty params" state (ie same as it was after prologue)
    VM_Address sp = fp.sub(optMethod.getFrameFixedSize());
    registers.gprs[STACK_POINTER] = sp.toInt();

    // store exception object for later retrieval by catch block
    int offset = optMethod.getUnsignedExceptionOffset();
    if (offset != 0) {
      // only put the exception object in the stackframe if the catch block is expecting it.
      // (if the method hasn't allocated a stack slot for caught exceptions, then we can safely
      //  drop the exceptionObject on the floor).
      VM_Magic.setObjectAtOffset(VM_Magic.addressAsObject(fp), -offset, exceptionObject);
      if (TRACE) {
	VM.sysWrite("Storing exception object ");
	VM.sysWrite(VM_Magic.objectAsAddress(exceptionObject));
	VM.sysWrite(" at offset ");
	VM.sysWrite(offset);
	VM.sysWrite(" from framepoint ");
	VM.sysWrite(fp);
	VM.sysWrite("\n");
      }
    } 

    if (TRACE) {
      VM.sysWrite("Registers before delivering exception in ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }

    // set address at which to resume executing frame
    registers.ip = catchBlockInstructionAddress;

    if (TRACE) {
      VM.sysWrite("Set ip to ");
      VM.sysWrite(registers.ip);
      VM.sysWrite("\n");
    }

    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called

    if (VM.VerifyAssertions) VM.assert(registers.inuse == true);
    registers.inuse = false;

    // 'give back' the portion of the stack we borrowed to run 
    // exception delivery code when invoked for a hardware trap.
    // If this was a straight software trap (athrow) then setting 
    // the stacklimit should be harmless, since the stacklimit should already have exactly
    // the value we are setting it too. 
    if (!myThread.hardwareExceptionRegisters.inuse) {
      myThread.stackLimit = VM_Magic.objectAsAddress(myThread.stack).add(STACK_SIZE_GUARD);
      VM_Processor.getCurrentProcessor().activeThreadStackLimit = myThread.stackLimit;
    }

    // "branches" to catchBlockInstructionAddress
    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }
  

  /**
   * Unwind a stackframe.
   */
  void unwindStackFrame(VM_CompiledMethod compiledMethod, 
			VM_Registers registers) {
    VM_Address fp = registers.getInnermostFramePointer();
    VM_OptCompiledMethod optMethod = (VM_OptCompiledMethod)compiledMethod;
    
    if (TRACE) {
      VM.sysWrite("Registers before unwinding frame for ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }

    // restore non-volatile registers
    int frameOffset = optMethod.getUnsignedNonVolatileOffset();
    for (int i = optMethod.getFirstNonVolatileGPR(); 
	 i<NUM_NONVOLATILE_GPRS; 
	 i++, frameOffset += 4) {
      registers.gprs[NONVOLATILE_GPRS[i]] = VM_Magic.getMemoryWord(fp.sub(frameOffset));
    }
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0);
    
    registers.unwindStackFrame();

    if (TRACE) {
      VM.sysWrite("Registers after unwinding frame for ");
      VM.sysWrite(optMethod.getMethod());
      VM.sysWrite("\n");
      for (int i=0; i<NUM_GPRS; i++) {
	VM.sysWrite(GPR_NAMES[i]);
	VM.sysWrite(" = ");
	VM.sysWrite(registers.gprs[i]);
	VM.sysWrite("\n");
      }
    }
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * An instance of this class provides iteration across the references 
 * represented by a frame built by the OPT compiler.
 *
 * The architecture-specific version of the GC Map iterator.  It inherits
 * its architecture-independent code from VM_OptGenericGCMapIterator.
 * This version is for IA32
 *
 * @author Michael Hind
 */
public final class VM_OptGCMapIterator extends VM_OptGenericGCMapIterator
  implements VM_Uninterruptible {

  private static final boolean DEBUG = false;
 
  public VM_OptGCMapIterator(int[] registerLocations) {
    super(registerLocations);
  }

  /** 
   * If any non-volatile gprs were saved by the method being processed
   * then update the registerLocations array with the locations where the
   * registers were saved.  Also, check for special methods that also
   * save the volatile gprs.
   */
  void updateLocateRegisters() {

    //           HIGH MEMORY
    //
    //       +---------------+                                           |
    //  FP-> |   saved FP    |  <-- this frame's caller's frame          |
    //       +---------------+                                           |
    //       |    cmid       |  <-- this frame's compiledmethod id       |
    //       +---------------+                                           |
    //       |               |                                           |
    //       |  Spill Area   |  <-- spills and other method-specific     |
    //       |     ...       |      compiler-managed storage             |
    //       +---------------+                                           |
    //       |   Saved FP    |     only SaveVolatile Frames              |   
    //       |    State      |                                           |
    //       +---------------+                                           |
    //       |  VolGPR[0]    |                                           
    //       |     ...       |     only SaveVolatile Frames              
    //       |  VolGPR[n]    |                                           
    //       +---------------+                                           
    //       |  NVolGPR[k]   |  <-- cm.getUnsignedNonVolatileOffset()  
    //       |     ...       |   k == cm.getFirstNonVolatileGPR()      
    //       |  NVolGPR[n]   |                                           
    //       +---------------+                                           
    //
    //           LOW MEMORY
    
    int frameOffset = compiledMethod.getUnsignedNonVolatileOffset();
    if (frameOffset >= 0) {
      // get to the non vol area
      VM_Address nonVolArea = framePtr.sub(frameOffset);
    
      // update non-volatiles
      int first = compiledMethod.getFirstNonVolatileGPR();
      if (first >= 0) {
	// move to the beginning of the nonVol area
	VM_Address location = nonVolArea;
	
	for (int i = first; i < NUM_NONVOLATILE_GPRS; i++) {
	  // determine what register index corresponds to this location
	  int registerIndex = NONVOLATILE_GPRS[i];
	  registerLocations[registerIndex] = location.toInt();
          if (DEBUG) {
            VM.sysWrite("UpdateRegisterLocations: Register ");
            VM.sysWrite(registerIndex);
            VM.sysWrite(" to Location ");
            VM.sysWrite(location.toInt());
            VM.sysWrite("\n");
          }
	  location = location.sub(4);
	}
      }
      
      // update volatiles if needed
      if (compiledMethod.isSaveVolatile()) {
	// move to the beginning of the nonVol area
	VM_Address location = nonVolArea.add(4 * NUM_VOLATILE_GPRS);
	
	for (int i = 0; i < NUM_VOLATILE_GPRS; i++) {
	  // determine what register index corresponds to this location
	  int registerIndex = VOLATILE_GPRS[i];
	  registerLocations[registerIndex] = location.toInt();
          if (DEBUG) {
            VM.sysWrite("UpdateRegisterLocations: Register ");
            VM.sysWrite(registerIndex);
            VM.sysWrite(" to Location ");
            VM.sysWrite(location.toInt());
            VM.sysWrite("\n");
          }
	  location = location.sub(4);
	}
      }
    }
  }

  /** 
   *  Determine the spill location given the frame ptr and spill offset.
   *  (The location of spills varies among architectures.)
   *  @param framePtr the frame pointer
   *  @param offset  the offset for the spill 
   *  @return the resulting spill location
   */
  public VM_Address getStackLocation(VM_Address framePtr, int offset) {
    return framePtr.sub(offset);
  }

  /** 
   *  Get address of the first spill location for the given frame ptr
   *  @param the frame pointer
   *  @return the first spill location
   */
  public VM_Address getFirstSpillLoc() {
    return framePtr.sub(-VM.STACKFRAME_BODY_OFFSET);
  }

  /** 
   *  Get address of the last spill location for the given frame ptr
   *  @param the frame pointer
   *  @return the last spill location
   */
  public VM_Address getLastSpillLoc() {
    if (compiledMethod.isSaveVolatile()) {
      return framePtr.sub(compiledMethod.getUnsignedNonVolatileOffset() - 4 - SAVE_VOL_SIZE);
    } else {
      return framePtr.sub(compiledMethod.getUnsignedNonVolatileOffset() - 4);
    }
  }

  final static int VOL_SIZE = 4 * NUM_VOLATILE_GPRS;
  final static int SAVE_VOL_SIZE = VOL_SIZE + VM.FPU_STATE_SIZE;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * This interface holds constants for the Opt GC map code specific to IA32
 *
 * @author Michael Hind
 */
interface VM_OptGCMapIteratorConstants extends OPT_PhysicalRegisterConstants {
  
  // NOTE: The following two constants seem to imply that registers 
  //       that can hold references are contiguous.  This is not true,
  //       in general, however, for the GC map code we only need to make
  //       sure that all such registers are included in the range defined
  //       below these contants.

  /*
   * The index of the first nonvolatile register that may hold a reference, 
   */
  static final int FIRST_GCMAP_REG = 0;  

  /*
   * the index of last register that may hold a reference
   */
  static final int LAST_GCMAP_REG = 7;

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Disassembler for the Intel instruction set
 *
 * Source code in C borrowed from C++ VisualAge, 
 * provided by Steve Turner (IBM Austin)
 * Minor change:  change output to lower case
 *
 * @author Ton Ngo 
 * @date 2/12/2001 
 */

class IntelDisassembler {

  public static byte[] intToByteArray(int[] intArray) {
    byte byteArray[] = new byte[intArray.length*4];
    for (int i=0; i<intArray.length; i++) {
      int word = intArray[i];
      int offset = i*4;
      byteArray[offset]   = (byte) ( word & 0x000000FF);
      byteArray[offset+1] = (byte) ((word & 0x0000FF00) >> 8);
      byteArray[offset+2] = (byte) ((word & 0x00FF0000) >> 16);
      byteArray[offset+3] = (byte) ((word & 0xFF000000) >> 24);
    }
    return byteArray;
  }

  /**
   * Disassemble up to the number of instruction, 
   * -if count is nonzero, disassemble up to count
   * -if count is zero, disassemble until there is no more valid 
   *  instructions in the buffer
   * 
   */
  public static native String disasm(byte[] instr, int count, int address);

  public static String disasm(byte[] instr) {
    return disasm(instr, 0, 0);
  }

  /**
   * Convenience methods:  convert from int to byte
   */
  public static String disasm(int instrArray[], int address) {
    return disasm(intToByteArray(instrArray), 0, address);
  }

  public static String disasm(int instrArray[], int count, int address) {
    return disasm(intToByteArray(instrArray), count, address);
  }


  /**
   * Compute the instruction length for a block of instruction
   * 
   *
   */
  public static native byte[] instructionLength(byte[] instr);
  public static byte[] instructionLength(int instrArray[]) {
    return instructionLength(intToByteArray(instrArray));
  }


  /**
   * Test if this is a CALL instruction
   */
  public static boolean isCallInstruction(int instrArray[]) {
    String instructionString = disasm(instrArray, 1, 0);
    if (instructionString.indexOf("CALL") == -1)
      return false;
    else 
      return true;
  }

  /**
   * Compute the target address for a call, jump, or jump conditional
   */
  public static native int getBranchTarget(byte instrArray[], int[] regs);
  // just a way to return more info from decoding the branch target
  public static native boolean isLastAddressIndirect();   

  public static int getBranchTarget(int instrArray[], int[] regs) {
    byte bArray[] = intToByteArray(instrArray);
    return getBranchTarget(bArray, regs);
  }

  /**
   * for stand alone testing 
   */
  static byte testInstructions[] = {
    (byte) 0x55, (byte) 0x89, (byte) 0xe5, (byte) 0x83, 
    (byte) 0xec, (byte) 0x08, (byte) 0xc7, (byte) 0x45, 
    (byte) 0xfc, (byte) 0x00, (byte) 0x00, (byte) 0x00, 
    (byte) 0x00, (byte) 0xc7, (byte) 0x45, (byte) 0xf8, 
    (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, 
    (byte) 0x83, (byte) 0x7d, (byte) 0xf8, (byte) 0x09, 
    (byte) 0x7e, (byte) 0x06, (byte) 0xeb, (byte) 0x14, 
    (byte) 0x8d, (byte) 0x74, (byte) 0x26, (byte) 0x00, 
    (byte) 0x8b, (byte) 0x45, (byte) 0xf8, (byte) 0x01, 
    (byte) 0x45, (byte) 0xfc, (byte) 0xff, (byte) 0x45, 
    (byte) 0xf8, (byte) 0xeb, (byte) 0xe9, (byte) 0x90, 
    (byte) 0x8d, (byte) 0x74, (byte) 0x26, (byte) 0x00, 
    (byte) 0x8b, (byte) 0x45, (byte) 0xfc, (byte) 0x50, 
    (byte) 0x68, (byte) 0x70, (byte) 0x84, (byte) 0x04, 
    (byte) 0x08, (byte) 0xe8, (byte) 0xfa, (byte) 0xfe, 
    (byte) 0xff, (byte) 0xff, (byte) 0x83, (byte) 0xc4, 
    (byte) 0x08, (byte) 0xc9, (byte) 0xc3, (byte) 0x90, 
    (byte) 0x90};

  // static byte testInstructions[] = {(byte) 0x55  };


  // for testing 
  public static void main(String args[]) {

    System.loadLibrary("IntelDisassembler");
    System.out.println("Disassemble Intel: " + testInstructions.length +
		       " bytes");
    System.out.println("return mnemonic: " + disasm(testInstructions));
    
  }


}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class compiles the prolog and epilog for all code that makes
 * the transition between Java and Native C
 * 2 cases:
 *  -from Java to C:  all user-defined native methods
 *  -C to Java:  all JNI functions in VM_JNIFunctions.java
 *
 * @author Ton Ngo
 * @author Steve Smith
 */
public class VM_JNICompiler implements VM_JNILinuxConstants, VM_BaselineConstants {

  // offsets to saved regs and addresses in java to C glue frames
  // EDI (JTOC) and EBX are nonvolatile registers in RVM
  //
  private static final int SAVED_GPRS = 5; 
  static final int EDI_SAVE_OFFSET = STACKFRAME_BODY_OFFSET;
  static final int EBX_SAVE_OFFSET = STACKFRAME_BODY_OFFSET - WORDSIZE;
  static final int EBP_SAVE_OFFSET = EBX_SAVE_OFFSET - WORDSIZE;
  static final int JNI_RETURN_ADDRESS_OFFSET = EBP_SAVE_OFFSET - WORDSIZE;
  static final int JNI_PR_OFFSET = JNI_RETURN_ADDRESS_OFFSET - WORDSIZE;

  // following used in prolog & epilog for JNIFunctions
  // offset of saved offset to preceeding java frame
  static final int SAVED_JAVA_FP_OFFSET = STACKFRAME_BODY_OFFSET;

  // following used in VM_Compiler to compute offset to first local:
  // includes 5 words:
  //   SAVED_JAVA_FP,  PR (ESI), S0 (ECX), EBX, and JTOC (EDI)
  static final int SAVED_GPRS_FOR_JNI = 5;

  /*****************************************************************
   * Handle the Java to C transition:  native methods
   *
   */

  static VM_MachineCode generateGlueCodeForNative (VM_CompiledMethod cm) {
    int compiledMethodId = cm.getId();
    VM_Method method     = cm.getMethod();
    VM_Assembler asm	 = new VM_Assembler(100);   // some size for the instruction array
    int nativeIP         = method.getNativeIP();
    // recompute some constants
    int parameterWords   = method.getParameterWords();

    // Meaning of constant offset into frame:
    // STACKFRAME_HEADER_SIZE =  12              (CHECK ??)
    // SAVED_GPRS = 4 words/registers
    // Stack frame:
    //        on entry          after prolog
    //
    //      high address	high address
    //      |          |	|          | Caller frame
    //      |          |	|          |
    // +    |arg 0     |	|arg 0     |    -> firstParameterOffset
    // +    |arg 1     |	|arg 1     |
    // +    |...       |	|...       |
    // +8   |arg n-1   |	|arg n-1   |    
    // +4   |returnAddr|	|returnAddr|
    //  0   +	       +	+saved FP  + <---- FP for glue frame
    // -4   |	       |	|methodID  |
    // -8   |	       |	|saved EDI |    -> STACKFRAME_BODY_OFFSET = -8
    // -C   |	       |	|saved EBX |
    // -10  |	       |	|saved EBP |
    // -14  |	       |	|returnAddr|  (return from OutOfLine to generated epilog)    
    // -18  |	       |	|saved PR  |
    // -1C  |	       |	|arg n-1   |  reordered args to native method
    // -20  |	       |	| ...      |  ...
    // -24  |	       |	|arg 1     |  ...
    // -28  |	       |	|arg 0     |  ...
    // -2C  |	       |	|class/obj |  required second arg to native method
    // -30  |	       |	|jniEnv    |  required first arg to native method
    // -34  |	       |	|          |    
    //      |	       |	|          |    
    //      |	       |	|          |    
    //       low address	 low address


    // TODO:  check and resize stack once on the lowest Java to C transition
    // on the stack.  Not needed if we use the thread original stack

    // Fill in frame header - similar to normal prolog
    prepareStackHeader(asm, method, compiledMethodId);

    // Process the arguments - specific to method being called
    storeParametersForLintel(asm, method);
    
    // load address of native into S0
    asm.emitMOV_Reg_Imm (S0, nativeIP);  

    // branch to outofline code in bootimage
    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.invokeNativeFunctionInstructionsField.getOffset());

    // return here from VM_OutOfLineMachineCode upon return from native code

    // PR and RVM JTOC restored, T0,T1 contain return from native call

    //If the return type is reference, look up the real value in the JNIref array 

    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv    
    if (method.getReturnType().isReferenceType()) {
      asm.emitADD_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsField.getOffset());      // T0 <- address of entry (not index)
      asm.emitMOV_Reg_RegInd (T0, T0);   // get the reference
    } else if (method.getReturnType().isLongType()) {
      asm.emitPUSH_Reg(T1);    // need to use T1 in popJNIrefForEpilog and to swap order T0-T1  
    }

    // CHECK - may not do anything - leave below will remove whole frame
    // asm.emitPUSH_Reg(T0);                // push the return value onto the stack

    // pop frame in JNIRefs array (need to use
    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv    
    popJNIrefForEpilog(asm);                                
    
    // then swap order of T0 and T1 for long
    if (method.getReturnType().isLongType()) {
      asm.emitMOV_Reg_Reg(T1, T0);  
      asm.emitPOP_Reg(T0);
    }

    // CHECK EXCEPTION AND BRANCH TO ATHROW CODE OR RETURN NORMALLY

    // get pending exception from JNIEnv
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());
    asm.emitMOV_Reg_RegDisp (S0,  S0, VM_Entrypoints.jniEnvField.getOffset());        	  // S0 <- jniEnv    
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIPendingExceptionField.getOffset());  // EBX <- JNIPendingException
    asm.emitMOV_RegDisp_Imm (S0, VM_Entrypoints.JNIPendingExceptionField.getOffset(), 0);    // clear the current pending exception

    asm.emitCMP_Reg_Imm(EBX, 0);   // check for exception pending:  JNIPendingException = non zero
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);            // Br if yes

    // if pending exception, discard the return value and current stack frame
    // then jump to athrow 
    asm.emitMOV_Reg_Reg     (T0, EBX);
    asm.emitMOV_Reg_RegDisp (T1, JTOC, VM_Entrypoints.athrowMethod.getOffset()); // acquire jump addr before restoring nonvolatiles

    asm.emitMOV_Reg_Reg     (SP, EBP);                      // discard current stack frame
    asm.emitMOV_Reg_RegDisp (JTOC, SP, EDI_SAVE_OFFSET);   // restore nonvolatile EDI/JTOC register
    asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET);    // restore nonvolatile EBX register
    asm.emitMOV_Reg_RegDisp (EBP, SP, EBP_SAVE_OFFSET);    // restore nonvolatile EBP register

    asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset());

    // don't use CALL since it will push on the stack frame the return address to here 
    asm.emitJMP_Reg(T1); // jumps to VM_Runtime.athrow

    fr.resolve(asm);  // branch to here if no exception 

    // no exception, proceed to return to caller    
    asm.emitMOV_Reg_Reg(SP, EBP);                           // discard current stack frame

    asm.emitMOV_Reg_RegDisp (JTOC, SP, EDI_SAVE_OFFSET);   // restore nonvolatile EDI/JTOC register
    asm.emitMOV_Reg_RegDisp (EBX, SP, EBX_SAVE_OFFSET);    // restore nonvolatile EBX register
    asm.emitMOV_Reg_RegDisp (EBP, SP, EBP_SAVE_OFFSET);    // restore nonvolatile EBP register

    asm.emitPOP_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset());

    // return to caller 
    // pop parameters from stack (Note that parameterWords does not include "this")
    if (method.isStatic())
      asm.emitRET_Imm(parameterWords << LG_WORDSIZE); 
    else
      asm.emitRET_Imm((parameterWords+1) << LG_WORDSIZE); 

    // return asm.makeMachineCode();
    return new VM_MachineCode(asm.getMachineCodes(), null);

  }

  /**************************************************************
   * Prepare the stack header for Java to C transition
   *         before               after
   *	   high address		high address
   *	   |          |		|          | Caller frame
   *	   |          |		|          |
   *  +    |arg 0     |		|arg 0     |    
   *  +    |arg 1     |		|arg 1     |
   *  +    |...       |		|...       |
   *  +8   |arg n-1   |		|arg n-1   |    
   *  +4   |returnAddr|		|returnAddr|
   *   0   +	      +		+saved FP  + <---- FP for glue frame
   *  -4   |	      |		|methodID  |
   *  -8   |	      |		|saved EDI |  (EDI == JTOC - for baseline methods)  
   *  -C   |	      |		|saved EBX |    
   *  -10  |	      |	        |	   |	
   *  
   *  
   *  
   */
  static void prepareStackHeader(VM_Assembler asm, VM_Method method, int compiledMethodId) {

    // set 2nd word of header = return address already pushed by CALL
    asm.emitPUSH_RegDisp (PR, VM_Entrypoints.framePointerField.getOffset());

    // start new frame:  set FP to point to the new frame
    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              SP);

    // set first word of header: method ID
    asm.emitMOV_RegDisp_Imm (SP, STACKFRAME_METHOD_ID_OFFSET, compiledMethodId); 


    // save nonvolatile registrs: JTOC/EDI, EBX, EBP
    asm.emitMOV_RegDisp_Reg (SP, EDI_SAVE_OFFSET, JTOC); 
    asm.emitMOV_RegDisp_Reg (SP, EBX_SAVE_OFFSET, EBX);
    asm.emitMOV_RegDisp_Reg (SP, EBP_SAVE_OFFSET, EBP);
    
    asm.emitMOV_Reg_Reg     (EBP, SP); // Establish EBP as the framepointer for use in the rest of the glue frame

    // restore JTOC with the value saved in VM_Processor.jtoc for use in prolog 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());
  }

  /**************************************************************
   * Process the arguments:
   *   -insert the 2 JNI args
   *   -replace pointers
   *   -reverse the order of the args from Java to fit the C convention
   *   -
   *
   *         before               after
   *
   *	   high address		high address
   *	   |          | 	|          | Caller frame
   *	   |          |		|          | 
   *  +    |arg 0     | 	|arg 0     | 	-> firstParameterOffset
   *  +    |arg 1     |		|arg 1     | 
   *  +    |...       |		|...       | 
   *  +8   |arg n-1   | 	|arg n-1   | 	
   *  +4   |returnAddr|		|returnAddr| 
   *   0   +saved FP  + 	+saved FP  + <---- FP for glue frame
   *  -4   |methodID  |		|methodID  | 
   *  -8   |saved EDI | 	|saved EDI | 	-> STACKFRAME_BODY_OFFSET = -8
   *  -C   |saved EBX | 	|saved EBX | 	
   *  -10  |	      | 	|returnAddr|  (return from OutOfLine to generated epilog)    
   *  -14  |	      |	        |saved PR  |
   *  -18  |	      |	        |arg n-1   |  reordered args to native method (firstLocalOffset
   *  -1C  |	      |	        | ...      |  ...
   *  -20  |	      |  	|arg 1     |  ...
   *  -24  |	      |	        |arg 0     |  ...
   *  -28  |	      |	        |class/obj |  required second arg 
   *  -2C  |	      |   SP -> |jniEnv    |  required first arg  (emptyStackOffset)
   *  -30  |	      |	        |          |    
   *	   |          |  	|          | 	
   *	    low address		 low address
   */
  static void storeParametersForLintel(VM_Assembler asm, VM_Method method) {
    VM_Class klass	     = method.getDeclaringClass();
    int parameterWords       = method.getParameterWords();
    int savedRegistersSize   = SAVED_GPRS<<LG_WORDSIZE;
    int firstLocalOffset     = STACKFRAME_BODY_OFFSET - savedRegistersSize ;
    int emptyStackOffset     = firstLocalOffset - ((parameterWords+2) << LG_WORDSIZE) + WORDSIZE;
    int firstParameterOffset = STACKFRAME_BODY_OFFSET + STACKFRAME_HEADER_SIZE + (parameterWords<<LG_WORDSIZE);
    int firstActualParameter;


    VM_Type[] types = method.getParameterTypes();   // does NOT include implicit this or class ptr
    int numArguments = types.length;                // number of arguments for this method
    int numRefArguments = 1;                        // initialize with count of 1 for the JNI arg
    int numFloats = 0;                              // number of float or double arguments

    // quick count of number of references
    for (int i=0; i<numArguments; i++) {
      if (types[i].isReferenceType())
	numRefArguments++;
      if (types[i].isFloatType() || types[i].isDoubleType())
	numFloats++;
    }


    // first push the parameters passed in registers back onto the caller frame
    // to free up the registers for use
    // The number of registers holding parameter is 
    // VM_RegisterConstants.NUM_PARAMETER_GPRS
    // Their indices are in VM_RegisterConstants.VOLATILE_GPRS[]
    int gpr = 0;
    // note that firstParameterOffset does not include "this"
    int parameterOffset = firstParameterOffset;   

    // handle the "this" parameter
    if (!method.isStatic()) {
      asm.emitMOV_RegDisp_Reg(EBP, firstParameterOffset+WORDSIZE, 
                              VOLATILE_GPRS[gpr]);
      gpr++;
    }
    
    for (int i=0; i<numArguments && gpr<NUM_PARAMETER_GPRS; i++) {
      if (types[i].isDoubleType()) {
	parameterOffset -= 2*WORDSIZE;
	continue;
      } else if (types[i].isFloatType()) {
	parameterOffset -= WORDSIZE;
	continue;
      } else if (types[i].isLongType()) {
	if (gpr<NUM_PARAMETER_GPRS) {   // get the hi word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
	if (gpr<NUM_PARAMETER_GPRS) {    // get the lo word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
      } else {
	if (gpr<NUM_PARAMETER_GPRS) {   // all other types fit in one word
	  asm.emitMOV_RegDisp_Reg(EBP, parameterOffset, VOLATILE_GPRS[gpr]);
	  gpr++;
	  parameterOffset -= WORDSIZE;
	}
      }
    }


    // bump SP to set aside room for the args + 2 additional JNI args
    asm.emitADD_Reg_Imm (SP, emptyStackOffset);                       

    // SP should now point to the bottom of the argument stack, 
    // which is arg[n-1]


    // Prepare the side stack to hold new refs
    // Leave S0 holding the jniEnv pointer
    // S0 <- VM_Thread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset());

    asm.emitMOV_Reg_RegDisp (S0, S0, VM_Entrypoints.jniEnvField.getOffset());        // S0 <- jniEnv

    // save PR in the jniEnv for JNI call from native
    VM_ProcessorLocalState.emitStoreProcessor(asm, S0, VM_Entrypoints.JNIEnvSavedPRField.getOffset());

    // save FP for glue frame in JNI env - used by GC when in C
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNITopJavaFPField.getOffset(), EBP);  // jniEnv.JNITopJavaFP <- FP

    //********************************************
    // Between HERE and THERE, S0 and T0 are in use
    // >>>> HERE <<<<
    startJNIrefForProlog(asm, numRefArguments);
    
    // Insert the JNI arg at the first entry:  JNI_Environment as the pointer to 
    // the JNI functions array
    // pr -> thread -> jniEnv -> function array
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIEnvAddressField.getOffset()); // ebx <- JNIEnvAddress
    asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset, EBX);                  // store as 1st arg

    // added 7/05 - check later SES
    // store current processors status word address in word after 
    // passed ->JNIFunctions (in EBX). upon return or reentry to java this
    // word is tested & it is wrong to use the processor object to find its address
    // since it may have been moved by GC while in native code.
    //
    // ASSUME T1 (EDX) is available ??? looks like PR still valid
    // COULD use JTOC since it is reloaded immediately below - 
    
    // T1<-addr or processor statusword 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T1,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    asm.emitMOV_RegDisp_Reg (EBX, WORDSIZE, T1);

    // Insert the JNI arg at the second entry: class or object as a jref index
    // first reload JTOC,  baseline compiler assumes JTOC register -> jtoc
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());
    if (method.isStatic()) {
      // For static method, push on arg stack the VM_Class object
      //    jtoc[tibOffset] -> class TIB ptr -> first TIB entry -> class object -> classForType
      klass.getClassForType();     // ensure the Java class object is created
      int tibOffset = klass.getTibOffset();
      asm.emitMOV_Reg_RegDisp (EBX, JTOC, tibOffset);
      asm.emitMOV_Reg_RegInd (EBX, EBX);
      asm.emitMOV_Reg_RegDisp (EBX, EBX, VM_Entrypoints.classForTypeField.getOffset());
      firstActualParameter = 0;
    } else {
      // For nonstatic method, "this" pointer should be the first arg in the caller frame,
      // make it the 2nd arg in the glue frame
      asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset+WORDSIZE);
      firstActualParameter = 1;
    }

    // Generate the code to push this pointer in ebx on to the JNIRefs stack 
    // and use the JREF index in its place
    // Assume: S0 is the jniEnv pointer (left over from above)
    //         T0 contains the address to TOP of JNIRefs stack
    // Kill value in ebx
    // On return, ebx contains the JREF index
    pushJNIref(asm);
    asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + WORDSIZE, EBX);  // store as 2nd arg

    // VM.sysWrite("VM_JNICompiler:  processing args "); 
    // VM.sysWrite(numArguments);
    // VM.sysWrite("\n");

    // Now fill in the rest:  copy parameters from caller frame into glue frame 
    // in reverse order for C
    int i=parameterWords - 1;   
    int fpr = numFloats-1;
    for (int argIndex=numArguments-1; argIndex>=0; argIndex--) {

      // for reference, substitute with a jref index
      if (types[argIndex].isReferenceType()) {
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitCMP_Reg_Imm(EBX, 0);
	VM_ForwardReference beq = asm.forwardJcc(asm.EQ);
	pushJNIref(asm);
	beq.resolve(asm);
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	i--;
      
      // for float and double, the first NUM_PARAMETER_FPRS args have
      // been loaded in the FPU stack, need to pop them from there
      } else if (types[argIndex].isDoubleType()) {
	if (fpr < NUM_PARAMETER_FPRS) {
	  // pop this 2-word arg from the FPU stack
	  asm.emitFSTP_RegDisp_Reg_Quad(EBP, emptyStackOffset + (WORDSIZE*(2+ i - 1)), FP0);	
	} else {
	  // copy this 2-word arg from the caller frame
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i -1)), EBX);
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - ((i-1)*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i)), EBX);	  
	}
	i-=2;
	fpr--;
      } else if (types[argIndex].isFloatType()) {
	if (fpr < NUM_PARAMETER_FPRS) {
	  // pop this 1-word arg from the FPU stack
	  asm.emitFSTP_RegDisp_Reg(EBP, emptyStackOffset + (WORDSIZE*(2+ i)), FP0);
	} else {
	  // copy this 1-word arg from the caller frame
	  asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	  asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	}
	i--;
	fpr--;
      } else if (types[argIndex].isLongType()) {
	//  copy other 2-word parameters: observe the high/low order when moving
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i - 1)), EBX);
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - ((i-1)*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2 + i)), EBX);
	i-=2;
      } else {
	// copy other 1-word parameters
	asm.emitMOV_Reg_RegDisp (EBX, EBP, firstParameterOffset - (i*WORDSIZE));
	asm.emitMOV_RegDisp_Reg (EBP, emptyStackOffset + (WORDSIZE*(2+ i)), EBX);
	i--;
      }
    }

    // don't need any more since the top was bumped at the beginning
    // endJNIrefForProlog(asm);

    // >>>> THERE <<<<
    // End use of T0 and S0
  }

  /**************************************************************
   * Generate code to convert a pointer value to a JREF index
   * This includes the following steps:
   *   (1) start by calling startJNIrefForProlog()
   *   (2) for each reference, put it in ebx and call pushJNIref() 
   *       to convert; the handler will be left in ebx
   *   (3) finish by calling endJNIrefForProlog()
   *  
   *   +-------+
   *   |       |  <-JNIRefsMax (byte index of last entry)
   *   |       |
   *   |       |
   *   |       |  <-JNIRefsTop (byte index of valid top entry)
   *   |       |
   *   |       |
   *   |FPindex|  <-JNIRefsSavedFP (byte index of Java to C transition)
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |
   *   |       |  <-JNIRefs
   *   +-------+
   *
   */

  /**
   * Start a new frame for this Java to C transition:
   * Expect: 
   *    -S0 contains a pointer to the VM_Thread.jniEnv
   * Perform these steps:
   *    -push current SavedFP index 
   *    -set SaveFP index <- current TOP
   * Leave registers ready for more push onto the jniEnv.JNIRefs array
   *    -S0 holds jniEnv so we can update jniEnv.JNIRefsTop and 
   *    -T0 holds address of top =  starting address of jniEnv.JNIRefs array + jniEnv.JNIRefsTop
   *     T0 is to be incremented before each push
   *    S0              ebx                         T0
   *  jniEnv        
   *    .         jniEnv.JNIRefs             jniEnv.JNIRefsTop
   *    .               .                    jniEnv.JNIRefsTop + 4
   *    .         jniEnv.JNIRefsSavedFP            .
   *    .               .                    jniEnv.JNIRefsTop
   *    .               .                    address(JNIRefsTop)             
   *    .
   */
  static void startJNIrefForProlog(VM_Assembler asm, int numRefsExpected) {

    // on entry, S0 contains a pointer to the VM_Thread.jniEnv
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base

    // get and check index of top for overflow
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());  // T0 <- index of top
    asm.emitADD_Reg_Imm(T0, numRefsExpected * WORDSIZE);                // increment index of top 
    asm.emitCMP_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsMaxField.getOffset());   // check against JNIRefsMax for overflow 
    // TODO:  Do something if overflow!!!

    // get and increment index of top 
    // asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopOffset);  // T0 <- index of top
    // asm.emitADD_Reg_Imm(T0, WORDSIZE);                                  // increment index of top        
    // asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopOffset, T0);  // jniEnv.JNIRefsTop <- T0
    

    asm.emitADD_RegDisp_Imm (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), WORDSIZE); // increment index of top
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());  // T0 <- index of top
    asm.emitADD_Reg_Reg(T0, EBX);                                       // T0 <- address of top (not index)

    // start new frame:  push current JNIRefsSavedFP onto stack and set it to the new top index    
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset()); // ebx <- jniEnv.JNIRefsSavedFP
    asm.emitMOV_RegInd_Reg  (T0, EBX);                                   // push (T0) <- ebx
    asm.emitMOV_Reg_RegDisp (T0, S0, VM_Entrypoints.JNIRefsTopField.getOffset());   // reload T0 <- index of top
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), T0); // jniEnv.JNIRefsSavedFP <- index of top 

    // leave T0 with address pointing to the top of the frame for more push later
    asm.emitADD_Reg_RegDisp(T0, S0, VM_Entrypoints.JNIRefsField.getOffset());       // recompute T0 <- address of top (not index)

    // and go ahead and bump up the Top offset by the amount expected
    asm.emitADD_RegDisp_Imm(S0, VM_Entrypoints.JNIRefsTopField.getOffset(), numRefsExpected * WORDSIZE);
  }

  /**
   * Push a pointer value onto the JNIRefs array, 
   * Expect:
   *   -T0 pointing to the address of the valid top 
   *   -the pointer value in register ebx
   *   -the space in the JNIRefs array has checked for overflow 
   *   by startJNIrefForProlog()
   * Perform these steps:
   *   -increment the JNIRefsTop index in ebx by 4
   *   -push a pointer value in ebx onto the top of the JNIRefs array
   *   -put the JNIRefsTop index into the sourceReg as the replacement for the pointer
   * Note:  jniEnv.JNIRefsTop is not updated yet
   *
   */
  static void pushJNIref(VM_Assembler asm) {
    asm.emitADD_Reg_Imm (T0, WORDSIZE);                            // increment top address
    asm.emitMOV_RegInd_Reg(T0, EBX);                               // store ref at top
    asm.emitMOV_Reg_Reg (EBX, T0);                                 // replace ref in ebx with top address
    asm.emitSUB_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());   // subtract base address to get index
  }

  /**
   * Wrap up the access to the JNIRefs array
   * Expect:
   *   -T0 pointing to the address of the valid top 
   *   -S0 holding the pointer to jniEnv
   * Perform these steps:
   *   -recompute value in T0 as byte offset from jniEnv.JNIRefs base
   *   -store value in T0 back into jniEnv.JNIRefsTop
   *
   */
  // static void endJNIrefForProlog(VM_Assembler asm) {
  //   asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base
  //   asm.emitSUB_Reg_Reg     (T0, EBX);                                  // S0 <- index of top
  //   asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), T0);  // jniEnv.JNIRefsTop <- T0
  // }

  /**
   * Generate the code to pop the frame in JNIRefs array for this Java to C transition
   * Expect:
   *  -JTOC, PR registers are valid
   *  -S0 contains a pointer to the VM_Thread.jniEnv
   *  -EBX and T1 are available as scratch registers
   * Perform these steps:
   *  -jniEnv.JNIRefsTop <- jniEnv.JNIRefsSavedFP - 4
   *  -jniEnv.JNIRefsSavedFP <- (jniEnv.JNIRefs + jniEnv.JNIRefsSavedFP)
   *
   */
  static void popJNIrefForEpilog(VM_Assembler asm) {
    
    // on entry, S0 contains a pointer to the VM_Thread.jniEnv
    // set TOP to point to entry below the last frame
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset());    // ebx <- JNIRefsSavedFP
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), T1);        // JNIRefsTop <- ebx
    asm.emitSUB_RegDisp_Imm (S0, VM_Entrypoints.JNIRefsTopField.getOffset(), WORDSIZE);  // JNIRefsTop -= 4

    // load savedFP with the index to the last frame
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.JNIRefsField.getOffset());    // ebx <- JNIRefs base
    asm.emitMOV_Reg_RegIdx  (EBX, EBX, T1, asm.BYTE, 0);                // ebx <- (JNIRefs base + SavedFP index)
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), EBX);  // JNIRefsSavedFP <- ebx

  }
  

  /*****************************************************************
   * Handle the C to Java transition:  JNI methods in VM_JNIFunctions.java
   * NOTE:
   *   -We need PR to access Java environment, but not certain whether
   *    Linux C treats it as nonvolatile and restores it before calling, 
   *    so for now it is saved in the JNIenv and restored from there.
   *   -Unlike the powerPC scheme which has a special prolog preceding
   *    the normal Java prolog, the Intel scheme replaces the Java prolog
   *    completely with the special prolog
   *
   *            Stack on entry            Stack at end of prolog after call
   *             high memory 			   high memory
   *            |            |                   |            |
   *	EBP ->	|saved FP    | 			 |saved FP    |
   *            |  ...       |                   |  ...       |
   *            |            |                   |            |
   *		|arg n-1     | 			 |arg n-1     |
   * native    	|  ...       | 			 |  ...       |       
   * caller    	|arg 0       | 			 |arg 0       |
   *	ESP -> 	|return addr |        		 |return addr |
   *            |            |           EBP ->  |saved FP    |
   *            |            |                   |methodID    | normal MethodID for JNI function
   *            |            |                   |saved JavaFP| offset to preceeding java frame
   *            |            |                   |saved edi   |	to be used for JTOC
   *            |            |                   |  "   ebx   |	to be used for nonvolatile
   *            |            |                   |  "   ecx   |	to be used for scrach
   *            |            |                   |  "   esi   |	to be used for PR
   *            |            |                   |arg 0       | copied in reverse order
   *            |            |                   |  ...       |
   *            |            |           ESP ->  |arg n-1     |
   *            |            |                   |            | normally compiled Java code continue
   *            |            |                   |            |
   *            |            |                   |            |
   *            |            |                   |            |
   *             low memory                        low memory
   *
   */

  static void generateGlueCodeForJNIMethod(VM_Assembler asm, VM_Method method, int methodID) {
    VM_Address bootRecordAddress = VM_Magic.objectAsAddress(VM_BootRecord.the_boot_record);

    // set 2nd word of header = return address already pushed by CALL
    // NOTE: C calling convention is that EBP contains the caller's framepointer.
    //       Therefore our C to Java transition frames must follow this protocol,
    //       not the RVM protocol in which the caller's framepointer is in 
    //       pr.framePointer and EBP is a nonvolatile register.
    asm.emitPUSH_Reg(EBP);          

    // start new frame:  set FP to point to the new frame
    asm.emitMOV_Reg_Reg (EBP, SP); 

    // set first word of header: method ID
    asm.emitPUSH_Imm (methodID); 
    asm.emitSUB_Reg_Imm (SP, WORDSIZE);  // leave room for saved -> preceeding java frame, set later

    // save registers that will be used in RVM, to be restored on return to C
    asm.emitPUSH_Reg(JTOC); 
    asm.emitPUSH_Reg(EBX);         
    asm.emitPUSH_Reg(S0);         
    VM_ProcessorLocalState.emitPushProcessor(asm);
    
     // copy the arguments in reverse order
    VM_Type[] types = method.getParameterTypes();   // does NOT include implicit this or class ptr
    int numArguments = types.length;                // number of arguments for this method
    int argOffset = 2;           // add 2 to get to arg area in caller frame
    for (int i=0; i<numArguments; i++) {
      if (types[i].isLongType() || types[i].isDoubleType()) {
        // handle 2-words case:
        asm.emitMOV_Reg_RegDisp (EBX, EBP, ((argOffset+1)*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        asm.emitMOV_Reg_RegDisp (EBX, EBP, (argOffset*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        argOffset+=2;
      } else {
        // Handle 1-word case:
        // add 2 to get to arg area in caller frame
        asm.emitMOV_Reg_RegDisp (EBX, EBP, (argOffset*WORDSIZE));  
        asm.emitPUSH_Reg(EBX);
        argOffset++;
      }
    }

    // START of code sequence to atomically change processor status from IN_NATIVE
    // to IN_JAVA, looping in a call to sysVirtualProcessorYield if BLOCKED_IN_NATIVE

    int retryLabel = asm.getMachineCodeIndex();     // backward branch label

    // Restore JTOC through the JNIEnv passed back from the C code as the first parameter:
    // an extra entry at the end of the JNIFunctions array contains the RVM JTOC
    //
    // NOTE - we need the JTOC here only to get the sysYield.. entry point out of the
    // bootrecord. we could alternatively also put the bootrecord address at the end
    // of JNIFunctions or put it there INSTEAD of the JTOC

    asm.emitMOV_Reg_RegDisp (EBX, EBP, (2*WORDSIZE));   // pick up arg 0 (from callers frame)
    asm.emitMOV_Reg_RegDisp (JTOC, EBX, 0);                         // JTOC<-addr of JNIFunctions[0]
    asm.emitMOV_Reg_RegDisp (JTOC, JTOC, JNIFUNCTIONS_JTOC_OFFSET); // JTOC<-JNIFunctions[saved JTOC]

    // address of current processors status word is stored at jniEnv (first arg) + 4;
    asm.emitMOV_Reg_RegDisp (S0, EBX, WORDSIZE);     // S0 <- addr of status word

    asm.emitMOV_Reg_RegInd(T0,S0);                         // T0<-contents of statusword 
    asm.emitCMP_Reg_Imm (T0, VM_Processor.IN_NATIVE);      // jmp if still IN_NATIVE
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);       // if so, skip 3 instructions

    // blocked in native, do pthread yield
    asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());  // T0<-bootrecord addr
    asm.emitCALL_RegDisp(T0, VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset());
    asm.emitJMP_Imm (retryLabel);                          // retry from beginning

    fr.resolve(asm);      // branch here if IN_NATIVE, attempt to go to IN_JAVA

    // T0 (EAX) contains "old value" (required for CMPXCNG instruction)
    // S0 contains address of status word to be swapped
    asm.emitMOV_Reg_Imm (T1, VM_Processor.IN_JAVA);  // T1<-new value (IN_JAVA)
    asm.emitCMPXCHG_RegInd_Reg(S0,T1);               // atomic compare-and-exchange
    asm.emitJCC_Cond_Imm(asm.NE,retryLabel);

    // END of code sequence to change state from IN_NATIVE to IN_JAVA

    // status is now IN_JAVA. GC can not occur while we execute on a processor
    // in this state, so it is safe to access fields of objects
    // JTOC reg has been restored to RVM JTOC

    // done saving, bump SP to reserve room for the local variables
    // SP should now be at the point normally marked as emptyStackOffset
    int numLocalVariables = method.getLocalWords() - method.getParameterWords();
    asm.emitSUB_Reg_Imm (SP, (numLocalVariables << LG_WORDSIZE));
   
    // Compute the byte offset for this thread (offset into the VM_Scheduler.threads array)
    asm.emitMOV_Reg_RegDisp (S0, JTOC, VM_Entrypoints.JNIFunctionPointersField.getOffset());   // S0 <- base addr
    asm.emitSUB_Reg_Reg (EBX, S0);                     // ebx <- offset
    asm.emitSHR_Reg_Imm (EBX, 1);                      // byte offset:  divide by 2 (=> off in threads array)
    // then get the thread and its real JNIEnv
    asm.emitMOV_Reg_RegDisp (S0, JTOC, VM_Entrypoints.threadsField.getOffset());   // S0 <- VM_Thread array
    asm.emitMOV_Reg_RegIdx  (S0, S0, EBX, asm.BYTE, 0);                 // S0 <- VM_Thread object
    asm.emitMOV_Reg_RegDisp (EBX, S0, VM_Entrypoints.jniEnvField.getOffset());     // ebx <- JNIenv

    // now EBX -> jniEnv for thread

    // Retrieve -> preceeding "top" java FP from jniEnv and save in current 
    // frame of JNIFunction
    asm.emitMOV_Reg_RegDisp (ESI, EBX, VM_Entrypoints.JNITopJavaFPField.getOffset());  
    // asm.emitMOV_Reg_RegDisp (PR, EBX, VM_Entrypoints.JNITopJavaFPField.getOffset());  
    
    // get offset from current FP (PR <- PR - FP)
    asm.emitSUB_Reg_Reg (ESI, EBP);    

    asm.emitMOV_RegDisp_Reg (EBP, SAVED_JAVA_FP_OFFSET, ESI);                // save in hdr of current frame 

    // Restore the VM_Processor value saved on the Java to C transition
    VM_ProcessorLocalState.emitSetProcessor(asm, EBX, 
					    VM_Entrypoints.JNIEnvSavedPRField.getOffset());


    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              EBP);
    
    // Test if calling Java JNIFunction on a RVM processor or 
    // a Native processor.
    // at this point: JTOC and PR have been restored & 
    // processor status = IN_JAVA,
    // arguments for the call have been setup, space on the stack for locals
    // has been acquired.

    // load mode of current processor for testing (RVM or NATIVE)
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0,
                                              VM_Entrypoints.processorModeField.getOffset());

    asm.emitCMP_Reg_Imm (T0, VM_Processor.RVM);           // test for RVM
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);     // Br if yes

    // If here, on a native processor, it is necessary to transfer back to a
    // RVM processor before executing the Java JNI Function.

    // !!! what about saving regs, especially FPRs ??? (CHECK)

    // branch to becomeRVMThread to make the transfer.

    // If GC occurs while we are on the transfer queue of the RVM processor,
    // what will the MapIterator do, will we appear to be in the prolog of the 
    // Java JNI Function? Will there be any refs to report? any saved regs to report?

    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.becomeRVMThreadMethod.getOffset());

    // execution here is now on the RVM processor, and on a different
    // os pThread. PR now points to the new RVM processor we have
    // been transferred to.  JTOC was set when we were re-dispatched.

    // XXX Restoring regs? especially FPRs ??? (CHECK)

    fr1.resolve(asm);  // branch to here if returning on a RVM processor

    // finally proceed with the normal Java compiled code
    // skip the thread switch test for now, see VM_Compiler.genThreadSwitchTest(true)

    asm.emitNOP(); // end of prologue marker

  }

  static void generateEpilogForJNIMethod(VM_Assembler asm, VM_Method method) {

    // assume RVM PR regs still valid. potentially T1 & T0 contain return
    // values and should not be modified. we use regs saved in prolog and restored
    // before return to do whatever needs to be done.  does not assume JTOC is valid,
    // and may use it as scratch reg.

    // if returning long, switch the order of the hi/lo word in T0 and T1
    if (method.getReturnType().isLongType()) {
      asm.emitPUSH_Reg(T1);    
      asm.emitMOV_Reg_Reg(T1, T0);  
      asm.emitPOP_Reg(T0);      
    }

    // current processor status should be IN_JAVA, implying no GC and being safe
    // to reference java objects and use PR

    // S0<-addr activethread
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.activeThreadField.getOffset()); 
    asm.emitMOV_Reg_RegDisp(S0, S0, VM_Entrypoints.jniEnvField.getOffset());       // S0<-addr threads jniEnv

    // set jniEnv TopJavaFP using value saved in frame in prolog
    asm.emitMOV_Reg_RegDisp(JTOC, EBP, SAVED_JAVA_FP_OFFSET);      // JTOC<-saved TopJavaFP (offset)
    asm.emitADD_Reg_Reg(JTOC, EBP);                                // change offset from FP into address
    asm.emitMOV_RegDisp_Reg (S0, VM_Entrypoints.JNITopJavaFPField.getOffset(), JTOC); // jniEnv.TopJavaFP <- JTOC

    // in case thread has migrated to different PR, reset saved PRs to current PR
    // first reset PR saved in jniEnv
    VM_ProcessorLocalState.emitStoreProcessor(asm, S0,
                                              VM_Entrypoints.JNIEnvSavedPRField.getOffset());

    // now save PR saved in preceeding JavaToNative transition frame, whose FP
    // is now in JTOC
    VM_ProcessorLocalState.emitStoreProcessor(asm, JTOC, JNI_PR_OFFSET);

    // get address of current processors statusword
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, EBX,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    // if processor changed, then statusword address, stored after the threads
    // jniEnv JNIFunction ptr needs to be changed. reuse JTOC as scratch
    asm.emitMOV_Reg_RegDisp(JTOC, S0, VM_Entrypoints.JNIEnvAddressField.getOffset());  // JTOC<-jniEnv.JNIEnvAddress
    asm.emitMOV_RegDisp_Reg (JTOC, WORDSIZE, EBX);         // [JTOC+4] <- processor statusword addr

    // change current processor status to IN_NATIVE
    asm.emitMOV_RegInd_Imm(EBX, VM_Processor.IN_NATIVE);

    // reload native/C nonvolatile regs - saved in prolog
    // what about FPRs
    VM_ProcessorLocalState.emitPopProcessor(asm);
    asm.emitPOP_Reg(S0);         
    asm.emitPOP_Reg(EBX);         
    asm.emitPOP_Reg(JTOC); 

    // NOTE: C expects the framepointer to be restored to EBP, so 
    //       the epilogue for the C to Java glue code must follow that 
    //       convention, not the RVM one!
    //       Also note that RVM treats EBP is a nonvolatile, so we don't
    //       explicitly save/restore it.
    asm.emitMOV_Reg_Reg(SP, EBP);                           // discard current stack frame
    asm.emitPOP_Reg(EBP);
    asm.emitRET();              // return to caller
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.lang.reflect.*;

/**
 *   This class implements the JNI environment, it includes:
 * -The array of JNI function pointers accessible from C
 * -Implementation of all the JNI functions
 *
 * @author Ton Ngo
 * @author Steve Smith 
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCUtil;

public class VM_JNIEnvironment implements VM_JNILinuxConstants, VM_RegisterConstants {

  private static boolean initialized = false;
  private static String[] names;

  /**
   * This is the JNI function table, the address of this array will be
   * passed to the native code
   */
  private static INSTRUCTION[][] JNIFunctions;

  /**
   * This is a table of pointers to the shared JNI function table.  All entries 
   * point to the same function table.  Each thread uses the pointer at its thread id
   * offset to allow us to determine a threads id from the pointer it is using.
   * Needed when native calls Java (JNIFunctions) and passes its JNIEnv pointer.
   * Its offset into the JNIFunctionPts array is the same as the threads offset
   * in the Scheduler.threads array.
   */
    //  private static int[] JNIFunctionPointers;
    static int[] JNIFunctionPointers;        // made public so vpStatus could be set 11/16/00 SES
                                             // maybe need set & get functions ??

  /**
   * These are thread specific information, such as:
   *  -the list of references passed to native code, for GC purpose
   *  -saved RVM system registers
   */
  VM_Address JNIEnvAddress;      // contain a pointer to the JNIFunctions array
  int savedTIreg;         // for saving thread index register on entry to native, to be restored on JNI call from native
  VM_Processor savedPRreg; // for saving processor register on entry to native, to be restored on JNI call from native
  boolean alwaysHasNativeFrame;  // true if the bottom stack frame is native, such as thread for CreateJVM or AttachCurrentThread

  int[] JNIRefs;          // references passed to native code
  int   JNIRefsTop;       // -> address of current top ref in JNIRefs array 
  int   JNIRefsMax;       // -> address of end (last entry) of JNIRefs array
  int   JNIRefsSavedFP;   // -> previous frame boundary in JNIRefs array
  public VM_Address JNITopJavaFP;     // -> Top java frame when in C frames on top of the stack

  Throwable pendingException = null;

  // Saved context for thread attached to external pthread.  This context is
  // saved by the JNIService thread and points to the point in JNIStartUp thread
  // where it yields to the queue in the native VM_Processor.
  // When DetachCurrentThread is called, the JNIService thread restores this context 
  // to allow the thread to run VM_Thread.terminate on its original stack.
  VM_Registers savedContextForTermination;

  // temporarily use a fixed size array for JNI refs, later grow as needed
  static final int JNIREFS_ARRAY_LENGTH = 100;

  public static void init() {

    // allocate the first dimension of the function array in the boot image so that
    // we have an address pointing to it.  This is necessary for thread creation
    // since the VM_JNIEnvironment object will contain a field pointing to this array

    // An extra entry is allocated, to hold the RVM JTOC 07/01 SES

    // JNIFunctions = new INSTRUCTION[FUNCTIONCOUNT][];
    // Why is INSTRUCTION not working?  getting filled with null
    JNIFunctions = new byte[FUNCTIONCOUNT+1][];

    // First word is a pointer to the JNIFunction table
    // Second word is address of current processors vpStatus word
    // (JTOC is now stored at end of shared JNIFunctions array)
    JNIFunctionPointers = new int[VM_Scheduler.MAX_THREADS * 2];
  }

  /**
   *  Initialize the array of JNI functions
   *  To be called from VM_DynamicLibrary.java when a library is loaded,
   *  expecting native calls to be made
   *
   */
  public static void boot() {

    if (initialized)
      return;

    // fill an array of JNI names
    setNames();

    // fill in the IP entries for each AIX linkage triplet
    try {
      VM_Class cls = VM_Class.forName("VM_JNIFunctions");
      VM_Method[] mths = cls.getDeclaredMethods();
      for (int i=0; i<mths.length; i++) {
	String methodName = mths[i].getName().toString();
	int jniIndex = indexOf(methodName);
	if (jniIndex!=-1) {
	  JNIFunctions[jniIndex] = mths[i].getCurrentCompiledMethod().getInstructions();
	  // VM.sysWrite("   " + methodName + "=" + VM.intAsHexString(JNIFunctions[jniIndex]));
	} 
      }

    } catch (VM_ResolutionException e) {
      throw new InternalError("VM_JNIEnvironment fails to initialize, has the class been renamed\n");
    }

    // store RVM JTOC address in last (extra) entry in JNIFunctions array
    // to be restored when native C invokes JNI functions implemented in java
    //
    // following causes exception in checkstore, so forced to setMemoryWord instead
    // JNIFunctions[FUNCTIONCOUNT+1] = VM_Magic.addressAsByteArray(VM_Magic.getTocPointer());
    VM_Magic.setMemoryAddress(VM_Magic.objectAsAddress(JNIFunctions).add(JNIFUNCTIONS_JTOC_OFFSET),
			      VM_Magic.getTocPointer());

    initialized = true;
  }

  // Instance:  create a thread specific JNI environment.  threadSlot = creating threads
  // thread id == index of its entry in Scheduler.threads array
  //
  public VM_JNIEnvironment (int threadSlot) {

    JNIFunctionPointers[threadSlot * 2] = VM_Magic.objectAsAddress(JNIFunctions).toInt();
    JNIFunctionPointers[(threadSlot * 2)+1] = 0;  // later contains addr of processor vpStatus word
    JNIEnvAddress = VM_Magic.objectAsAddress(JNIFunctionPointers).add(threadSlot*8);
    JNIRefs = new int[JNIREFS_ARRAY_LENGTH];
    JNIRefs[0] = 0;                       // 0 entry for bottom of stack
    JNIRefsTop = 0;
    JNIRefsSavedFP = 0;
    JNIRefsMax = (JNIRefs.length - 1) * 4;   // byte offset to last entry

    // initially TOP and SavedFP -> entry 0 containing 0

    alwaysHasNativeFrame = false;
  }

  // push a reference onto thread local JNIRefs stack.  To be used by JNI
  // Functions when returning a reference back to JNI native C code
  // Taken:    Object to put on stack
  // Returned: offset of entry in JNIRefs stack
  // 
  public int pushJNIRef( Object ref ) {
    if (ref == null) return 0;
    if (VM.VerifyAssertions) VM.assert( VM_GCUtil.validRef( VM_Magic.objectAsAddress(ref) ) );
    JNIRefsTop += 4;
    if (JNIRefsTop >> 2 >= JNIRefs.length) {
	int[] newrefs = new int[ JNIRefs.length * 2 ];
	for(int i = 0; i < JNIRefs.length; i++) newrefs[i] = JNIRefs[i];
	JNIRefs = newrefs;
    }
    JNIRefs[ JNIRefsTop >> 2 ] = VM_Magic.objectAsAddress(ref).toInt();
    return JNIRefsTop;
  }

  // get a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  // Returned: reference at that offset
  public Object getJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP, ");
      VM.sysWrite(offset); 
      VM.sysWrite("(top is ");
      VM.sysWrite(JNIRefsTop);
      VM.sysWrite(")\n");
      return null;
    }
    if (offset < 0)
	return VM_JNIGlobalRefTable.ref( offset );
    else
	return VM_Magic.addressAsObject( VM_Address.fromInt(JNIRefs[ offset>>2 ]) );
    
  }

  // remove a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  public void deleteJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP, ");
      VM.sysWrite(offset); 
      VM.sysWrite("(top is ");
      VM.sysWrite(JNIRefsTop);
      VM.sysWrite(")\n");
    }
    
    JNIRefs[ offset>>2 ] = 0;

    if (offset == JNIRefsTop) JNIRefsTop -= 4;
  }

  // record an exception as pending so that it will be delivered on the return
  // to the Java caller;  clear the exception by recording null
  // Taken:  an exception or error
  // Returned:  nothing
  //
  public void recordException(Throwable e) {
    // don't overwrite the first exception except to clear it
    if (pendingException==null || e==null)
      pendingException = e;
  }

  // return the pending exception
  // Taken:  nothing
  // Returned:  an exception or error
  //
  public Throwable getException() {
    return pendingException;
  }

  //
  // get the address of the JNIFunctions array, which should be in the JTOC
  // Taken:    nothing 
  // Returned: the address of the JNIFunctions array 
  // 
  public VM_Address getJNIenvAddress() {
    return JNIEnvAddress;
  }

  public INSTRUCTION[] getInstructions(int id) {    
    return JNIFunctions[id];
  }

  //
  // get the JNI index for a function name
  // Taken:    a JNI function name
  // Returned: the index for this function, -1 if not found
  //
  private static int indexOf(String functionName) {
    for (int i=0; i<FUNCTIONCOUNT; i++) {
      if (names[i].equals(functionName))
	return i;
    }
    return -1;
  }

  private static String[] setNames() {
    names = new String[FUNCTIONCOUNT];
    names[0]                             = new String("undefined");
    names[RESERVED0]                     = new String("reserved0")                     ;	  
    names[RESERVED1]                     = new String("reserved1")                     ;	  
    names[RESERVED2]                     = new String("reserved2")                     ;	  
    names[RESERVED3]                     = new String("reserved3")                     ;	  
    names[GETVERSION]                    = new String("GetVersion")                    ;	  
    names[DEFINECLASS]                   = new String("DefineClass")                   ;	  
    names[FINDCLASS]                     = new String("FindClass")                     ;	  
    names[FROMREFLECTEDMETHOD]         	 = new String("FromReflectedMethod"); //  JDK1.2, #7      
    names[FROMREFLECTEDFIELD]          	 = new String("FromReflectedField");  //  JDK1.2, #8      
    names[TOREFLECTEDMETHOD]           	 = new String("ToReflectedMethod");   //  JDK1.2, #9      
    names[GETSUPERCLASS]                 = new String("GetSuperclass")                 ;	  
    names[ISASSIGNABLEFROM]              = new String("IsAssignableFrom")              ;	  
    names[TOREFLECTEDFIELD]            	 = new String("ToReflectedField");    //  JDK1.2, #12      
    names[THROW]                         = new String("Throw")                         ;	  
    names[THROWNEW]                      = new String("ThrowNew")                      ;	  
    names[EXCEPTIONOCCURRED]             = new String("ExceptionOccurred")             ;	  
    names[EXCEPTIONDESCRIBE]             = new String("ExceptionDescribe")             ;	  
    names[EXCEPTIONCLEAR]                = new String("ExceptionClear")                ;	  
    names[FATALERROR]                    = new String("FatalError")                    ;	  
    names[PUSHLOCALFRAME]              	 = new String("PushLocalFrame");      //  JDK1.2, #19      
    names[POPLOCALFRAME]               	 = new String("PopLocalFrame");       //  JDK1.2, #20      
    names[NEWGLOBALREF]                  = new String("NewGlobalRef")                  ;	  
    names[DELETEGLOBALREF]               = new String("DeleteGlobalRef")               ;	  
    names[DELETELOCALREF]                = new String("DeleteLocalRef")                ;	  
    names[ISSAMEOBJECT]                  = new String("IsSameObject")                  ;	  
    names[NEWLOCALREF]                 	 = new String("NewLocalRef");         //  JDK1.2, #25      
    names[ENSURELOCALCAPACITY]         	 = new String("EnsureLocalCapacity"); //  JDK1.2, #26   
    names[ALLOCOBJECT]                   = new String("AllocObject")                   ;	  
    names[NEWOBJECT]                     = new String("NewObject")                     ;	  
    names[NEWOBJECTV]                    = new String("NewObjectV")                    ;	  
    names[NEWOBJECTA]                    = new String("NewObjectA")                    ;	  
    names[GETOBJECTCLASS]                = new String("GetObjectClass")                ;	  
    names[ISINSTANCEOF]                  = new String("IsInstanceOf")                  ;	  
    names[GETMETHODID]                   = new String("GetMethodID")                   ;	  
    names[CALLOBJECTMETHOD]              = new String("CallObjectMethod")              ;	  
    names[CALLOBJECTMETHODV]             = new String("CallObjectMethodV")             ;	  
    names[CALLOBJECTMETHODA]             = new String("CallObjectMethodA")             ;	  
    names[CALLBOOLEANMETHOD]             = new String("CallBooleanMethod")             ;	  
    names[CALLBOOLEANMETHODV]            = new String("CallBooleanMethodV")            ;	  
    names[CALLBOOLEANMETHODA]            = new String("CallBooleanMethodA")            ;	  
    names[CALLBYTEMETHOD]                = new String("CallByteMethod")                ;	  
    names[CALLBYTEMETHODV]               = new String("CallByteMethodV")               ;	  
    names[CALLBYTEMETHODA]               = new String("CallByteMethodA")               ;	  
    names[CALLCHARMETHOD]                = new String("CallCharMethod")                ;	  
    names[CALLCHARMETHODV]               = new String("CallCharMethodV")               ;	  
    names[CALLCHARMETHODA]               = new String("CallCharMethodA")               ;	  
    names[CALLSHORTMETHOD]               = new String("CallShortMethod")               ;	  
    names[CALLSHORTMETHODV]              = new String("CallShortMethodV")              ;	  
    names[CALLSHORTMETHODA]              = new String("CallShortMethodA")              ;	  
    names[CALLINTMETHOD]                 = new String("CallIntMethod")                 ;	  
    names[CALLINTMETHODV]                = new String("CallIntMethodV")                ;	  
    names[CALLINTMETHODA]                = new String("CallIntMethodA")                ;	  
    names[CALLLONGMETHOD]                = new String("CallLongMethod")                ;	  
    names[CALLLONGMETHODV]               = new String("CallLongMethodV")               ;	  
    names[CALLLONGMETHODA]               = new String("CallLongMethodA")               ;	  
    names[CALLFLOATMETHOD]               = new String("CallFloatMethod")               ;	  
    names[CALLFLOATMETHODV]              = new String("CallFloatMethodV")              ;	  
    names[CALLFLOATMETHODA]              = new String("CallFloatMethodA")              ;	  
    names[CALLDOUBLEMETHOD]              = new String("CallDoubleMethod")              ;	  
    names[CALLDOUBLEMETHODV]             = new String("CallDoubleMethodV")             ;	  
    names[CALLDOUBLEMETHODA]             = new String("CallDoubleMethodA")             ;	  
    names[CALLVOIDMETHOD]                = new String("CallVoidMethod")                ;	  
    names[CALLVOIDMETHODV]               = new String("CallVoidMethodV")               ;	  
    names[CALLVOIDMETHODA]               = new String("CallVoidMethodA")               ;	  
    names[CALLNONVIRTUALOBJECTMETHOD]    = new String("CallNonvirtualObjectMethod")    ;	  
    names[CALLNONVIRTUALOBJECTMETHODV]   = new String("CallNonvirtualObjectMethodV")   ;	  
    names[CALLNONVIRTUALOBJECTMETHODA]   = new String("CallNonvirtualObjectMethodA")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHOD]   = new String("CallNonvirtualBooleanMethod")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHODV]  = new String("CallNonvirtualBooleanMethodV")  ;	  
    names[CALLNONVIRTUALBOOLEANMETHODA]  = new String("CallNonvirtualBooleanMethodA")  ;	  
    names[CALLNONVIRTUALBYTEMETHOD]      = new String("CallNonvirtualByteMethod")      ;	  
    names[CALLNONVIRTUALBYTEMETHODV]     = new String("CallNonvirtualByteMethodV")     ;	  
    names[CALLNONVIRTUALBYTEMETHODA]     = new String("CallNonvirtualByteMethodA")     ;	  
    names[CALLNONVIRTUALCHARMETHOD]      = new String("CallNonvirtualCharMethod")      ;	  
    names[CALLNONVIRTUALCHARMETHODV]     = new String("CallNonvirtualCharMethodV")     ;	  
    names[CALLNONVIRTUALCHARMETHODA]     = new String("CallNonvirtualCharMethodA")     ;	  
    names[CALLNONVIRTUALSHORTMETHOD]     = new String("CallNonvirtualShortMethod")     ;	  
    names[CALLNONVIRTUALSHORTMETHODV]    = new String("CallNonvirtualShortMethodV")    ;	  
    names[CALLNONVIRTUALSHORTMETHODA]    = new String("CallNonvirtualShortMethodA")    ;	  
    names[CALLNONVIRTUALINTMETHOD]       = new String("CallNonvirtualIntMethod")       ;	  
    names[CALLNONVIRTUALINTMETHODV]      = new String("CallNonvirtualIntMethodV")      ;	  
    names[CALLNONVIRTUALINTMETHODA]      = new String("CallNonvirtualIntMethodA")      ;	  
    names[CALLNONVIRTUALLONGMETHOD]      = new String("CallNonvirtualLongMethod")      ;	  
    names[CALLNONVIRTUALLONGMETHODV]     = new String("CallNonvirtualLongMethodV")     ;	  
    names[CALLNONVIRTUALLONGMETHODA]     = new String("CallNonvirtualLongMethodA")     ;	  
    names[CALLNONVIRTUALFLOATMETHOD]     = new String("CallNonvirtualFloatMethod")     ;	  
    names[CALLNONVIRTUALFLOATMETHODV]    = new String("CallNonvirtualFloatMethodV")    ;	  
    names[CALLNONVIRTUALFLOATMETHODA]    = new String("CallNonvirtualFloatMethodA")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHOD]    = new String("CallNonvirtualDoubleMethod")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHODV]   = new String("CallNonvirtualDoubleMethodV")   ;	  
    names[CALLNONVIRTUALDOUBLEMETHODA]   = new String("CallNonvirtualDoubleMethodA")   ;	  
    names[CALLNONVIRTUALVOIDMETHOD]      = new String("CallNonvirtualVoidMethod")      ;	  
    names[CALLNONVIRTUALVOIDMETHODV]     = new String("CallNonvirtualVoidMethodV")     ;	  
    names[CALLNONVIRTUALVOIDMETHODA]     = new String("CallNonvirtualVoidMethodA")     ;	  
    names[GETFIELDID]                    = new String("GetFieldID")                    ;	  
    names[GETOBJECTFIELD]                = new String("GetObjectField")                ;	  
    names[GETBOOLEANFIELD]               = new String("GetBooleanField")               ;	  
    names[GETBYTEFIELD]                  = new String("GetByteField")                  ;	  
    names[GETCHARFIELD]                  = new String("GetCharField")                  ;	  
    names[GETSHORTFIELD]                 = new String("GetShortField")                 ;	  
    names[GETINTFIELD]                   = new String("GetIntField")                   ;	  
    names[GETLONGFIELD]                  = new String("GetLongField")                  ;	  
    names[GETFLOATFIELD]                 = new String("GetFloatField")                 ;	  
    names[GETDOUBLEFIELD]                = new String("GetDoubleField")                ;	  
    names[SETOBJECTFIELD]                = new String("SetObjectField")                ;	  
    names[SETBOOLEANFIELD]               = new String("SetBooleanField")               ;	  
    names[SETBYTEFIELD]                  = new String("SetByteField")                  ;	  
    names[SETCHARFIELD]                  = new String("SetCharField")                  ;	  
    names[SETSHORTFIELD]                 = new String("SetShortField")                 ;	  
    names[SETINTFIELD]                   = new String("SetIntField")                   ;	  
    names[SETLONGFIELD]                  = new String("SetLongField")                  ;	  
    names[SETFLOATFIELD]                 = new String("SetFloatField")                 ;	  
    names[SETDOUBLEFIELD]                = new String("SetDoubleField")                ;	  
    names[GETSTATICMETHODID]             = new String("GetStaticMethodID")             ;	  
    names[CALLSTATICOBJECTMETHOD]        = new String("CallStaticObjectMethod")        ;	  
    names[CALLSTATICOBJECTMETHODV]       = new String("CallStaticObjectMethodV")       ;	  
    names[CALLSTATICOBJECTMETHODA]       = new String("CallStaticObjectMethodA")       ;	  
    names[CALLSTATICBOOLEANMETHOD]       = new String("CallStaticBooleanMethod")       ;	  
    names[CALLSTATICBOOLEANMETHODV]      = new String("CallStaticBooleanMethodV")      ;	  
    names[CALLSTATICBOOLEANMETHODA]      = new String("CallStaticBooleanMethodA")      ;	  
    names[CALLSTATICBYTEMETHOD]          = new String("CallStaticByteMethod")          ;	  
    names[CALLSTATICBYTEMETHODV]         = new String("CallStaticByteMethodV")         ;	  
    names[CALLSTATICBYTEMETHODA]         = new String("CallStaticByteMethodA")         ;	  
    names[CALLSTATICCHARMETHOD]          = new String("CallStaticCharMethod")          ;	  
    names[CALLSTATICCHARMETHODV]         = new String("CallStaticCharMethodV")         ;	  
    names[CALLSTATICCHARMETHODA]         = new String("CallStaticCharMethodA")         ;	  
    names[CALLSTATICSHORTMETHOD]         = new String("CallStaticShortMethod")         ;	  
    names[CALLSTATICSHORTMETHODV]        = new String("CallStaticShortMethodV")        ;	  
    names[CALLSTATICSHORTMETHODA]        = new String("CallStaticShortMethodA")        ;	  
    names[CALLSTATICINTMETHOD]           = new String("CallStaticIntMethod")           ;	  
    names[CALLSTATICINTMETHODV]          = new String("CallStaticIntMethodV")          ;	  
    names[CALLSTATICINTMETHODA]          = new String("CallStaticIntMethodA")          ;	  
    names[CALLSTATICLONGMETHOD]          = new String("CallStaticLongMethod")          ;	  
    names[CALLSTATICLONGMETHODV]         = new String("CallStaticLongMethodV")         ;	  
    names[CALLSTATICLONGMETHODA]         = new String("CallStaticLongMethodA")         ;	  
    names[CALLSTATICFLOATMETHOD]         = new String("CallStaticFloatMethod")         ;	  
    names[CALLSTATICFLOATMETHODV]        = new String("CallStaticFloatMethodV")        ;	  
    names[CALLSTATICFLOATMETHODA]        = new String("CallStaticFloatMethodA")        ;	  
    names[CALLSTATICDOUBLEMETHOD]        = new String("CallStaticDoubleMethod")        ;	  
    names[CALLSTATICDOUBLEMETHODV]       = new String("CallStaticDoubleMethodV")       ;	  
    names[CALLSTATICDOUBLEMETHODA]       = new String("CallStaticDoubleMethodA")       ;	  
    names[CALLSTATICVOIDMETHOD]          = new String("CallStaticVoidMethod")          ;	  
    names[CALLSTATICVOIDMETHODV]         = new String("CallStaticVoidMethodV")         ;	  
    names[CALLSTATICVOIDMETHODA]         = new String("CallStaticVoidMethodA")         ;	  
    names[GETSTATICFIELDID]              = new String("GetStaticFieldID")              ;	  
    names[GETSTATICOBJECTFIELD]          = new String("GetStaticObjectField")          ;	  
    names[GETSTATICBOOLEANFIELD]         = new String("GetStaticBooleanField")         ;	  
    names[GETSTATICBYTEFIELD]            = new String("GetStaticByteField")            ;	  
    names[GETSTATICCHARFIELD]            = new String("GetStaticCharField")            ;	  
    names[GETSTATICSHORTFIELD]           = new String("GetStaticShortField")           ;	  
    names[GETSTATICINTFIELD]             = new String("GetStaticIntField")             ;	  
    names[GETSTATICLONGFIELD]            = new String("GetStaticLongField")            ;	  
    names[GETSTATICFLOATFIELD]           = new String("GetStaticFloatField")           ;	  
    names[GETSTATICDOUBLEFIELD]          = new String("GetStaticDoubleField")          ;	  
    names[SETSTATICOBJECTFIELD]          = new String("SetStaticObjectField")          ;	  
    names[SETSTATICBOOLEANFIELD]         = new String("SetStaticBooleanField")         ;	  
    names[SETSTATICBYTEFIELD]            = new String("SetStaticByteField")            ;	  
    names[SETSTATICCHARFIELD]            = new String("SetStaticCharField")            ;	  
    names[SETSTATICSHORTFIELD]           = new String("SetStaticShortField")           ;	  
    names[SETSTATICINTFIELD]             = new String("SetStaticIntField")             ;	  
    names[SETSTATICLONGFIELD]            = new String("SetStaticLongField")            ;	  
    names[SETSTATICFLOATFIELD]           = new String("SetStaticFloatField")           ;	  
    names[SETSTATICDOUBLEFIELD]          = new String("SetStaticDoubleField")          ;	  
    names[NEWSTRING]                     = new String("NewString")                     ;	  
    names[GETSTRINGLENGTH]               = new String("GetStringLength")               ;	  
    names[GETSTRINGCHARS]                = new String("GetStringChars")                ;	  
    names[RELEASESTRINGCHARS]            = new String("ReleaseStringChars")            ;	  
    names[NEWSTRINGUTF]                  = new String("NewStringUTF")                  ;	  
    names[GETSTRINGUTFLENGTH]            = new String("GetStringUTFLength")            ;	  
    names[GETSTRINGUTFCHARS]             = new String("GetStringUTFChars")             ;	  
    names[RELEASESTRINGUTFCHARS]         = new String("ReleaseStringUTFChars")         ;	  
    names[GETARRAYLENGTH]                = new String("GetArrayLength")                ;	  
    names[NEWOBJECTARRAY]                = new String("NewObjectArray")                ;	  
    names[GETOBJECTARRAYELEMENT]         = new String("GetObjectArrayElement")         ;	  
    names[SETOBJECTARRAYELEMENT]         = new String("SetObjectArrayElement")         ;	  
    names[NEWBOOLEANARRAY]               = new String("NewBooleanArray")               ;	  
    names[NEWBYTEARRAY]                  = new String("NewByteArray")                  ;	  
    names[NEWCHARARRAY]                  = new String("NewCharArray")                  ;	  
    names[NEWSHORTARRAY]                 = new String("NewShortArray")                 ;	  
    names[NEWINTARRAY]                   = new String("NewIntArray")                   ;	  
    names[NEWLONGARRAY]                  = new String("NewLongArray")                  ;	  
    names[NEWFLOATARRAY]                 = new String("NewFloatArray")                 ;	  
    names[NEWDOUBLEARRAY]                = new String("NewDoubleArray")                ;	  
    names[GETBOOLEANARRAYELEMENTS]       = new String("GetBooleanArrayElements")       ;	  
    names[GETBYTEARRAYELEMENTS]          = new String("GetByteArrayElements")          ;	  
    names[GETCHARARRAYELEMENTS]          = new String("GetCharArrayElements")          ;	  
    names[GETSHORTARRAYELEMENTS]         = new String("GetShortArrayElements")         ;	  
    names[GETINTARRAYELEMENTS]           = new String("GetIntArrayElements")           ;	  
    names[GETLONGARRAYELEMENTS]          = new String("GetLongArrayElements")          ;	  
    names[GETFLOATARRAYELEMENTS]         = new String("GetFloatArrayElements")         ;	  
    names[GETDOUBLEARRAYELEMENTS]        = new String("GetDoubleArrayElements")        ;	  
    names[RELEASEBOOLEANARRAYELEMENTS]   = new String("ReleaseBooleanArrayElements")   ;	  
    names[RELEASEBYTEARRAYELEMENTS]      = new String("ReleaseByteArrayElements")      ;	  
    names[RELEASECHARARRAYELEMENTS]      = new String("ReleaseCharArrayElements")      ;	  
    names[RELEASESHORTARRAYELEMENTS]     = new String("ReleaseShortArrayElements")     ;	  
    names[RELEASEINTARRAYELEMENTS]       = new String("ReleaseIntArrayElements")       ;	  
    names[RELEASELONGARRAYELEMENTS]      = new String("ReleaseLongArrayElements")      ;	  
    names[RELEASEFLOATARRAYELEMENTS]     = new String("ReleaseFloatArrayElements")     ;	  
    names[RELEASEDOUBLEARRAYELEMENTS]    = new String("ReleaseDoubleArrayElements")    ;	  
    names[GETBOOLEANARRAYREGION]         = new String("GetBooleanArrayRegion")         ;	  
    names[GETBYTEARRAYREGION]            = new String("GetByteArrayRegion")            ;	  
    names[GETCHARARRAYREGION]            = new String("GetCharArrayRegion")            ;	  
    names[GETSHORTARRAYREGION]           = new String("GetShortArrayRegion")           ;	  
    names[GETINTARRAYREGION]             = new String("GetIntArrayRegion")             ;	  
    names[GETLONGARRAYREGION]            = new String("GetLongArrayRegion")            ;	  
    names[GETFLOATARRAYREGION]           = new String("GetFloatArrayRegion")           ;	  
    names[GETDOUBLEARRAYREGION]          = new String("GetDoubleArrayRegion")          ;	  
    names[SETBOOLEANARRAYREGION]         = new String("SetBooleanArrayRegion")         ;	  
    names[SETBYTEARRAYREGION]            = new String("SetByteArrayRegion")            ;	  
    names[SETCHARARRAYREGION]            = new String("SetCharArrayRegion")            ;	  
    names[SETSHORTARRAYREGION]           = new String("SetShortArrayRegion")           ;	  
    names[SETINTARRAYREGION]             = new String("SetIntArrayRegion")             ;	  
    names[SETLONGARRAYREGION]            = new String("SetLongArrayRegion")            ;	  
    names[SETFLOATARRAYREGION]           = new String("SetFloatArrayRegion")           ;	  
    names[SETDOUBLEARRAYREGION]          = new String("SetDoubleArrayRegion")          ;	  
    names[REGISTERNATIVES]               = new String("RegisterNatives")               ;	  
    names[UNREGISTERNATIVES]             = new String("UnregisterNatives")             ;	  
    names[MONITORENTER]                  = new String("MonitorEnter")                  ;	  
    names[MONITOREXIT]                   = new String("MonitorExit")                   ;	  
    names[GETJAVAVM]                     = new String("GetJavaVM")                     ;	  
    names[GETSTRINGREGION]             	 = new String("GetStringRegion");           // JDK 1.2, #220
    names[GETSTRINGUTFREGION]         	 = new String("GetStringUTFRegion");        // JDK 1.2, #221
    names[GETPRIMITIVEARRAYCRITICAL]   	 = new String("GetPrimitiveArrayCritical"); // JDK 1.2, #222
    names[RELEASEPRIMITIVEARRAYCRITICAL] = new String("ReleasePrimitiveArrayCritical"); // JDK 1.2, #223
    names[GETSTRINGCRITICAL]           	 = new String("GetStringCritical");         // JDK 1.2, # 224
    names[RELEASESTRINGCRITICAL]       	 = new String("ReleaseStringCritical");     // JDK 1.2, #225
    names[NEWWEAKGLOBALREF]            	 = new String("NewWeakGlobalRef");    	    // JDK 1.2, #226
    names[DELETEWEAKGLOBALREF]         	 = new String("DeleteWeakGlobalRef"); 	    // JDK 1.2, #227
    names[EXCEPTIONCHECK]              	 = new String("ExceptionCheck");      	    // JDK 1.2, #228

    return names;

  }

  /*****************************************************************************
   * Utility function called from VM_JNIFunction
   * (cannot be placed in VM_JNIFunction because methods there are specially compiled
   * to be called from native)
   *****************************************************************************/


  /**
   * Get a VM_Field of an object given the index for this field
   * @param obj an Object
   * @param fieldIndex an index into the VM_Field array that describes the fields of this object
   * @return the VM_Field pointed to by the index, or null if the index or the object is invalid
   *
   */
  public static VM_Field getFieldAtIndex (Object obj, int fieldIndex) {
    // VM.sysWrite("GetObjectField: field at index " + fieldIndex + "\n");

    VM_Type objType = VM_Magic.getObjectType(obj);
    if (objType.isClassType()) {
      VM_Field[] fields = objType.asClass().getInstanceFields();
      if (fieldIndex>=fields.length) {
	return null;                                      // invalid field index
      } else {
	return fields[fieldIndex];
      } 
    } else {
      // object is not a class type, probably an array or an invalid reference
      return null;
    }
  }

  /**
   * Common code shared by the JNI functions NewObjectA, NewObjectV, NewObject
   * (object creation)
   * @param methodID the method ID for a constructor
   * @return a new object created by the specified constructor
   */
  public static Object invokeInitializer(Class cls, int methodID, VM_Address argAddress, 
					 boolean isJvalue, boolean isDotDotStyle) 
    throws Exception  {

    // get the parameter list as Java class
    VM_Method mth = VM_MethodDictionary.getValue(methodID);
    VM_Type[] argTypes = mth.getParameterTypes();
    Class[]   argClasses = new Class[argTypes.length];
    for (int i=0; i<argClasses.length; i++) {
      argClasses[i] = argTypes[i].getClassForType();
    }

    Constructor constMethod = cls.getConstructor(argClasses);
    if (constMethod==null)
      throw new Exception("Constructor not found");


    // Package the parameters for the constructor
    VM_Address varargAddress;
    if (isDotDotStyle) 
      // flag is false because this JNI function has 3 args before the var args
      varargAddress = getVarArgAddress(false);    
    else
      varargAddress = argAddress;

    Object argObjs[];
    if (isJvalue)
      argObjs = packageParameterFromJValue(mth, argAddress);
    else
      argObjs = packageParameterFromVarArg(mth, varargAddress);

    // construct the new object
    Object newobj = constMethod.newInstance(argObjs);
    
    return newobj;

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>Method
   * (static method invocation)
   * @param methodID the method ID
   * @param expectReturnType the return type of the method to be invoked
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(int methodID, VM_Type expectReturnType)
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();	// expect a certain stack frame structure

    VM_Address varargAddress = getVarArgAddress(false);    
    return packageAndInvoke(null, methodID, varargAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>Method
   * (virtual method invocation)
   * @param obj the object instance 
   * @param methodID the method ID
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args  true if the calling JNI Function takes 4 args before the vararg
   *                   false if the calling JNI Function takes 3 args before the vararg
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(Object obj, int methodID, 
					      VM_Type expectReturnType, boolean skip4Args)
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();	// expect a certain stack frame structure

    VM_Address varargAddress = getVarArgAddress(skip4Args);    
    return packageAndInvoke(obj, methodID, varargAddress, expectReturnType, skip4Args, true);

  }

  /**
   * This method supports var args passed from C
   *
   * In the Linux Intel C convention, the caller places the args immediately above the
   * saved return address, starting with the first arg
   *
   *
   *
   *
   * For the JNI functions that takes var args, their prolog code will save the
   * var arg in the glue frame because the values in the register may be lost by 
   * subsequent calls.
   *
   * This method copies the var arg values that were saved earlier in glue frame into
   * the spill area of the original caller, thereby doing the work that the callee
   * normally performs in the AIX C convention.
   *
   * NOTE: This method contains internal stack pointer.
   * For now we assume that the stack will not be relocatable while native code is running
   * because native code can hold an address into the stack, so this code is OK,
   * but this is an issue to be resolved later
   *
   * NOTE:  this method assumes that it is immediately above the 
   * invokeWithDotDotVarArg frame, the JNI frame, the glue frame and 
   * the C caller frame in the respective order.  
   * Therefore, this method will not work if called from anywhere else
   *
   *  low address
   *
   *   |  fp  | <- VM_JNIEnvironment.getVarArgAddress
   *   | mid  |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- VM_JNIEnvironment.invokeWithDotDotVarArg frame
   *   | mid  |
   *   | ...  |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- JNI method frame
   *   | mid  |
   *   | ...  |
   *   | arg 0|    args copied by JNI prolog (3 for static, nonvirtual, 
   *   | arg 1|    or 4 for virtual)
   *   | arg 2|
   *   |      |
   *   |      |
   *   |------|
   *   | fp   | <- Native C caller frame
   *   |return|
   *   | arg 0|    
   *   | arg 1|    
   *   | arg 2|
   *   | arg 3|
   *   | arg 4|
   *   | arg 5|
   *   | arg 6|
   *   | arg 7|
   *   | arg 8|    
   *   | arg 9|
   *   |      |
   *   |      |
   *   |      |
   *
   *
   *   high address
   *
   *
   * @param skip4Args if true, the calling JNI function has 4 args before the vararg
   *                  if false, the calling JNI function has 3 args before the vararg
   * @return the starting address of the vararg in the caller stack frame
   */
  private static VM_Address getVarArgAddress(boolean skip4Args) {
    
    VM_Address fp = VM_Magic.getFramePointer();
    fp = VM_Magic.getMemoryAddress(fp);
    fp = VM_Magic.getMemoryAddress(fp);
    return (fp.add(2*4 + (skip4Args ? 4*4 : 3*4)));

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodV
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {

    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>MethodV
   * @param obj the object instance 
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {

    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, true);

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodA
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {
    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, false);
  }

  /**
   * Common code shared by the JNI functions Call<type>MethodA
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {

    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, false);

  }

  /**
   * Common code shared by invokeWithJValue, invokeWithVarArg and invokeWithDotDotVarArg
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args This flag is received from the JNI function and passed directly to 
   *                     VM_Reflection.invoke().  
   *                     It is true if the actual method is to be invoked, which could be
   *                     from the superclass.
   *                     It is false if the method from the real class of the object 
   *                     is to be invoked, which may not be the actual method specified by methodID
   * @param isVarArg  This flag describes whether the array of parameters is in var arg format or
   *                  jvalue format
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object packageAndInvoke(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args, 
					boolean isVarArg) 
    throws Exception, VM_PragmaNoInline {
    VM_Magic.pragmaNoOptCompile();   // expect a certain stack frame structure

    VM_Method targetMethod;
    int returnValue;

    // VM.sysWrite("JNI CallXXXMethod:  method ID " + methodID + " with args at " + 
    // 		   VM.intAsHexString(argAddress) + "\n");
    
    targetMethod = VM_MethodDictionary.getValue(methodID);
    VM_Type returnType = targetMethod.getReturnType();

    // VM.sysWrite("JNI CallXXXMethod:  " + targetMethod.getDeclaringClass().toString() +
    //		"." + targetMethod.getName().toString() + "\n");

    if (expectReturnType==null) {   // for reference return type 
      if (!returnType.isReferenceType())
	throw new Exception("Wrong return type for method: expect reference type instead of " + returnType);      
    } 
    else {    // for primitive return type
      if (returnType!=expectReturnType) 
	throw new Exception("Wrong return type for method: expect " + expectReturnType + 
			    " instead of " + returnType);
    }  

    // Repackage the arguments into an array of objects based on the signature of this method
    Object[] argObjectArray;
    if (isVarArg) {
      argObjectArray = packageParameterFromVarArg(targetMethod, argAddress);
    } else {
      argObjectArray = packageParameterFromJValue(targetMethod, argAddress);
    }

    // now invoke the method
    Object returnObj = VM_Reflection.invoke(targetMethod, obj, argObjectArray, skip4Args);
    
    return returnObj;

  }

  /**
   * Repackage the arguments passed as a variable argument list into an array of Object,
   * used by the JNI functions CallStatic<type>MethodV
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromVarArg(VM_Method targetMethod, VM_Address argAddress) {
    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromVarArg: packaging " + argCount + " arguments\n");

    VM_Address addr = argAddress;
    for (int i=0; i<argCount; i++) {

      int loword = VM_Magic.getMemoryWord(addr);
      int hiword;

      // VM.sysWrite("JNI packageParameterFromVarArg:  arg " + i + " = " + loword + 
      // " or " + VM.intAsHexString(loword) + "\n");

      addr = addr.add(4);

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	// NOTE:  in VarArg convention, C compiler will expand a float to a double that occupy 2 words
	// so we have to extract it as a double and convert it back to a float
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);                       
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapFloat((float) (Double.longBitsToDouble(doubleBits)));
	
      } else if (argTypes[i].isDoubleType()) {
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	hiword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte		
	argObjectArray[i] = VM_Reflection.wrapBoolean(loword);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) loword);

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) loword);

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) loword);

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(loword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(loword);

      } else {
	return null;
      }

    }

    return argObjectArray;
    

  }

  /**
   * Repackage the arguments passed as an array of jvalue into an array of Object,
   * used by the JNI functions CallStatic<type>MethodA
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromJValue(VM_Method targetMethod, VM_Address argAddress) {

    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromJValue: packaging " + argCount + " arguments\n");

    VM_Address addr = argAddress;
    for (int i=0; i<argCount; i++, addr = addr.add(8)) {

      int loword = VM_Magic.getMemoryWord(addr);
      int hiword;

      // VM.sysWrite("JNI packageParameterFromJValue:  arg " + i + " = " + loword + 
      //  " or " + VM.intAsHexString(loword) + ", at address " + 
      //	  VM.intAsHexString(addr) + "\n");

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	argObjectArray[i] = VM_Reflection.wrapFloat(Float.intBitsToFloat(loword));

      } else if (argTypes[i].isDoubleType()) {
	hiword = VM_Magic.getMemoryWord(addr.add(4));
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	hiword = VM_Magic.getMemoryWord(addr.add(4));
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte	
	argObjectArray[i] = VM_Reflection.wrapBoolean(loword & 0x000000FF);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) (loword & 0x000000FF));

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) (loword & 0x0000FFFF));

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) (loword & 0x0000FFFF));

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(loword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(loword);

      } else {
	return null;
      }

    }

    return argObjectArray;


  }

  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java byte[] with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java byte[]
   */
  static byte[] createByteArrayFromC(VM_Address stringAddress) {
    int word;
    int length = 0;
    VM_Address addr = stringAddress;

    // scan the memory for the null termination of the string
    while (true) {
      word = VM_Magic.getMemoryWord(addr);
      int byte3 = ((word >> 24) & 0xFF);
      int byte2 = ((word >> 16) & 0xFF);
      int byte1 = ((word >> 8) & 0xFF);
      int byte0 = (word & 0xFF);
      if (byte0==0)
	break;
      length++;
      if (byte1==0) 
	break;
      length++;
      if (byte2==0)
	break;
      length++;
      if (byte3==0)
	break;
      length++;
      addr = addr.add(4);
    }

   byte[] contents = new byte[length];
   VM_Memory.memcopy(VM_Magic.objectAsAddress(contents), stringAddress, length);
   
   return contents;
  }

  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java String with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java String
   */
  static String createStringFromC(VM_Address stringAddress) {

    byte[] contents = createByteArrayFromC( stringAddress );
    return new String(contents);

  }

  public void dumpJniRefsStack () {
    int jniRefOffset = JNIRefsTop;
    VM.sysWrite("\n* * dump of JNIEnvironment JniRefs Stack * *\n");
    VM.sysWrite("* JNIRefs = ");
    VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs));
    VM.sysWrite(" * JNIRefsTop = ");
    VM.sysWrite(JNIRefsTop,false);
    VM.sysWrite(" * JNIRefsSavedFP = ");
    VM.sysWrite(JNIRefsSavedFP,false);
    VM.sysWrite(".\n*\n");
    while ( jniRefOffset >= 0 ) {
      VM.sysWrite(jniRefOffset,false);
      VM.sysWrite(" ");
      VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs).add(jniRefOffset));
      VM.sysWrite(" ");
      VM_GCUtil.dumpRef(VM_Address.fromInt(JNIRefs[ jniRefOffset >> 2 ]));
      jniRefOffset -= 4;
    }
    VM.sysWrite("\n* * end of dump * *\n");
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frames inserted at the transition from Java to
 * JNI Native C.  It will report JREFs associated with the executing
 * C frames which are in the "JREFs stack" attached to the executing
 * Threads JNIEnvironment.  It will update register location addresses
 * for the non-volatile registers to point to the registers saved
 * in the transition frame.
 *
 * @see VM_JNICompiler
 * @author Steve Smith
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_JNIGCMapIterator extends MM.VM_GCMapIterator 
    implements VM_BaselineConstants, VM_Uninterruptible {

  // Java to Native C transition frame...(see VM_JNICompiler)
  //
  //  0   	+ saved FP   + <---- FP for Jave to Native C glue frame
  // -4   	| methodID   |
  // -8   	| saved EDI  |  non-volatile GPR (JTOC for baseline callers or ? for opt callers)
  // -C  	| saved EBX  |  non-volatile GPR  
  // -10  	| saved EBP  |  non-volatile GPR  
  // -14        | returnAddr |  (for return from OutOfLineMachineCode)
  // -18        | saved PR   |  
  // -1C	| arg n-1    |  reordered arguments to native method
  // -20	|  ...       |  ...
  // -24	| arg 1      |  ...
  // -28  	| arg 0      |  ...
  // -2C  	| class/obj  |  required 2nd argument to all native methods
  // -30  	| jniEnv     |  required 1st argument to all native methods
  // -34   	| returnAddr |  return address pushed by call to native method  
  //    	+ saved FP   +  <---- FP for called native method  

  // additional instance fields added by this subclass of VM_GCMapIterator
  int[]         jniRefs;
  int           jniNextRef;
  int           jniFramePtr;
  VM_Address    jniSavedProcessorRegAddr;     // -> saved PR reg
  VM_Address    jniSavedReturnAddr;           // -> return addr in generated transition prolog
  
  public VM_JNIGCMapIterator(int[] registerLocations) {
    this.registerLocations = registerLocations;
  }
  
  // Override newStackWalk() in parent class VM_GCMapIterator to
  // initialize iterator for scan of JNI JREFs stack of refs
  // Taken:    thread
  // Returned: nothing
  //
  public void newStackWalk(VM_Thread thread) {
    super.newStackWalk(thread);   // sets this.thread, inits registerLocations[]
    VM_JNIEnvironment env = this.thread.getJNIEnv();
    // the "primordial" thread, created by JDK in the bootimage, does not have
    // a JniEnv object, all threads created by the VM will.
    if (env != null) {
      this.jniRefs = env.JNIRefs;
      this.jniNextRef = env.JNIRefsTop;
      this.jniFramePtr = env.JNIRefsSavedFP;  
      this.jniSavedProcessorRegAddr = VM_Address.zero();  
                                    // necessary so getNextRefAddr() can be used to report
                                    // jniRefs in a "frame", without calling setup. 
    } 
  }
     
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address framePtr) {
    this.framePtr = framePtr;

    // processor reg (PR) was saved at JNI_PR_OFFSET, and will be used to
    // set processor reg upon return to java.  it must be reported during
    // GC so it will be relocated, if necessary.
    //
    jniSavedProcessorRegAddr = framePtr.add(VM_JNICompiler.JNI_PR_OFFSET);

    // return address into generated prolog must be relocated if the code object
    // for that prolog/epilog is moved by GC
    jniSavedReturnAddr       = framePtr.add(VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

  } //- implements VM_GCMapIterator
   
  // return (address of) next ref in the current "frame" on the
  // threads JNIEnvironment stack of refs	  
  // When at the end of the current frame, update register locations to point
  // to the non-volatile registers saved in the JNI transition frame.
  //
  public VM_Address getNextReferenceAddress() {
    int nextFP;
    VM_Address ref_address;

    // first report jni refs in the current frame in the jniRef side stack
    // until all in the frame are reported
    //
    if ( jniNextRef > jniFramePtr ) {
      ref_address = VM_Magic.objectAsAddress(jniRefs).add(jniNextRef);
      jniNextRef = jniNextRef - 4;
      return ref_address;
    }

    // report location of saved processor reg in the Java to C frame
    if ( !jniSavedProcessorRegAddr.isZero() ) {
      ref_address = jniSavedProcessorRegAddr;
      jniSavedProcessorRegAddr = VM_Address.zero();
      return ref_address;
    }

    // no more refs to report, before returning 0, setup for processing
    // the next jni frame, if any

    // jniNextRef -> savedFramePtr for another "frame" of refs for another
    // sequence of Native C frames lower in the stack, or to 0 if this is the
    // last jni frame in the JNIRefs stack.  If more frames, initialize for a
    // later scan of those refs.
    //
    if ( jniFramePtr > 0 ) {
      jniFramePtr = jniRefs[jniFramePtr >> 2];
      jniNextRef = jniNextRef - 4;
    }

    // set register locations for non-volatiles to point to registers saved in
    // the JNI transition frame at a fixed negative offset from the callers FP.
    // the save non-volatiles are EBX, EBP,  and EDI (JTOC)
    //
    registerLocations[JTOC] = framePtr.add(VM_JNICompiler.EDI_SAVE_OFFSET).toInt();
    registerLocations[EBX]  = framePtr.add(VM_JNICompiler.EBX_SAVE_OFFSET).toInt();
    registerLocations[EBP]  = framePtr.add(VM_JNICompiler.EBP_SAVE_OFFSET).toInt();

    return VM_Address.zero();  // no more refs to report
  } //- implements VM_GCMapIterator
  
  public VM_Address getNextReturnAddressAddress() {
    VM_Address ref_address;
    if ( !jniSavedReturnAddr.isZero() ) {
      ref_address = jniSavedReturnAddr;
      jniSavedReturnAddr = VM_Address.zero();
      return ref_address;
    }
    return VM_Address.zero();
  } //- implements VM_GCMapIterator
  
  public void reset() {
  } //- implements VM_GCMapIterator
  
  public void cleanupPointers() {
  } //- implements VM_GCMapIterator
  
  public int getType() {
    return VM_CompiledMethod.JNI;
  } //- implements VM_GCMapIterator
   
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Constants for JNI support
 *
 * @author Ton Ngo
 * @author Steve Smith
 */
interface VM_JNILinuxConstants extends VM_JNIConstants {
  // byte offset of saved jtoc at end of JNIFunctions array
  static final int JNIFUNCTIONS_JTOC_OFFSET = FUNCTIONCOUNT * 4;

  // index of IP in the AIX linkage triplet
  // static final int IP = 0;                    

  // index of TOC in the AIX linage triplet
  // static final int TOC = 1;                   
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine specific helper functions for dynamic linking.
 * 
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 */
class VM_DynamicLinkerHelper implements VM_Constants, VM_Uninterruptible {

  /**
   * Reach up two stack frames into a frame that is compiled
   * with the DynamicBridge register protocol and grap 
   * the receiver object of the invoke (ie the first param).
   * NOTE: assumes that caller has disabled GC.
   */
  static Object getReceiverObject() throws VM_PragmaNoInline {

    VM_Address callingFrame = VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());
    callingFrame = VM_Magic.getCallerFramePointer(callingFrame);
    VM_Address location = VM_Address.zero();
    if (0 < NUM_PARAMETER_GPRS) {
      location = VM_Magic.getMemoryAddress(callingFrame.add(VM_BaselineConstants.STACKFRAME_FIRST_PARAMETER_OFFSET));

    } else {
      VM.sysFail("VM_DynamicLinerHelper: assumes at least one param passed in registers");
    }
    return VM_Magic.addressAsObject(location);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$ 

/**
 * An interface conflict resolution stub uses a hidden parameter to
 * distinguish among multiple interface methods of a class that map to
 * the same slot in the class's IMT. </p>
 * 
 * <p><STRONG>Assumption:</STRONG>
 * Register EAX contains the "this" parameter of the
 * method being called invoked.
 *
 * <p><STRONG>Assumption:</STRONG>
 * Register ECX is available as a scratch register (we need one!)
 * 
 * @author Bowen Alpern
 * @author Dave Grove
 */
class VM_InterfaceMethodConflictResolver implements VM_Constants {

  // Create a conflict resolution stub for the set of interface method signatures l.
  // 
  static INSTRUCTION[] createStub(int[] sigIds, VM_Method[] targets) {
    int numEntries = sigIds.length;
    // (1) Create an assembler.
    VM_Assembler asm = new VM_Assembler(numEntries); 
    
    // (2) signatures must be in ascending order (to build binary search tree).
    if (VM.VerifyAssertions) {
      for (int i=1; i<sigIds.length; i++) {
	VM.assert(sigIds[i-1] < sigIds[i]);
      }
    }

    // (3) Assign synthetic bytecode numbers to each switch such that we'll generate them
    // in ascending order.  This lets us use the general forward branching mechanisms
    // of the VM_Assembler.
    int[] bcIndices = new int[numEntries];
    assignBytecodeIndices(0, bcIndices, 0, numEntries -1);
    
    // (4) Generate the stub.
    insertStubPrologue(asm);
    insertStubCase(asm, sigIds, targets, bcIndices, 0, numEntries-1);
    
    return asm.getMachineCodes();
  }


  // Assign ascending bytecode indices to each case (in the order they will be generated)
  private static int assignBytecodeIndices(int bcIndex, int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    bcIndices[middle] = bcIndex++;
    if (low == middle && middle == high) {
      return bcIndex;
    } else {
      // Recurse.
      if (low < middle) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, middle+1, high);
      }
      return bcIndex;
    }
  }

  // Make a stub prologue: get TIB into ECX
  // factor out to reduce code space in each call.
  //
  private static void insertStubPrologue (VM_Assembler asm) {
    VM_ObjectModel.baselineEmitLoadTIB(asm,ECX,EAX);
  }

  // Generate a subtree covering from low to high inclusive.
  private static void insertStubCase(VM_Assembler asm,  
				     int[] sigIds, VM_Method[] targets,
				     int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    asm.resolveForwardReferences(bcIndices[middle]);
    if (low == middle && middle == high) {
      // a leaf case; can simply invoke the method directly.
      VM_Method target = targets[middle];
      if (target.isStatic()) { // an error case...
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX, VM_Entrypoints.jtocField.getOffset());
      }
      asm.emitJMP_RegDisp(ECX, target.getOffset());
    } else {
      int disp = VM_Entrypoints.hiddenSignatureIdField.getOffset();
      VM_ProcessorLocalState.emitCompareFieldWithImm(asm, disp, sigIds[middle]);
      if (low < middle) {
	asm.emitJCC_Cond_Label(asm.LT, bcIndices[(low+middle-1)/2]);
      }
      if (middle < high) {
	asm.emitJCC_Cond_Label(asm.GT, bcIndices[(middle+1+high)/2]);
      }
      // invoke the method for middle.
      VM_Method target = targets[middle];
      if (target.isStatic()) { // an error case...
	VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX, VM_Entrypoints.jtocField.getOffset());
      }
      asm.emitJMP_RegDisp(ECX, target.getOffset());
      // Recurse.
      if (low < middle) {
	insertStubCase(asm, sigIds, targets, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	insertStubCase(asm, sigIds, targets, bcIndices, middle+1, high);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generate a "trampoline" that jumps to the shared lazy compilation stub.
 * We do this to enable the optimizing compiler to use ptr equality of
 * target instructions to imply logical (source) equality of target methods.
 * This is used to perform guarded inlining using the "method test."
 * Without per-method lazy compilation trampolines, ptr equality of target
 * instructions does not imply source equality, since both targets may in fact
 * be the globally shared lazy compilation stub.
 * 
 * @author Dave Grove
 */
class VM_LazyCompilationTrampolineGenerator implements VM_BaselineConstants {

  /** Generate a new lazy compilation trampoline. */
  static INSTRUCTION[] getTrampoline (){
    VM_Assembler asm = new VM_Assembler(0); 
    // get JTOC into ECX
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, ECX,
                                              VM_Entrypoints.jtocField.getOffset());
    // jmp to real lazy mathod invoker
    asm.emitJMP_RegDisp(ECX, VM_Entrypoints.lazyMethodInvokerMethod.getOffset()); 
    return asm.getMachineCodes();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine dependent portion of Reflective method invoker.
 *
 * @author Maria Butrico
 */
public class VM_MachineReflection implements VM_Constants {
  //-----------//
  // interface //
  //-----------//
   

   //----------------//
   // implementation //
   //----------------//
   
   // Determine number/type of registers and parameters required to
   // call specified method.
   //  Unlike the PowerPC code we count all the parameters, not just the
   // ones that spill.  This allow us to make enough space on the stack
   // following the calling convention.
   //
  static int 
    countParameters(VM_Method method) {
    int GPRs   = 0;
    int FPRs   = 0;
    int parameters = 0;	// parameters size in 32-bits quant.

    int gp = NUM_PARAMETER_GPRS; // 0, 1, 2
    int fp = NUM_PARAMETER_FPRS; // 0-8

    if (!method.isStatic()) {
      if (gp > 0) {GPRs++; gp--;}
      parameters++;
    }

    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
	if (gp > 0) {
	  GPRs++; gp--;
	  if (gp > 0) {GPRs++; gp--;}
	}
	parameters+=2; 
      } else if (t.isFloatType()) {
	if (fp > 0) {FPRs++; fp--;}
	parameters++;
      } else if (t.isDoubleType()) {
	if (fp > 0) {FPRs++; fp--;}
	parameters+=2;
      } else { // t is object, int, short, char, byte, or boolean
	if (gp > 0) {GPRs++; gp--;}
	parameters++;
      }
    }

    // hack to return triple
    return (parameters<<(REFLECTION_FPRS_BITS+REFLECTION_GPRS_BITS)) |
      (FPRs<<REFLECTION_GPRS_BITS) | GPRs;
  }


  // Collect parameters into arrays of registers/spills, as required to
  // call specified method.
  static void 
    packageParameters(VM_Method method, Object thisArg, Object[] otherArgs,
		      int[] GPRs, double[] FPRs, int[] Parameters) {
    int GPR	   	= 0;
    int FPR		= FPRs.length;
    int parameter	= 0;


    int gp = NUM_PARAMETER_GPRS; // 0, 1, 2
    int fp = NUM_PARAMETER_FPRS; // 0-8

    if (!method.isStatic()) {
      if (gp > 0) {
	gp--;
	GPRs[GPR++] = VM_Reflection.unwrapObject(thisArg);
      }
      Parameters[parameter++] = VM_Reflection.unwrapObject(thisArg);
    }

    VM_Type [] types = method.getParameterTypes();

    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];

      if (t.isLongType()) {
	long l = VM_Reflection.unwrapLong(otherArgs[i]);
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int)(l>>>32);
	  if (gp > 0) {
	    gp--;
	    GPRs[GPR++] = (int)(l);
	  }
	}
	Parameters[parameter++] = (int)(l>>>32);
	Parameters[parameter++] = (int)l;

      } else if (t.isFloatType()) {
	if (fp > 0) {
	  fp--;
	  FPRs[--FPR] = VM_Reflection.unwrapFloat(otherArgs[i]);
	}
	float f = VM_Reflection.unwrapFloat(otherArgs[i]);
	Parameters[parameter++] = Float.floatToIntBits(f);

      } else if (t.isDoubleType()) {
	if (fp > 0) {
	  fp--;
	  FPRs[--FPR] = VM_Reflection.unwrapDouble(otherArgs[i]);
	}
	double d = VM_Reflection.unwrapDouble(otherArgs[i]);
	long l = Double.doubleToLongBits(d);
	Parameters[parameter++] = (int)(l>>>32);
	Parameters[parameter++] = (int)l;

      } else if (t.isBooleanType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);

      } else if (t.isByteType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapByte(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapByte(otherArgs[i]);

      } else if (t.isCharType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapChar(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapChar(otherArgs[i]);

      } else if (t.isShortType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = (int) VM_Reflection.unwrapShort(otherArgs[i]);
	}
	Parameters[parameter++] = (int) VM_Reflection.unwrapShort(otherArgs[i]);

      } else if (t.isIntType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapInt(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapInt(otherArgs[i]);

      } else if (!t.isPrimitiveType()) {
	if (gp > 0) {
	  gp--;
	  GPRs[GPR++] = VM_Reflection.unwrapObject(otherArgs[i]);
	}
	Parameters[parameter++] = VM_Reflection.unwrapObject(otherArgs[i]);

      } else  {
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
      }
    }
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A place to put hand written machine code typically invoked by VM_Magic 
 * methods.
 *
 * <p>Hand coding of small inline instruction sequences is typically handled by 
 * each compiler's implementation of VM_Magic methods.  
 * A few VM_Magic methods are so complex that their implementations require 
 * many instructions.  But our compilers do not inline 
 * arbitrary amounts of machine code. We therefore write such code blocks 
 * here, out of line.
 * 
 * <p>These code blocks can be shared by all compilers. They can be branched to
 * via a jtoc offset (obtained from VM_Entrypoints.XXXInstructionsField).
 * 
 * <p> 17 Mar 1999 Derek Lieber (adapted from powerPC version in 2000 
 * by somebody)
 * 
 * <p> 15 Jun 2001 Dave Grove and Bowen Alpern (Derek believed that compilers 
 * could inline these methods if they wanted.  We do not believe this would 
 * be very easy since they return assuming the return address is on the stack.)
 *
 * @author Maria Butrico
 */
class VM_OutOfLineMachineCode implements VM_BaselineConstants {
  //-----------//
  // interface //
  //-----------//
   
  static void init() {
    reflectiveMethodInvokerInstructions        = generateReflectiveMethodInvokerInstructions();
    saveThreadStateInstructions                = generateSaveThreadStateInstructions();
    threadSwitchInstructions                   = generateThreadSwitchInstructions();
    restoreHardwareExceptionStateInstructions  = generateRestoreHardwareExceptionStateInstructions();
    invokeNativeFunctionInstructions           = generateInvokeNativeFunctionInstructions();
  }

  //----------------//
  // implementation //
  //----------------//

  private static INSTRUCTION[] reflectiveMethodInvokerInstructions;
  private static INSTRUCTION[] saveThreadStateInstructions;
  private static INSTRUCTION[] threadSwitchInstructions;
  private static INSTRUCTION[] restoreHardwareExceptionStateInstructions;
  private static INSTRUCTION[] invokeNativeFunctionInstructions;
   
  private static final int PARAMS_FP_OFFSET	= WORDSIZE * 2;
  private static final int FPRS_FP_OFFSET	= WORDSIZE * 3;
  private static final int GPRS_FP_OFFSET	= WORDSIZE * 4;
  private static final int CODE_FP_OFFSET	= WORDSIZE * 5;


  /**
   * Machine code for reflective method invocation.
   *
   * VM compiled with NUM_PARAMETERS_GPRS == 0
   *   Registers taken at runtime:
   *     none
   *   Stack taken at runtime:
   *     hi-mem
   *         address of method entrypoint to be called
   *         address of gpr registers to be loaded
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * VM compiled with NUM_PARAMETERS_GPRS == 1
   *   T0 == address of method entrypoint to be called
   *   Stack taken at runtime:
   *     hi-mem
   *         space ???
   *         address of gpr registers to be loaded
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * VM compiled with NUM_PARAMETERS_GPRS == 2
   *   T0 == address of method entrypoint to be called
   *   T1 == address of gpr registers to be loaded
   *   Stack taken at runtime:
   *     hi-mem
   *         space ???
   *         space ???
   *         address of fpr registers to be loaded
   *         address of parameters area in calling frame
   *         return address
   *     low-mem
   * 
   * Registers returned at runtime:
   *   standard return value conventions used
   *
   * Side effects at runtime:
   *   artificial stackframe created and destroyed
   *   volatile, and scratch registers destroyed
   *
  */
  private static INSTRUCTION[] generateReflectiveMethodInvokerInstructions() {
    VM_Assembler asm = new VM_Assembler(100);

    /* write at most 2 parameters from registers in the stack.  This is
     * logically equivalent to ParamaterRegisterUnload in the compiler
     */
    int gprs;
    int fpOffset = VM_Entrypoints.framePointerField.getOffset();
    byte T = T0;
    gprs = NUM_PARAMETER_GPRS;
    int offset = 4 << LG_WORDSIZE;		// we have exactly 4 paramaters
    if (gprs > 0) {
      gprs--;
      asm.emitMOV_RegDisp_Reg(SP, offset, T); 
      T = T1;
      offset -= WORDSIZE;
    }

    if (gprs > 0)
      asm.emitMOV_RegDisp_Reg(SP, offset, T); 
    /* available registers S0, T0, T1 */


    /* push a new frame */
    asm.emitPUSH_RegDisp(PR, fpOffset); // link this frame with next
    VM_ProcessorLocalState.emitMoveRegToField(asm, fpOffset, SP); // establish base of new frame
    asm.emitPUSH_Imm    (INVISIBLE_METHOD_ID);
    asm.emitADD_Reg_Imm (SP, STACKFRAME_BODY_OFFSET);
    
    /* write parameters on stack 
     * move data from memory addressed by Paramaters array, the fourth
     * parameter to this, into the stack.
     * SP target address
     * S0 source address
     * T1 length
     * T0 scratch
     */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, PARAMS_FP_OFFSET);// S0 <- Parameters
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- Parameters.length()
    asm.emitCMP_Reg_Imm     (T1, 0);			// length == 0 ?

    int parameterLoopLabel = asm.getMachineCodeIndex();
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);	// done? --> branch to end
    asm.emitMOV_Reg_RegInd (T0, S0);			// T0 <- Paramaters[i]
    asm.emitPUSH_Reg (T0);				// mem[j++] <- Parameters[i]
    asm.emitADD_Reg_Imm (S0, WORDSIZE);			// i++
    asm.emitADD_Reg_Imm (T1, -1);			// length--
    asm.emitJMP_Imm (parameterLoopLabel);

    fr1.resolve(asm);					// end of the loop
    
    /* write fprs onto fprs registers */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, FPRS_FP_OFFSET);	// S0 <- FPRs
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- FPRs.length()
    asm.emitSHL_Reg_Imm (T1, LG_WORDSIZE + 1 );		// length in bytes
    asm.emitADD_Reg_Reg (S0, T1);			// S0 <- last FPR + 8
    asm.emitCMP_Reg_Imm (T1, 0);			// length == 0 ?

    int fprsLoopLabel = asm.getMachineCodeIndex();
    VM_ForwardReference fr2 = asm.forwardJcc(asm.EQ);	// done? --> branch to end
    asm.emitSUB_Reg_Imm ( S0, 2 * WORDSIZE);		// i--
    asm.emitFLD_Reg_RegInd_Quad (FP0, S0);		// frp[fpr_sp++] <-FPRs[i]
    asm.emitSUB_Reg_Imm (T1, 2* WORDSIZE);		// length--
    asm.emitJMP_Imm (fprsLoopLabel);

    fr2.resolve(asm);					// end of the loop


    /* write gprs: S0 = Base address of GPRs[], T1 = GPRs.length */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, GPRS_FP_OFFSET);	// S0 <- GPRs
    asm.emitMOV_Reg_RegDisp (T1, S0, VM_ObjectModel.getArrayLengthOffset());	// T1 <- GPRs.length()
    asm.emitCMP_Reg_Imm (T1, 0);			// length == 0 ?
    VM_ForwardReference fr3 = asm.forwardJcc(asm.EQ);	// result 0 --> branch to end
    asm.emitMOV_Reg_RegInd (T0, S0);			// T0 <- GPRs[0]
    asm.emitADD_Reg_Imm (S0, WORDSIZE);			// S0 += WORDSIZE
    asm.emitADD_Reg_Imm (T1, -1);			// T1--
    VM_ForwardReference fr4 = asm.forwardJcc(asm.EQ);	// result 0 --> branch to end
    asm.emitMOV_Reg_RegInd (T1, S0);			// T1 <- GPRs[1]
    fr3.resolve(asm);
    fr4.resolve(asm);

    /* branch to method.  On a good day we might even be back */
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0, fpOffset);
    asm.emitMOV_Reg_RegDisp (S0, S0, CODE_FP_OFFSET);	// S0 <- code
    asm.emitCALL_Reg (S0);				// go there
    // T0/T1 have returned value

    /* and get out */
    // NOTE: RVM callee has popped the params, so we can simply
    //       add back in the initial SP to FP delta to get SP to be a framepointer again!
    asm.emitADD_Reg_Imm (SP, -STACKFRAME_BODY_OFFSET + 4); 
    asm.emitPOP_RegDisp (PR, fpOffset);

    asm.emitRET_Imm(4 << LG_WORDSIZE);			// again, exactly 4 parameters

    return asm.getMachineCodes();
  }


  /**
   * Machine code to implement "VM_Magic.saveThreadState()".
   * 
   *  Registers taken at runtime:
   *    T0 == address of VM_Registers object
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    S0, T1 destroyed
   *    Thread state stored into VM_Registers object 
   */
  private static INSTRUCTION[] generateSaveThreadStateInstructions() {
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0); // assuming no NV FPRs (otherwise would have to save them here)
    VM_Assembler asm = new VM_Assembler(0);
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    asm.emitMOV_Reg_RegDisp(S0, PR, VM_Entrypoints.framePointerField.getOffset()); 
    asm.emitMOV_RegDisp_Reg(T0, fpOffset, S0);        // registers.fp := pr.framePointer
    asm.emitPOP_Reg        (T1);                      // T1 := return address 
    asm.emitADD_Reg_Imm    (SP, 4);                   // throw away space for registers parameter (in T0)
    asm.emitMOV_Reg_RegDisp(S0, T0, gprsOffset);      // S0 := registers.gprs[]
    asm.emitMOV_RegDisp_Reg(S0, SP<<LG_WORDSIZE, SP); // registers.gprs[#SP] := SP
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_RegDisp_Reg(S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE, NONVOLATILE_GPRS[i]); // registers.gprs[i] := i'th register
    }
    asm.emitJMP_Reg        (T1);                      // return to return address
    return asm.getMachineCodes();
  }
      

  /**
   * Machine code to implement "VM_Magic.threadSwitch()".
   * 
   *  Parameters taken at runtime:
   *    T0 == address of VM_Thread object for the current thread
   *    T1 == address of VM_Registers object for the new thread
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    sets current Thread's beingDispatched field to false
   *    saves current Thread's nonvolatile hardware state in its VM_Registers object
   *    restores new thread's VM_Registers nonvolatile hardware state.
   *    execution resumes at address specificed by restored thread's VM_Registers ip field
   */
  private static INSTRUCTION[] generateThreadSwitchInstructions() {
    if (VM.VerifyAssertions) VM.assert(NUM_NONVOLATILE_FPRS == 0); // assuming no NV FPRs (otherwise would have to save them here)
    VM_Assembler asm = new VM_Assembler(0);
    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    int regsOffset = VM_Entrypoints.threadContextRegistersField.getOffset();

    // (1) Save hardware state of thread we are switching off of.
    asm.emitMOV_Reg_RegDisp  (S0, T0, regsOffset);      // S0 = T0.contextRegisters
    asm.emitPOP_RegDisp      (S0, ipOffset);            // T0.contextRegisters.ip = returnAddress
    asm.emitPUSH_RegDisp     (PR, VM_Entrypoints.framePointerField.getOffset()); // push PR.framePointer
    asm.emitPOP_RegDisp      (S0, fpOffset);            // T0.contextRegisters.fp = pushed framepointer
    asm.emitADD_Reg_Imm      (SP, 8);                   // discard 2 words of parameters (T0, T1)
    asm.emitMOV_Reg_RegDisp  (S0, S0, gprsOffset);      // S0 = T0.contextRegisters.gprs;
    asm.emitMOV_RegDisp_Reg  (S0, SP<<LG_WORDSIZE, SP); // T0.contextRegisters.gprs[#SP] := SP
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_RegDisp_Reg(S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE, NONVOLATILE_GPRS[i]); // T0.contextRegisters.gprs[i] := i'th register
    }

    // (2) Set currentThread.beingDispatched to false
    asm.emitMOV_RegDisp_Imm(T0, VM_Entrypoints.beingDispatchedField.getOffset(), 0); // previous thread's stack is nolonger in use, so it can now be dispatched on any virtual processor 
    
    // (3) Restore hardware state of thread we are switching to.
    asm.emitMOV_Reg_RegDisp(S0, T1, fpOffset);        // S0 := restoreRegs.fp
    VM_ProcessorLocalState.emitMoveRegToField(asm, VM_Entrypoints.framePointerField.getOffset(), S0); // PR.framePointer = restoreRegs.fp
    asm.emitMOV_Reg_RegDisp(S0, T1, gprsOffset);      // S0 := restoreRegs.gprs[]
    asm.emitMOV_Reg_RegDisp(SP, S0, SP<<LG_WORDSIZE); // SP := restoreRegs.gprs[#SP]
    for (int i=0; i<NUM_NONVOLATILE_GPRS; i++) {
      asm.emitMOV_Reg_RegDisp(NONVOLATILE_GPRS[i], S0, NONVOLATILE_GPRS[i]<<LG_WORDSIZE); // i'th register := restoreRegs.gprs[i]
    }
    asm.emitJMP_RegDisp    (T1, ipOffset);            // return to (save) return address
    return asm.getMachineCodes();
  }
      

  /**
   * Machine code to implement "VM_Magic.restoreHardwareExceptionState()".
   * 
   *  Registers taken at runtime:
   *    T0 == address of VM_Registers object
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    all registers are restored except PROCESSOR_REGISTER and EFLAGS;
   *    execution resumes at "registers.ip"
   */
  private static INSTRUCTION[] generateRestoreHardwareExceptionStateInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int   fpOffset = VM_Entrypoints.registersFPField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();

    // Set PR.framePointer to be registers.fp
    asm.emitMOV_Reg_RegDisp(S0, T0, fpOffset); 
    VM_ProcessorLocalState.emitMoveRegToField(asm,
                                              VM_Entrypoints.framePointerField.getOffset(),
                                              S0);

    // Restore SP
    asm.emitMOV_Reg_RegDisp (S0, T0, gprsOffset);
    asm.emitMOV_Reg_RegDisp (SP, S0, SP<<LG_WORDSIZE);
    
    // Push registers.ip to stack (now that SP has been restored)
    asm.emitPUSH_RegDisp(T0, ipOffset);

    // Restore the GPRs except for S0, PR, and SP 
    // (restored above and then modified by pushing registers.ip!)
    for (byte i= 0; i < NUM_GPRS; i++) {
      if (i != S0 && i != ESI && i != SP) {
	asm.emitMOV_Reg_RegDisp(i, S0, i<<LG_WORDSIZE);
      }
    }
    
    // Restore S0
    asm.emitMOV_Reg_RegDisp(S0, S0, S0<<LG_WORDSIZE);

    // Return to registers.ip (popping stack)
    asm.emitRET();
    return asm.getMachineCodes();
  }

  // Out of line prolog/epilog called from generated prologues for user
  // written native methods (see VM_JNICompiler).  Completes the call
  // into native code from java and handles the return from native back
  // to java.
  //
  // on entry assume:
  //   TOC = TOC for native call
  //   S0  = address of native function to branch to
  //
  private static INSTRUCTION[]
  generateInvokeNativeFunctionInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    // save PR in glue frame - to be relocated by GC
    VM_ProcessorLocalState.emitStoreProcessor(asm, EBP, 
                                              VM_JNICompiler.JNI_PR_OFFSET);

    // save callers ret addr in glue frame
    asm.emitPOP_RegDisp (EBP, VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

    // change processor status to IN_NATIVE
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0, 
                                              VM_Entrypoints.vpStatusAddressField.getOffset());
    asm.emitMOV_RegInd_Imm(T0, VM_Processor.IN_NATIVE);

    // make the call...
    asm.emitCALL_Reg(S0);

    // return from native code here...
    // T0 contains single word return value from native
    // T1 ...

    // push return values on stack
    asm.emitPUSH_Reg(T0);
    asm.emitPUSH_Reg(T1);

    int retryLabel = asm.getMachineCodeIndex();     // backward branch label

    // reload PR ref from glue frame
    VM_ProcessorLocalState.emitLoadProcessor(asm, EBP,
                                             VM_JNICompiler.JNI_PR_OFFSET);

    // reload JTOC from processor NOTE: JTOC saved in glue frame may not be
    // the RVM JTOC 
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, JTOC,
                                              VM_Entrypoints.jtocField.getOffset());

    // S0<-addr of statusword
    VM_ProcessorLocalState.emitMoveFieldToReg(asm, S0,
                                              VM_Entrypoints.vpStatusAddressField.getOffset());

    asm.emitMOV_Reg_RegInd(T0,S0);                         // T0<-contents of statusword 
    asm.emitCMP_Reg_Imm (T0, VM_Processor.IN_NATIVE);      // jmp if still IN_NATIVE
    VM_ForwardReference fr = asm.forwardJcc(asm.EQ);       // if so, skip 3 instructions

    // blocked in native, do pthread yield
    asm.emitMOV_Reg_RegDisp(T0, JTOC, VM_Entrypoints.the_boot_recordField.getOffset());  // T0<-bootrecord addr
    asm.emitCALL_RegDisp(T0, VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset());
    asm.emitJMP_Imm (retryLabel);                          // retry from beginning

    fr.resolve(asm);      // branch here if IN_NATIVE, attempt to go to IN_JAVA

    // T0 (EAX) contains "old value" (required for CMPXCNG instruction)
    // S0 contains address of status word to be swapped
    asm.emitMOV_Reg_Imm (T1, VM_Processor.IN_JAVA);  // T1<-new value (IN_JAVA)
    asm.emitCMPXCHG_RegInd_Reg(S0,T1);               // atomic compare-and-exchange
    asm.emitJCC_Cond_Imm(asm.NE,retryLabel);
									
    // status is now IN_JAVA. GC can not occur while we execute on a processor
    // in this state, so it is safe to access fields of objects

    // Test if returning to Java on a RVM processor or a Native processor.

    VM_ProcessorLocalState.emitMoveFieldToReg(asm, T0,
                                              VM_Entrypoints.processorModeField.getOffset());
    asm.emitCMP_Reg_Imm (T0, VM_Processor.RVM);           // test for RVM
    VM_ForwardReference fr1 = asm.forwardJcc(asm.EQ);     // Br if yes

    // If here, on a native processor, it is necessary to transfer back to a
    // RVM processor before returning to the Java calling method.

    // !!! volatile FPRs will be lost during the yield to the transfer
    // queue of the RVM processor, and later redispatch on that processor.
    // ADD A SAVE OF FPR return reg (top on FPR stack?) before trnsferring
    // branch to becomeRVMThread to make the transfer

    asm.emitCALL_RegDisp(JTOC, VM_Entrypoints.becomeRVMThreadMethod.getOffset());

    // execution here is now on the RVM processor, and on a different
    // os pThread. non-volatile GRPs & FPRs have been saved and restored
    // during the transfer. PR now points to the RVM processor we have
    // been transferred to.

    // XXX Restore the saved FPR return reg, before returning

    fr1.resolve(asm);  // branch to here if returning on a RVM processor

    // pop return values off stack into expected regs before returning to caller
    asm.emitPOP_Reg(T1);
    asm.emitPOP_Reg(T0);

    // push callers return address onto stack, prevoiusly saved in glue frame
    asm.emitPUSH_RegDisp (EBP, VM_JNICompiler.JNI_RETURN_ADDRESS_OFFSET);

    asm.emitRET();

    return asm.getMachineCodes();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

//-#if RVM_WITH_OPT_COMPILER
import instructionFormats.*;
//-#endif 

/**
 * This class provides a layer of abstraction that the rest of the VM must
 * use in order to access the current <code>VM_Processor</code> object.
 *
 * @see VM_Processor
 *
 * @author Stephen Fink
 */
public final class VM_ProcessorLocalState 
//-#if RVM_WITH_OPT_COMPILER
extends OPT_IRTools
//-#endif 
{
  
  static byte PROCESSOR_REGISTER = VM_RegisterConstants.ESI;

  /**
   * The C bootstrap program has placed a pointer to the initial
   * VM_Processor in ESI.  
   */
  static void boot() {
    // do nothing - everything is already set up.
  }


  /**
   * Return the current VM_Processor object
   */
  public static VM_Processor getCurrentProcessor() throws VM_PragmaUninterruptible {
    return VM_Magic.getESIAsProcessor();
  }

  /**
   * Set the current VM_Processor object
   */
  public static void setCurrentProcessor(VM_Processor p) throws VM_PragmaUninterruptible {
    VM_Magic.setESIAsProcessor(p);
  }

  /**
   * Emit an instruction sequence to move the value of a register into a field 
   * in the current processor offset 
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param reg number of the register supplying the new value
   */
  static void emitMoveRegToField(VM_Assembler asm, int offset, byte reg) {
    asm.emitMOV_RegDisp_Reg(PROCESSOR_REGISTER,offset,reg);
  }

  /**
   * Emit an instruction sequence to move an immediate value into a field 
   * in the current processor offset 
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param imm immediate value
   */
  static void emitMoveImmToField(VM_Assembler asm, int offset, int imm) {
    asm.emitMOV_RegDisp_Imm(PROCESSOR_REGISTER,offset,imm);
  }

  /**
   * Emit an instruction sequence to move the value of a field in the 
   * current processor offset to a register
   *
   * @param asm assembler object
   * @param dest number of destination register
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitMoveFieldToReg(VM_Assembler asm, byte dest, int offset) {
    asm.emitMOV_Reg_RegDisp(dest,PROCESSOR_REGISTER,offset);
  }

  /**
   * Emit an instruction sequence to compare the value of a field in the 
   * current processor offset with an immediate value
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   * @param imm immediate value to compare with
   */
  static void emitCompareFieldWithImm(VM_Assembler asm, int offset, int imm) {
    asm.emitCMP_RegDisp_Imm(PROCESSOR_REGISTER,offset,imm);
  }
  /**
   * Emit an instruction sequence to decrement the value of a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitDecrementField(VM_Assembler asm, int offset) {
    asm.emitDEC_RegDisp(PROCESSOR_REGISTER,offset);
  }
  /**
   * Emit an instruction sequence to PUSH the value of a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitPushField(VM_Assembler asm, int offset) {
    asm.emitPUSH_RegDisp(PROCESSOR_REGISTER,offset);
  }
  /**
   * Emit an instruction sequence to POP a value into a field in the 
   * current processor offset
   *
   * @param asm assembler object
   * @param offset of field in the <code>VM_Processor</code> object
   */
  static void emitPopField(VM_Assembler asm, int offset) {
    asm.emitPOP_RegDisp(PROCESSOR_REGISTER,offset);
  }

  /**
   * Emit an instruction sequence to set the current VM_Processor 
   * to be the value at [base] + offset
   *
   * <P>TODO: this method is used only by the JNI compiler.  Consider
   * rewriting the JNI compiler to allow us to deprecate this method.
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitSetProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_Reg_RegDisp(PROCESSOR_REGISTER, base, offset);
  }

  /**
   * Emit an instruction sequence to PUSH a pointer to the current VM_Processor
   * object on the stack.
   *
   * @param asm assembler object
   */
  static void emitPushProcessor(VM_Assembler asm) {
    asm.emitPUSH_Reg(PROCESSOR_REGISTER);
  }

  /**
   * Emit an instruction sequence to POP a value on the stack, and set the
   * current processor reference to be this value.
   *
   * @param asm assembler object
   */
  static void emitPopProcessor(VM_Assembler asm) {
    asm.emitPOP_Reg(PROCESSOR_REGISTER);
  }

  /**
   * Emit an instruction sequence to store a pointer to the current VM_Processor
   * object at a location defined by [base]+offset
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitStoreProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_RegDisp_Reg(base,offset,PROCESSOR_REGISTER);
  }
  /**
   * Emit an instruction sequence to load current VM_Processor
   * object from a location defined by [base]+offset
   *
   * @param asm assembler object
   * @param base number of base register
   * @param offset offset
   */
  static void emitLoadProcessor(VM_Assembler asm, byte base, int offset) {
    asm.emitMOV_Reg_RegDisp(PROCESSOR_REGISTER,base,offset);
  }

  //-#if RVM_WITH_OPT_COMPILER
  /**
   * Insert code during BURS to load a pointer to the current processor
   * into a symbolic register, and return the resultant operand
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_BURS burs) {
    OPT_RegisterOperand result =
      burs.ir.regpool.makeTemp(OPT_ClassLoaderProxy.VM_ProcessorType);
    OPT_Register ESI = burs.ir.regpool.getPhysicalRegisterSet().getESI();

    burs.append(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }

  /**
   * Insert code before instruction s to load a pointer to the current 
   * processor into a symbolic register, and return the resultant operand
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s) {
    OPT_RegisterOperand result = ir.regpool.makeTemp
                                 (OPT_ClassLoaderProxy.VM_ProcessorType);
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    s.insertBefore(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }
  /**
   * Insert code before instruction s to load a pointer to the current 
   * processor into a particular register operand.
   */
  static OPT_RegisterOperand insertGetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s,
                                                       OPT_RegisterOperand rop)
  {
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    OPT_RegisterOperand result = rop.copyRO();
    s.insertBefore(MIR_Move.create(IA32_MOV,result,R(ESI)));
    return result;
  }
  /**
   * Insert code after instruction s to set the current 
   * processor to be the value of a particular register operand.
   */
  static OPT_RegisterOperand appendSetCurrentProcessor(OPT_IR ir,
                                                       OPT_Instruction s,
                                                       OPT_RegisterOperand rop)
  {
    OPT_Register ESI = ir.regpool.getPhysicalRegisterSet().getESI();

    s.insertBefore(MIR_Move.create(IA32_MOV,R(ESI),rop.copyRO()));
    return rop;
  }
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frame  built by the Baseline compiler
 * An Instance of this class will iterate through a particular 
 * reference map of a method returning the offsets of any refereces
 * that are part of the input parameters, local variables, and 
 * java stack for the stack frame.
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_BaselineGCMapIterator extends VM_GCMapIterator 
  implements VM_BaselineConstants,
	     VM_Uninterruptible  {

  // Iterator state for mapping any stackframe.
  //
  private   int              mapOffset; // current offset in current map
  private   int              mapId;     // id of current map out of all maps
  private   VM_ReferenceMaps maps;      // set of maps for this method

  // Additional iterator state for mapping dynamic bridge stackframes.
  //
  private VM_DynamicLink dynamicLink;                    // place to keep info returned by VM_CompiledMethod.getDynamicLink
  private VM_Method      bridgeTarget;                   // method to be invoked via dynamic bridge (null: current frame is not a dynamic bridge)
  private VM_Method      currentMethod;                  // method for the frame
  private VM_Type[]      bridgeParameterTypes;           // parameter types passed by that method
  private boolean        bridgeParameterMappingRequired; // have all bridge parameters been mapped yet?
  private boolean        bridgeRegistersLocationUpdated; // have the register location been updated
  private int            bridgeParameterInitialIndex;    // first parameter to be mapped (-1 == "this")
  private int            bridgeParameterIndex;           // current parameter being mapped (-1 == "this")
  private int            bridgeRegisterIndex;            // gpr register it lives in
  private VM_Address     bridgeRegisterLocation;         // memory address at which that register was saved


  //
  // Remember the location array for registers. This array needs to be updated
  // with the location of any saved registers.
  // This information is not used by this iterator but must be updated for the
  // other types of iterators (ones for the quick and opt compiler built frames)
  // The locations are kept as addresses within the stack.
  //
  public VM_BaselineGCMapIterator(int registerLocations[]) {
    this.registerLocations = registerLocations; // (in superclass)
    dynamicLink  = new VM_DynamicLink();
  }

  //
  // Set the iterator to scan the map at the machine instruction offset provided.
  // The iterator is positioned to the beginning of the map
  //
  //   method - identifies the method and class
  //   instruction offset - identifies the map to be scanned.
  //   fp  - identifies a specific occurrance of this method and
  //         allows for processing instance specific information
  //         i.e JSR return address values
  //
  //  NOTE: An iterator may be reused to scan a different method and map.
  //
  public void setupIterator(VM_CompiledMethod compiledMethod, int instructionOffset, VM_Address fp) {
    currentMethod = compiledMethod.getMethod();

    // setup superclass
    //
    framePtr = fp;
      
    // setup stackframe mapping
    //
    maps      = ((VM_BaselineCompiledMethod)compiledMethod).referenceMaps;
    mapId     = maps.locateGCPoint(instructionOffset, currentMethod);
    mapOffset = 0;
    if (mapId < 0) {
      // lock the jsr lock to serialize jsr processing
      VM_ReferenceMaps.jsrLock.lock();
      maps.setupJSRSubroutineMap( framePtr, mapId, compiledMethod);
    }
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator setupIterator mapId = ");
      VM.sysWrite(mapId);
      VM.sysWrite(" for ");
      VM.sysWrite(currentMethod);
      VM.sysWrite(".\n");
    }
      
    // setup dynamic bridge mapping
    //
    bridgeTarget                   = null;
    bridgeParameterTypes           = null;
    bridgeParameterMappingRequired = false;
    bridgeRegistersLocationUpdated = false;
    bridgeParameterIndex           = 0;
    bridgeRegisterIndex            = 0;
    bridgeRegisterLocation         = VM_Address.zero();

    if (currentMethod.getDeclaringClass().isDynamicBridge()) {
      fp                       = VM_Magic.getCallerFramePointer(fp);
      VM_Address        ip                       = VM_Magic.getNextInstructionAddress(fp);
      int               callingCompiledMethodId  = VM_Magic.getCompiledMethodID(fp);
      VM_CompiledMethod callingCompiledMethod    = VM_CompiledMethods.getCompiledMethod(callingCompiledMethodId);
      int               callingInstructionOffset = ip.diff(VM_Magic.objectAsAddress(callingCompiledMethod.getInstructions()));

      callingCompiledMethod.getDynamicLink(dynamicLink, callingInstructionOffset);
      bridgeTarget                = dynamicLink.methodRef();
      bridgeParameterInitialIndex = dynamicLink.isInvokedWithImplicitThisParameter() ? -1 : 0;
      bridgeParameterTypes        = bridgeTarget.getParameterTypes();
    }
        
    reset();
  }
  
  // Reset iteration to initial state.
  // This allows a map to be scanned multiple times
  //
  public void reset() {

    mapOffset = 0;

    if (bridgeTarget != null) {
      // point to first saved gpr
      bridgeParameterMappingRequired = true;
      bridgeParameterIndex   = bridgeParameterInitialIndex;
      bridgeRegisterIndex    = FIRST_VOLATILE_GPR;
      bridgeRegisterLocation = VM_Address.fromInt(VM_Magic.getMemoryWord(framePtr));
      bridgeRegisterLocation = bridgeRegisterLocation.sub(8 * (LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) +
							  4 * (LAST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR + 1));
    }
  }

  // Get location of next reference.
  // A zero return indicates that no more references exist.
  //
  public VM_Address getNextReferenceAddress() {

    if (mapId < 0)
      mapOffset = maps.getNextJSRRef(mapOffset);
    else
      mapOffset = maps.getNextRef(mapOffset, mapId);
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReferenceOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
      if (mapId < 0) 
	VM.sysWrite("Offset is a JSR return address ie internal pointer.\n");
    }

    if (mapOffset != 0) {
      return (framePtr.add(mapOffset));

    } else if (bridgeParameterMappingRequired) {

      if (VM.TraceStkMaps) {
	VM.sysWrite("getNextReferenceAddress: bridgeTarget="); VM.sysWrite(bridgeTarget); VM.sysWrite("\n");
      }
      if (!bridgeRegistersLocationUpdated) {
	// point registerLocations[] to our callers stackframe
	//
	VM_Address location = framePtr.add(VM_Compiler.getFrameSize(currentMethod));
	location = location.sub((LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) * 8); 
	// skip non-volatile and volatile fprs
	for (int i = LAST_NONVOLATILE_GPR; i >= FIRST_VOLATILE_GPR; --i) {
	  location = location.sub(4);
	  registerLocations[i] = location.toInt();
	}

	bridgeRegistersLocationUpdated = true;
      }

      // handle implicit "this" parameter, if any
      //
      if (bridgeParameterIndex == -1) {
	bridgeParameterIndex   += 1;
	bridgeRegisterIndex    += 1;
	bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	return bridgeRegisterLocation.sub(4);
      }
         
      // now the remaining parameters
      //
      while (true) {
	if (bridgeParameterIndex == bridgeParameterTypes.length || bridgeRegisterIndex > LAST_VOLATILE_GPR) {
	  bridgeParameterMappingRequired = false;
	  break;
	}
	VM_Type bridgeParameterType = bridgeParameterTypes[bridgeParameterIndex++];
	if (bridgeParameterType.isReferenceType()) {
	  bridgeRegisterIndex    += 1;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	  return bridgeRegisterLocation.sub(4);
	} else if (bridgeParameterType.isLongType()) {
	  bridgeRegisterIndex    += 2;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(8);
	} else if (bridgeParameterType.isDoubleType() || bridgeParameterType.isFloatType()) {
	  // no gpr's used
	} else {
	  // boolean, byte, char, short, int
	  bridgeRegisterIndex    += 1;
	  bridgeRegisterLocation = bridgeRegisterLocation.add(4);
	}
      }
    }
      
    return VM_Address.zero();
  }

  //
  // Gets the location of the next return address
  // after the current position.
  //  a zero return indicates that no more references exist
  //
  public VM_Address getNextReturnAddressAddress() {

    if (mapId >= 0) {
      if (VM.TraceStkMaps) {
	VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset mapId = ");
	VM.sysWrite(mapId);
	VM.sysWrite(".\n");
      }
      return VM_Address.zero();
    }
    mapOffset = maps.getNextJSRReturnAddr(mapOffset);
    if (VM.TraceStkMaps) {
      VM.sysWrite("VM_BaselineGCMapIterator getNextReturnAddressOffset = ");
      VM.sysWrite(mapOffset);
      VM.sysWrite(".\n");
    }
    return (mapOffset == 0) ? VM_Address.zero() : framePtr.add(mapOffset);
  }

  // cleanup pointers - used with method maps to release data structures
  //    early ... they may be in temporary storage ie storage only used
  //    during garbage collection
  //
  public void cleanupPointers() {
    maps.cleanupPointers();
    maps = null;
    if (mapId < 0) 
      VM_ReferenceMaps.jsrLock.unlock();
    bridgeTarget         = null;
    bridgeParameterTypes = null;
  }
       
  public int getType() {
    return VM_CompiledMethod.BASELINE;
  }

  // For debugging (used with checkRefMap)
  //
  public int getStackDepth() {
    return maps.getStackDepth(mapId);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine code generators:
 *
 * Corresponding to a PowerPC assembler instruction of the form
 *    xx A,B,C
 * there will be a method
 *    void emitXX (int A, int B, int C).
 * 
 * The emitXX method appends this instruction to an VM_MachineCode object.
 * The name of a method for generating assembler instruction with the record
 * bit set (say xx.) will be end in a lower-case r (emitXXr).
 * 
 * mIP will be incremented to point to the next machine instruction.
 * 
 * Machine code generators:
 *
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 * @author Derek Lieber
 * @modified Dave Grove
 */
final class VM_Assembler implements VM_BaselineConstants,
				    VM_AssemblerConstants {

  private VM_MachineCode mc;
  private int mIP; // current machine code instruction
  private boolean shouldPrint;

  VM_Assembler (int length) {
    this(length, false);
  }

  VM_Assembler (int length, boolean sp) {
    mc = new VM_MachineCode();
    mIP = 0;
    shouldPrint = sp;
  }

  /* assembler stuff:

     labels and comments can be added to the an assembler instruction by
     calling the label and comment methods with the appropriate Strings
     BEFORE calling the emit method that generates the instruction.

  */

  final static boolean fits (int val, int bits) {
    val = val >> bits-1;
    return (val == 0 || val == -1);
  }

  final static String hex (int i) {
    if (i == 0) return "0x0";
    String s = VM.intAsHexString(i).substring(2);
    while (s.substring(0,1).equals("0")) 
      s = s.substring(1);
    return "0x" + s;
  }

  private final static String signedHex (int i) {
    if (i > 0) return hex(i);
    if (i < 0) return "-" + hex(-i);
    return "0x0";
  }

  private final static String left (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(0,w);
    for (int i=n; i<w; i++) {
      s = s + " ";
    }
    return s; 
  }

  private final static String right (String s, int w) {
    int n = s.length();
    if (w < n) return s.substring(n-w);
    for (int i=n; i<w; i++) {
      s = " " + s;
    } 
    return s; 
  }

  void noteBytecode (int i, String bcode) {
    if (!VM.TraceAssembler) return;
    VM.sysWrite("[" + i + "] " + bcode + "\n");
    if (!VM.TraceAssembler) return;
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, String args) {
    String instr = right(hex(inum<<2),6) + "| " + right(hex((int)mi),8);
    instr += " " + left(opcode,6) + left(args,20);
    System.out.println(instr);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode) {
    String args = "";
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT) {
    String args = right(""+RT,2);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, String s) {
    String args = right(""+RT,2) + right(s,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA) {
    String args = right(""+RT,2) + right(""+RA,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, String D, int RA) {
    String args = right(""+RT,2) + right(D,7) + right("("+RA,3) + ")";
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, int RB) {
    String args = right(""+RT,2) + right(" "+RA,3) + ", " + right(""+RB,2);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, String V){
    String args = right(""+RT,2) + right(" "+RA,3) + ", " + left(V,7);
    asm (inum, mi, opcode, args);
  }

  private void asm (int inum, INSTRUCTION mi, String opcode, int RT, int RA, int RC, int RB) {
    String args = right(""+RT,2) + right(""+RA,7) 
                                 + ", " + right(""+RC,2) 
                                 + ", " + right(""+RB,2);
    asm (inum, mi, opcode, args);
  }

  /* Handling backward branch references */

  int getMachineCodeIndex () {
    return mIP;
  }

  /* Handling forward branch references */

  VM_ForwardReference forwardRefs = null;

  /* call before emiting code for the branch */
  final void reserveForwardBranch (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.UnconditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the branch */
  final void reserveForwardConditionalBranch (int where) {
    emitNOP();
    VM_ForwardReference fr = new VM_ForwardReference.ConditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the branch */
  final void reserveShortForwardConditionalBranch (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.ConditionalBranch(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting data for the case branch */
  final void reserveForwardCase (int where) {
    VM_ForwardReference fr = new VM_ForwardReference.SwitchCase(mIP, where);
    forwardRefs = VM_ForwardReference.enqueue(forwardRefs, fr);
  }

  /* call before emiting code for the target */
  final void resolveForwardReferences (int label) {
    if (forwardRefs == null) return; 
    forwardRefs = VM_ForwardReference.resolveMatching(this, forwardRefs, label);
  }

  final void patchUnconditionalBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" <- " + hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if (VM.VerifyAssertions) VM.assert((delta>>>23) == 0); // delta (positive) fits in 24 bits
    instr |= (delta<<2);
    mc.putInstruction(sourceMachinecodeIndex, instr);
  }
  
  final void patchConditionalBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" << " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if ((delta>>>13) == 0) { // delta (positive) fits in 14 bits
      instr |= (delta<<2);
      mc.putInstruction(sourceMachinecodeIndex, instr);
    } else {
      if (VM.VerifyAssertions) VM.assert((delta>>>23) == 0); // delta (positive) fits in 24 bits
      instr ^= 0x01000008; // make skip instruction with opposite sense
      mc.putInstruction(sourceMachinecodeIndex-1, instr); // skip unconditional branch to target
      mc.putInstruction(sourceMachinecodeIndex,  Btemplate | (delta&0xFFFFFF)<<2);
    }
  }

  final void patchShortBranch(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" << " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = mIP - sourceMachinecodeIndex;
    INSTRUCTION instr = mc.getInstruction(sourceMachinecodeIndex);
    if ((delta>>>13) == 0) { // delta (positive) fits in 14 bits
      instr |= (delta<<2);
      mc.putInstruction(sourceMachinecodeIndex, instr);
    } else {
      throw new InternalError("Long offset doesn't fit in short branch\n");
    }
  }

  final void patchSwitchCase(int sourceMachinecodeIndex) {
    if (VM.TraceAssembler) System.out.print(" <+ " + VM_Assembler.hex(sourceMachinecodeIndex << 2));
    int delta = (mIP - sourceMachinecodeIndex) << 2;
    // correction is number of bytes of source off switch base
    int         correction = (int)mc.getInstruction(sourceMachinecodeIndex);
    INSTRUCTION offset = (INSTRUCTION)(delta+correction);
    mc.putInstruction(sourceMachinecodeIndex, offset);
  }


  /* machine instructions */

  static final int Atemplate = 31<<26 | 10<<1;

  final void emitA (int RT, int RA, int RB) {
    INSTRUCTION mi = Atemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "a", RT,  RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int AEtemplate = 31<<26 | 138<<1;

  final void emitAE (int RT, int RA, int RB) {
    INSTRUCTION mi = AEtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "ae", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int AIrtemplate = 13<<26;

  final void emitAIr (int RT, int RA, int SI) {
    if (VM.VerifyAssertions) VM.assert(fits(SI, 16));
    INSTRUCTION mi = AIrtemplate | RT<<21 | RA<<16 | (SI & 0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "ai.", RT, RA, signedHex(SI));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ANDtemplate = 31<<26 | 28<<1;

  final void emitAND (int RA, int RS, int RB) {
    INSTRUCTION mi = ANDtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "and", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ANDItemplate = 28<<26;

  final void emitANDI (int RA, int RS, int U) {
    if (VM.VerifyAssertions) VM.assert((U>>>16) == 0);
    INSTRUCTION mi = ANDItemplate | RS<<21 | RA<<16 | U;
    if (VM.TraceAssembler)
      asm(mIP, mi, "andi.", RA, RS, hex(U));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int Btemplate = 18<<26;

  private final void _emitB (int relative_address) {
    if (VM.VerifyAssertions) VM.assert(fits(relative_address,24));
    INSTRUCTION mi = Btemplate | (relative_address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "b", signedHex(relative_address<<2));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitB (int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitB(relative_address);
  }

  final void emitB (int relative_address) {
    relative_address -= mIP;
    if (VM.VerifyAssertions) VM.assert(relative_address < 0);
    _emitB(relative_address);
  }

  final VM_ForwardReference emitForwardB() {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitB(0);
    return fr;
  }

  static final int BLAtemplate = 18<<26 | 3;

  final void emitBLA (int address) {
    if (VM.VerifyAssertions) VM.assert(fits(address,24));
    INSTRUCTION mi = BLAtemplate | (address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bla", hex(address));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BLtemplate = 18<<26 | 1;

  private final void _emitBL (int relative_address) {
    if (VM.VerifyAssertions) VM.assert(fits(relative_address,24));
    INSTRUCTION mi = BLtemplate | (relative_address&0xFFFFFF)<<2;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bl", signedHex(relative_address<<2));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitBL (int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBL(relative_address);
  }


  final VM_ForwardReference emitForwardBL() {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitBL(0);
    return fr;
  }

  static final int BCtemplate = 16<<26;

  public static final int flipCode(int cc) {
    switch(cc) {
    case LT: return GE;
    case GT: return LE;
    case EQ: return NE;
    case LE: return GT;
    case GE: return LT;
    case NE: return EQ;
    }
    if (VM.VerifyAssertions) VM.assert(false);
    return -1;
  }

  private final void _emitBC (int cc, int relative_address) {
    if (fits(relative_address, 14)) {
      INSTRUCTION mi = BCtemplate | cc | (relative_address&0x3FFF)<<2;
      if (VM.TraceAssembler) {
	switch(cc) {
	case LT: asm(mIP, mi, "blt", signedHex(relative_address<<2)); break;
	case GT: asm(mIP, mi, "bgt", signedHex(relative_address<<2)); break;
	case EQ: asm(mIP, mi, "beq", signedHex(relative_address<<2)); break;
	case LE: asm(mIP, mi, "ble", signedHex(relative_address<<2)); break;
	case GE: asm(mIP, mi, "bge", signedHex(relative_address<<2)); break;
	case NE: asm(mIP, mi, "bne", signedHex(relative_address<<2)); break;
	}
      }
      mIP++;
      mc.addInstruction(mi);
    } else {
      _emitBC(flipCode(cc), 2);
      _emitB(relative_address-1);
    }
  }

  final void emitBC (int cc, int relative_address, int label) {
    if (relative_address == 0) {
      reserveForwardConditionalBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBC(cc, relative_address);
  }

  final void emitShortBC (int cc, int relative_address, int label) {
    if (relative_address == 0) {
      reserveShortForwardConditionalBranch(label);
    } else {
      relative_address -= mIP;
    }
    _emitBC(cc, relative_address);
  }

  final void emitBC (int cc, int relative_address) {
    relative_address -= mIP;
    if (VM.VerifyAssertions) VM.assert(relative_address < 0);
    _emitBC(cc, relative_address);
  }

  final VM_ForwardReference emitForwardBC(int cc) {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    _emitBC(cc, 0);
    return fr;
  }

  // delta i: difference between address of case i and of delta 0
  final void emitSwitchCase(int i, int relative_address, int bTarget) {
    int data = i << 2;
    if (relative_address == 0) {
      reserveForwardCase(bTarget);
    } else {
      data += ((relative_address - mIP) << 2);
    }
    if (VM.TraceAssembler) asm(mIP, data, "DATA", "" + data);
    mIP++;
    mc.addInstruction(data);
  }

  static final int BLRtemplate = 19<<26 | 0x14<<21 | 16<<1;

  final void emitBLR () {
    INSTRUCTION mi = BLRtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "blr");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BLRLtemplate = 19<<26 | 0x14<<21 | 16<<1 | 1;

  final void emitBLRL () {
    INSTRUCTION mi = BLRLtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "blrl");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BCTRtemplate = 19<<26 | 0x14<<21 | 528<<1;

  final void emitBCTR () {
    INSTRUCTION mi = BCTRtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bctr");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int BCTRLtemplate = 19<<26 | 0x14<<21 | 528<<1 | 1;

  final void emitBCTRL () {
    INSTRUCTION mi = BCTRLtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bctrl");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CALtemplate = 14<<26;

  final void emitCAL (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = CALtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cal", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CAUtemplate = 15<<26;

  final void emitCAU (int RT, int RA, int UI) {
    if (VM.VerifyAssertions) VM.assert(UI == (UI&0xFFFF));
    INSTRUCTION mi = CAUtemplate | RT<<21 | RA<<16 | UI;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cau", RT, RA, hex(UI));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCAU (int RT, int UI) {
    if (VM.VerifyAssertions) VM.assert(UI == (UI&0xFFFF));
    INSTRUCTION mi = CAUtemplate | RT<<21 | UI;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cau", RT, 0, hex(UI));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPtemplate = 31<<26;
  final void emitCMP (int BF, int RA, int RB) {
    INSTRUCTION mi = CMPtemplate | BF<<23 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmp", BF, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMP (int RA, int RB) {
    INSTRUCTION mi = CMPtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmp", 0, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPItemplate = 11<<26;

  final void emitCMPI (int BF, int RA, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = CMPItemplate | BF<<23 | RA<<16 | (V&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpi", BF, RA, signedHex(V));
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMPI (int RA, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = CMPItemplate | RA<<16 | (V&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpi", 0, RA, signedHex(V));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CMPLtemplate = 31<<26 | 32<<1;

  final void emitCMPL (int BF, int RA, int RB) {
    INSTRUCTION mi = CMPLtemplate | BF<<23 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpl", BF, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitCMPL (int RA, int RB) {
    INSTRUCTION mi = CMPLtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cmpl", 0, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRANDtemplate = 19<<26 | 257<<1;

  final void emitCRAND (int BT, int BA, int BB) {
    INSTRUCTION mi = CRANDtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crand", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRANDCtemplate = 19<<26 | 129<<1;

  final void emitCRANDC (int BT, int BA, int BB) {
    INSTRUCTION mi = CRANDCtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crandc", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRORtemplate = 19<<26 | 449<<1;

  final void emitCROR (int BT, int BA, int BB) {
    INSTRUCTION mi = CRORtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "cror", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int CRORCtemplate = 19<<26 | 417<<1;

  final void emitCRORC (int BT, int BA, int BB) {
    INSTRUCTION mi = CRORCtemplate | BT<<21 | BA<<16 | BB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "crorc", BT, BA, BB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FAtemplate = 63<<26 | 21<<1;

  final void emitFA (int FRT, int FRA,int FRB) {
    INSTRUCTION mi = FAtemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fa", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FAstemplate = 59<<26 | 21<<1; // single-percision add

  final void emitFAs (int FRT, int FRA,int FRB) {
    INSTRUCTION mi = FAstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "faS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FABStemplate = 63<<26 | 264<<1;

  final void emitFABS (int FRT, int FRB) {
    INSTRUCTION mi = FABStemplate | FRT<<21 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fabs", FRT, FRB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FCMPUtemplate = 63<<26;

  final void emitFCMPU (int FRA,int FRB) {
    INSTRUCTION mi = FCMPUtemplate | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fcmpu", FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FDtemplate = 63<<26 | 18<<1;

  final void emitFD (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FDtemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fd", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FDstemplate = 59<<26 | 18<<1; // single-precision divide

  final void emitFDs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FDstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fdS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMtemplate = 63<<26 | 25<<1;

  final void emitFM (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FMtemplate | FRT<<21 | FRA<<16 | FRB<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fm", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMstemplate = 59<<26 | 25<<1; // single-precision fm

  final void emitFMs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FMstemplate | FRT<<21 | FRA<<16 | FRB<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fmS", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FMAtemplate = 63<<26 | 29<<1;

  final void emitFMA (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FMAtemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fma", FRT, FRA, FRC, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FNMStemplate = 63<<26 | 30<<1;

  final void emitFNMS (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FNMStemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fnms", FRT, FRA, FRC, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FNEGtemplate = 63<<26 | 40<<1;

  final void emitFNEG (int FRT, int FRB) {
    INSTRUCTION mi = FNEGtemplate | FRT<<21 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fneg", FRT, FRB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FStemplate = 63<<26 | 20<<1;

  final void emitFS (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FStemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fs", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FSstemplate = 59<<26 | 20<<1;

  final void emitFSs (int FRT, int FRA, int FRB) {
    INSTRUCTION mi = FSstemplate | FRT<<21 | FRA<<16 | FRB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "FSs", FRT, FRA, FRB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FSELtemplate = 63<<26 | 23<<1;

  final void emitFSEL (int FRT, int FRA, int FRC, int FRB) {
    INSTRUCTION mi = FSELtemplate | FRT<<21 | FRA<<16 | FRB<<11 | FRC<<6;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fsel", FRT, FRA, FRB, FRC );
    mIP++;
    mc.addInstruction(mi);
  }

  // LOAD/ STORE MULTIPLE

  // TODO!! verify that D is sign extended 
  // (the Assembler Language Reference seems ambiguous) 
  //
  final void emitLM(int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = (46<<26)  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lm", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  // TODO!! verify that D is sign extended 
  // (the Assembler Language Reference seems ambiguous) 
  //
  final void emitSTM(int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = (47<<26)  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lm", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }


  static final int Ltemplate = 32<<26;

  final void emitL (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = Ltemplate  | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "l", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LBZtemplate = 34<<26;

  final void emitLBZ (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LBZtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lbz", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LBZXtemplate = 31<<26 | 87<<1;

  final void emitLBZX (int RT, int RA, int RB) {
    INSTRUCTION mi = LBZXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lbzx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHAtemplate = 42<<26;

  final void emitLHA (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LHAtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lha", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHZtemplate = 40<<26;

  final void emitLHZ (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LHZtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhz", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDtemplate = 50<<26;

  final void emitLFD (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFDtemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfd", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDUtemplate = 51<<26;

  final void emitLFDU (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFDUtemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfdu", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFDXtemplate = 31<<26 | 599<<1;

  final void emitLFDX (int FRT, int RA, int RB) {
    INSTRUCTION mi = LFDXtemplate | FRT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfdx", FRT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LFStemplate = 48<<26;

  final void emitLFS (int FRT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LFStemplate | FRT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lfs", FRT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHAXtemplate = 31<<26 | 343<<1;

  final void emitLHAX (int RT, int RA, int RB) {
    INSTRUCTION mi = LHAXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhax", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LHZXtemplate = 31<<26 | 279<<1;

  final void emitLHZX (int RT, int RA, int RB) {
    INSTRUCTION mi = LHZXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lhzx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  final void emitLIL (int RT, int D) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = CALtemplate | RT<<21 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lil", RT, signedHex(D));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LUtemplate = 33<<26;

  final void emitLU (int RT, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = LUtemplate | RT<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "lu", RT, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LXtemplate = 31<<26 | 23<<1;

  final void emitLX (int RT, int RA, int RB) {
    INSTRUCTION mi = LXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int LUXtemplate = 31<<26 | 55<<1;

  final void emitLUX (int RT, int RA, int RB) {
    INSTRUCTION mi = LUXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lux", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  
  static final int LWARXtemplate = 31<<26 | 20<<1;

  final void emitLWARX (int RT, int RA, int RB) {
    INSTRUCTION mi = LWARXtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "lwarx", RT, RA, RB );
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFLRtemplate = 31<<26 | 0x08<<16 | 339<<1;

  final void emitMFLR (int RT) {
    INSTRUCTION mi = MFLRtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mflr", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFCRtemplate = 31<<26 | 19<<1;

  final void emitMFCR (int RT) {
    INSTRUCTION mi = MFCRtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mfcr", RT);
    mIP++;
    mc.addInstruction(mi);
  }
  
  static final int MFSPRtemplate = 31<<26 | 339<<1;

  final void emitMFSPR (int RT, int SPR) {
    INSTRUCTION mi = MFSPRtemplate | RT<<21 | SPR<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mfspr", RT, SPR);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MTLRtemplate = 31<<26 | 0x08<<16 | 467<<1;

  final void emitMTLR (int RS) {
    INSTRUCTION mi = MTLRtemplate | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtlr", RS);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MTCRFtemplate = 31<<26 | 144<<1;

  final void emitMTCRF (int mask, int RS) {
    INSTRUCTION mi = MTCRFtemplate | mask<<12 | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtcrf", mask, RS);
    mIP++;
    mc.addInstruction(mi);
  }


  static final int MTCTRtemplate = 31<<26 | 0x09<<16 | 467<<1;

  final void emitMTCTR (int RS) {
    INSTRUCTION mi = MTCTRtemplate | RS<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mtctr", RS);
    mIP++;
    mc.addInstruction(mi);
  }
 
  static final int MULHWUtemplate = 31<<26 | 11<<1;

  final void emitMULHWU (int RT, int RA, int RB) {
    INSTRUCTION mi = MULHWUtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mulhwu", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DIVtemplate = 31<<26 | 491<<1;

  final void emitDIV (int RT, int RA, int RB) {
    INSTRUCTION mi = DIVtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "div", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MULStemplate = 31<<26 | 235<<1;

  final void emitMULS (int RT, int RA, int RB) {
    INSTRUCTION mi = MULStemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "muls", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int NEGtemplate = 31<<26 | 104<<1;

  final void emitNEG (int RT, int RA) {
    INSTRUCTION mi = NEGtemplate | RT<<21 | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "neg", RT, RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ORtemplate = 31<<26 | 444<<1;

  final void emitOR (int RA, int RS, int RB) {
    INSTRUCTION mi = ORtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "or", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int RLWINM_template = 21<<26;

  final void emitRLWINM (int RA, int RS, int SH, int MB, int ME) {
    INSTRUCTION mi = RLWINM_template | RS<<21 | RA<<16 | SH<<11 | MB<<6 | ME<<1;
    /*
    if (VM.TraceAssembler)
      asm(mIP, mi, "rlwinm", RA, RS, SH, MB, ME);
    */
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFrtemplate = 31<<26 | 8<<1 | 1;

  final void emitSFr (int RT, int RA, int RB) {
    INSTRUCTION mi = SFrtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sf.", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFtemplate = 31<<26 | 8<<1;

  final void emitSF (int RT, int RA, int RB) {
    INSTRUCTION mi = SFtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sf", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFItemplate = 8<<26;

  final void emitSFI (int RA, int RS, int S) {
    if (VM.VerifyAssertions) VM.assert(fits(S,16));
    INSTRUCTION mi = SFItemplate | RS<<21 | RA<<16 | S;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfi", RA, RS, signedHex(S));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFErtemplate = 31<<26 | 136<<1 | 1;

  final void emitSFEr (int RT, int RA, int RB) {
    INSTRUCTION mi = SFErtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfe.", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFEtemplate = 31<<26 | 136<<1;

  final void emitSFE (int RT, int RA, int RB) {
    INSTRUCTION mi = SFEtemplate | RT<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfe", RT, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SFZEtemplate = 31<<26 | 200<<1;

  final void emitSFZE (int RT, int RA) {
    INSTRUCTION mi = SFZEtemplate | RT<<21 | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sfze", RT, RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SLtemplate = 31<<26 | 24<<1;

  final void emitSL (int RA, int RS, int RB) {
    INSTRUCTION mi = SLtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sl", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SLItemplate = 21<<26;

  final void emitSLI (int RA, int RS, int N) {
    INSTRUCTION mi = SLItemplate | RS<<21 | RA<<16 | N<<11 | (31-N)<<1;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sli", RA, RS, signedHex(N));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRtemplate = 31<<26 | 536<<1;

  final void emitSR (int RA, int RS, int RB) {
    INSTRUCTION mi = SRtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sr", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAtemplate = 31<<26 | 792<<1;

  final void emitSRA (int RA, int RS, int RB) {
    INSTRUCTION mi = SRAtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sra", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAItemplate = 31<<26 | 824<<1;

  final void emitSRAI (int RA, int RS, int SH) {
    INSTRUCTION mi = SRAItemplate | RS<<21 | RA<<16 | SH<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "srai", RA, RS, signedHex(SH));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SRAIrtemplate = 31<<26 | 824<<1 | 1;

  final void emitSRAIr (int RA, int RS, int SH) {
    INSTRUCTION mi = SRAIrtemplate | RS<<21 | RA<<16 | SH<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "srai.", RA, RS, signedHex(SH));
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STtemplate = 36<<26;

  final void emitST (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "st", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STBtemplate = 38<<26;

  final void emitSTB (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STBtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stb", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STBXtemplate = 31<<26 | 215<<1;

  final void emitSTBX (int RS, int RA, int RB) {
    INSTRUCTION mi = STBXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stbx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STHXtemplate = 31<<26 | 407<<1;

  final void emitSTHX (int RS, int RA, int RB) {
    INSTRUCTION mi = STHXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sthx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STXtemplate = 31<<26 | 151<<1;

  final void emitSTX (int RS, int RA, int RB) {
    INSTRUCTION mi = STXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stx", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDtemplate = 54<<26;

  final void emitSTFD (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFDtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfd", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDUtemplate = 55<<26;

  final void emitSTFDU (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFDUtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfdu", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFDXtemplate = 31<<26 | 727<<1;

  final void emitSTFDX (int FRS, int RA, int RB) {
    INSTRUCTION mi = STFDXtemplate | FRS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfdx", FRS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFStemplate = 52<<26;

  final void emitSTFS (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFStemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfs", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STFSUtemplate = 53<<26;

  final void emitSTFSU (int FRS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STFSUtemplate | FRS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stfsu", FRS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STUtemplate = 37<<26;

  final void emitSTU (int RS, int D, int RA) {
    if (VM.VerifyAssertions) VM.assert(fits(D, 16));
    INSTRUCTION mi = STUtemplate | RS<<21 | RA<<16 | (D&0xFFFF);
    if (VM.TraceAssembler)
      asm(mIP, mi, "stu", RS, signedHex(D), RA);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STUXtemplate = 31<<26 | 183<<1;

  final void emitSTUX (int RS, int RA, int RB) {
    INSTRUCTION mi = STUXtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stux", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int STWCXrtemplate = 31<<26 | 150<<1 | 1;

  final void emitSTWCXr (int RS, int RA, int RB) {
    INSTRUCTION mi = STWCXrtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "stwcx.", RS, RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int Ttemplate = 31<<26 | 4<<1;

  static final int TItemplate = 3<<26;

  final void emitTI (int TO, int RA, int SI) {
    INSTRUCTION mi = TItemplate | TO<<21 | RA<<16 | SI&0xFFFF;
    if (VM.TraceAssembler)
      asm(mIP, mi, "ti", TO, RA, signedHex(SI));
    mIP++;
    mc.addInstruction(mi);
  }
  
  static final int TLEtemplate = 31<<26 | 0x14<<21 | 4<<1;

  final void emitTLE (int RA, int RB) {
    INSTRUCTION mi = TLEtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tle", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TLTtemplate = 31<<26 | 0x10<<21 | 4<<1;

  final void emitTLT (int RA, int RB) {
    INSTRUCTION mi = TLTtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tlt", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TLLEtemplate = 31<<26 | 0x6<<21 | 4<<1;

  final void emitTLLE (int RA, int RB) {
    INSTRUCTION mi = TLLEtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "tlle", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TEQItemplate = 3<<26 | 0x4<<21;

  final void emitTEQ0 (int RA) {
    INSTRUCTION mi = TEQItemplate | RA<<16;
    if (VM.TraceAssembler)
      asm(mIP, mi, "teqi", RA, "0");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int TWItemplate = 3<<26 | 0x3EC<<16;	// RA == 12

  final void emitTWI (int imm) {
    INSTRUCTION mi = TWItemplate | imm;
    if (VM.TraceAssembler)
      asm(mIP, mi, "twi", imm);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int XORtemplate = 31<<26 | 316<<1;

  final void emitXOR (int RA, int RS, int RB) {
    INSTRUCTION mi = XORtemplate | RS<<21 | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "xor", RA, RS, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int XORItemplate = 26<<26;

  final void emitXORI (int RA, int RS, int V) {
    if (VM.VerifyAssertions) VM.assert(fits(V, 16));
    INSTRUCTION mi = XORItemplate |  RS<<21 | RA<<16  | V&0xFFFF;
    if (VM.TraceAssembler)
      asm(mIP, mi, "xori", RA, RS, V);
    mIP++;
    mc.addInstruction(mi);
  }

  /* macro instructions */

  static final int NOPtemplate = 19<<26 | 449<<1;

  final void emitNOP () {
    INSTRUCTION mi = NOPtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "nop");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int SENTINALtemplate = 19<<26 | 0x1F<<21 | 0x1F<<16 | 0x1F<<11 | 449<<1;

  final void emitSENTINAL () {
    INSTRUCTION mi = SENTINALtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "end", "prologue");
    mIP++;
    mc.addInstruction(mi);
  }

  // branch conditional -- don't thread switch
  static final int BNTStemplate = BCtemplate | GE | THREAD_SWITCH_BIT<<16;
  final VM_ForwardReference emitBNTS () {
    VM_ForwardReference fr = new VM_ForwardReference.ShortBranch(mIP);
    INSTRUCTION mi = BNTStemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "bge", THREAD_SWITCH_BIT, signedHex(0));
    mIP++;
    mc.addInstruction(mi);
    return fr;
  }

  final void emitLoffset(int RT, int RA, int offset) {
    if (fits(offset, 16)) {
      emitL  (RT, offset, RA);
    } else if ((offset & 0x8000) == 0) {
      emitCAU(RT, RA, offset>>16);
      emitL  (RT, offset&0xFFFF, RT);
    } else {
      emitCAU(RT, RA, (offset>>16)+1);
      emitL  (RT, offset|0xFFFF0000, RT);
    }
  }
    

  final void emitLtoc (int RT, int offset) {
    emitLoffset(RT, JTOC, offset);
  }

  final void emitSTtoc (int RT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitST(RT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU(Rz, JTOC, offset>>16);
      emitST (RT, offset&0xFFFF, Rz);
    } else {
      emitCAU(Rz, JTOC, (offset>>16)+1);
      emitST (RT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitCALtoc (int RT, int offset) {
    if (fits(offset, 16)) {
      emitCAL(RT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU(RT, JTOC, offset>>16);
      emitCAL(RT, offset&0xFFFF, RT);
    } else {
      emitCAU(RT, JTOC, (offset>>16)+1);
      emitCAL(RT, offset|0xFFFF0000, RT);
    }
  }

  final void emitLFDtoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitLFD(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU( Rz, JTOC, offset>>16);
      emitLFD(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU( Rz, JTOC, (offset>>16)+1);
      emitLFD(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitSTFDtoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitSTFD(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU ( Rz, JTOC, offset>>16);
      emitSTFD(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU ( Rz, JTOC, (offset>>16)+1);
      emitSTFD(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitLFStoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitLFS(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU( Rz, JTOC, offset>>16);
      emitLFS(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU( Rz, JTOC, (offset>>16)+1);
      emitLFS(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitSTFStoc (int FRT, int offset, int Rz) {
    if (fits(offset, 16)) {
      emitSTFS(FRT, offset, JTOC);
    } else if (0 == (offset&0x8000)) {
      emitCAU ( Rz, JTOC, offset>>16);
      emitSTFS(FRT, offset&0xFFFF, Rz);
    } else {
      emitCAU ( Rz, JTOC, (offset>>16)+1);
      emitSTFS(FRT, offset|0xFFFF0000, Rz);
    }
  }

  final void emitLVAL (int RT, int val) {
    if (fits(val, 16)) { 
      emitLIL(RT, val);
    } else if ((val&0x8000) == 0) {
      emitLIL(RT, val&0xFFFF);
      emitCAU(RT, RT,  val>>>16);
    } else {// top half of RT is 0xFFFF
      emitLIL(RT, val|0xFFFF0000);
      emitCAU(RT, RT, (val>>>16)+1);
    }
  }

  // Convert generated machine code into final form.
  //
  VM_MachineCode finalizeMachineCode (int[] bytecodeMap) {
    mc.setBytecodeMap(bytecodeMap);
    return makeMachineCode();
  }

  VM_MachineCode makeMachineCode () {
    mc.finish();
    if (shouldPrint) {
      INSTRUCTION[] instructions = mc.getInstructions();
      boolean saved = VM_BaselineCompiler.options.PRINT_MACHINECODE;
      try {
	VM_BaselineCompiler.options.PRINT_MACHINECODE = false;
	for (int i = 0; i < instructions.length; i++) {
	  VM.sysWrite(VM_Services.getHexString(i << LG_INSTRUCTION_WIDTH, true));
	  VM.sysWrite(" : ");
	  VM.sysWrite(VM_Services.getHexString(instructions[i], false));
	  VM.sysWrite("  ");
	  VM.sysWrite(PPC_Disassembler.disasm(instructions[i], i << LG_INSTRUCTION_WIDTH));
	  VM.sysWrite("\n");
	}
      } finally {
	VM_BaselineCompiler.options.PRINT_MACHINECODE = saved;
      }
    }
    return mc;
  }

  /**
   * Append an array of INSTRUCTION to the current machine code
   */
  void appendInstructions (INSTRUCTION[] instructionSegment) {
    for (int i=0; i<instructionSegment.length; i++) {
      mIP++;
      mc.addInstruction(instructionSegment[i]);
    }
  }

  // new PowerPC instuctions

  static final int SYNCtemplate = 31<<26 | 598<<1;
  
  final void emitSYNC () {
    INSTRUCTION mi = SYNCtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "sync");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ICBItemplate = 31<<26 | 982<<1;
  
  final void emitICBI (int RA, int RB) {
    INSTRUCTION mi = ICBItemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "icbi", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int ISYNCtemplate = 19<<26 | 150<<1;
  
  final void emitISYNC () {
    INSTRUCTION mi = ISYNCtemplate;
    if (VM.TraceAssembler)
      asm(mIP, mi, "isync");
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DCBFtemplate = 31<<26 | 86<<1;
  
  final void emitDCBF (int RA, int RB) {
    INSTRUCTION mi = DCBFtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "dcbf", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int DCBSTtemplate = 31<<26 | 54<<1;
  
  final void emitDCBST (int RA, int RB) {
    INSTRUCTION mi = DCBSTtemplate | RA<<16 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "dcbst", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFTBtemplate = 31<<26 | 392<<11 | 371<<1;
  
  final void emitMFTB (int RT) {
    INSTRUCTION mi = MFTBtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mftb", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int MFTBUtemplate = 31<<26 | 424<<11 | 371<<1;
  
  final void emitMFTBU (int RT) {
    INSTRUCTION mi = MFTBUtemplate | RT<<21;
    if (VM.TraceAssembler)
      asm(mIP, mi, "mftbu", RT);
    mIP++;
    mc.addInstruction(mi);
  }

  static final int FCTIZtemplate = 63<<26 | 15<<1;
  
  final void emitFCTIZ (int RA, int RB) {
    INSTRUCTION mi = FCTIZtemplate | RA<<21 | RB<<11;
    if (VM.TraceAssembler)
      asm(mIP, mi, "fctiz", RA, RB);
    mIP++;
    mc.addInstruction(mi);
  }

  // -----------------------------------------------------------//
  // The following section contains assembler "macros" used by: //
  //    VM_Compiler                                             //
  //    VM_MagicCompiler                                        //
  //    VM_Barriers                                             //
  // -----------------------------------------------------------//
  
  // Emit baseline stack overflow instruction sequence.
  // Before:   FP is current (calling) frame
  //           PR is the current VM_Processor, which contains a pointer to the active thread.
  // After:    R0, S0 destroyed
  //
  void emitStackOverflowCheck (int frameSize) {
    emitL   ( 0,  VM_Entrypoints.activeThreadStackLimitField.getOffset(), PROCESSOR_REGISTER);   // R0 := &stack guard page
    emitCAL (S0, -frameSize, FP);                        // S0 := &new frame
    emitTLT (S0,  0);                                    // trap if new frame below guard page
  }

  // Emit baseline stack overflow instruction sequence for native method prolog.
  // For the lowest Java to C transition frame in the stack, check that there is space of
  // STACK_SIZE_NATIVE words available on the stack;  enlarge stack if necessary.
  // For subsequent Java to C transition frames, check for the requested size and don't resize
  // the stack if overflow
  // Before:   FP is current (calling) frame
  //           PR is the current VM_Processor, which contains a pointer to the active thread.
  // After:    R0, S0 destroyed
  //
  void emitNativeStackOverflowCheck (int frameSize) {
    emitL    (S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // S0 := thread pointer
    emitL    (S0, VM_Entrypoints.jniEnvField.getOffset(), S0);      // S0 := thread.jniEnv
    emitL    ( 0, VM_Entrypoints.JNIRefsTopField.getOffset(),S0);   // R0 := thread.jniEnv.JNIRefsTop
    emitL    (S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // S0 := thread pointer
    emitCMPI ( 0, 0);                                 	 // check if S0 == 0 -> first native frame on stack
    VM_ForwardReference fr1 = emitForwardBC(EQ);
    // check for enough space for requested frame size
    emitL   ( 0,  VM_Entrypoints.stackLimitField.getOffset(), S0);  // R0 := &stack guard page
    emitCAL (S0, -frameSize, FP);                        // S0 := &new frame pointer
    emitTLT (S0,  0);                                    // trap if new frame below guard page
    VM_ForwardReference fr2 = emitForwardB();

    // check for enough space for STACK_SIZE_JNINATIVE 
    fr1.resolve(this);
    emitL   ( 0,  VM_Entrypoints.stackLimitField.getOffset(), S0);  // R0 := &stack guard page
    emitLIL(S0, 1);
    emitSLI(S0, S0, STACK_LOG_JNINATIVE);
    emitSF (S0, S0, FP);             // S0 := &new frame pointer

    emitCMP(0, S0);
    VM_ForwardReference fr3 = emitForwardBC(LE);
    emitTWI ( 1 );                                    // trap if new frame pointer below guard page
    fr2.resolve(this);
    fr3.resolve(this);
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  static final int CALL_INSTRUCTIONS = 3; // number of instructions generated by emitCall()
  void emitCall (int spSaveAreaOffset) {
    emitST(SP, spSaveAreaOffset, FP); // save SP
    emitBLRL  ();
    emitL (SP, spSaveAreaOffset, FP); // restore SP
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           "hidden" parameter (e.g. for fast invokeinterface collision resolution
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitCallWithHiddenParameter (int spSaveAreaOffset, int hiddenParameter) {
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(SP, hiddenParameter);      // pass "hidden" parameter in SP scratch  register
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }

  //-#if RVM_WITH_SPECIALIZATION

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           call site number for specialization
  //
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitSpecializationCall (int spSaveAreaOffset, VM_Method m, int bIP) {
    int callSiteNumber = 0;
    if (VM_SpecializationSentry.isValid()) {
      callSiteNumber = VM_SpecializationCallSites.getCallSiteNumber(null, m, bIP);
    }
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(0, callSiteNumber<<2);      // pass call site in reg. 0
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }

  // Emit baseline call instruction sequence.
  // Taken:    offset of sp save area within current (baseline) stackframe, in bytes
  //           call site number for specialization
  //
  // Before:   LR is address to call
  //           FP is address of current frame
  // After:    no registers changed
  //
  void emitSpecializationCallWithHiddenParameter(int spSaveAreaOffset, 
						 int hiddenParameter,
						 VM_Method m,
						 int bIP) {
    int callSiteNumber = 0;
    if (VM_SpecializationSentry.isValid()) {
      callSiteNumber = VM_SpecializationCallSites.getCallSiteNumber(null, m, bIP);
    }
    emitST  (SP, spSaveAreaOffset, FP); // save SP
    emitLVAL(SP, hiddenParameter);    // pass "hidden" parameter in reg. SP 
    emitLVAL(0, callSiteNumber<<2);      // pass call site in reg. 0
    emitBLRL();
    emitL   (SP, spSaveAreaOffset, FP); // restore SP
  }
  //-#endif
}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$
/**
 * Constants exported by the assembler
 * @author Bowen Alpern
 * @author Maria Butrico
 * @author Anthony Cocchi
 * @author Dave Grove
 * @author Derek Lieber
 */
interface VM_AssemblerConstants {

  public static final int LT = 0xC<<21 | 0<<16;
  public static final int GT = 0xC<<21 | 1<<16;
  public static final int EQ = 0xC<<21 | 2<<16;
  public static final int GE = 0x4<<21 | 0<<16;
  public static final int LE = 0x4<<21 | 1<<16;
  public static final int NE = 0x4<<21 | 2<<16;

}  
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Registers used by virtual machine.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_BaselineConstants extends VM_Constants {

  // Dedicated registers
  static final int FP   = FRAME_POINTER; 
  static final int JTOC = JTOC_POINTER;
  static final int TI   = THREAD_ID_REGISTER;

  // Scratch general purpose registers
  static final int S0   = FIRST_SCRATCH_GPR;
  static final int SP   = FIRST_SCRATCH_GPR+1;

  // Temporary general purpose registers 
  static final int T0   = FIRST_VOLATILE_GPR;
  static final int T1   = FIRST_VOLATILE_GPR+1;
  static final int T2   = FIRST_VOLATILE_GPR+2;
  static final int T3   = FIRST_VOLATILE_GPR+3;
  
  // Temporary floating-point registers;
  static final int F0   = FIRST_VOLATILE_FPR;
  static final int F1   = FIRST_VOLATILE_FPR+1;
  static final int F2   = FIRST_VOLATILE_FPR+2;
  static final int F3   = FIRST_VOLATILE_FPR+3;

  static final int VOLATILE_GPRS = LAST_VOLATILE_GPR - FIRST_VOLATILE_GPR + 1;
  static final int VOLATILE_FPRS = LAST_VOLATILE_FPR - FIRST_VOLATILE_FPR + 1;
  static final int MIN_PARAM_REGISTERS = (VOLATILE_GPRS < VOLATILE_FPRS ? VOLATILE_GPRS : VOLATILE_FPRS);
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  Handle exception delivery and stack unwinding for methods compiled 
 * by baseline compiler.
 *
 * @author Derek Lieber
 * @date 18 Sep 1998 
 */
class VM_BaselineExceptionDeliverer extends VM_ExceptionDeliverer 
  implements VM_BaselineConstants {

  /**
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod compiledMethod,
			VM_Address        catchBlockInstructionAddress,
			Throwable         exceptionObject,
			VM_Registers      registers) {
    VM_Address fp    = registers.getInnermostFramePointer();
    VM_Method method = compiledMethod.getMethod();

    // reset sp to "empty expression stack" state
    //
    VM_Address sp = fp.add(VM_Compiler.getEmptyStackOffset(method));

    // push exception object as argument to catch block
    //
    sp = sp.sub(4);
    VM_Magic.setMemoryWord(sp, VM_Magic.objectAsAddress(exceptionObject).toInt());
    registers.gprs[SP] = sp.toInt();

    // set address at which to resume executing frame
    //
    registers.ip = catchBlockInstructionAddress;

    // branch to catch block
    //
    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called
    if (VM.VerifyAssertions) VM.assert(registers.inuse == true); 

    registers.inuse = false;
    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }
   
  /**
   * Unwind a stackframe.
   */
  void unwindStackFrame(VM_CompiledMethod compiledMethod, VM_Registers registers) {
    VM_Method method = compiledMethod.getMethod();
    if (method.isSynchronized()) { 
      VM_Address ip = registers.getInnermostInstructionAddress();
      VM_Address base = VM_Magic.objectAsAddress(compiledMethod.getInstructions());
      int instr = ip.diff(base);
      int lockOffset = ((VM_BaselineCompiledMethod)compiledMethod).getLockAcquisitionOffset();
      if (instr > lockOffset) { // we actually have the lock, so must unlock it.
	Object lock;
	if (method.isStatic()) {
	  lock = method.getDeclaringClass().getClassForType();
	} else {
	  VM_Address fp = registers.getInnermostFramePointer();
	  int offset = VM_Compiler.getFirstLocalOffset(method);
	  lock = VM_Magic.addressAsObject(VM_Magic.getMemoryAddress(fp.add(offset)));
	}
	VM_ObjectModel.genericUnlock(lock);
      }
    }
    registers.unwindStackFrame();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * VM_Compiler is the baseline compiler class for powerPC architectures.
 *
 * <p> The <code> compile() </code> method translates the bytecodes of a 
 * method to straightforward machine code.
 * 
 * @author Bowen Alpern
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;

public class VM_Compiler extends VM_BaselineCompiler 
  implements VM_BaselineConstants,
	     VM_AssemblerConstants {

  // stackframe pseudo-constants //
  /*private*/ int frameSize;            //!!TODO: make private once VM_MagicCompiler is merged in
  /*private*/ int spSaveAreaOffset;     //!!TODO: make private once VM_MagicCompiler is merged in
  private int emptyStackOffset;
  private int firstLocalOffset;
  private int spillOffset;

  // If we're doing a short forward jump of less than 
  // this number of bytecodes, then we can always use a short-form
  // conditional branch (don't have to emit a nop & bc).
  private static final int SHORT_FORWARD_LIMIT = 500;

  /**
   * Create a VM_Compiler object for the compilation of method.
   */
  VM_Compiler(VM_BaselineCompiledMethod cm) {
    super(cm);
    if (VM.VerifyAssertions) VM.assert(T3 <= LAST_VOLATILE_GPR);           // need 4 gp temps
    if (VM.VerifyAssertions) VM.assert(F3 <= LAST_VOLATILE_FPR);           // need 4 fp temps
    if (VM.VerifyAssertions) VM.assert(S0 < SP && SP <= LAST_SCRATCH_GPR); // need 2 scratch
    frameSize        = getFrameSize(method);
    spSaveAreaOffset = getSPSaveAreaOffset(method);
    firstLocalOffset = getFirstLocalOffset(method);
    emptyStackOffset = getEmptyStackOffset(method);
  }


  //----------------//
  // more interface //
  //----------------//
  
  // position of spill area within method's stackframe.
  static int getMaxSpillOffset (VM_Method m) throws VM_PragmaUninterruptible {
    int params = m.getOperandWords()<<2; // maximum parameter area
    int spill  = params - (MIN_PARAM_REGISTERS << 2);
    if (spill < 0) spill = 0;
    return STACKFRAME_HEADER_SIZE + spill - 4;
  }
  
  // position of operand stack within method's stackframe.
  static int getEmptyStackOffset (VM_Method m) throws VM_PragmaUninterruptible {
    int stack = m.getOperandWords()<<2; // maximum stack size
    return getMaxSpillOffset(m) + stack + 4; // last local
  }
  
  // position of locals within method's stackframe.
  static int getFirstLocalOffset (VM_Method m) throws VM_PragmaUninterruptible {
    int locals = m.getLocalWords()<<2;       // input param words + pure locals
    return getEmptyStackOffset(m) - 4 + locals; // bottom-most local
  }
  
  // position of SP save area within method's stackframe.
  static int getSPSaveAreaOffset (VM_Method m) throws VM_PragmaUninterruptible {
     return getFirstLocalOffset(m) + 4;
  }
  
  // size of method's stackframe.
  static int getFrameSize (VM_Method m) throws VM_PragmaUninterruptible {
    int size;
    if (!m.isNative()) {
      size = getSPSaveAreaOffset(m) + 4;
      if (m.getDeclaringClass().isDynamicBridge()) {
	size += (LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) * 8;
	size += (LAST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR + 1) * 4;
      }
      size = (size + STACKFRAME_ALIGNMENT_MASK) & 
              ~STACKFRAME_ALIGNMENT_MASK; // round up
      return size;
    } else {
      // space for:
      //   -AIX header (6 words)
      //   -parameters and 2 new JNI parameters (jnienv + obj), minimum 8 words
      //   -JNI_SAVE_AREA_OFFSET = savedSP + savedJTOC  + Processor_Register
      //                           nonvolatile registers + GC flag + 
      //                           affinity (20 words) +
      //                           saved volatile registers
      int argSpace = 4 * (m.getParameterWords()+ 2);
      if (argSpace<32)
	argSpace = 32;
      size = AIX_FRAME_HEADER_SIZE + argSpace + JNI_SAVE_AREA_SIZE;     
    }

    size = (size + STACKFRAME_ALIGNMENT_MASK) & ~STACKFRAME_ALIGNMENT_MASK; // round up
    return size;

  }

  /*
   * implementation of abstract methods of VM_BaselineCompiler
   */

  /*
   * Misc routines not directly tied to a particular bytecode
   */

  /**
   * Emit the prologue for the method
   */
  protected final void emit_prologue() {
    genPrologue();
  }

  /**
   * Emit code to complete the dynamic linking of a
   * prematurely resolved VM_Type.
   * @param dictionaryId of type to link (if necessary)
   */
  protected final void emit_initializeClassIfNeccessary(int dictionaryId) {
    asm.emitLtoc(S0, VM_Entrypoints.initializeClassIfNecessaryMethod.getOffset());
    asm.emitMTLR(S0);
    asm.emitLVAL(T0, dictionaryId); 
    asm.emitCall(spSaveAreaOffset);
  }

  /**
   * Emit the code for a threadswitch tests (aka a yieldpoint).
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  protected final void emit_threadSwitchTest(int whereFrom) {
    genThreadSwitchTest(whereFrom);
  }

  /**
   * Emit the code to implement the spcified magic.
   * @param magicMethod desired magic
   */
  protected final void emit_Magic(VM_Method magicMethod) {
    VM_MagicCompiler.generateInlineCode(this, magicMethod);
  }


  /*
   * Loading constants
   */


  /**
   * Emit code to load the null constant.
   */
  protected final void emit_aconst_null() {
    asm.emitLIL(T0,  0);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to load an int constant.
   * @param val the int constant to load
   */
  protected final void emit_iconst(int val) {
    asm.emitLIL(T0, val);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to load a long constant
   * @param val the lower 32 bits of long constant (upper32 are 0).
   */
  protected final void emit_lconst(int val) {
    if (val == 0) {
      asm.emitLFStoc(F0, VM_Entrypoints.zeroFloatField.getOffset(), T0);
    } else if (val == 1) {
      if (VM.VerifyAssertions) VM.assert(val == 1);
      asm.emitLFDtoc(F0, VM_Entrypoints.longOneField.getOffset(), T0);
    }
    asm.emitSTFDU (F0, -8, SP);
  }

  /**
   * Emit code to load 0.0f
   */
  protected final void emit_fconst_0() {
    asm.emitLFStoc(F0, VM_Entrypoints.zeroFloatField.getOffset(), T0);
    asm.emitSTFSU (F0, -4, SP);
  }

  /**
   * Emit code to load 1.0f
   */
  protected final void emit_fconst_1() {
    asm.emitLFStoc(F0, VM_Entrypoints.oneFloatField.getOffset(), T0);
    asm.emitSTFSU (F0, -4, SP);
  }

  /**
   * Emit code to load 2.0f
   */
  protected final void emit_fconst_2() {
    asm.emitLFStoc(F0, VM_Entrypoints.twoFloatField.getOffset(), T0);
    asm.emitSTFSU (F0, -4, SP);
  }

  /**
   * Emit code to load 0.0d
   */
  protected final void emit_dconst_0() {
    asm.emitLFStoc(F0, VM_Entrypoints.zeroFloatField.getOffset(), T0);
    asm.emitSTFDU (F0, -8, SP);
  }

  /**
   * Emit code to load 1.0d
   */
  protected final void emit_dconst_1() {
    asm.emitLFStoc(F0, VM_Entrypoints.oneFloatField.getOffset(), T0);
    asm.emitSTFDU (F0, -8, SP);
  }

  /**
   * Emit code to load a 32 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc(int offset) {
    asm.emitLtoc(T0,  offset);
    asm.emitSTU (T0, -4, SP);
  }

  /**
   * Emit code to load a 64 bit constant
   * @param offset JTOC offset of the constant 
   */
  protected final void emit_ldc2(int offset) {
    asm.emitLFDtoc(F0,  offset, T0);
    asm.emitSTFDU (F0, -8, SP);
  }


  /*
   * loading local variables
   */


  /**
   * Emit code to load an int local variable
   * @param index the local index to load
   */
  protected final void emit_iload(int index) {
    int offset = localOffset(index);
    asm.emitL(T0, offset, FP);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to load a long local variable
   * @param index the local index to load
   */
  protected final void emit_lload(int index) {
    int offset = localOffset(index) - 4;
    asm.emitLFD  (F0, offset, FP);
    asm.emitSTFDU(F0, -8, SP);
  }

  /**
   * Emit code to local a float local variable
   * @param index the local index to load
   */
  protected final void emit_fload(int index) {
    int offset = localOffset(index);
    asm.emitL(T0, offset, FP);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to load a double local variable
   * @param index the local index to load
   */
  protected final void emit_dload(int index) {
    int offset = localOffset(index) - 4;
    asm.emitLFD  (F0, offset, FP);
    asm.emitSTFDU(F0, -8, SP);
  }

  /**
   * Emit code to load a reference local variable
   * @param index the local index to load
   */
  protected final void emit_aload(int index) {
    int offset = localOffset(index);
    asm.emitL(T0, offset, FP);
    asm.emitSTU(T0, -4, SP);
  }


  /*
   * storing local variables
   */


  /**
   * Emit code to store an int to a local variable
   * @param index the local index to load
   */
  protected final void emit_istore(int index) {
    asm.emitL(T0, 0, SP);
    asm.emitCAL(SP, 4, SP);
    int offset = localOffset(index);
    asm.emitST(T0, offset, FP);
  }

  /**
   * Emit code to store a long to a local variable
   * @param index the local index to load
   */
  protected final void emit_lstore(int index) {
    asm.emitLFD(F0, 0, SP);
    asm.emitCAL(SP, 8, SP);
    int offset = localOffset(index)-4;
    asm.emitSTFD(F0, offset, FP);
  }

  /**
   * Emit code to store a float to a local variable
   * @param index the local index to load
   */
  protected final void emit_fstore(int index) {
    asm.emitL  (T0, 0, SP);
    asm.emitCAL(SP, 4, SP);
    int offset = localOffset(index);
    asm.emitST(T0, offset, FP);
  }

  /**
   * Emit code to store an double  to a local variable
   * @param index the local index to load
   */
  protected final void emit_dstore(int index) {
    asm.emitLFD(F0, 0, SP);
    asm.emitCAL(SP, 8, SP);
    int offset = localOffset(index)-4;
    asm.emitSTFD(F0, offset, FP);
  }

  /**
   * Emit code to store a reference to a local variable
   * @param index the local index to load
   */
  protected final void emit_astore(int index) {
    asm.emitL(T0, 0, SP);
    asm.emitCAL(SP, 4, SP);
    int offset = localOffset(index);
    asm.emitST(T0, offset, FP);
  }


  /*
   * array loads
   */


  /**
   * Emit code to load from an int array
   */
  protected final void emit_iaload() {
    aloadSetup(2);
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    asm.emitLX  (T2, T0, T1);  // load desired int array element
    asm.emitSTU (T2,  4, SP);  
  }

  /**
   * Emit code to load from a long array
   */
  protected final void emit_laload() {
    aloadSetup(3);
    asm.emitSLI (T0, T0,  3);  // convert two word index to byte index
    asm.emitLFDX(F0, T0, T1);  // load desired (long) array element
    asm.emitSTFD(F0,  0, SP);  
  }

  /**
   * Emit code to load from a float array
   */
  protected final void emit_faload() {
    aloadSetup(2);
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    asm.emitLX  (T2, T0, T1);  // load desired (float) array element
    asm.emitSTU (T2,  4, SP);  
  }

  /**
   * Emit code to load from a double array
   */
  protected final void emit_daload() {
    aloadSetup(3);
    asm.emitSLI (T0, T0,  3);  // convert two word index to byte index
    asm.emitLFDX(F0, T0, T1);  // load desired (double) array element
    asm.emitSTFD(F0,  0, SP);  
  }

  /**
   * Emit code to load from a reference array
   */
  protected final void emit_aaload() {
    aloadSetup(2);
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    asm.emitLX  (T2, T0, T1);  // load desired (ref) array element
    asm.emitSTU (T2,  4, SP);  
  }

  /**
   * Emit code to load from a byte/boolean array
   */
  protected final void emit_baload() {
    aloadSetup(0);
    asm.emitLBZX(T2, T0, T1);  // no load byte algebraic ...
    asm.emitSLI (T2, T2, 24);
    asm.emitSRAI(T2, T2, 24);  // propogate the sign bit
    asm.emitSTU (T2,  4, SP);  
  }

  /**
   * Emit code to load from a char array
   */
  protected final void emit_caload() {
    aloadSetup(1);
    asm.emitSLI (T0, T0,  1);  // convert halfword index to byte index
    asm.emitLHZX(T2, T0, T1);  // load desired (char) array element
    asm.emitSTU (T2,  4, SP);  
  }

  /**
   * Emit code to load from a short array
   */
  protected final void emit_saload() {
    aloadSetup(1);
    asm.emitSLI (T0, T0,  1);  // convert halfword index to byte index
    asm.emitLHAX(T2, T0, T1);  // load desired (short) array element
    asm.emitSTU (T2,  4, SP);  
  }


  /*
   * array stores
   */


  /**
   * Emit code to store to an int array
   */
  protected final void emit_iastore() {
    astoreSetup(2);
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    asm.emitSTX (T3, T0, T1);  // store int value in array
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }

  /**
   * Emit code to store to a long array
   */
  protected final void emit_lastore() {
    astoreLong();
  }

  /**
   * Emit code to store to a float array
   */
  protected final void emit_fastore() {
    astoreSetup(2);
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    asm.emitSTX (T3, T0, T1);  // store float value in array
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }

  /**
   * Emit code to store to a double array
   */
  protected final void emit_dastore() {
    astoreLong();
  }

  /**
   * Emit code to store to a reference array
   */
  protected final void emit_aastore() {
    asm.emitLtoc(T0,  VM_Entrypoints.checkstoreMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitL   (T0,  8, SP);  //  T0 := arrayref
    asm.emitL   (T1,  0, SP);  //  T1 := value
    asm.emitCall(spSaveAreaOffset);   // checkstore(arrayref, value)
    if (VM_Collector.NEEDS_WRITE_BARRIER) 
      VM_Barriers.compileArrayStoreBarrier(asm, spSaveAreaOffset);
    astoreSetup(-1);	// NOT (dfb): following 4 lines plus emitTLLE seem redundant and possibly bogus
    asm.emitL   (T1,  8, SP);                    // T1 is array ref
    asm.emitL   (T0,  4, SP);                    // T0 is array index
    asm.emitL   (T2,  VM_ObjectModel.getArrayLengthOffset(), T1);  // T2 is array length
    asm.emitL   (T3,  0, SP);                    // T3 is value to store
    emitSegmentedArrayAccess (asm, T1, T0, T2, 2);
    asm.emitTLLE(T2, T0);      // trap if index < 0 or index >= length
    asm.emitSLI (T0, T0,  2);  // convert word index to byte index
    if (VM.BuildForConcurrentGC) {
      //-#if RVM_WITH_CONCURRENT_GC // because VM_RCBarriers not available for non concurrent GC builds
      VM_RCBarriers.compileArrayStoreBarrier(asm, spSaveAreaOffset);
      //-#endif
    } else {
      asm.emitSTX (T3, T0, T1);  // store ref value in array
    }
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }

  /**
   * Emit code to store to a byte/boolean array
   */
  protected final void emit_bastore() {
    astoreSetup(0);
    asm.emitSTBX(T3, T0, T1);  // store byte value in array
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }

  /**
   * Emit code to store to a char array
   */
  protected final void emit_castore() {
    astoreSetup(1);
    asm.emitSLI (T0, T0,  1);  // convert halfword index to byte index
    asm.emitSTHX(T3, T0, T1);  // store char value in array
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }

  /**
   * Emit code to store to a short array
   */
  protected final void emit_sastore() {
    astoreSetup(1);
    asm.emitSLI (T0, T0,  1);  // convert halfword index to byte index
    asm.emitSTHX(T3, T0, T1);  // store short value in array
    asm.emitCAL (SP, 12, SP);  // complete 3 pops
  }


  /*
   * expression stack manipulation
   */


  /**
   * Emit code to implement the pop bytecode
   */
  protected final void emit_pop() {
    asm.emitCAL(SP, 4, SP);
  }

  /**
   * Emit code to implement the pop2 bytecode
   */
  protected final void emit_pop2() {
    asm.emitCAL(SP, 8, SP);
  }

  /**
   * Emit code to implement the dup bytecode
   */
  protected final void emit_dup() {
    asm.emitL  (T0,  0, SP);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to implement the dup_x1 bytecode
   */
  protected final void emit_dup_x1() {
    asm.emitL  (T0,  0, SP);
    asm.emitL  (T1,  4, SP);
    asm.emitST (T0,  4, SP);
    asm.emitST (T1,  0, SP);
    asm.emitSTU(T0, -4, SP);
  }

  /**
   * Emit code to implement the dup_x2 bytecode
   */
  protected final void emit_dup_x2() {
    asm.emitL   (T0,  0, SP);
    asm.emitLFD (F0,  4, SP);
    asm.emitST  (T0,  8, SP);
    asm.emitSTFD(F0,  0, SP);
    asm.emitSTU (T0, -4, SP);
  }

  /**
   * Emit code to implement the dup2 bytecode
   */
  protected final void emit_dup2() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitSTFDU(F0, -8, SP);
  }

  /**
   * Emit code to implement the dup2_x1 bytecode
   */
  protected final void emit_dup2_x1() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitL    (T0,  8, SP);
    asm.emitSTFD (F0,  4, SP);
    asm.emitST   (T0,  0, SP);
    asm.emitSTFDU(F0, -8, SP);
  }

  /**
   * Emit code to implement the dup2_x2 bytecode
   */
  protected final void emit_dup2_x2() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitLFD  (F1,  8, SP);
    asm.emitSTFD (F0,  8, SP);
    asm.emitSTFD (F1,  0, SP);
    asm.emitSTFDU(F0, -8, SP);
  }

  /**
   * Emit code to implement the swap bytecode
   */
  protected final void emit_swap() {
    asm.emitL  (T0,  0, SP);
    asm.emitL  (T1,  4, SP);
    asm.emitST (T0,  4, SP);
    asm.emitST (T1,  0, SP);
  }


  /*
   * int ALU
   */


  /**
   * Emit code to implement the iadd bytecode
   */
  protected final void emit_iadd() {
    asm.emitL  (T0,  0, SP);
    asm.emitL  (T1,  4, SP);
    asm.emitA  (T2, T1, T0);
    asm.emitSTU(T2,  4, SP);
  }

  /**
   * Emit code to implement the isub bytecode
   */
  protected final void emit_isub() {
    asm.emitL  (T0,  0, SP);
    asm.emitL  (T1,  4, SP);
    asm.emitSF (T2, T0, T1);
    asm.emitSTU(T2,  4, SP);
  }

  /**
   * Emit code to implement the imul bytecode
   */
  protected final void emit_imul() {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitMULS(T1,T0, T1);
    asm.emitSTU(T1, 4, SP);
  }

  /**
   * Emit code to implement the idiv bytecode
   */
  protected final void emit_idiv() {
    asm.emitL   (T0, 4, SP);
    asm.emitL   (T1, 0, SP);
    asm.emitTEQ0(T1);
    asm.emitDIV (T0, T0, T1);  // T0 := T0/T1
    asm.emitSTU (T0, 4, SP);
  }

  /**
   * Emit code to implement the irem bytecode
   */
  protected final void emit_irem() {
    asm.emitL   (T0, 4, SP);
    asm.emitL   (T1, 0, SP);
    asm.emitTEQ0(T1);
    asm.emitDIV (T2, T0, T1);   // T2 := T0/T1
    asm.emitMULS(T2, T2, T1);   // T2 := [T0/T1]*T1
    asm.emitSF  (T1, T2, T0);   // T1 := T0 - [T0/T1]*T1
    asm.emitSTU (T1, 4, SP);
  }

  /**
   * Emit code to implement the ineg bytecode
   */
  protected final void emit_ineg() {
    asm.emitL  (T0,  0, SP);
    asm.emitNEG(T0, T0);
    asm.emitST (T0,  0, SP);
  }

  /**
   * Emit code to implement the ishl bytecode
   */
  protected final void emit_ishl() {
    asm.emitL   (T0,  4, SP);
    asm.emitL   (T1,  0, SP);
    asm.emitANDI(T1, T1, 0x1F);
    asm.emitSL  (T0, T0, T1);
    asm.emitSTU (T0,  4, SP);
  }

  /**
   * Emit code to implement the ishr bytecode
   */
  protected final void emit_ishr() {
    asm.emitL   (T0,  4, SP);
    asm.emitL   (T1,  0, SP);
    asm.emitANDI(T1, T1, 0x1F);
    asm.emitSRA (T0, T0, T1);
    asm.emitSTU (T0,  4, SP);
  }

  /**
   * Emit code to implement the iushr bytecode
   */
  protected final void emit_iushr() {
    asm.emitL   (T0,  4, SP);
    asm.emitL   (T1,  0, SP);
    asm.emitANDI(T1, T1, 0x1F);
    asm.emitSR  (T0, T0, T1);
    asm.emitSTU (T0,  4, SP);
  }

  /**
   * Emit code to implement the iand bytecode
   */
  protected final void emit_iand() {
    asm.emitL   (T0,  0, SP);
    asm.emitL   (T1,  4, SP);
    asm.emitAND (T2, T0, T1);
    asm.emitSTU (T2,  4, SP);
  }

  /**
   * Emit code to implement the ior bytecode
   */
  protected final void emit_ior() {
    asm.emitL   (T0,  0,SP);
    asm.emitL   (T1,  4,SP);
    asm.emitOR  (T2, T0,T1);
    asm.emitSTU (T2,  4,SP);
  }

  /**
   * Emit code to implement the ixor bytecode
   */
  protected final void emit_ixor() {
    asm.emitL   (T0,  0,SP);
    asm.emitL   (T1,  4,SP);
    asm.emitXOR (T2, T0,T1);
    asm.emitSTU (T2,  4,SP);
  }

  /**
   * Emit code to implement the iinc bytecode
   * @param index index of local
   * @param val value to increment it by
   */
  protected final void emit_iinc(int index, int val) {
    int offset = localOffset(index);
    asm.emitL  (T0, offset, FP);
    asm.emitCAL(T0, val, T0);
    asm.emitST (T0, offset, FP);
  }


  /*
   * long ALU
   */


  /**
   * Emit code to implement the ladd bytecode
   */
  protected final void emit_ladd() {
    asm.emitL  (T0,  4, SP);
    asm.emitL  (T1, 12, SP);
    asm.emitL  (T2,  0, SP);
    asm.emitL  (T3,  8, SP);
    asm.emitA  (T0, T1, T0);
    asm.emitAE (T1, T2, T3);
    asm.emitST (T0, 12, SP);
    asm.emitSTU(T1,  8, SP);
  }

  /**
   * Emit code to implement the lsub bytecode
   */
  protected final void emit_lsub() {
    asm.emitL  (T0,  4, SP);
    asm.emitL  (T1, 12, SP);
    asm.emitL  (T2,  0, SP);
    asm.emitL  (T3,  8, SP);
    asm.emitSF (T0, T0, T1);
    asm.emitSFE(T1, T2, T3);
    asm.emitST (T0, 12, SP);
    asm.emitSTU(T1,  8, SP);
  }

  /**
   * Emit code to implement the lmul bytecode
   */
  protected final void emit_lmul() {
    asm.emitL     (T1, 12, SP);
    asm.emitL     (T3,  4, SP);
    asm.emitL     (T0,  8, SP);
    asm.emitL     (T2,  0, SP);
    asm.emitMULHWU(S0, T1, T3);
    asm.emitMULS  (T0, T0, T3);
    asm.emitA     (T0, T0, S0);
    asm.emitMULS  (S0, T1, T2);
    asm.emitMULS  (T1, T1, T3);
    asm.emitA     (T0, T0, S0);
    asm.emitST    (T1, 12, SP);
    asm.emitSTU   (T0,  8, SP);
  }

  /**
   * Emit code to implement the ldiv bytecode
   */
  protected final void emit_ldiv() {
    asm.emitL   (T3,  4, SP);
    asm.emitL   (T2,  0, SP);
    asm.emitOR  (T0, T3, T2); // or two halfs of denomenator together
    asm.emitTEQ0(T0);         // trap if 0.
    asm.emitL   (T1, 12, SP);
    asm.emitL   (T0,  8, SP);
    VM_MagicCompiler.generateSysCall(asm, 16, VM_Entrypoints.sysLongDivideIPField);
    asm.emitST  (T1, 12, SP); 
    asm.emitSTU (T0,  8, SP); 
  }

  /**
   * Emit code to implement the lrem bytecode
   */
  protected final void emit_lrem() {
    asm.emitL   (T3,  4, SP);
    asm.emitL   (T2,  0, SP);
    asm.emitOR  (T0, T3, T2); // or two halfs of denomenator together
    asm.emitTEQ0(T0);         // trap if 0.
    asm.emitL   (T1, 12, SP);
    asm.emitL   (T0,  8, SP);
    VM_MagicCompiler.generateSysCall(asm, 16, VM_Entrypoints.sysLongRemainderIPField);
    asm.emitST  (T1, 12, SP); 
    asm.emitSTU (T0,  8, SP); 
  }

  /**
   * Emit code to implement the lneg bytecode
   */
  protected final void emit_lneg() {
    asm.emitL   (T0,  4, SP);
    asm.emitL   (T1,  0, SP);
    asm.emitSFI (T0, T0, 0x0);
    asm.emitSFZE(T1, T1);
    asm.emitST  (T0,  4, SP);
    asm.emitSTU (T1,  0, SP);
  }

  /**
   * Emit code to implement the lshsl bytecode
   */
  protected final void emit_lshl() {
    asm.emitL   (T0,  0, SP);    // T0 is n
    asm.emitL   (T1,  8, SP);    // T1 is low bits of l
    asm.emitANDI(T3, T0, 0x20);  // shift more than 31 bits?
    asm.emitXOR (T0, T3, T0);    // restrict shift to at most 31 bits
    asm.emitSL  (T3, T1, T0);    // low bits of l shifted n or n-32 bits
    asm.emitL   (T2,  4, SP);    // T2 is high bits of l
    VM_ForwardReference fr1 = asm.emitForwardBC(EQ); // if shift less than 32, goto
    asm.emitSTU (T3,  4, SP);    // store high bits of result
    asm.emitLIL (T0,  0);        // low bits are zero
    asm.emitST  (T0,  4, SP);    // store 'em
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    asm.emitSL  (T2, T2, T0);    // high bits of l shifted n bits left
    asm.emitSFI (T0, T0, 0x20);  // T0 := 32 - T0; 
    asm.emitSR  (T1, T1, T0);    // T1 is middle bits of result
    asm.emitOR  (T2, T2, T1);    // T2 is high bits of result
    asm.emitSTU (T2,  4, SP);    // store high bits of result
    asm.emitST  (T3,  4, SP);    // store low bits of result           
    fr2.resolve(asm);
  }

  /**
   * Emit code to implement the lshr bytecode
   */
  protected final void emit_lshr() {
    asm.emitL   (T0,  0, SP);    // T0 is n
    asm.emitL   (T2,  4, SP);    // T2 is high bits of l
    asm.emitANDI(T3, T0, 0x20);  // shift more than 31 bits?
    asm.emitXOR (T0, T3, T0);    // restrict shift to at most 31 bits
    asm.emitSRA (T3, T2, T0);    // high bits of l shifted n or n-32 bits
    asm.emitL   (T1,  8, SP);    // T1 is low bits of l
    VM_ForwardReference fr1 = asm.emitForwardBC(EQ);
    asm.emitST  (T3,  8, SP);    // store low bits of result
    asm.emitSRAI(T0, T3, 0x1F);  // propogate a full work of sign bit
    asm.emitSTU (T0,  4, SP);    // store high bits of result
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    asm.emitSR  (T1, T1, T0);    // low bits of l shifted n bits right
    asm.emitSFI (T0, T0, 0x20);  // T0 := 32 - T0;
    asm.emitSL  (T2, T2, T0);    // T2 is middle bits of result
    asm.emitOR  (T1, T1, T2);    // T1 is low bits of result
    asm.emitST  (T1,  8, SP);    // store low bits of result 
    asm.emitSTU (T3,  4, SP);    // store high bits of result          
    fr2.resolve(asm);
  }

  /**
   * Emit code to implement the lushr bytecode
   */
  protected final void emit_lushr() {
    asm.emitL   (T0,  0, SP);    // T0 is n
    asm.emitL   (T2,  4, SP);    // T2 is high bits of l
    asm.emitANDI(T3, T0, 0x20);  // shift more than 31 bits?
    asm.emitXOR (T0, T3, T0);    // restrict shift to at most 31 bits
    asm.emitSR  (T3, T2, T0);    // high bits of l shifted n or n-32 bits
    asm.emitL   (T1,  8, SP);    // T1 is low bits of l
    VM_ForwardReference fr1 = asm.emitForwardBC(EQ);
    asm.emitST  (T3,  8, SP);    // store low bits of result
    asm.emitLIL (T0,  0);        // high bits are zero
    asm.emitSTU (T0,  4, SP);    // store 'em
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    asm.emitSR  (T1, T1, T0);    // low bits of l shifted n bits right
    asm.emitSFI (T0, T0, 0x20);  // T0 := 32 - T0;
    asm.emitSL  (T2, T2, T0);    // T2 is middle bits of result
    asm.emitOR  (T1, T1, T2);    // T1 is low bits of result
    asm.emitST  (T1,  8, SP);    // store low bits of result 
    asm.emitSTU (T3,  4, SP);    // store high bits of result          
    fr2.resolve(asm);
  }

  /**
   * Emit code to implement the land bytecode
   */
  protected final void emit_land() {
    asm.emitL  (T0,  4, SP);
    asm.emitL  (T1, 12, SP);
    asm.emitL  (T2,  0, SP);
    asm.emitL  (T3,  8, SP);
    asm.emitAND(T0, T1, T0);
    asm.emitAND(T1, T2, T3);
    asm.emitST (T0, 12, SP);
    asm.emitSTU(T1,  8, SP);
  }

  /**
   * Emit code to implement the lor bytecode
   */
  protected final void emit_lor() {
    asm.emitL  (T0,  4, SP);
    asm.emitL  (T1, 12, SP);
    asm.emitL  (T2,  0, SP);
    asm.emitL  (T3,  8, SP);
    asm.emitOR (T0, T1, T0);
    asm.emitOR (T1, T2, T3);
    asm.emitST (T0, 12, SP);
    asm.emitSTU(T1,  8, SP);
  }

  /**
   * Emit code to implement the lxor bytecode
   */
  protected final void emit_lxor() {
    asm.emitL  (T0,  4, SP);
    asm.emitL  (T1, 12, SP);
    asm.emitL  (T2,  0, SP);
    asm.emitL  (T3,  8, SP);
    asm.emitXOR(T0, T1, T0);
    asm.emitXOR(T1, T2, T3);
    asm.emitST (T0, 12, SP);
    asm.emitSTU(T1,  8, SP);
  }


  /*
   * float ALU
   */


  /**
   * Emit code to implement the fadd bytecode
   */
  protected final void emit_fadd() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitLFS  (F1,  4, SP);
    asm.emitFAs  (F0, F1, F0);
    asm.emitSTFSU(F0,  4, SP);
  }

  /**
   * Emit code to implement the fsub bytecode
   */
  protected final void emit_fsub() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitLFS  (F1,  4, SP);
    asm.emitFSs  (F0, F1, F0);
    asm.emitSTFSU(F0,  4, SP);
  }

  /**
   * Emit code to implement the fmul bytecode
   */
  protected final void emit_fmul() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitLFS  (F1,  4, SP);
    asm.emitFMs  (F0, F1, F0); // single precision multiply
    asm.emitSTFSU(F0,  4, SP);
  }

  /**
   * Emit code to implement the fdiv bytecode
   */
  protected final void emit_fdiv() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitLFS  (F1,  4, SP);
    asm.emitFDs  (F0, F1, F0);
    asm.emitSTFSU(F0,  4, SP);
  }

  /**
   * Emit code to implement the frem bytecode
   */
  protected final void emit_frem() {
    asm.emitLFS   (F1,  0, SP);              // F1 is b
    asm.emitLFS   (F0,  4, SP);              // F0 is a
    VM_MagicCompiler.generateSysCall(asm, 8, VM_Entrypoints.sysDoubleRemainderIPField);
    asm.emitSTFSU (F0,  4, SP);
  }

  /**
   * Emit code to implement the fneg bytecode
   */
  protected final void emit_fneg() {
    asm.emitLFS (F0,  0, SP);
    asm.emitFNEG(F0, F0);
    asm.emitSTFS(F0,  0, SP);
  }


  /*
   * double ALU
   */


  /**
   * Emit code to implement the dadd bytecode
   */
  protected final void emit_dadd() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitLFD  (F1,  8, SP);
    asm.emitFA   (F0, F1, F0);
    asm.emitSTFDU(F0,  8, SP);
  }

  /**
   * Emit code to implement the dsub bytecode
   */
  protected final void emit_dsub() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitLFD  (F1,  8, SP);
    asm.emitFS   (F0, F1, F0);
    asm.emitSTFDU(F0,  8, SP);
  }

  /**
   * Emit code to implement the dmul bytecode
   */
  protected final void emit_dmul() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitLFD  (F1,  8, SP);
    asm.emitFM   (F0, F1, F0);
    asm.emitSTFDU(F0,  8, SP);
  }

  /**
   * Emit code to implement the ddiv bytecode
   */
  protected final void emit_ddiv() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitLFD  (F1,  8, SP);
    asm.emitFD   (F0, F1, F0);
    asm.emitSTFDU(F0,  8, SP);
  }

  /**
   * Emit code to implement the drem bytecode
   */
  protected final void emit_drem() {
    asm.emitLFD   (F1,  0, SP);              // F1 is b
    asm.emitLFD   (F0,  8, SP);              // F0 is a
    VM_MagicCompiler.generateSysCall(asm, 16, VM_Entrypoints.sysDoubleRemainderIPField);
    asm.emitSTFDU (F0,  8, SP);
  }

  /**
   * Emit code to implement the dneg bytecode
   */
  protected final void emit_dneg() {
    asm.emitLFD (F0,  0, SP);
    asm.emitFNEG(F0, F0);
    asm.emitSTFD(F0,  0, SP);
  }


  /*
   * conversion ops
   */


  /**
   * Emit code to implement the i2l bytecode
   */
  protected final void emit_i2l() {
    asm.emitL   (T0,  0, SP);
    asm.emitSRAI(T1, T0, 31);
    asm.emitSTU (T1, -4, SP);
  }

  /**
   * Emit code to implement the i2f bytecode
   */
  protected final void emit_i2f() {
    asm.emitL     (T0,  0, SP);               // T0 is X (an int)
    asm.emitCMPI  (T0,  0);                   // is X < 0
    asm.emitLFDtoc(F0, VM_Entrypoints.IEEEmagicField.getOffset(), T1);  // F0 is MAGIC
    asm.emitSTFD  (F0, -4, SP);               // MAGIC on stack
    asm.emitST    (T0,  0, SP);               // if 0 <= X, MAGIC + X 
    VM_ForwardReference fr = asm.emitForwardBC(GE);
    asm.emitL     (T0, -4, SP);               // T0 is top of MAGIC
    asm.emitCAL   (T0, -1, T0);               // decrement top of MAGIC
    asm.emitST    (T0, -4, SP);               // MAGIC + X is on stack
    fr.resolve(asm);
    asm.emitLFD   (F1, -4, SP);               // F1 is MAGIC + X
    asm.emitFS    (F1, F1, F0);               // F1 is X
    asm.emitSTFS  (F1,  0, SP);               // float(X) is on stack 
  }

  /**
   * Emit code to implement the i2d bytecode
   */
  protected final void emit_i2d() {
    asm.emitL     (T0,  0, SP);               // T0 is X (an int)
    asm.emitCMPI  (T0,  0);                   // is X < 0
    asm.emitLFDtoc(F0, VM_Entrypoints.IEEEmagicField.getOffset(), T1);  // F0 is MAGIC
    asm.emitSTFD  (F0, -4, SP);               // MAGIC on stack
    asm.emitST    (T0,  0, SP);               // if 0 <= X, MAGIC + X 
    VM_ForwardReference fr = asm.emitForwardBC(GE); // ow, handle X < 0
    asm.emitL     (T0, -4, SP);               // T0 is top of MAGIC
    asm.emitCAL   (T0, -1, T0);               // decrement top of MAGIC
    asm.emitST    (T0, -4, SP);               // MAGIC + X is on stack
    fr.resolve(asm);
    asm.emitLFD   (F1, -4, SP);               // F1 is MAGIC + X
    asm.emitFS    (F1, F1, F0);               // F1 is X
    asm.emitSTFDU (F1, -4, SP);               // float(X) is on stack 
  }

  /**
   * Emit code to implement the l2i bytecode
   */
  protected final void emit_l2i() {
    asm.emitCAL(SP, 4, SP); // throw away top of the long
  }

  /**
   * Emit code to implement the l2f bytecode
   */
  protected final void emit_l2f() {
    asm.emitL   (T1, 4, SP);
    asm.emitL   (T0, 0, SP);
    VM_MagicCompiler.generateSysCall(asm, 8, VM_Entrypoints.sysLongToFloatIPField);
    asm.emitSTFSU(F0,  4, SP);
  }

  /**
   * Emit code to implement the l2d bytecode
   */
  protected final void emit_l2d() {
    asm.emitL   (T1, 4, SP);
    asm.emitL   (T0, 0, SP);
    VM_MagicCompiler.generateSysCall(asm, 8, VM_Entrypoints.sysLongToDoubleIPField);
    asm.emitSTFD(F0,  0, SP);
  }

  /**
   * Emit code to implement the f2i bytecode
   */
  protected final void emit_f2i() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitFCTIZ(F0, F0);
    asm.emitSTFD (F0, -4, SP); 
  }

  /**
   * Emit code to implement the f2l bytecode
   */
  protected final void emit_f2l() {
    asm.emitLFS (F0,  0, SP);
    VM_MagicCompiler.generateSysCall(asm, 4, VM_Entrypoints.sysFloatToLongIPField);
    asm.emitST  (T1,  0, SP);
    asm.emitSTU (T0, -4, SP);
  }

  /**
   * Emit code to implement the f2d bytecode
   */
  protected final void emit_f2d() {
    asm.emitLFS  (F0,  0, SP);
    asm.emitSTFDU(F0, -4, SP);
  }

  /**
   * Emit code to implement the d2i bytecode
   */
  protected final void emit_d2i() {
    asm.emitLFD  (F0,  0, SP);
    asm.emitFCTIZ(F0, F0);
    asm.emitSTFD (F0,  0, SP);
    asm.emitCAL  (SP,  4, SP);
  }

  /**
   * Emit code to implement the d2l bytecode
   */
  protected final void emit_d2l() {
    asm.emitLFD (F0,  0, SP);
    VM_MagicCompiler.generateSysCall(asm, 8, VM_Entrypoints.sysDoubleToLongIPField);
    asm.emitST  (T1, 4, SP);
    asm.emitSTU (T0, 0, SP);
  }

  /**
   * Emit code to implement the d2f bytecode
   */
  protected final void emit_d2f() {
    asm.emitLFD  (F0, 0, SP);
    asm.emitSTFSU(F0, 4, SP);
  }

  /**
   * Emit code to implement the i2b bytecode
   */
  protected final void emit_i2b() {
    asm.emitL   (T0,  3, SP);
    asm.emitSRAI(T0, T0, 24);
    asm.emitST  (T0,  0, SP);
  }

  /**
   * Emit code to implement the i2c bytecode
   */
  protected final void emit_i2c() {
    asm.emitLHZ(T0, 2, SP);
    asm.emitST (T0, 0, SP);
  }

  /**
   * Emit code to implement the i2s bytecode
   */
  protected final void emit_i2s() {
    asm.emitLHA(T0, 2, SP);
    asm.emitST (T0, 0, SP);
  }


  /*
   * comparision ops
   */


  /**
   * Emit code to implement the lcmp bytecode
   */
  protected final void emit_lcmp() {
    asm.emitL    (T1,  8, SP);  // T1 is ah
    asm.emitL    (T3,  0, SP);  // T3 is bh
    asm.emitCMP  (T1, T3);      // ah ? al
    VM_ForwardReference fr1 = asm.emitForwardBC(LT);
    VM_ForwardReference fr2 = asm.emitForwardBC(GT);
    asm.emitL    (T0, 12, SP);  // (ah == bh), T0 is al
    asm.emitL    (T2,  4, SP);  // T2 is bl
    asm.emitCMPL (T0, T2);      // al ? bl (logical compare)
    VM_ForwardReference fr3 = asm.emitForwardBC(LT);
    VM_ForwardReference fr4 = asm.emitForwardBC(GT);
    asm.emitLIL  (T0,  0);      // a == b
    asm.emitSTU  (T0, 12, SP);  // push  0
    VM_ForwardReference fr5 = asm.emitForwardB();
    fr1.resolve(asm);
    fr3.resolve(asm);
    asm.emitLIL  (T0, -1);      // a <  b
    asm.emitSTU  (T0, 12, SP);  // push -1
    VM_ForwardReference fr6 = asm.emitForwardB();
    fr2.resolve(asm);
    fr4.resolve(asm);
    asm.emitLIL  (T0,  1);      // a >  b
    asm.emitSTU  (T0, 12, SP);  // push  1
    fr5.resolve(asm);
    fr6.resolve(asm);
  }

  /**
   * Emit code to implement the fcmpl bytecode
   */
  protected final void emit_fcmpl() {
    asm.emitLFS  (F0,  4, SP);
    asm.emitLFS  (F1,  0, SP);
    asm.emitFCMPU(F0, F1);
    VM_ForwardReference fr1 = asm.emitForwardBC(LE);
    asm.emitLIL  (T0,  1); // the GT bit of CR0
    asm.emitSTU  (T0,  4, SP);
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    VM_ForwardReference fr3 = asm.emitForwardBC(EQ);
    asm.emitLIL  (T0, -1); // the LT or UO bits of CR0
    asm.emitSTU  (T0,  4, SP);
    VM_ForwardReference fr4 = asm.emitForwardB();
    fr3.resolve(asm);
    asm.emitLIL  (T0,  0);
    asm.emitSTU  (T0,  4, SP); // the EQ bit of CR0
    fr2.resolve(asm);
    fr4.resolve(asm);
  }

  /**
   * Emit code to implement the fcmpg bytecode
   */
  protected final void emit_fcmpg() {
    asm.emitLFS  (F0,  4, SP);
    asm.emitLFS  (F1,  0, SP);
    asm.emitFCMPU(F0, F1);
    VM_ForwardReference fr1 = asm.emitForwardBC(GE);
    asm.emitLIL  (T0, -1);     // the LT bit of CR0
    asm.emitSTU  (T0,  4, SP);
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    VM_ForwardReference fr3 = asm.emitForwardBC(EQ);
    asm.emitLIL  (T0,  1);     // the GT or UO bits of CR0
    asm.emitSTU  (T0,  4, SP);
    VM_ForwardReference fr4 = asm.emitForwardB();
    fr3.resolve(asm);
    asm.emitLIL  (T0,  0);     // the EQ bit of CR0
    asm.emitSTU  (T0,  4, SP);
    fr2.resolve(asm);
    fr4.resolve(asm);
  }

  /**
   * Emit code to implement the dcmpl bytecode
   */
  protected final void emit_dcmpl() {
    asm.emitLFD  (F0,  8, SP);
    asm.emitLFD  (F1,  0, SP);
    asm.emitFCMPU(F0, F1);
    VM_ForwardReference fr1 = asm.emitForwardBC(LE);
    asm.emitLIL  (T0,  1); // the GT bit of CR0
    asm.emitSTU  (T0, 12, SP);
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    VM_ForwardReference fr3 = asm.emitForwardBC(EQ);
    asm.emitLIL  (T0, -1); // the LT or UO bits of CR0
    asm.emitSTU  (T0, 12, SP);
    VM_ForwardReference fr4 = asm.emitForwardB();
    fr3.resolve(asm);
    asm.emitLIL  (T0,  0);
    asm.emitSTU  (T0, 12, SP); // the EQ bit of CR0
    fr2.resolve(asm);
    fr4.resolve(asm);
  }

  /**
   * Emit code to implement the dcmpg bytecode
   */
  protected final void emit_dcmpg() {
    asm.emitLFD  (F0,  8, SP);
    asm.emitLFD  (F1,  0, SP);
    asm.emitFCMPU(F0, F1);
    VM_ForwardReference fr1 = asm.emitForwardBC(GE);
    asm.emitLIL  (T0, -1); // the LT bit of CR0
    asm.emitSTU  (T0, 12, SP);
    VM_ForwardReference fr2 = asm.emitForwardB();
    fr1.resolve(asm);
    VM_ForwardReference fr3 = asm.emitForwardBC(EQ);
    asm.emitLIL  (T0,  1); // the GT or UO bits of CR0
    asm.emitSTU  (T0, 12, SP);
    VM_ForwardReference fr4 = asm.emitForwardB();
    fr3.resolve(asm);
    asm.emitLIL  (T0,  0); // the EQ bit of CR0
    asm.emitSTU  (T0, 12, SP);
    fr2.resolve(asm);
    fr4.resolve(asm);
  }


  /*
   * branching
   */


  /**
   * Emit code to implement the ifeg bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifeq(int bTarget) {
    asm.emitL  (T0,  0, SP);
    asm.emitAIr(T0, T0,  0); // compares T0 to 0 and sets CR0 
    asm.emitCAL(SP,  4, SP); // completes pop
    genCondBranch(EQ, bTarget);
  }

  /**
   * Emit code to implement the ifne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifne(int bTarget) {
    asm.emitL  (T0,  0, SP);
    asm.emitAIr(T0, T0,  0); // compares T0 to 0 and sets CR0 
    asm.emitCAL(SP,  4, SP); // completes pop
    genCondBranch(NE, bTarget);
  }

  /**
   * Emit code to implement the iflt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_iflt(int bTarget) {
    asm.emitL  (T0,  0, SP);
    asm.emitAIr(T0, T0,  0); // compares T0 to 0 and sets CR0 
    asm.emitCAL(SP,  4, SP); // completes pop
    genCondBranch(LT, bTarget);
  }

  /**
   * Emit code to implement the ifge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifge(int bTarget) {
    asm.emitL  (T0,  0, SP);
    asm.emitAIr(T0, T0,  0); // compares T0 to 0 and sets CR0 
    asm.emitCAL(SP,  4, SP); // completes pop
    genCondBranch(GE, bTarget);
  }

  /**
   * Emit code to implement the ifgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifgt(int bTarget) {
    asm.emitL  (T0,  0, SP);
    asm.emitAIr(T0, T0,  0); // compares T0 to 0 and sets CR0 
    asm.emitCAL(SP,  4, SP); // completes pop
    genCondBranch(GT, bTarget);
  }

  /**
   * Emit code to implement the ifle bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifle(int bTarget) {
    asm.emitL  (T0, 0, SP);
    asm.emitAIr(0,  T0,  0); // T0 to 0 and sets CR0 
    asm.emitCAL(SP, 4, SP);  // completes pop
    genCondBranch(LE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpeq(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(EQ, bTarget);
  }

  /**
   * Emit code to implement the if_icmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpne(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(NE, bTarget);
  }

  /**
   * Emit code to implement the if_icmplt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmplt(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(LT, bTarget);
  }

  /**
   * Emit code to implement the if_icmpge bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpge(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(GE, bTarget);
  }

  /**
   * Emit code to implement the if_icmpgt bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmpgt(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(GT, bTarget);
  }

  /**
   * Emit code to implement the if_icmple bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_icmple(int bTarget) {
    asm.emitL  (T0, 4, SP);
    asm.emitL  (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(LE, bTarget);
  }

  /**
   * Emit code to implement the if_acmpeq bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpeq(int bTarget) {
    asm.emitL (T0, 4, SP);
    asm.emitL (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(EQ, bTarget);
  }

  /**
   * Emit code to implement the if_acmpne bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_if_acmpne(int bTarget) {
    asm.emitL (T0, 4, SP);
    asm.emitL (T1, 0, SP);
    asm.emitCMP(T0, T1);    // sets CR0
    asm.emitCAL(SP, 8, SP); // completes 2 pops
    genCondBranch(NE, bTarget);
  }

  /**
   * Emit code to implement the ifnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnull(int bTarget) {
    asm.emitL   (T0,  0, SP);
    asm.emitLIL (T1,  0);
    asm.emitCMP (T0, T1);  
    asm.emitCAL (SP,  4, SP);
    genCondBranch(EQ, bTarget);
  }

  /**
   * Emit code to implement the ifnonnull bytecode
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_ifnonnull(int bTarget) {
    asm.emitL   (T0,  0, SP);
    asm.emitLIL (T1,  0);
    asm.emitCMP (T0, T1);  
    asm.emitCAL (SP,  4, SP);
    genCondBranch(NE, bTarget);
  }

  /**
   * Emit code to implement the goto and gotow bytecodes
   * @param bTarget target bytecode of the branch
   */
  protected final void emit_goto(int bTarget) {
    int mTarget = bytecodeMap[bTarget];
    asm.emitB(mTarget, bTarget);
  }

  /**
   * Emit code to implement the jsr and jsrw bytecode
   * @param bTarget target bytecode of the jsr
   */
  protected final void emit_jsr(int bTarget) {
    VM_ForwardReference fr = asm.emitForwardBL();
    fr.resolve(asm); // get PC into LR...
    asm.emitMFLR(T1);           // LR +  0
    asm.emitCAL (T1, 16, T1);   // LR +  4  (LR + 16 is ret address)
    asm.emitSTU (T1, -4, SP);   // LR +  8
    asm.emitBL(bytecodeMap[bTarget], bTarget); // LR + 12
  }

  /**
   * Emit code to implement the ret bytecode
   * @param index local variable containing the return address
   */
  protected final void emit_ret(int index) {
    int offset = localOffset(index);
    asm.emitL(T0, offset, FP);
    asm.emitMTLR(T0);
    asm.emitBLR ();
  }

  /**
   * Emit code to implement the tableswitch bytecode
   * @param defaultval bcIndex of the default target
   * @param low low value of switch
   * @param high high value of switch
   */
  protected final void emit_tableswitch(int defaultval, int low, int high) {
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    int n = high-low+1;       // n = number of normal cases (0..n-1)
    int firstCounter = edgeCounterIdx; // only used if options.EDGE_COUNTERS;

    asm.emitL   (T0, 0, SP);  // T0 is index
    asm.emitCAL (SP,  4, SP); // pop index from stack
    if (asm.fits(16, -low)) {
      asm.emitCAL(T0, -low, T0);
    } else {
      asm.emitLVAL(T1, low);
      asm.emitSF  (T0, T1, T0); 
    }
    asm.emitLVAL(T2, n);
    asm.emitCMPL(T0, T2);
    if (options.EDGE_COUNTERS) {
      edgeCounterIdx += n+1; // allocate n+1 counters
      // Load counter array for this method
      asm.emitLtoc (T2, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitLoffset(T2, T2, getEdgeCounterOffset());
      
      VM_ForwardReference fr = asm.emitForwardBC(LT); // jump around jump to default target
      incEdgeCounter(T2, S0, firstCounter + n);
      asm.emitB (mTarget, bTarget);
      fr.resolve(asm);
    } else {
      // conditionally jump to default target
      if (bTarget - SHORT_FORWARD_LIMIT < biStart) {
	asm.emitShortBC(GE, mTarget, bTarget);
      } else {
	asm.emitBC  (GE, mTarget, bTarget); 
      }
    }
    VM_ForwardReference fr1 = asm.emitForwardBL();
    for (int i=0; i<n; i++) {
      int offset = fetch4BytesSigned();
      bTarget = biStart + offset;
      mTarget = bytecodeMap[bTarget];
      asm.emitSwitchCase(i, mTarget, bTarget);
    }
    fr1.resolve(asm);
    asm.emitMFLR(T1);         // T1 is base of table
    asm.emitSLI (T0, T0,  2); // convert to bytes
    if (options.EDGE_COUNTERS) {
      incEdgeCounterIdx(T2, S0, firstCounter, T0);
    }
    asm.emitLX  (T0, T0, T1); // T0 is relative offset of desired case
    asm.emitA   (T1, T1, T0); // T1 is absolute address of desired case
    asm.emitMTLR(T1);
    asm.emitBLR ();
  }

  /**
   * Emit code to implement the lookupswitch bytecode
   * @param defaultval bcIndex of the default target
   * @param npairs number of pairs in the lookup switch
   */
  protected final void emit_lookupswitch(int defaultval, int npairs) {
    if (options.EDGE_COUNTERS) {
      // Load counter array for this method
      asm.emitLtoc (T2, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitLoffset(T2, T2, getEdgeCounterOffset());
    }

    asm.emitL   (T0,  0, SP); // T0 is key
    asm.emitCAL (SP,  4, SP); // pop key
    for (int i=0; i<npairs; i++) {
      int match   = fetch4BytesSigned();
      if (asm.fits(match, 16)) {
	asm.emitCMPI(T0, match);
      } else {
	asm.emitLVAL(T1, match);
	asm.emitCMP(T0, T1);
      }
      int offset  = fetch4BytesSigned();
      int bTarget = biStart + offset;
      int mTarget = bytecodeMap[bTarget];
      if (options.EDGE_COUNTERS) {
	// Flip conditions so we can jump over the increment of the taken counter.
	VM_ForwardReference fr = asm.emitForwardBC(NE);
	// Increment counter & jump to target
	incEdgeCounter(T2, S0, edgeCounterIdx++);
	asm.emitB(mTarget, bTarget);
	fr.resolve(asm);
      } else {
	if (bTarget - SHORT_FORWARD_LIMIT < biStart) {
	  asm.emitShortBC(EQ, mTarget, bTarget);
	} else {
	  asm.emitBC(EQ, mTarget, bTarget);
	}
      }
    }
    int bTarget = biStart + defaultval;
    int mTarget = bytecodeMap[bTarget];
    if (options.EDGE_COUNTERS) {
      incEdgeCounter(T2, S0, edgeCounterIdx++);
    }
    asm.emitB(mTarget, bTarget);
  }


  /*
   * returns (from function; NOT ret)
   */


  /**
   * Emit code to implement the ireturn bytecode
   */
  protected final void emit_ireturn() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    asm.emitL(T0, 0, SP);
    genEpilogue();
  }

  /**
   * Emit code to implement the lreturn bytecode
   */
  protected final void emit_lreturn() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    asm.emitL(T1, 4, SP); // hi register := lo word (which is at higher memory address)
    asm.emitL(T0, 0, SP); // lo register := hi word (which is at lower  memory address)
    genEpilogue();
  }

  /**
   * Emit code to implement the freturn bytecode
   */
  protected final void emit_freturn() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    asm.emitLFS(F0, 0, SP);
    genEpilogue();
  }

  /**
   * Emit code to implement the dreturn bytecode
   */
  protected final void emit_dreturn() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    asm.emitLFD(F0, 0, SP);
    genEpilogue();
  }

  /**
   * Emit code to implement the areturn bytecode
   */
  protected final void emit_areturn() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    asm.emitL(T0, 0, SP);
    genEpilogue();
  }

  /**
   * Emit code to implement the return bytecode
   */
  protected final void emit_return() {
    if (method.isSynchronized()) genSynchronizedMethodEpilogue();
    genEpilogue();
  }


  /*
   * field access
   */


  /**
   * Emit code to implement a dynamically linked getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getstatic(VM_Field fieldRef) {
    emitDynamicLinkingSequence(fieldRef); // leaves field offset in T2
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitLX (T0, T2, JTOC);
      asm.emitSTU (T0, -4, SP);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitLFDX (F0, T2, JTOC);
      asm.emitSTFDU (F0, -8, SP);
    }
  }

  /**
   * Emit code to implement a getstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();

    if (!(VM.BuildForStrongVolatileSemantics && fieldRef.isVolatile())) { // normal case
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitLtoc(T0, fieldOffset);
	asm.emitSTU (T0, -4, SP);
      } else { // field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLFDtoc(F0, fieldOffset, T0);
	asm.emitSTFDU (F0, -8, SP);
      }
    } else { // strong volatiles case
      // asm.emitISYNC(); // to make getstatic of a volatile field a read barrier uncomment this line (Note this is untested and the Opt compiler must also behave.))
      if (fieldRef.getSize() == 4) {  // see Appendix E of PowerPC Microprocessor Family: The Programming nvironments
	asm.emitLVAL  (T1, fieldOffset); // T1 = fieldOffset
	int label = asm.getMachineCodeIndex();
	asm.emitLWARX (T0, JTOC, T1);    // T0 = value
	asm.emitSTWCXr(T0, JTOC, T1);    // atomically rewrite value
	asm.emitBC    (NE, label);       // retry, if reservation lost
	asm.emitSTU   (T0, -4, SP);      // push value
      } else { // volatile field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL     (S0, VM_Entrypoints.processorLockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
	asm.emitLFDtoc(F0, fieldOffset, T0);
	asm.emitSTFDU (F0, -8, SP);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL     (S0, VM_Entrypoints.processorUnlockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putstatic(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutstaticBarrier(asm, spSaveAreaOffset, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(fieldRef);		      // leaves field offset in T2
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitL    (T0, 0, SP);
      asm.emitCAL  (SP, 4, SP);
      if (VM.BuildForConcurrentGC && ! fieldRef.getType().isPrimitiveType()) {
	//-#if RVM_WITH_CONCURRENT_GC
	VM_RCBarriers.compileDynamicPutstaticBarrier2(asm, spSaveAreaOffset, method, fieldRef);
	//-#endif
      } else {
	asm.emitSTX(T0, T2, JTOC);
      }
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitLFD    (F0, 0, SP );
      asm.emitCAL    (SP, 8, SP);
      asm.emitSTFDX(F0, T2, JTOC);
    }
  }

  /**
   * Emit code to implement a putstatic
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putstatic(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutstaticBarrier(asm, spSaveAreaOffset, fieldOffset);
    }
    if (!(VM.BuildForStrongVolatileSemantics && fieldRef.isVolatile())) { // normal case
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitL    (T0, 0, SP);
	asm.emitCAL  (SP, 4, SP);
	if (VM.BuildForConcurrentGC && ! fieldRef.getType().isPrimitiveType()) {
	  //-#if RVM_WITH_CONCURRENT_GC
	  VM_RCBarriers.compilePutstaticBarrier(asm, spSaveAreaOffset, fieldOffset, method, fieldRef);
	  //-#endif
	} else {
	  asm.emitSTtoc(T0, fieldOffset, T1);
	}
      } else { // field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLFD    (F0, 0, SP );
	asm.emitCAL    (SP, 8, SP);
	asm.emitSTFDtoc(F0, fieldOffset, T0);
      }
    } else {		// strong volatiles case
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitL     (T0, 0, SP);
	asm.emitCAL   (SP, 4, SP);
	if (VM.BuildForConcurrentGC && ! fieldRef.getType().isPrimitiveType()) {
	  //-#if RVM_WITH_CONCURRENT_GC 
	  VM_RCBarriers.compilePutstaticBarrier(asm, spSaveAreaOffset, fieldOffset, method, fieldRef);
	  //-#endif
	} else { // see Appendix E of PowerPC Microprocessor Family: The Programming Environments
	  asm.emitLVAL  (T1, fieldOffset); // T1 = fieldOffset
	  int label = asm.getMachineCodeIndex();
	  asm.emitLWARX (S0, JTOC, T1);    // S0 = old value
	  asm.emitSTWCXr(T0, JTOC, T1);    // atomically replace with new value (T0)
	  asm.emitBC    (NE, label);       // retry, if reservation lost
	}
	// asm.emitSYNC(); // to make putstatic of a volatile field a write barrier uncomment this line (Note this is untested and the Opt compiler must also behave.))
      } else { // volatile field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLtoc   (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL      (S0, VM_Entrypoints.processorLockMethod.getOffset(), T1);
	asm.emitMTLR   (S0);
	asm.emitCall   (spSaveAreaOffset);
	asm.emitLFD    (F0, 0, SP );
	asm.emitCAL    (SP, 8, SP);
	asm.emitSTFDtoc(F0, fieldOffset, T0);
	asm.emitLtoc   (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL      (S0, VM_Entrypoints.processorUnlockMethod.getOffset(), T1);
	asm.emitMTLR   (S0);
	asm.emitCall   (spSaveAreaOffset);
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_getfield(VM_Field fieldRef) {
    emitDynamicLinkingSequence(fieldRef);		      // leaves field offset in T2
    asm.emitL (T1, 0, SP); // T1 = object reference
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitLX(T0, T2, T1); // use field offset in T2 from emitDynamicLinkingSequence()
      asm.emitST(T0, 0, SP);
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitLFDX (F0, T2, T1); // use field offset in T2 from emitDynamicLinkingSequence()
      asm.emitSTFDU(F0, -4, SP);
    }
  }

  /**
   * Emit code to implement a getfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_getfield(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (!(VM.BuildForStrongVolatileSemantics && fieldRef.isVolatile())) { // normal case
      asm.emitL (T1, 0, SP); // T1 = object reference
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitL (T0, fieldOffset, T1);
	asm.emitST(T0, 0, SP);
      } else { // field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLFD  (F0, fieldOffset, T1);
	asm.emitSTFDU(F0, -4, SP);
      }
    } else {		// strong volatiles case
      if (fieldRef.getSize() == 4) { // field is one word
	// asm.emitISYNC(); // to make getfield of a volatile field a read barrier uncomment this line (Note this is untested and the Opt compiler must also behave.))
	asm.emitL  (T1, 0, SP);                // T1 = object to read
	// see Appendix E of PowerPC Microprocessor Family: The Programming Environments
	asm.emitCAL   (T1, fieldOffset, T1); // T1 = pointer to slot
	int label = asm.getMachineCodeIndex();
	asm.emitLWARX (T0, 0, T1);           // T0 = value
	asm.emitSTWCXr(T0, 0, T1);           // atomically replace with new value (T0)
	asm.emitBC    (NE, label);           // retry, if reservation lost
	asm.emitST    (T0, 0, SP);           // push value
      } else { // volatile field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL     (S0, VM_Entrypoints.processorLockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
	asm.emitL     (T1, 0, SP);
	asm.emitLFD   (F0, fieldOffset, T1);
	asm.emitSTFDU (F0, -4, SP);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);	
	asm.emitL     (S0, VM_Entrypoints.processorUnlockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
      }
    }
  }


  /**
   * Emit code to implement a dynamically linked putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_unresolved_putfield(VM_Field fieldRef) {
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compileUnresolvedPutfieldBarrier(asm, spSaveAreaOffset, fieldRef.getDictionaryId());
    }
    emitDynamicLinkingSequence(fieldRef);		      // leaves field offset in T2
    if (fieldRef.getSize() == 4) { // field is one word
      asm.emitL  (T1, 4, SP); // T1 = object reference
      asm.emitL  (T0, 0, SP); // T0 = value
      asm.emitCAL(SP, 8, SP);  
      if (!(VM.BuildForConcurrentGC && !fieldRef.getType().isPrimitiveType())) { // normal case
	asm.emitSTX(T0, T2, T1);
      } else {	// barrier needed for reference counting GC
	//-#if RVM_WITH_CONCURRENT_GC
	VM_RCBarriers.compileDynamicPutfieldBarrier2(asm, spSaveAreaOffset, method, fieldRef);
	//-#endif
      }
    } else { // field is two words (double or long)
      if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
      asm.emitLFD (F0,  0, SP); // F0 = doubleword value
      asm.emitL   (T1,  8, SP); // T1 = object reference
      asm.emitCAL (SP, 12, SP);
      asm.emitSTFDX(F0, T2, T1);
    }
  }

  /**
   * Emit code to implement a putfield
   * @param fieldRef the referenced field
   */
  protected final void emit_resolved_putfield(VM_Field fieldRef) {
    int fieldOffset = fieldRef.getOffset();
    if (VM_Collector.NEEDS_WRITE_BARRIER && !fieldRef.getType().isPrimitiveType()) {
      VM_Barriers.compilePutfieldBarrier(asm, spSaveAreaOffset, fieldOffset);
    }
    if (!(VM.BuildForStrongVolatileSemantics && fieldRef.isVolatile())) { // normal case
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitL  (T1, 4, SP); // T1 = object reference
	asm.emitL  (T0, 0, SP); // T0 = value
	asm.emitCAL(SP, 8, SP);  
	if (!(VM.BuildForConcurrentGC && !fieldRef.getType().isPrimitiveType())) { // normal case
	  asm.emitST (T0, fieldOffset, T1);
	} else {	// barrier needed for reference counting GC
	  //-#if RVM_WITH_CONCURRENT_GC
	  VM_RCBarriers.compilePutfieldBarrier(asm, spSaveAreaOffset, fieldOffset, method, fieldRef);
	  //-#endif
	}
      } else { // field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLFD (F0,  0, SP); // F0 = doubleword value
	asm.emitL   (T1,  8, SP); // T1 = object reference
	asm.emitCAL (SP, 12, SP);
	asm.emitSTFD(F0, fieldOffset, T1);
      }
    } else {		// strong volatiles case
      if (fieldRef.getSize() == 4) { // field is one word
	asm.emitL  (T1, 4, SP);                // T1 = object to update
	asm.emitL  (T0, 0, SP);                // T0 = new value
	asm.emitCAL(SP, 8, SP);                // pop stack
	if (VM.BuildForConcurrentGC && ! fieldRef.getType().isPrimitiveType()) {
	  //-#if RVM_WITH_CONCURRENT_GC // because VM_RCBarriers not available for non concurrent GC builds
	  VM_RCBarriers.compilePutfieldBarrier(asm, spSaveAreaOffset, fieldOffset, method, fieldRef);
	  //-#endif
	} else { // see Appendix E of PowerPC Microprocessor Family: The Programming Environments
	  asm.emitCAL   (T1, fieldOffset, T1); // T1 = pointer to slot
	  int label = asm.getMachineCodeIndex();
	  asm.emitLWARX (S0, 0, T1);           // S0 = old value
	  asm.emitSTWCXr(T0, 0, T1);           // atomically replace with new value (T0)
	  asm.emitBC    (NE, label);           // retry, if reservation lost
	}
	// asm.emitSYNC(); // to make putfield of a volatile field a write barrier uncomment this line (Note this is untested and the Opt compiler must also behave.))
      } else { // volatile field is two words (double or long)
	if (VM.VerifyAssertions) VM.assert(fieldRef.getSize() == 8);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL     (S0, VM_Entrypoints.processorLockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
	asm.emitLFD   (F0,  0, SP);
	asm.emitL     (T1,  8, SP);
	asm.emitCAL   (SP, 12, SP);
	asm.emitSTFD  (F0, fieldOffset, T1);
	asm.emitLtoc  (T0, VM_Entrypoints.doublewordVolatileMutexField.getOffset());
	VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0);
	asm.emitL     (S0, VM_Entrypoints.processorUnlockMethod.getOffset(), T1);
	asm.emitMTLR  (S0);
	asm.emitCall  (spSaveAreaOffset);
      }
    }
  }


  /*
   * method invocation
   */

  /**
   * Emit code to implement a dynamically linked invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokevirtual(VM_Method methodRef) {
    int methodRefParameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int objectOffset = (methodRefParameterWords << 2) - 4;
    asm.emitL   (T0, objectOffset,      SP); // load this
    VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0); // load TIB
    emitDynamicLinkingSequence(methodRef); // leaves method offset in T2
    asm.emitLX  (T2, T2, T1);  
    asm.emitMTLR(T2);
    genMoveParametersToRegisters(true, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(true, methodRef);
  }

  /**
   * Emit code to implement invokevirtual
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokevirtual(VM_Method methodRef) {
    int methodRefParameterWords = methodRef.getParameterWords() + 1; // +1 for "this" parameter
    int objectOffset = (methodRefParameterWords << 2) - 4;
    asm.emitL   (T0, objectOffset,      SP); // load this
    VM_ObjectModel.baselineEmitLoadTIB(asm, T1, T0); // load TIB
    int methodOffset = methodRef.getOffset();
    asm.emitL   (T2, methodOffset,   T1);
    asm.emitMTLR(T2);
    genMoveParametersToRegisters(true, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(true, methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokespecial
   * @param methodRef the referenced method
   * @param targetRef the method to invoke
   */
  protected final void emit_resolved_invokespecial(VM_Method methodRef, VM_Method target) {
    if (target.isObjectInitializer()) { // invoke via method's jtoc slot
      asm.emitLtoc(T0, target.getOffset());
    } else { // invoke via class's tib slot
      if (VM.VerifyAssertions) VM.assert(!target.isStatic());
      asm.emitLtoc(T0, target.getDeclaringClass().getTibOffset());
      asm.emitL   (T0, target.getOffset(), T0);
    }
    asm.emitMTLR(T0);
    genMoveParametersToRegisters(true, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, methodRef, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(true, methodRef);
  }

  /**
   * Emit code to implement invokespecial
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokespecial(VM_Method methodRef) {
    // must be a static method; if it was a super then declaring class _must_ be resolved
    emitDynamicLinkingSequence(methodRef); // leaves method offset in T2
    asm.emitLX    (T0, T2, JTOC); 
    asm.emitMTLR(T0);
    genMoveParametersToRegisters(true, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, methodRef, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(true, methodRef);
  }


  /**
   * Emit code to implement a dynamically linked invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_unresolved_invokestatic(VM_Method methodRef) {
    emitDynamicLinkingSequence(methodRef);		      // leaves method offset in T2
    asm.emitLX  (T0, T2, JTOC); // method offset left in T2 by emitDynamicLinkingSequence
    asm.emitMTLR(T0);
    genMoveParametersToRegisters(false, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(false, methodRef);
  }

  /**
   * Emit code to implement invokestatic
   * @param methodRef the referenced method
   */
  protected final void emit_resolved_invokestatic(VM_Method methodRef) {
    int methodOffset = methodRef.getOffset();
    asm.emitLtoc(T0, methodOffset);
    asm.emitMTLR(T0);
    genMoveParametersToRegisters(false, methodRef);
    //-#if RVM_WITH_SPECIALIZATION
    asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
    //-#else
    asm.emitCall(spSaveAreaOffset);
    //-#endif
    genPopParametersAndPushReturnValue(false, methodRef);
  }


  /**
   * Emit code to implement the invokeinterface bytecode
   * @param methodRef the referenced method
   */
  protected final void emit_invokeinterface(VM_Method methodRef, int count) {
    // (1) Emit dynamic type checking sequence if required to 
    // do so inline.
    if (VM.BuildForIMTInterfaceInvocation || 
	(VM.BuildForITableInterfaceInvocation && 
	 VM.DirectlyIndexedITables)) {
      VM_Method resolvedMethodRef = null;
      try {
	resolvedMethodRef = methodRef.resolveInterfaceMethod(false);
      } catch (VM_ResolutionException e) {
	// actually can't be thrown when we pass false for canLoad.
      }
      if (resolvedMethodRef == null) {
	// might be a ghost ref. Call uncommon case typechecking routine 
	// to deal with this
	asm.emitLtoc(T0, VM_Entrypoints.unresolvedInvokeinterfaceImplementsTestMethod.getOffset());
	asm.emitMTLR(T0);
	asm.emitLIL (T0, methodRef.getDictionaryId());  // dictionaryId of method we are trying to call
	asm.emitL   (T1, (count-1) << 2, SP);           // the "this" object
	VM_ObjectModel.baselineEmitLoadTIB(asm,T1,T1);
	asm.emitCall(spSaveAreaOffset);                 // throw exception, if link error
      } else {
	// normal case.  Not a ghost ref.
	asm.emitLtoc(T0, VM_Entrypoints.invokeinterfaceImplementsTestMethod.getOffset());
	asm.emitMTLR(T0);
	asm.emitLtoc(T0, methodRef.getDeclaringClass().getTibOffset()); // tib of the interface method
	asm.emitL   (T0, TIB_TYPE_INDEX << 2, T0);                   // type of the interface method
	asm.emitL   (T1, (count-1) << 2, SP);                        // the "this" object
	VM_ObjectModel.baselineEmitLoadTIB(asm,T1,T1);
	asm.emitCall(spSaveAreaOffset);                              // throw exception, if link error
      }
    }
    // (2) Emit interface invocation sequence.
    if (VM.BuildForIMTInterfaceInvocation) {
      int signatureId = VM_ClassLoader.
	findOrCreateInterfaceMethodSignatureId(methodRef.getName(), 
					       methodRef.getDescriptor());
      int offset      = VM_InterfaceInvocation.getIMTOffset(signatureId);
      genMoveParametersToRegisters(true, methodRef); // T0 is "this"
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
      if (VM.BuildForIndirectIMT) {
	// Load the IMT base into S0
	asm.emitL(S0, TIB_IMT_TIB_INDEX << 2, S0);
      }
      asm.emitL   (S0, offset, S0);                  // the method address
      asm.emitMTLR(S0);
      //-#if RVM_WITH_SPECIALIZATION
      asm.emitSpecializationCallWithHiddenParameter(spSaveAreaOffset, 
						    signatureId, method, 
						    biStart);
      //-#else
      asm.emitCallWithHiddenParameter(spSaveAreaOffset, signatureId);
      //-#endif
    } else if (VM.BuildForITableInterfaceInvocation && 
	       VM.DirectlyIndexedITables && 
	       methodRef.getDeclaringClass().isResolved()) {
      methodRef = methodRef.resolve();
      VM_Class I = methodRef.getDeclaringClass();
      genMoveParametersToRegisters(true, methodRef);        //T0 is "this"
      VM_ObjectModel.baselineEmitLoadTIB(asm,S0,T0);
      asm.emitL   (S0, TIB_ITABLES_TIB_INDEX << 2, S0); // iTables 
      asm.emitL   (S0, I.getInterfaceId() << 2, S0);  // iTable
      asm.emitL   (S0, VM_InterfaceInvocation.getITableIndex(I, methodRef) << 2, S0); // the method to call
      asm.emitMTLR(S0);
      //-#if RVM_WITH_SPECIALIZATION
      asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
      //-#else
      asm.emitCall(spSaveAreaOffset);
      //-#endif
    } else {
      VM_Class I = methodRef.getDeclaringClass();
      int itableIndex = -1;
      if (VM.BuildForITableInterfaceInvocation) {
	// get the index of the method in the Itable
	if (I.isLoaded()) {
	  itableIndex = VM_InterfaceInvocation.getITableIndex(I, methodRef);
	}
      }
      if (itableIndex == -1) {
	// itable index is not known at compile-time.
	// call "invokeInterface" to resolve object + method id into 
	// method address
	int methodRefId = methodRef.getDictionaryId();
	asm.emitLtoc(T0, VM_Entrypoints.invokeInterfaceMethod.getOffset());
	asm.emitMTLR(T0);
	asm.emitL   (T0, (count-1) << 2, SP); // object
	asm.emitLVAL(T1, methodRefId);        // method id
	asm.emitCall(spSaveAreaOffset);       // T0 := resolved method address
	asm.emitMTLR(T0);
	genMoveParametersToRegisters(true, methodRef);
	//-#if RVM_WITH_SPECIALIZATION
	asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
	//-#else
	asm.emitCall(spSaveAreaOffset);
	//-#endif
      } else {
	// itable index is known at compile-time.
	// call "findITable" to resolve object + interface id into 
	// itable address
	asm.emitLtoc(T0, VM_Entrypoints.findItableMethod.getOffset());
	asm.emitMTLR(T0);
	asm.emitL   (T0, (count-1) << 2, SP);     // object
	VM_ObjectModel.baselineEmitLoadTIB(asm,T0,T0);
	asm.emitLVAL(T1, I.getInterfaceId());    // interface id
	asm.emitCall(spSaveAreaOffset);   // T0 := itable reference
	asm.emitL   (T0, itableIndex << 2, T0); // T0 := the method to call
	asm.emitMTLR(T0);
	genMoveParametersToRegisters(true, methodRef);        //T0 is "this"
	//-#if RVM_WITH_SPECIALIZATION
	asm.emitSpecializationCall(spSaveAreaOffset, method, biStart);
	//-#else
	asm.emitCall(spSaveAreaOffset);
	//-#endif
      }
    }
    genPopParametersAndPushReturnValue(true, methodRef);
  }
 

  /*
   * other object model functions
   */ 


  /**
   * Emit code to allocate a scalar object
   * @param typeRef the VM_Class to instantiate
   */
  protected final void emit_resolved_new(VM_Class typeRef) {
    int instanceSize = typeRef.getInstanceSize();
    int tibOffset = typeRef.getTibOffset();
    asm.emitLtoc(T0, VM_Entrypoints.quickNewScalarMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitLVAL(T0, instanceSize);
    asm.emitLtoc(T1, tibOffset);
    asm.emitLVAL(T2, typeRef.hasFinalizer()?1:0);
    asm.emitCall(spSaveAreaOffset);
    asm.emitSTU (T0, -4, SP);
  }

  /**
   * Emit code to dynamically link and allocate a scalar object
   * @param the dictionaryId of the VM_Class to dynamically link & instantiate
   */
  protected final void emit_unresolved_new(int dictionaryId) {
    asm.emitLtoc(T0, VM_Entrypoints.newScalarMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitLVAL(T0, dictionaryId);
    asm.emitCall(spSaveAreaOffset);
    asm.emitSTU (T0, -4, SP);
  }

  /**
   * Emit code to allocate an array
   * @param array the VM_Array to instantiate
   */
  protected final void emit_newarray(VM_Array array) {
    int width      = array.getLogElementSize();
    int tibOffset  = array.getTibOffset();
    int headerSize = VM_ObjectModel.computeArrayHeaderSize(array);
    asm.emitLtoc (T0, VM_Entrypoints.quickNewArrayMethod.getOffset());
    asm.emitMTLR (T0);
    asm.emitL    (T0,  0, SP);                // T0 := number of elements
    asm.emitSLI  (T1, T0, width);             // T1 := number of bytes
    asm.emitCAL  (T1, headerSize, T1);        //    += header bytes
    asm.emitLtoc (T2, tibOffset);             // T2 := tib
    asm.emitCall(spSaveAreaOffset);
    asm.emitST   (T0, 0, SP);
  }

  /**
   * Emit code to allocate a multi-dimensional array
   * @param typeRef the VM_Array to instantiate
   * @param dimensions the number of dimensions
   * @param dictionaryId, the dictionaryId of typeRef
   */
  protected final void emit_multianewarray(VM_Array typeRef, int dimensions, int dictionaryId) {
    asm.emitLtoc(T0, VM_Entrypoints.newArrayArrayMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitLVAL(T0, dimensions);
    asm.emitLVAL(T1, dictionaryId);
    asm.emitSLI (T2, T0,  2); // number of bytes of array dimension args
    asm.emitA   (T2, SP, T2); // offset of word *above* first...
    asm.emitSF  (T2, FP, T2); // ...array dimension arg
    asm.emitCall(spSaveAreaOffset);
    asm.emitSTU (T0, (dimensions - 1)<<2, SP); // pop array dimension args, push return val
  }

  /**
   * Emit code to implement the arraylength bytecode
   */
  protected final void emit_arraylength() {
    asm.emitL (T0, 0, SP);
    asm.emitL (T1, VM_ObjectModel.getArrayLengthOffset(), T0);
    if (VM.BuildForRealtimeGC) {
      asm.emitCMPI(T1, 0);
      VM_ForwardReference fr = asm.emitForwardBC(GE);
      asm.emitNEG(T1, T1);
      fr.resolve(asm);
    }
    asm.emitST(T1, 0, SP);
  }

  /**
   * Emit code to implement the athrow bytecode
   */
  protected final void emit_athrow() {
    asm.emitLtoc(T0, VM_Entrypoints.athrowMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitL   (T0, 0, SP);
    asm.emitCall(spSaveAreaOffset);
  }

  /**
   * Emit code to implement the checkcast bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this checkcast
   */
  protected final void emit_checkcast(VM_Type typeRef, VM_Method target) {
    asm.emitLtoc(T0,  target.getOffset());
    asm.emitMTLR(T0);
    asm.emitL   (T0,  0, SP); // checkcast(obj, klass) consumes obj
    asm.emitLVAL(T1, typeRef.getTibOffset());
    asm.emitCall(spSaveAreaOffset);               // but obj remains on stack afterwords
  }

  /**
   * Emit code to implement the instanceof bytecode
   * @param typeRef the LHS type
   * @param target the method to invoke to implement this instanceof
   */
  protected final void emit_instanceof(VM_Type typeRef, VM_Method target) {
    asm.emitLtoc(T0,  target.getOffset());            
    asm.emitMTLR(T0);
    asm.emitL   (T0, 0, SP);
    asm.emitLVAL(T1, typeRef.getTibOffset());
    asm.emitCall(spSaveAreaOffset);
    asm.emitST  (T0, 0, SP);
  }

  /**
   * Emit code to implement the monitorenter bytecode
   */
  protected final void emit_monitorenter() {
    asm.emitL   (S0, VM_Entrypoints.lockMethod.getOffset(), JTOC);
    asm.emitMTLR(S0);
    asm.emitCall(spSaveAreaOffset);
    asm.emitCAL (SP, 4, SP);
  }

  /**
   * Emit code to implement the monitorexit bytecode
   */
  protected final void emit_monitorexit() {
    asm.emitL     (T0, 0, SP);
    asm.emitL   (S0, VM_Entrypoints.unlockMethod.getOffset(), JTOC);
    asm.emitMTLR(S0);
    asm.emitCall(spSaveAreaOffset);
    asm.emitCAL (SP, 4, SP);
  }

  
  // offset of i-th local variable with respect to FP
  private int localOffset (int i) {
    int offset = firstLocalOffset - (i << 2);
    if (VM.VerifyAssertions) VM.assert(offset < 0x8000);
    return offset;
  }

  private void emitDynamicLinkingSequence(VM_Field fieldRef) {
    emitDynamicLinkingSequence(fieldRef.getDictionaryId(), 
			       VM_Entrypoints.fieldOffsetsField.getOffset(),
			       VM_Entrypoints.resolveFieldMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(VM_Method methodRef) {
    emitDynamicLinkingSequence(methodRef.getDictionaryId(), 
			       VM_Entrypoints.methodOffsetsField.getOffset(),
			       VM_Entrypoints.resolveMethodMethod.getOffset());
  }

  private void emitDynamicLinkingSequence(int memberId,
					  int tableOffset,
					  int resolverOffset) {
    int label = asm.getMachineCodeIndex();

    // load offset table
    asm.emitLtoc (T2, tableOffset);
    asm.emitLoffset(T2, T2, memberId << 2);

    // test for non-zero offset and branch around call to resolver
    asm.emitCMPI (T2, 0);				      // T2 ?= 0, is field's class loaded?
    VM_ForwardReference fr1 = asm.emitForwardBC(NE);
    asm.emitLtoc (T0, resolverOffset);
    asm.emitMTLR (T0);
    asm.emitLVAL (T0, memberId);                              // dictionaryId of member we are resolving
    asm.emitCall (spSaveAreaOffset);			      // link; will throw exception if link error
    asm.emitB    (label);                       	      // go back and try again
    fr1.resolve(asm);
  }

  // Load/Store assist
  private void aloadSetup (int logSize) {
    asm.emitL   (T1,  4, SP);                    // T1 is array ref
    asm.emitL   (T0,  0, SP);                    // T0 is array index
    asm.emitL   (T2,  VM_ObjectModel.getArrayLengthOffset(), T1);  // T2 is array length
    if (logSize >= 0)
	emitSegmentedArrayAccess(asm, T1, T0, T2, logSize);
    if (VM.BuildForRealtimeGC || !options.ANNOTATIONS ||
	!method.queryAnnotationForBytecode(biStart, VM_Method.annotationBoundsCheck)) {
      asm.emitTLLE(T2, T0);      // trap if index < 0 or index >= length
    }
  }

  private void astoreSetup (int logSize) {
    asm.emitL   (T1,  8, SP);                    // T1 is array ref
    asm.emitL   (T0,  4, SP);                    // T0 is array index
    asm.emitL   (T2,  VM_ObjectModel.getArrayLengthOffset(), T1);  // T2 is array length
    asm.emitL   (T3,  0, SP);                    // T3 is value to store
    if (logSize >= 0)
	emitSegmentedArrayAccess(asm, T1, T0, T2, logSize);
    if ( VM.BuildForRealtimeGC || !options.ANNOTATIONS ||
	 !method.queryAnnotationForBytecode(biStart,
					VM_Method.annotationBoundsCheck)) {
      asm.emitTLLE(T2, T0);      // trap if index < 0 or index >= length
    }
  }

  private void astoreLong () {
    asm.emitL    (T1, 12, SP);                    // T1 is array ref
    asm.emitL    (T0,  8, SP);                    // T0 is array index
    asm.emitL    (T2,  VM_ObjectModel.getArrayLengthOffset(), T1);  // T2 is array length
    asm.emitLFD  (F0,  0, SP);                    // F0 is value to store
    emitSegmentedArrayAccess(asm, T1, T0, T2, 3);
    if ( VM.BuildForRealtimeGC || !options.ANNOTATIONS ||
	 !method.queryAnnotationForBytecode(biStart,
					VM_Method.annotationBoundsCheck)) {
      asm.emitTLLE(T2, T0);     // trap if index < 0 or index >= length
    }
    asm.emitSLI  (T0, T0,  3);  // convert double index to byte index
    asm.emitSTFDX(F0, T0, T1);  // store double value in array
    asm.emitCAL  (SP, 16, SP);  // complete 3 pops (1st is 2 words)
  }

  private static void emitSegmentedArrayAccess (VM_Assembler asm, int Tarr, int Tidx, int Tlen, int shift) {
    if (VM.BuildForRealtimeGC) {
    //-#if RVM_WITH_REALTIME_GC
      VM_SegmentedArray.emitSegmentedArrayAccess(asm, Tarr, Tidx, Tlen, shift);
    //-#endif
    }
  }  

  // Emit code to buy a stackframe, store incoming parameters, 
  // and acquire method synchronization lock.
  //
  private void genPrologue () {
    if (klass.isBridgeFromNative()) {
      VM_JNICompiler.generateGlueCodeForJNIMethod (asm, method);
    }

    // Generate trap if new frame would cross guard page.
    //
    if (isInterruptible) {
      asm.emitStackOverflowCheck(frameSize);                            // clobbers R0, S0
    }

    // Buy frame.
    //
    asm.emitSTU (FP, -frameSize, FP); // save old FP & buy new frame (trap if new frame below guard page) !!TODO: handle frames larger than 32k when addressing local variables, etc.
    
    // If this is a "dynamic bridge" method, then save all registers except GPR0, FPR0, JTOC, and FP.
    //
    if (klass.isDynamicBridge()) {
      int offset = frameSize;
      for (int i = LAST_NONVOLATILE_FPR; i >= FIRST_VOLATILE_FPR; --i)
         asm.emitSTFD (i, offset -= 8, FP);
      for (int i = LAST_NONVOLATILE_GPR; i >= FIRST_VOLATILE_GPR; --i)
         asm.emitST (i, offset -= 4, FP);
    }
    
    // Fill in frame header.
    //
    asm.emitLVAL(S0, compiledMethod.getId());
    asm.emitMFLR(0);
    asm.emitST  (S0, STACKFRAME_METHOD_ID_OFFSET, FP);                   // save compiled method id
    asm.emitST  (0, frameSize + STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP); // save LR !!TODO: handle discontiguous stacks when saving return address
    
    // Setup expression stack and locals.
    //
    asm.emitCAL (SP, emptyStackOffset, FP);                              // setup expression stack
    genMoveParametersToLocals();                                                   // move parameters to locals
   
    // Perform a thread switch if so requested.
    //
    genThreadSwitchTest(VM_Thread.PROLOGUE); //           (VM_BaselineExceptionDeliverer WONT release the lock (for synchronized methods) during prologue code)

    // Acquire method syncronization lock.  (VM_BaselineExceptionDeliverer will release the lock (for synchronized methods) after  prologue code)
    //
    if (method.isSynchronized()) 
      genSynchronizedMethodPrologue();

    // Mark start of code for which source lines exist (for jdp debugger breakpointing).
    //
    asm.emitSENTINAL(); 
  }

  // Emit code to acquire method synchronization lock.
  //
  private void genSynchronizedMethodPrologue() {
    if (method.isStatic()) { // put java.lang.Class object for VM_Type into T0
      if (VM.writingBootImage) {
	VM.deferClassObjectCreation(klass);
      } else {
	klass.getClassForType();
      }
      int tibOffset = klass.getTibOffset();
      asm.emitLtoc(T0, tibOffset);
      asm.emitL   (T0, 0, T0);
      asm.emitL   (T0, VM_Entrypoints.classForTypeField.getOffset(), T0); 
    } else { // first local is "this" pointer
      asm.emitL(T0, localOffset(0), FP);
    }
    asm.emitL     (S0, VM_Entrypoints.lockMethod.getOffset(), JTOC); // call out...
    asm.emitMTLR  (S0);                                  // ...of line lock
    asm.emitCall(spSaveAreaOffset);
    lockOffset = 4*(asm.getMachineCodeIndex() - 1); // after this instruction, the method has the monitor
  }

  // Emit code to release method synchronization lock.
  //
  private void genSynchronizedMethodEpilogue () {
    if (method.isStatic()) { // put java.lang.Class for VM_Type into T0
      int tibOffset = klass.getTibOffset();
      asm.emitLtoc(T0, tibOffset);
      asm.emitL   (T0, 0, T0);
      asm.emitL   (T0, VM_Entrypoints.classForTypeField.getOffset(), T0); 
    } else { // first local is "this" pointer
      asm.emitL(T0, localOffset(0), FP); //!!TODO: think about this - can anybody store into local 0 (ie. change the value of "this")?
    }
    asm.emitL   (S0, VM_Entrypoints.unlockMethod.getOffset(), JTOC);  // call out...
    asm.emitMTLR(S0);                                     // ...of line lock
    asm.emitCall(spSaveAreaOffset);
  }
    
  // Emit code to discard stackframe and return to caller.
  //
  private void genEpilogue () {
    if (klass.isDynamicBridge()) {// Restore non-volatile registers.
      // we never return from a DynamicBridge frame
      asm.emitTWI(-1);
    } else {
      if (frameSize <= 0x8000) {
	asm.emitCAL(FP, frameSize, FP); // discard current frame
      } else {
	asm.emitL(FP, 0, FP);           // discard current frame
      }
      asm.emitL   (S0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP); 
      asm.emitMTLR(S0);
      asm.emitBLR (); // branch always, through link register
    }
  }


  /**
   * Emit the code for a bytecode level conditional branch
   * @param cc the condition code to branch on
   * @param bTarget the target bytecode index
   */
  private void genCondBranch(int cc, int bTarget) {
    if (options.EDGE_COUNTERS) {
      // Allocate 2 counters, taken and not taken
      int entry = edgeCounterIdx;
      edgeCounterIdx += 2;

      // Load counter array for this method
      asm.emitLtoc (T0, VM_Entrypoints.edgeCountersField.getOffset());
      asm.emitLoffset(T0, T0, getEdgeCounterOffset());

      // Flip conditions so we can jump over the increment of the taken counter.
      VM_ForwardReference fr = asm.emitForwardBC(asm.flipCode(cc));

      // Increment taken counter & jump to target
      incEdgeCounter(T0, T1, entry+VM_EdgeCounts.TAKEN);
      asm.emitB(bytecodeMap[bTarget], bTarget);

      // Not taken
      fr.resolve(asm);
      incEdgeCounter(T0, T1, entry+VM_EdgeCounts.NOT_TAKEN);
    } else {
      if (bTarget - SHORT_FORWARD_LIMIT < biStart) {
	asm.emitShortBC(cc, bytecodeMap[bTarget], bTarget);
      } else {
	asm.emitBC(cc, bytecodeMap[bTarget], bTarget);
      }
    }
  }

  /**
   * increment an edge counter.  
   * @param counters register containing base of counter array
   * @param scratch scratch register
   * @param counterIdx index of counter to increment
   */
  private final void incEdgeCounter(int counters, int scratch, int counterIdx) {
    asm.emitL      (scratch, counterIdx<<2, counters);
    asm.emitCAL    (scratch, 1, scratch);
    asm.emitRLWINM (scratch, scratch, 0, 1, 31);
    asm.emitST     (scratch, counterIdx<<2, counters);
  }

  private final void incEdgeCounterIdx(int counters, int scratch, int base, int counterIdx) {
    asm.emitCAL     (counters, base<<2, counters);
    asm.emitLX      (scratch, counterIdx, counters);
    asm.emitCAL     (scratch, 1, scratch);
    asm.emitRLWINM  (scratch, scratch, 0, 1, 31);
    asm.emitSTX     (scratch, counterIdx, counters);
  }    

  /**
   * @param whereFrom is this thread switch from a PROLOGUE, BACKEDGE, or EPILOGUE?
   */
  private void genThreadSwitchTest (int whereFrom) {
    if (isInterruptible) {
      VM_ForwardReference fr;
      // alternate ways of setting the thread switch bit
      if (VM.BuildForDeterministicThreadSwitching) { // set THREAD_SWITCH_BIT every N method calls
	
	// Decrement counter
	asm.emitL  (T2, VM_Entrypoints.deterministicThreadSwitchCountField.getOffset(), PROCESSOR_REGISTER);
	asm.emitCAL(T2, -1, T2);  // decrement it
	asm.emitST (T2, VM_Entrypoints.deterministicThreadSwitchCountField.getOffset(), PROCESSOR_REGISTER);
        
	// If counter reaches zero, set threadswitch bit
	asm.emitCMPI(T2, 0);
	fr = asm.emitForwardBC(GT);
	asm.emitCRORC(THREAD_SWITCH_BIT, 0, 0); // set thread switch bit
      } else if (!VM.BuildForThreadSwitchUsingControlRegisterBit) {
	asm.emitL(S0, VM_Entrypoints.threadSwitchRequestedField.getOffset(), PROCESSOR_REGISTER);
	asm.emitCMPI(THREAD_SWITCH_REGISTER, S0, 0); // set THREAD_SWITCH_SWITCH_REGISTER, S0, 0); // set THREAD_SWITCH_BIT in CR
	fr = asm.emitBNTS(); // skip, unless THREAD_SWITCH_BIT in CR is set
      } else { // else rely on the timer interrupt to set the THREAD_SWITCH_BIT
	fr = asm.emitBNTS(); // skip, unless THREAD_SWITCH_BIT in CR is set
      }
      if (whereFrom == VM_Thread.PROLOGUE) {
	asm.emitL   (S0, VM_Entrypoints.threadSwitchFromPrologueMethod.getOffset(), JTOC);
      } else if (whereFrom == VM_Thread.BACKEDGE) {
	asm.emitL   (S0, VM_Entrypoints.threadSwitchFromBackedgeMethod.getOffset(), JTOC);
      } else { // EPILOGUE
	asm.emitL   (S0, VM_Entrypoints.threadSwitchFromEpilogueMethod.getOffset(), JTOC);
      }
      asm.emitMTLR(S0);
      asm.emitCall(spSaveAreaOffset);
      fr.resolve(asm);
    }
  }

  // parameter stuff //

  // store parameters from registers into local variables of current method.
  private void genMoveParametersToLocals () {
    // AIX computation will differ
    spillOffset = getFrameSize(method) + STACKFRAME_HEADER_SIZE;
    int gp = FIRST_VOLATILE_GPR;
    int fp = FIRST_VOLATILE_FPR;
    int localIndex = 0;
    if (!method.isStatic()) {
      if (gp > LAST_VOLATILE_GPR) genUnspillWord(localIndex++);
      else asm.emitST(gp++, localOffset(localIndex++), FP);
    }
    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++, localIndex++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
        if (gp > LAST_VOLATILE_GPR) genUnspillDoubleword(localIndex++);
	else {
	  asm.emitST(gp++, localOffset(localIndex + 1), FP); // lo mem := lo register (== hi word)
	  if (gp > LAST_VOLATILE_GPR) genUnspillWord(localIndex);
	  else asm.emitST(gp++, localOffset(localIndex), FP);// hi mem := hi register (== lo word)
	  localIndex += 1;
	}
      } else if (t.isFloatType()) {
        if (fp > LAST_VOLATILE_FPR) genUnspillWord(localIndex);
	else asm.emitSTFS(fp++, localOffset(localIndex), FP);
      } else if (t.isDoubleType()) {
        if (fp > LAST_VOLATILE_FPR) genUnspillDoubleword(localIndex++);
	else asm.emitSTFD(fp++, localOffset(localIndex++) - 4, FP);
      } else { // t is object, int, short, char, byte, or boolean
        if (gp > LAST_VOLATILE_GPR) genUnspillWord(localIndex);
	else asm.emitST(gp++, localOffset(localIndex), FP);
      }
    }
  }

  // load parameters into registers before calling method "m".
  private void genMoveParametersToRegisters (boolean hasImplicitThisArg, VM_Method m) {
    // AIX computation will differ
    spillOffset = STACKFRAME_HEADER_SIZE;
    int gp = FIRST_VOLATILE_GPR;
    int fp = FIRST_VOLATILE_FPR;
    int stackOffset = m.getParameterWords()<<2;
    if (hasImplicitThisArg) {
      if (gp > LAST_VOLATILE_GPR) genSpillWord(stackOffset);
      else asm.emitL(gp++, stackOffset, SP);
    }
    VM_Type [] types = m.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
	stackOffset -= 8;
        if (gp > LAST_VOLATILE_GPR) genSpillDoubleword(stackOffset);
	else {
	  asm.emitL(gp++, stackOffset,   SP);       // lo register := lo mem (== hi order word)
	  if (gp > LAST_VOLATILE_GPR) genSpillWord(stackOffset+4);
	  else asm.emitL(gp++, stackOffset+4, SP);  // hi register := hi mem (== lo order word)
	}
      } else if (t.isFloatType()) {
	stackOffset -= 4;
        if (fp > LAST_VOLATILE_FPR) genSpillWord(stackOffset);
	else asm.emitLFS(fp++, stackOffset, SP);
      } else if (t.isDoubleType()) {
	stackOffset -= 8;
        if (fp > LAST_VOLATILE_FPR) genSpillDoubleword(stackOffset);
	else asm.emitLFD(fp++, stackOffset, SP);
      } else { // t is object, int, short, char, byte, or boolean
	stackOffset -= 4;
        if (gp > LAST_VOLATILE_GPR) genSpillWord(stackOffset);
	else asm.emitL(gp++, stackOffset, SP);
      }
    }
    if (VM.VerifyAssertions) VM.assert(stackOffset == 0);
  }

  // push return value of method "m" from register to operand stack.
  private void genPopParametersAndPushReturnValue (boolean hasImplicitThisArg, VM_Method m) {
    VM_Type t = m.getReturnType();
    int parameterSize = 
      (m.getParameterWords() + (hasImplicitThisArg ? 1 : 0) ) << 2;
    if (t.isVoidType()) {
      if (0 < parameterSize) asm.emitCAL(SP, parameterSize, SP);
    } else if (t.isLongType()) {
      asm.emitST (FIRST_VOLATILE_GPR+1, parameterSize-4, SP); // hi mem := hi register (== lo word)
      asm.emitSTU(FIRST_VOLATILE_GPR,   parameterSize-8, SP); // lo mem := lo register (== hi word)
    } else if (t.isFloatType()) {
      asm.emitSTFSU(FIRST_VOLATILE_FPR, parameterSize-4, SP);
    } else if (t.isDoubleType()) {
      asm.emitSTFDU(FIRST_VOLATILE_FPR, parameterSize-8, SP);
    } else { // t is object, int, short, char, byte, or boolean
      asm.emitSTU(FIRST_VOLATILE_GPR, parameterSize-4, SP);
    }
  }

  private void genSpillWord (int stackOffset) {
     asm.emitL (0, stackOffset, SP);
     asm.emitST(0, spillOffset, FP);
     spillOffset += 4;
  }
     
  private void genSpillDoubleword (int stackOffset) {
     asm.emitLFD (0, stackOffset, SP);
     asm.emitSTFD(0, spillOffset, FP);
     spillOffset += 8;
  }
               
  private void genUnspillWord (int localIndex) {
     asm.emitL (0, spillOffset, FP);
     asm.emitST(0, localOffset(localIndex), FP);
     spillOffset += 4;
  }
                      
  private void genUnspillDoubleword (int localIndex) {
     asm.emitLFD (0, spillOffset, FP);
     asm.emitSTFD(0, localOffset(localIndex) - 4, FP);
     spillOffset += 8;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Vector;

/*
 * A block of machine code in the running virtual machine image.
 *
 * Machine code is an array of "instructions", declared formally as integers,
 * produced by VM_Compiler, typically by translating the bytecodes
 * of a VM_Method. The code entrypoint is the first word in the array.
 *
 * @author Bowen Alpern
 * @author Tony Cocchi 
 * @author Derek Lieber
 */
final class VM_MachineCode {

  /**
   * Get the instructions comprising this block of machine code.
   */ 
  INSTRUCTION[] getInstructions() {
    if (VM.VerifyAssertions) VM.assert(instructions != null); // must call "finish" first
    return instructions;
  }

  /** 
   * Get the bytecode-to-instruction map for this block of machine code.
   * @return an array co-indexed with bytecode array. Each entry is an offset
   * into the array of machine codes giving the first instruction that the
   * bytecode compiled to.
   * @see #getInstructions
   */
  public int[] getBytecodeMap() {
    return bytecode_2_machine;
  }

  /**
   * Finish generation of assembler code.
   */ 
  void finish () {
    if (VM.VerifyAssertions) VM.assert(instructions == null); // finish must only be called once

    int n = (next_bundle-1)*size+next;
    instructions = VM_RuntimeStructures.newInstructions(n);
    int k = 0;
    for (int i=0; i<next_bundle; i++){
      INSTRUCTION[] b = (INSTRUCTION[]) bundles.elementAt(i);
      int m = (i == next_bundle-1 ? next : size);
      for (int j=0; j<m; j++) {
        instructions[k++] = b[j];
      }
    }

    // synchronize icache with generated machine code that was written through dcache
    //
    if (VM.runningVM)
      VM_Memory.sync(VM_Magic.objectAsAddress(instructions), instructions.length << VM.LG_INSTRUCTION_WIDTH);

    // release work buffers
    //
    bundles = null;
    current_bundle = null;
  }

  void addInstruction (INSTRUCTION instr) {
    if (next < current_bundle.length) {
      current_bundle[next++] = instr;
    } else {
      current_bundle = new INSTRUCTION[size];
      bundles.addElement(current_bundle);
      next_bundle++;
      next = 0;
      current_bundle[next++] = instr;
    }
  }

  INSTRUCTION getInstruction (int k) {
    int i = k >> shift;
    int j = k & mask;
    INSTRUCTION[] b = (INSTRUCTION[]) bundles.elementAt(i);
    return b[j];
  }

  void putInstruction(int k, INSTRUCTION instr) {
    int i = k >> shift;
    int j = k & mask;
    INSTRUCTION[] b = (INSTRUCTION[]) bundles.elementAt(i);
    b[j] = instr;
  }

  void setBytecodeMap(int b2m[]) {
    bytecode_2_machine = b2m;
    return;
  }


  private int bytecode_2_machine[];  // See setBytecodeMap/getBytecodeMap

  /* Unfortunately, the number of instructions is not known in advance.
     This class implements a vector of instructions (ints).  It uses a
     vector -- bundles -- whose elements are each int[size] arrays of 
     instructions.  size is assumed to be a power of two.
   */

  private static final int  mask = 0xFF;
  private static final int  size = mask+1;
  private static final int shift = 8;

  private INSTRUCTION[] instructions;
  private Vector        bundles;
  private INSTRUCTION[] current_bundle;
  private int           next;
  private int           next_bundle;

  VM_MachineCode () {
    bundles = new Vector();
    current_bundle = new INSTRUCTION[size];
    bundles.addElement(current_bundle);
    next_bundle++;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *  Generate inline machine instructions for special methods that cannot be 
 *  implemented in java bytecodes. These instructions are generated whenever  
 *  we encounter an "invokestatic" bytecode that calls a method with a 
 *  signature of the form "static native VM_Magic.xxx(...)".
 *  23 Jan 1998 Derek Lieber
 * 
 *  NOTE: when adding a new "methodName" to "generate()", be sure to also 
 * consider how it affects the values on the stack and update 
 * "checkForActualCall()" accordingly.
 * If no call is actually generated, the map will reflect the status of the 
 * locals (including parameters) at the time of the call but nothing on the 
 * operand stack for the call site will be mapped.
 *  7 Jul 1998 Janice Shepherd
 *
 * @author Derek Lieber
 * @author Janice Sheperd
 */
class VM_MagicCompiler implements VM_BaselineConstants, 
				  VM_AssemblerConstants {

  // These constants do not really belong here, but since I am making this change
  // I might as well make it a little better.  All size in bytes.
  static final int SIZE_IP = 4;
  static final int SIZE_TOC = 4;
  static final int SIZE_ADDRESS = 4;
  static final int SIZE_INTEGER = 4;

  static final int DEBUG = 0;

  //-----------//
  // interface //
  //-----------//
   
  // Generate inline code sequence for specified method.
  // Taken:    compiler we're generating code with
  //           method whose name indicates semantics of code to be generated
  // Returned: nothing
  //
  static void
  generateInlineCode(VM_Compiler compiler, VM_Method methodToBeCalled) {
    VM_Atom      methodName       = methodToBeCalled.getName();
    VM_Assembler asm              = compiler.asm;
    int          spSaveAreaOffset = compiler.spSaveAreaOffset;
      
    if (methodName == VM_MagicNames.sysCall0) {
      generateSysCall1(asm, 0, false );
      generateSysCall2(asm, 0);
      generateSysCallRet_I(asm, 0);
    } else if (methodName == VM_MagicNames.sysCall1) {
      int valueOffset = generateSysCall1(asm, SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);          // load value
      generateSysCall2(asm, SIZE_INTEGER);
      generateSysCallRet_I(asm, SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCall2) {
      int valueOffset = generateSysCall1(asm, 2 * SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);		 		 // load value1
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 1, valueOffset,  SP);		 		 // load value2
      generateSysCall2(asm, 2 * SIZE_INTEGER);
      generateSysCallRet_I(asm, 2 * SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCall3) {
      int valueOffset = generateSysCall1(asm, 3 * SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);		 		 // load value1
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 1, valueOffset,  SP);		 		 // load value2
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 2, valueOffset,  SP);		 		 // load value3
      generateSysCall2(asm, 3 * SIZE_INTEGER);
      generateSysCallRet_I(asm, 3 * SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCall4) {
      int valueOffset = generateSysCall1(asm, 4 * SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);		 		 // load value1
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 1, valueOffset,  SP);		 		 // load value2
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 2, valueOffset,  SP);		 		 // load value3
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 3, valueOffset,  SP);		 		 // load value4
      generateSysCall2(asm, 4 * SIZE_INTEGER);
      generateSysCallRet_I(asm, 4 * SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCall_L_0) {
      generateSysCall1(asm, 0, false );
      generateSysCall2(asm, 0);
      generateSysCallRet_L(asm, 0);
    } else if (methodName == VM_MagicNames.sysCall_L_I) {
      int valueOffset = generateSysCall1(asm, SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);          // load value
      generateSysCall2(asm, SIZE_INTEGER);
      generateSysCallRet_L(asm, SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCallAD) {
      int valueOffset = generateSysCall1(asm, 3 * SIZE_INTEGER, false );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);		 		 // load value1
      valueOffset -= SIZE_INTEGER;
      asm.emitLFD(0, valueOffset,  SP);		 		 // load value2
      generateSysCall2(asm, 3 * SIZE_INTEGER);
      generateSysCallRet_I(asm, 3 * SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.sysCallSigWait) {
      int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
      int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
 
      asm.emitL   (T0, 0, SP);	// t0 := address of VM_Registers object
      asm.emitCAL (SP, 4, SP);	// pop address of VM_Registers object
      VM_ForwardReference fr1 = asm.emitForwardBL();
      fr1.resolve(asm);
      asm.emitMFLR(0);
      asm.emitST  (0, ipOffset, T0 ); // store ip into VM_Registers Object
      asm.emitL   (T0, gprsOffset, T0); // TO <- registers.gprs[]
      asm.emitST  (FP, FP*4, T0);  
      int valueOffset = generateSysCall1(asm, 2 * SIZE_INTEGER, true );
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3, valueOffset,  SP);		 		 // load value1
      valueOffset -= SIZE_INTEGER;
      asm.emitL(3 + 1, valueOffset,  SP);		 		 // load value2
      generateSysCall2(asm, 2 * SIZE_INTEGER);
      generateSysCallRet_I(asm, 2 * SIZE_INTEGER);
    } else if (methodName == VM_MagicNames.getFramePointer) {
      asm.emitSTU(FP, -4, SP); // push FP
    } else if (methodName == VM_MagicNames.getCallerFramePointer) {
      asm.emitL (T0, 0, SP);                               // pop  frame pointer of callee frame
      asm.emitL (T1, STACKFRAME_FRAME_POINTER_OFFSET, T0); // load frame pointer of caller frame
      asm.emitST(T1, 0, SP);                               // push frame pointer of caller frame
    } else if (methodName == VM_MagicNames.setCallerFramePointer) {
      asm.emitL  (T0, +4, SP); // fp
      asm.emitL  (T1,  0, SP); // value
      asm.emitST (T1,  STACKFRAME_FRAME_POINTER_OFFSET, T0); // *(address+SFPO) := value
      asm.emitCAL(SP,  8, SP); // pop address, pop value
    } else if (methodName == VM_MagicNames.getCompiledMethodID) {
      asm.emitL (T0, 0, SP);                           // pop  frame pointer of callee frame
      asm.emitL (T1, STACKFRAME_METHOD_ID_OFFSET, T0); // load frame pointer of caller frame
      asm.emitST(T1, 0, SP);                           // push frame pointer of caller frame
    } else if (methodName == VM_MagicNames.setCompiledMethodID) {
      asm.emitL  (T0, +4, SP); // fp
      asm.emitL  (T1,  0, SP); // value
      asm.emitST (T1,  STACKFRAME_METHOD_ID_OFFSET, T0); // *(address+SNIO) := value
      asm.emitCAL(SP,  8, SP); // pop address, pop value
    } else if (methodName == VM_MagicNames.getNextInstructionAddress) {
      asm.emitL (T0, 0, SP);                                  // pop  frame pointer of callee frame
      asm.emitL (T1, STACKFRAME_NEXT_INSTRUCTION_OFFSET, T0); // load frame pointer of caller frame
      asm.emitST(T1, 0, SP);                                  // push frame pointer of caller frame
    } else if (methodName == VM_MagicNames.setNextInstructionAddress) {
      asm.emitL  (T0, +4, SP); // fp
      asm.emitL  (T1,  0, SP); // value
      asm.emitST (T1,  STACKFRAME_NEXT_INSTRUCTION_OFFSET, T0); // *(address+SNIO) := value
      asm.emitCAL(SP,  8, SP); // pop address, pop value
    } else if (methodName == VM_MagicNames.getReturnAddress) {
      asm.emitL (T0, 0, SP);                                  // pop  frame pointer of callee frame
      asm.emitL (T1, STACKFRAME_FRAME_POINTER_OFFSET, T0);    // load frame pointer of caller frame
      asm.emitL (T2, STACKFRAME_NEXT_INSTRUCTION_OFFSET, T1); // load frame pointer of caller frame
      asm.emitST(T2, 0, SP);                                  // push frame pointer of caller frame
    } else if (methodName == VM_MagicNames.setReturnAddress) {
      asm.emitL  (T0, +4, SP); // fp
      asm.emitL  (T0, STACKFRAME_FRAME_POINTER_OFFSET, T0);    // load frame pointer of caller frame
      asm.emitL  (T1,  0, SP); // value
      asm.emitST (T1,  STACKFRAME_NEXT_INSTRUCTION_OFFSET, T0); // *(address+SNIO) := value
      asm.emitCAL(SP,  8, SP); // pop address, pop value
    } else if (methodName == VM_MagicNames.getTocPointer ||
	       methodName == VM_MagicNames.getJTOC) {
      asm.emitSTU(JTOC, -4, SP); // push JTOC
    } else if (methodName == VM_MagicNames.getThreadId) {
      asm.emitSTU(TI, -4, SP); // push TI
    } else if (methodName == VM_MagicNames.setThreadId) {
      asm.emitL  (TI, 0, SP); // TI := (shifted) thread index
      asm.emitCAL(SP, 4, SP); // pop threadid arg
    } else if (methodName == VM_MagicNames.getProcessorRegister) {
      asm.emitSTU(PROCESSOR_REGISTER, -4, SP);
    } else if (methodName == VM_MagicNames.setProcessorRegister) {
      asm.emitL  (PROCESSOR_REGISTER, 0, SP); // register := arg
      asm.emitCAL(SP, 4, SP);                 // pop arg
    } else if (methodName == VM_MagicNames.getTimeBase) {
      int label = asm.getMachineCodeIndex();
      asm.emitMFTBU(T0);                      // T0 := time base, upper
      asm.emitMFTB (T1);                      // T1 := time base, lower
      asm.emitMFTBU(T2);                      // T2 := time base, upper
      asm.emitCMP  (T0, T2);                  // T0 == T2?
      asm.emitBC   (NE, label);               // lower rolled over, try again
      asm.emitSTU  (T1, -4, SP);              // push low
      asm.emitSTU  (T0, -4, SP);              // push high
    } else if (methodName == VM_MagicNames.getTime) {
      asm.emitL  (T0, 0, SP); // t0 := address of VM_Processor object
      asm.emitCAL(SP, 4, SP); // pop arg
      asm.emitLtoc(S0, VM_Entrypoints.getTimeInstructionsField.getOffset());
      asm.emitMTLR(S0);
      asm.emitCall(spSaveAreaOffset);             // call out of line machine code
      asm.emitSTFDU (F0, -8, SP); // push return value
    } else if (methodName == VM_MagicNames.invokeMain) {
      asm.emitL   (T0, 0, SP); // t0 := ip
      asm.emitMTLR(T0);
      asm.emitCAL (SP, 4, SP); // pop ip
      asm.emitL   (T0, 0, SP); // t0 := parameter
      asm.emitCall(spSaveAreaOffset);          // call
      asm.emitCAL (SP, 4, SP); // pop parameter
    } else if (methodName == VM_MagicNames.invokeClassInitializer) {
      asm.emitL   (T0, 0, SP); // t0 := address to be called
      asm.emitCAL (SP, 4, SP); // pop ip
      asm.emitMTLR(T0);
      asm.emitCall(spSaveAreaOffset);          // call
    } else if (methodName == VM_MagicNames.invokeMethodReturningVoid) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
    } else if (methodName == VM_MagicNames.invokeMethodReturningInt) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
      asm.emitSTU(T0, -4, SP);       // push result
    } else if (methodName == VM_MagicNames.invokeMethodReturningLong) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
      asm.emitSTU(T1, -4, SP);       // push result
      asm.emitSTU(T0, -4, SP);       // push result
    } else if (methodName == VM_MagicNames.invokeMethodReturningFloat) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
      asm.emitSTFSU(F0, -4, SP);     // push result
    } else if (methodName == VM_MagicNames.invokeMethodReturningDouble) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
      asm.emitSTFDU(F0, -8, SP);     // push result
    } else if (methodName == VM_MagicNames.invokeMethodReturningObject) {
      generateMethodInvocation(asm, spSaveAreaOffset); // call method
      asm.emitSTU(T0, -4, SP);       // push result
    } else if (methodName == VM_MagicNames.getIntAtOffset ||
	       methodName == VM_MagicNames.getObjectAtOffset ||
	       methodName == VM_MagicNames.getObjectArrayAtOffset) {
      asm.emitL  (T0, +4, SP); // pop object
      asm.emitL  (T1,  0, SP); // pop offset
      asm.emitLX (T0, T1, T0); // *(object+offset)
      asm.emitSTU (T0, 4, SP); // push *(object+offset)
    } else if (methodName == VM_MagicNames.getByteAtOffset) {
      asm.emitL   (T0, +4, SP);   // pop object
      asm.emitL   (T1,  0, SP);   // pop offset
      asm.emitLBZX(T0, T1, T0);   // load byte with zero extension.
      asm.emitSTU (T0, 4, SP);    // push *(object+offset) 
    } else if (methodName == VM_MagicNames.setIntAtOffset ||
	       methodName == VM_MagicNames.setObjectAtOffset) {
      asm.emitL  (T0, +8, SP); // pop object
      asm.emitL  (T1, +4, SP); // pop offset
      asm.emitL  (T2,  0, SP); // pop newvalue
      asm.emitSTX(T2, T1, T0); // *(object+offset) = newvalue
      asm.emitCAL(SP, 12, SP); // drop all args
    } else if (methodName == VM_MagicNames.setByteAtOffset) {
      asm.emitL  (T0, +8, SP); // pop object
      asm.emitL  (T1, +4, SP); // pop offset
      asm.emitL  (T2,  0, SP); // pop newvalue
      asm.emitSTBX(T2, T1, T0); // *(object+offset) = newvalue
      asm.emitCAL(SP, 12, SP); // drop all args
    } else if (methodName == VM_MagicNames.getLongAtOffset) {
      asm.emitL  (T1, +4, SP); // pop object
      asm.emitL  (T2,  0, SP); // pop offset
      asm.emitLX (T0, T1, T2); // *(object+offset)
      asm.emitCAL(T2, +4, T2); // offset += 4
      asm.emitLX (T1, T1, T2); // *(object+offset+4)
      asm.emitST (T0,  0, SP); // *sp := *(object+offset)
      asm.emitST (T1, +4, SP); // *sp+4 := *(object+offset+4)
    } else if (methodName == VM_MagicNames.setLongAtOffset) {
      asm.emitL  (T0,+12, SP); // pop object
      asm.emitL  (T1, +8, SP); // pop offset
      asm.emitL  (T2,  0, SP); // pop newvalue low 
      asm.emitSTX(T2, T1, T0); // *(object+offset) = newvalue low
      asm.emitCAL(T1, +4, T1); // offset += 4
      asm.emitL  (T2, +4, SP); // pop newvalue high 
      asm.emitSTX(T2, T1, T0); // *(object+offset) = newvalue high
      asm.emitCAL(SP, 16, SP); // drop all args
    } else if (methodName == VM_MagicNames.getMemoryWord ||
	       methodName == VM_MagicNames.getMemoryAddress) {
      asm.emitL  (T0,  0, SP); // address
      asm.emitL  (T0,  0, T0); // *address
      asm.emitST (T0,  0, SP); // *sp := *address
    } else if (methodName == VM_MagicNames.setMemoryWord ||
	       methodName == VM_MagicNames.setMemoryAddress) {
      asm.emitL  (T0,  4, SP); // address
      asm.emitL  (T1,  0, SP); // value
      asm.emitST (T1,  0, T0); // *address := value
      asm.emitCAL(SP,  8, SP); // pop address, pop value
    } else if (methodName == VM_MagicNames.prepare) {
      asm.emitL    (T0,  4, SP); // pop object
      asm.emitL    (T1,  0, SP); // pop offset
      if (VM.BuildForSingleVirtualProcessor) {
	asm.emitLX (T0, T1, T0); // *(object+offset)
      } else {
	asm.emitLWARX(T0,  T1, T0); // *(object+offset), setting processor's reservation address
      }
      asm.emitSTU (T0,  4, SP); // push *(object+offset)
    } else if (methodName == VM_MagicNames.attempt) {
      asm.emitL     (T0, 12, SP);  // pop object
      asm.emitL     (T1,  8, SP);  // pop offset
      asm.emitL     (T2,  0, SP);  // pop newValue (ignore oldValue)
      if (VM.BuildForSingleVirtualProcessor) {
	asm.emitSTX   (T2,  T1, T0); // store new value (on one VP this succeeds by definition)
	asm.emitCAL   (T0,  1, 0);   // T0 := true
	asm.emitSTU   (T0,  12, SP);  // push success of conditional store
      } else {
	asm.emitSTWCXr(T2,  T1, T0); // store new value and set CR0
	asm.emitCAL   (T0,  0, 0);  // T0 := false
	VM_ForwardReference fr = asm.emitForwardBC(NE); // skip, if store failed
	asm.emitCAL   (T0,  1, 0);   // T0 := true
	fr.resolve(asm);
	asm.emitSTU   (T0,  12, SP);  // push success of conditional store
      }
    } else if (methodName == VM_MagicNames.setThreadSwitchBit) {
      asm.emitCRORC(THREAD_SWITCH_BIT, 0, 0);
    } else if (methodName == VM_MagicNames.clearThreadSwitchBit) {
      asm.emitCRANDC(THREAD_SWITCH_BIT, 0, 0);
    } else if (methodName == VM_MagicNames.saveThreadState) {
      asm.emitL   (T0, 0, SP); // T0 := address of VM_Registers object
      asm.emitLtoc(S0, VM_Entrypoints.saveThreadStateInstructionsField.getOffset());
      asm.emitMTLR(S0);
      asm.emitCall(spSaveAreaOffset); // call out of line machine code
      asm.emitCAL(SP, 4, SP);  // pop arg
    } else if (methodName == VM_MagicNames.threadSwitch) {
      asm.emitL(T0, 4, SP); // T0 := address of previous VM_Thread object
      asm.emitL(T1, 0, SP); // T1 := address of VM_Registers of new thread
      asm.emitLtoc(S0, VM_Entrypoints.threadSwitchInstructionsField.getOffset());
      asm.emitMTLR(S0);
      asm.emitCall(spSaveAreaOffset);
      asm.emitCAL(SP, 8, SP);  // pop two args
    } else if (methodName == VM_MagicNames.restoreHardwareExceptionState) {
      asm.emitL(T0, 0, SP); // T0 := address of VM_Registers object
      asm.emitLtoc(S0, VM_Entrypoints.restoreHardwareExceptionStateInstructionsField.getOffset());
      asm.emitMTLR(S0);
      asm.emitBLR(); // branch to out of line machine code (does not return)
    } else if (methodName == VM_MagicNames.returnToNewStack) {
      asm.emitL   (FP, 0, SP);                                  // FP := new stackframe
      asm.emitL   (S0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP); // fetch...
      asm.emitMTLR(S0);                                         // ...return address
      asm.emitBLR ();                                           // return to caller
    } else if (methodName == VM_MagicNames.dynamicBridgeTo) {
      if (VM.VerifyAssertions) VM.assert(compiler.klass.isDynamicBridge());
         
      // fetch parameter (address to branch to) into CT register
      //
      asm.emitL(T0, 0, SP);
      asm.emitMTCTR(T0);

      // restore volatile and non-volatile registers
      // (note that these are only saved for "dynamic bridge" methods)
      //
      int offset = compiler.frameSize;

      // restore non-volatile and volatile fprs
      for (int i = LAST_NONVOLATILE_FPR; i >= FIRST_VOLATILE_FPR; --i)
	asm.emitLFD(i, offset -= 8, FP);
      
      // restore non-volatile gprs
      for (int i = LAST_NONVOLATILE_GPR; i >= FIRST_NONVOLATILE_GPR; --i)
	asm.emitL(i, offset -= 4, FP);
            
      // skip saved thread-id, processor, and scratch registers
      offset -= (FIRST_NONVOLATILE_GPR - LAST_VOLATILE_GPR - 1) * 4;
         
      // restore volatile gprs
      for (int i = LAST_VOLATILE_GPR; i >= FIRST_VOLATILE_GPR; --i)
	asm.emitL(i, offset -= 4, FP);
          
      // pop stackframe
      asm.emitL(FP, 0, FP);
         
      // restore link register
      asm.emitL(S0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP);
      asm.emitMTLR(S0);

      asm.emitBCTR(); // branch always, through count register
    } else if (methodName == VM_MagicNames.objectAsAddress         ||
	       methodName == VM_MagicNames.addressAsByteArray      ||
	       methodName == VM_MagicNames.addressAsIntArray       ||
	       methodName == VM_MagicNames.addressAsObject         ||
	       methodName == VM_MagicNames.addressAsObjectArray    ||
	       methodName == VM_MagicNames.addressAsType           ||
	       methodName == VM_MagicNames.objectAsType            ||
	       methodName == VM_MagicNames.objectAsByteArray       ||
	       methodName == VM_MagicNames.objectAsShortArray      ||
	       methodName == VM_MagicNames.objectAsIntArray        ||
	       methodName == VM_MagicNames.addressAsThread         ||
	       methodName == VM_MagicNames.objectAsThread          ||
	       methodName == VM_MagicNames.objectAsProcessor       ||
	       //-#if RVM_WITH_JIKESRVM_MEMORY_MANAGERS
	       methodName == VM_MagicNames.addressAsBlockControl   ||
	       methodName == VM_MagicNames.addressAsSizeControl    ||
	       methodName == VM_MagicNames.addressAsSizeControlArray   ||
	       //-#if RVM_WITH_CONCURRENT_GC
	       methodName == VM_MagicNames.threadAsRCCollectorThread ||
	       //-#endif
	       //-#endif
	       methodName == VM_MagicNames.threadAsCollectorThread ||
	       methodName == VM_MagicNames.addressAsRegisters      ||
	       methodName == VM_MagicNames.addressAsStack          ||
	       methodName == VM_MagicNames.floatAsIntBits          ||
	       methodName == VM_MagicNames.intBitsAsFloat          ||
	       methodName == VM_MagicNames.doubleAsLongBits        ||
	       methodName == VM_MagicNames.longBitsAsDouble) {
      // no-op (a type change, not a representation change)
    } else if (methodName == VM_MagicNames.getObjectType) {
      generateGetObjectType(asm);
    } else if (methodName == VM_MagicNames.getArrayLength) {
      generateGetArrayLength(asm);
    } else if (methodName == VM_MagicNames.sync) {
      asm.emitSYNC();
    } else if (methodName == VM_MagicNames.isync) {
      asm.emitISYNC();
    } else if (methodName == VM_MagicNames.dcbst) {
      asm.emitL(T0, 0, SP);    // address
      asm.emitCAL(SP, 4, SP);  // pop
      asm.emitDCBST(0, T0);
    } else if (methodName == VM_MagicNames.icbi) {
      asm.emitL(T0, 0, SP);    // address
      asm.emitCAL(SP, 4, SP);  // pop
      asm.emitICBI(0, T0);
    } else if (methodName == VM_MagicNames.pragmaNoOptCompile) {
      // meaningless;  for the optimizing compiler forces baseline compilation
    } else if (methodName == VM_MagicNames.addressFromInt) {
      // no-op
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.fromInt as no-op");
    } else if (methodName == VM_MagicNames.addressToInt) {
      // no-op
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.toInt as no-op");
    } else if (methodName == VM_MagicNames.addressAdd) {
      // same as an integer add
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.add as integer add");
      asm.emitL  (T0,  0, SP);
      asm.emitL  (T1,  4, SP);
      asm.emitA  (T2, T1, T0);
      asm.emitSTU(T2,  4, SP);
    } else if (methodName == VM_MagicNames.addressSub ||
	       methodName == VM_MagicNames.addressDiff) {
      // same as an integer subtraction
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.sub/diff as integer sub");
      asm.emitL  (T0,  0, SP);
      asm.emitL  (T1,  4, SP);
      asm.emitSF (T2, T0, T1);
      asm.emitSTU(T2,  4, SP);
    } else if (methodName == VM_MagicNames.addressLT) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.LT as unsigned comparison");
      generateAddrComparison(asm, LT);
    } else if (methodName == VM_MagicNames.addressLE) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.LE as unsigned comparison");
      generateAddrComparison(asm, LE);
    } else if (methodName == VM_MagicNames.addressEQ) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.EQ as unsigned comparison");
      generateAddrComparison(asm, EQ);
    } else if (methodName == VM_MagicNames.addressNE) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.NE as unsigned comparison");
      generateAddrComparison(asm, NE);
    } else if (methodName == VM_MagicNames.addressGT) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.GT as unsigned comparison");
      generateAddrComparison(asm, GT);
    } else if (methodName == VM_MagicNames.addressGE) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.GE as unsigned comparison");
      generateAddrComparison(asm, GE);
    } else if (methodName == VM_MagicNames.addressIsZero) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.isZero as unsigned comparison");
      asm.emitLIL (T0,  0);
      asm.emitSTU (T0, -4, SP);
      generateAddrComparison(asm, EQ);
    } else if (methodName == VM_MagicNames.addressIsMax) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.isMax as unsigned comparison");
      asm.emitLIL (T0, -1);
      asm.emitSTU (T0, -4, SP);
      generateAddrComparison(asm, EQ);
    } else if (methodName == VM_MagicNames.addressZero) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.zero as 0");
      asm.emitLIL (T0,  0);
      asm.emitSTU (T0, -4, SP);
    } else if (methodName == VM_MagicNames.addressMax) {
      // unsigned comparison generating a boolean
      if (DEBUG >= 1) VM.sysWriteln("VM_AddressCompiler.java: Translating VM_Address.max as -1");
      asm.emitLIL (T0, -1);
      asm.emitSTU (T0, -4, SP);
    } else {
      VM.sysWrite("VM_MagicCompiler.java: no magic for " + methodToBeCalled + "\n");
      if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
    }
  }

  private static void generateAddrComparison(VM_Assembler asm, int cc) {
    asm.emitL  (T1,  0, SP);
    asm.emitL  (T0,  4, SP);
    asm.emitLIL(T2,  1);
    asm.emitCMPL(T0, T1);    // unsigned comparison
    VM_ForwardReference fr = asm.emitForwardBC(cc);
    asm.emitLIL(T2,  0);
    fr.resolve(asm);
    asm.emitSTU(T2,  4, SP);
  }


  // Indicate if specified VM_Magic method causes a frame to be created on the runtime stack.
  // Taken:   VM_Method of the magic method being called
  // Returned: true if method causes a stackframe to be created
  //
  public static boolean checkForActualCall(VM_Method methodToBeCalled) {
    VM_Atom methodName = methodToBeCalled.getName();
    return methodName == VM_MagicNames.invokeMain                  ||
      methodName == VM_MagicNames.invokeClassInitializer      ||
      methodName == VM_MagicNames.invokeMethodReturningVoid   ||
      methodName == VM_MagicNames.invokeMethodReturningInt    ||
      methodName == VM_MagicNames.invokeMethodReturningLong   ||
      methodName == VM_MagicNames.invokeMethodReturningFloat  ||
      methodName == VM_MagicNames.invokeMethodReturningDouble ||
      methodName == VM_MagicNames.invokeMethodReturningObject;
  }


  //----------------//
  // implementation //
  //----------------//

  // Generate code to invoke arbitrary method with arbitrary parameters/return value.
  //
  // We generate inline code that calls "VM_OutOfLineMachineCode.reflectiveMethodInvokerInstructions"
  // which, at runtime, will create a new stackframe with an appropriately sized spill area
  // (but no register save area, locals, or operand stack), load up the specified
  // fpr's and gpr's, call the specified method, pop the stackframe, and return a value.
  //
  private static void generateMethodInvocation(VM_Assembler asm, 
					       int spSaveAreaOffset) {
    // On entry the stack looks like this:
    //
    //                       hi-mem
    //            +-------------------------+    \
    //            |         code[]          |     |
    //            +-------------------------+     |
    //            |         gprs[]          |     |
    //            +-------------------------+     |- java operand stack
    //            |         fprs[]          |     |
    //            +-------------------------+     |
    //    SP ->   |         spills[]        |     |
    //            +-------------------------+    /

    // fetch parameters and generate call to method invoker
    //
    asm.emitLtoc (S0, VM_Entrypoints.reflectiveMethodInvokerInstructionsField.getOffset());
    asm.emitL    (T0, 12, SP);        // t0 := code
    asm.emitMTLR (S0);
    asm.emitL    (T1,  8, SP);        // t1 := gprs
    asm.emitL    (T2,  4, SP);        // t2 := fprs
    asm.emitL    (T3,  0, SP);        // t3 := spills
    asm.emitCall(spSaveAreaOffset);
    asm.emitCAL  (SP,  16, SP);       // pop parameters
  }

  // Generate code for "VM_Type VM_Magic.getObjectType(Object object)".
  //
  static void generateGetObjectType(VM_Assembler asm) {
    // On entry the stack looks like this:
    //
    //                     hi-mem
    //            +-------------------------+    \
    //    SP ->   |    (Object object)      |     |- java operand stack
    //            +-------------------------+    /

    asm.emitL (T0,  0, SP);                   // get object pointer
    VM_ObjectModel.baselineEmitLoadTIB(asm,T0,T0);
    asm.emitL (T0,  TIB_TYPE_INDEX << 2, T0); // get "type" field from type information block
    asm.emitST(T0,  0, SP);                   // *sp := type
  }

  // Generate code for "int VM_Magic.getArrayLength(Object object)".
  //
  static void generateGetArrayLength(VM_Assembler asm) {
    // On entry the stack looks like this:
    //
    //                     hi-mem
    //            +-------------------------+    \
    //    SP ->   |    (Object object)      |     |- java operand stack
    //            +-------------------------+    /

    asm.emitL (T0,  0, SP);                   // get object pointer
    asm.emitL (T0,  VM_ObjectModel.getArrayLengthOffset(), T0); // get array length field
    asm.emitST(T0,  0, SP);                   // *sp := length
  }

  // Generate code for "int VM_Magic.sysCallN(int ip, int toc, int val0, int val1, ..., valN-1)".
  // Taken: number of bytes in parameters (not including JTOC, IP)
  //
  static int generateSysCall1(VM_Assembler asm, 
			      int rawParametersSize, 
			      boolean check_stack) {
    // Make sure stack has enough space to run the C function and any calls it makes.
    // We must do this prior to calling the function because there's no way to expand our stack
    // if the C function causes a guard page trap: the C stackframe cannot be relocated and
    // its contents cannot be scanned for object references.
    //
    if ( check_stack )
      asm.emitStackOverflowCheck(STACK_SIZE_NATIVE);
     
    // Create a linkage area that's compatible with RS6000 "C" calling conventions.
    // Just before the call, the stack looks like this:
    //
    //                     hi-mem
    //            +-------------------------+  . . . . . . . .
    //            |          ...            |                  \
	//            +-------------------------+                   |
	//            |          ...            |    \              |
	//            +-------------------------+     |             |
	//            |       (int ip)          |     |             |
	//            +-------------------------+     |             |
	//            |       (int toc)         |     |             |
	//            +-------------------------+     |             |
	//            |       (int val0)        |     |  java       |- java
	//            +-------------------------+     |-  operand   |   stack
	//            |       (int val1)        |     |    stack    |    frame
	//            +-------------------------+     |             |
	//            |          ...            |     |             |
	//            +-------------------------+     |             |
	//  SP ->     |      (int valN-1)       |     |             |
	//            +-------------------------+    /              |
	//            |          ...            |                   |
	//            +-------------------------+                   |
	//            |                         | <-- spot for this frame's callee's return address
	//            +-------------------------+                   |
	//            |          MI             | <-- this frame's method id
	//            +-------------------------+                   |
	//            |       saved FP          | <-- this frame's caller's frame
	//            +-------------------------+  . . . . . . . . /
	//            |      saved JTOC         |
	//            +-------------------------+
	//            |      saved SP           |
	//            +-------------------------+  . . . . . . . . . . . . . .
	//            | parameterN-1 save area  | +  \                         \
	//            +-------------------------+     |                         |
	//            |          ...            | +   |                         |
	//            +-------------------------+     |- register save area for |
	//            |  parameter1 save area   | +   |    use by callee        |
	//            +-------------------------+     |                         |
	//            |  parameter0 save area   | +  /                          |  rs6000
	//            +-------------------------+                               |-  linkage
	//        +20 |       TOC save area     | +                             |    area
	//            +-------------------------+                               |
	//        +16 |       (reserved)        | -    + == used by callee      |
	//            +-------------------------+      - == ignored by callee   |
	//        +12 |       (reserved)        | -                             |
	//            +-------------------------+                               |
	//         +8 |       LR save area      | +                             |
	//            +-------------------------+                               |
	//         +4 |       CR save area      | +                             |
	//            +-------------------------+                               |
	//  FP ->  +0 |       (backlink)        | -                             |
	//            +-------------------------+  . . . . . . . . . . . . . . /
	//
	// Notes:
	// 1. C parameters are passed in registers R3...R10
	// 2. space is also reserved on the stack for use by callee
	//    as parameter save area
	// 3. parameters are pushed on the java operand stack left to right
	//    java conventions) but if callee saves them, they will
	//    appear in the parameter save area right to left (C conventions)
	//
	// generateSysCall1  set ups the call
	// generateSysCall2  branches and cleans up
	// generateSysCallRet_<type> fix stack pushes return values
 
	int parameterAreaSize = rawParametersSize + SIZE_IP + SIZE_TOC;
	int ipOffset        = parameterAreaSize - SIZE_IP;		 // offset of ip parameter from SP
	int tocOffset       = ipOffset          - SIZE_TOC;		 // offset of toc parameter from SP
	int endValueOffset  = tocOffset;		 		 		 		 // offset of end of value0 parameter from SP

	int linkageAreaSize   = rawParametersSize +		 		 // values
	  (2 * SIZE_TOC) +		 		 // saveJTOC & toc
	  (6 * 4);		 		 		 // backlink + cr + lr + res + res + saveSP

	asm.emitSTU (FP,  -linkageAreaSize, FP);        // create linkage area
	asm.emitST  (JTOC, linkageAreaSize-4, FP);      // save JTOC
	asm.emitST  (SP,   linkageAreaSize-8, FP);      // save SP

	asm.emitL   (JTOC, tocOffset, SP);              // load new TOC
	asm.emitL   (0,    ipOffset,  SP);              // load new IP
     
	return endValueOffset;
  }


  static void generateSysCall2(VM_Assembler asm, int rawParametersSize) {
    int parameterAreaSize = rawParametersSize + SIZE_IP + SIZE_TOC;
    int linkageAreaSize   = rawParametersSize +		 		 // values
      (2 * SIZE_TOC) +		 		 // saveJTOC & toc
      (6 * 4);		 		 		 // backlink + cr + lr + res + res + saveSP

    asm.emitMTLR(0);                                // call desired...
    asm.emitBLRL();                                 // ...function

    asm.emitL   (JTOC, linkageAreaSize - 4, FP);    // restore JTOC
    asm.emitL   (SP,   linkageAreaSize - 8, FP);    // restore SP
    asm.emitCAL (FP,  +linkageAreaSize, FP);        // remove linkage area
  }


  // generate call and return sequence to invoke a C arithmetic helper function through the boot record
  // field specificed by target.  See comments above in sysCall1 about AIX linkage conventions.
  // Caller deals with expression stack (setting up args, pushing return, adjusting stack height)
  static void generateSysCall(VM_Assembler asm, int parametersSize, VM_Field target) {
    int linkageAreaSize   = parametersSize + (2 * SIZE_TOC) + (6 * 4);

    asm.emitSTU (FP,  -linkageAreaSize, FP);        // create linkage area
    asm.emitST  (JTOC, linkageAreaSize-4, FP);      // save JTOC
    asm.emitST  (SP,   linkageAreaSize-8, FP);      // save SP

    // acquire toc and ip from bootrecord
    asm.emitLtoc(S0, VM_Entrypoints.the_boot_recordField.getOffset());
    asm.emitL   (JTOC, VM_Entrypoints.sysTOCField.getOffset(), S0);
    asm.emitL   (0, target.getOffset(), S0);

    // call it
    asm.emitMTLR(0);
    asm.emitBLRL(); 

    // cleanup
    asm.emitL   (JTOC, linkageAreaSize - 4, FP);    // restore JTOC
    asm.emitL   (SP,   linkageAreaSize - 8, FP);    // restore SP
    asm.emitCAL (FP,   linkageAreaSize, FP);        // remove linkage area
  }


  static void generateSysCallRet_I(VM_Assembler asm, int rawParametersSize) {
    int parameterAreaSize = rawParametersSize + SIZE_IP + SIZE_TOC;
    asm.emitCAL (SP, parameterAreaSize - 4, SP);    // pop args, push space for return value
    asm.emitST  (3, 0, SP);                         // deposit C return value (R3) on stacktop
  }

  static void generateSysCallRet_L(VM_Assembler asm, int rawParametersSize) {
    int parameterAreaSize = rawParametersSize + SIZE_IP + SIZE_TOC;
    asm.emitCAL (SP, parameterAreaSize - 8, SP);    // pop args, push space for return value
    asm.emitST  (3, 0, SP);                         // deposit C return value (R3) on stacktop
    asm.emitST  (4, 4, SP);                         // deposit C return value (R4) on stacktop
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * Helper routine to pull the parameters to multianewarray off the
 * Java expression stack maintained by the baseline compiler and 
 * pass them to VM_Runtime.buildMultiDimensionalArray.
 * 
 * TODO: There is only 1 line of platform dependent code here; refactor?
 *
 * @author Bowen Alpern
 * @author Tony Cocchi 
 * @author Derek Lieber
 */
class VM_MultianewarrayHelper {

  /**
   * Allocate something like "new Foo[cnt0][cnt1]...[cntN-1]",
   *                      or "new int[cnt0][cnt1]...[cntN-1]".
   * @param numDimensions number of array dimensions
   * @param dictionaryId  type of array (VM_TypeDictionary id)
   * @param argOffset     position of word *above* `cnt0' argument within caller's frame
   *                      This is used to access the number of elements to 
   *                      be allocated for each dimension.
   * See also: bytecode 0xc5 ("multianewarray") in VM_Compiler
   */
  static Object newArrayArray(int numDimensions, int dictionaryId, int argOffset)
    throws VM_ResolutionException, 
	   NegativeArraySizeException, 
	   OutOfMemoryError {
    // fetch number of elements to be allocated for each array dimension
    //
    int[] numElements = new int[numDimensions];
    VM.disableGC();
    VM_Address argp = VM_Address.fromInt(VM_Magic.getMemoryWord(VM_Magic.getFramePointer())).add(argOffset);
    for (int i = 0; i < numDimensions; ++i)
      numElements[i] = VM_Magic.getMemoryWord(argp.sub(4 * (i + 1)));
    VM.enableGC();

    // validate arguments
    //
    for (int i = 0; i < numDimensions; ++i)
      if (numElements[i] < 0) throw new NegativeArraySizeException();

    // create array
    //
    return VM_Runtime.buildMultiDimensionalArray(numElements, 0, VM_TypeDictionary.getValue(dictionaryId).asArray());
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Class called from baseline compiler to generate architecture specific
 * write barrier for generational garbage collectors.  For baseline 
 * compiled methods, the write barrier calls methods of VM_WriteBarrier.
 *
 * @author Stephen Smith
 */
class VM_Barriers implements VM_BaselineConstants {

  static void compileArrayStoreBarrier (VM_Assembler asm, int spSaveAreaOffset) {
    //  on entry java stack contains ...|target_array_ref|array_index|ref_to_store|
    //  SP -> ref_to_store, SP+8 -> target_ref

    asm.emitLtoc(T0,  VM_Entrypoints.arrayStoreWriteBarrierMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitL (T0, 8, SP);         // load arrayref
    asm.emitL (T1, 4, SP);         // load index
    asm.emitL (T2, 0, SP);         // load value
    asm.emitCall(spSaveAreaOffset);// VM_WriteBarrier.arrayStoreWriteBarrier(Object ref, int index, Object value)

    }


  static void compilePutfieldBarrier (VM_Assembler asm, int spSaveAreaOffset, int fieldOffset) {

    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref

    asm.emitLtoc(T0, VM_Entrypoints.resolvedPutfieldWriteBarrierMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitCAL (T1, fieldOffset, 0); // load offset 
    asm.emitL   (T0, 4, SP);          // load objref
    asm.emitL   (T2, 0, SP);          // load value
    asm.emitCall(spSaveAreaOffset);   // VM_WriteBarrier.resolvedPutfieldWriteBarrier(T0,T1,T2)

    }

  static void compileUnresolvedPutfieldBarrier (VM_Assembler asm, int spSaveAreaOffset, int fieldID) {

    //  on entry java stack contains ...|target_ref|ref_to_store|
    //  SP -> ref_to_store, SP+4 -> target_ref

    asm.emitLtoc(T0,  VM_Entrypoints.unresolvedPutfieldWriteBarrierMethod.getOffset());
    asm.emitMTLR(T0);
    asm.emitCAL (T1, fieldID, 0); // load fieldID 
    asm.emitL   (T0, 4, SP);          // load objref
    asm.emitL   (T2, 0, SP);          // load value
    asm.emitCall(spSaveAreaOffset);   // VM_WriteBarrier.unresolvedPutfieldWriteBarrie(T0,T1,T2)

    }

  // currently do not have a "write barrier for putstatic, emit nothing, for now...
  // (still scanning all of statics/jtoc during each GC)
  //
  static void compilePutstaticBarrier (VM_Assembler asm, int spSaveAreaOffset, int fieldOffset) {
  }

  static void compileUnresolvedPutstaticBarrier (VM_Assembler asm, int spSaveAreaOffset, int fieldOffset) {
  }
 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Contains architecture-specific helper functions for BURS.
 * @author Dave Grove
 * @author Mauricio J. Serrano
 */
abstract class OPT_BURS_Helpers extends OPT_PhysicalRegisterTools
  implements OPT_Operators, OPT_PhysicalRegisterConstants {

  // Generic helper functions.
  // Defined here to allow us to use them in the arch-specific
  // helper functions which are the bulk of this file.
  /**
   * @return the given operand as a register
   */
  static final OPT_RegisterOperand R (OPT_Operand op) {
    return  (OPT_RegisterOperand)op;
  }

  /**
   * @return the given operand as an integer constant
   */
  static final OPT_IntConstantOperand I (OPT_Operand op) {
    return  (OPT_IntConstantOperand)op;
  }

  /**
   * @return the given operand as a long constant
   */
  static final OPT_LongConstantOperand L (OPT_Operand op) {
    return  (OPT_LongConstantOperand)op;
  }

  /**
   * @return the integer value of the given operand
   */
  static final int IV (OPT_Operand op) {
    return  I(op).value;
  }

  // 
  // Begin PowerPC specific helper functions.
  // 
  /**
   * returns true if an unsigned integer in 16 bits
   */
  final boolean UI16 (OPT_Operand a) {
    return  (IV(a) & 0xffff0000) == 0;
  }

  /**
   * returns true if an unsigned integer in 15 bits
   */
  final boolean UI15 (OPT_Operand a) {
    return  (IV(a) & 0xffff8000) == 0;
  }

  /**
   * returns true if a signed integer in 16 bits
   */
  final boolean SI16 (OPT_Operand a) {
    return  SI16(IV(a));
  }

  /**
   * returns true if a signed integer in 16 bits
   */
  final boolean SI16 (int value) {
    return  (value <= 32767) && (value >= -32768);
  }

  /**
   * returns true if lower 16-bits are zero
   */
  final boolean U16 (OPT_Operand a) {
    return  (IV(a) & 0xffff) == 0;
  }

  /**
   * returns true if the constant fits the mask of PowerPC's RLWINM
   */
  final boolean MASK (OPT_Operand a) {
    return  MASK(IV(a));
  }

  /**
   * returns true if the constant fits the mask of PowerPC's RLWINM
   */
  final boolean MASK (int value) {
    if (value < 0)
      value = ~value;
    return  POSITIVE_MASK(value);
  }

  /**
   * returns true if the constant fits the mask of PowerPC's RLWINM
   */
  final boolean POSITIVE_MASK (OPT_Operand a) {
    return  POSITIVE_MASK(IV(a));
  }

  /**
   * returns true if the constant fits the mask of PowerPC's RLWINM
   */
  final boolean POSITIVE_MASK (int value) {
    if (value == 0)
      return  false;
    do {
      if ((value & 0x1) == 1)
        break;
      value = value >>> 1;
    } while (true);
    do {
      if ((value & 0x1) == 0)
        return  false;
      value = value >>> 1;
    } while (value != 0);
    return  true;
  }

  final boolean MASK_AND_OR (OPT_Operand and, OPT_Operand or) {
    int value1 = IV(and);
    int value2 = IV(or);
    return  ((~value1 & value2) == value2) && MASK(value1);
  }

  /**
   * Integer Shift Right Immediate
   */
  final OPT_IntConstantOperand SRI (OPT_Operand o, int amount) {
    return  I(IV(o) >>> amount);
  }

  /**
   * Integer And Immediate
   */
  final OPT_IntConstantOperand ANDI (OPT_Operand o, int mask) {
    return  I(IV(o) & mask);
  }

  /**
   * Calculate Lower 16 Bits
   */
  final OPT_IntConstantOperand CAL16 (OPT_Operand o) {
    return  I(OPT_Bits.PPCMaskLower16(IV(o)));
  }

  /**
   * Calculate Upper 16 Bits
   */
  final OPT_IntConstantOperand CAU16 (OPT_Operand o) {
    return  I(OPT_Bits.PPCMaskUpper16(IV(o)));
  }

  /**
   * Mask Begin
   */
  final OPT_IntConstantOperand MB (OPT_Operand o) {
    return  I(MaskBegin(IV(o)));
  }

  final int MaskBegin (int integer) {
    int value;
    for (value = 0; integer >= 0; integer = integer << 1, value++);
    return  value;
  }

  /**
   * Mask End
   */
  final OPT_IntConstantOperand ME (OPT_Operand o) {
    return  I(MaskEnd(IV(o)));
  }

  final int MaskEnd (int integer) {
    int value;
    for (value = 31; (integer & 0x1) == 0; integer = integer >>> 1, value--);
    return  value;
  }
  
  // access functions
  OPT_Register getXER () {
    return  getIR().regpool.getPhysicalRegisterSet().getXER();
  }

  OPT_Register getLR () {
    return  getIR().regpool.getPhysicalRegisterSet().getLR();
  }

  OPT_Register getCTR () {
    return  getIR().regpool.getPhysicalRegisterSet().getCTR();
  }

  OPT_Register getTU () {
    return  getIR().regpool.getPhysicalRegisterSet().getTU();
  }

  OPT_Register getTL () {
    return  getIR().regpool.getPhysicalRegisterSet().getTL();
  }

  OPT_Register getCR () {
    return  getIR().regpool.getPhysicalRegisterSet().getCR();
  }

  /* RVM registers */
  OPT_Register getJTOC () {
    return  getIR().regpool.getPhysicalRegisterSet().getJTOC();
  }

  /**
   * Emit code to load a float value from the JTOC.
   * @param burs
   * @param operator
   * @param RT
   * @param field
   */
  private void emitLFtoc(OPT_BURS burs, OPT_Operator operator, 
			 OPT_Register RT, VM_Field field) {
    OPT_Register JTOC = burs.ir.regpool.getPhysicalRegisterSet().getJTOC();
    int offset = field.getOffset();
    int valueHigh = OPT_Bits.PPCMaskUpper16(offset);
    OPT_Instruction s;
    if (valueHigh == 0) {
      s = OPT_IRTools.nonPEIGC(MIR_Load.create(operator, 
						  D(RT), R(JTOC), 
						  I(offset)));
      burs.append(s);
    } else {
      OPT_Register reg = burs.ir.regpool.getInteger();
      int valueLow = OPT_Bits.PPCMaskLower16(offset);
      burs.append(MIR_Binary.create(PPC_ADDIS, R(reg), R(JTOC), I(valueHigh)));
      s = OPT_IRTools.nonPEIGC(MIR_Load.create(operator, D(RT), 
						  R(reg), I(valueLow)));
      burs.append(s);
    }
  }

  /**
   * Emit code to load an Integer Constant.
   * reg must be != 0
   */
  void IntConstant(OPT_BURS burs, OPT_Register reg, int value) {
    int lo = OPT_Bits.PPCMaskLower16(value);
    int hi = OPT_Bits.PPCMaskUpper16(value);
    if (hi != 0) {
      burs.append(MIR_Unary.create(PPC_LDIS, R(reg), I(hi)));
      if (lo != 0)
        burs.append(MIR_Binary.create(PPC_ADDI, R(reg), R(reg), I(lo)));
    } else {
      burs.append(MIR_Unary.create(PPC_LDI, R(reg), I(lo)));
    }
  }

  /**
   * Emit code to get a caught exception object into a register
   */
  void GET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    burs.ir.stackManager.forceFrameAllocation();
    int offset = burs.ir.stackManager.allocateSpaceForCaughtException();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_LocationOperand loc = new OPT_LocationOperand(-offset);
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Load.mutate(s, PPC_LWZ, Nullary.getClearResult(s), 
                                 R(FP), I(offset), loc, TG())));
  }

  /**
   * Emit code to move a value in a register to the stack location
   * where a caught exception object is expected to be.
   */
  void SET_EXCEPTION_OBJECT(OPT_BURS burs, OPT_Instruction s) {
    burs.ir.stackManager.forceFrameAllocation();
    int offset = burs.ir.stackManager.allocateSpaceForCaughtException();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_LocationOperand loc = new OPT_LocationOperand(-offset);
    OPT_RegisterOperand obj = (OPT_RegisterOperand)CacheOp.getRef(s);
    burs.append(OPT_IRTools.nonPEIGC(MIR_Store.mutate(s, PPC_STW, obj, 
							 R(FP), I(offset), 
							 loc, TG())));
  }

  /**
   * Emit code to move 32 bits from FPRs to GPRs
   * Note: intentionally use 'null' location to prevent DepGraph
   * from assuming that load/store not aliased. We're stepping outside
   * the Java type system here!
   */
  void FPR2GPR_32(OPT_BURS burs, OPT_Instruction s) {
    int offset = burs.ir.stackManager.allocateSpaceForConversion();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_RegisterOperand val = (OPT_RegisterOperand)Unary.getClearVal(s);
    burs.append(OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STFS, val, 
							 R(FP), I(offset), 
							 null, TG())));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Load.mutate(s, PPC_LWZ, Unary.getClearResult(s), 
				 R(FP), I(offset), null, TG())));
  }

  /**
   * Emit code to move 32 bits from GPRs to FPRs
   * Note: intentionally use 'null' location to prevent DepGraph
   * from assuming that load/store not aliased. We're stepping outside
   * the Java type system here!
   */
  void GPR2FPR_32(OPT_BURS burs, OPT_Instruction s) {
    int offset = burs.ir.stackManager.allocateSpaceForConversion();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_RegisterOperand val = (OPT_RegisterOperand)Unary.getClearVal(s);
    burs.append(OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STW, val, 
							 R(FP), I(offset), 
							 null, TG())));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Load.mutate(s, PPC_LFS, Unary.getClearResult(s), R(FP), 
                                 I(offset), null, TG())));
  }

  /**
   * Emit code to move 64 bits from FPRs to GPRs
   * Note: intentionally use 'null' location to prevent DepGraph
   * from assuming that load/store not aliased. We're stepping outside
   * the Java type system here!
   */
  void FPR2GPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = burs.ir.stackManager.allocateSpaceForConversion();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_RegisterOperand val = (OPT_RegisterOperand)Unary.getClearVal(s);
    burs.append(OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STFD, val, 
							 R(FP), I(offset), 
							 null, TG())));
    OPT_RegisterOperand i1 = Unary.getClearResult(s);
    OPT_RegisterOperand i2 = R(burs.ir.regpool.getSecondReg(i1.register));
    burs.append(OPT_IRTools.nonPEIGC(MIR_Load.create(PPC_LWZ, i1, 
							R(FP), I(offset), 
							null, TG())));
    burs.append(OPT_IRTools.nonPEIGC(MIR_Load.mutate(s, PPC_LWZ, i2, 
							R(FP), I(offset+4),
							null, TG())));
  }

  /**
   * Emit code to move 64 bits from GPRs to FPRs
   * Note: intentionally use 'null' location to prevent DepGraph
   * from assuming that load/store not aliased. We're stepping outside
   * the Java type system here!
   */
  void GPR2FPR_64(OPT_BURS burs, OPT_Instruction s) {
    int offset = burs.ir.stackManager.allocateSpaceForConversion();
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_RegisterOperand i1 = (OPT_RegisterOperand)Unary.getClearVal(s);
    OPT_RegisterOperand i2 = R(burs.ir.regpool.getSecondReg(i1.register));
    burs.append(OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STW, i1, 
							 R(FP), I(offset), 
							 null, TG())));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Store.create(PPC_STW, i2, R(FP), I(offset+4), null, 
                                  TG())));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Load.mutate(s, PPC_LFD, Unary.getClearResult(s), R(FP), 
                                 I(offset), null, TG())));
  }

  /**
   * Expand a prologue by expanding out longs into pairs of ints
   */
  void PROLOGUE(OPT_BURS burs, OPT_Instruction s) {
    int numFormals = Prologue.getNumberOfFormals(s);
    int numLongs = 0;
    for (int i=0; i<numFormals; i++) {
      if (Prologue.getFormal(s, i).type == VM_Type.LongType) numLongs ++;
    }
    if (numLongs != 0) {
      OPT_Instruction s2 = Prologue.create(IR_PROLOGUE, numFormals+numLongs);
      for (int sidx=0, s2idx=0; sidx<numFormals; sidx++) {
	OPT_RegisterOperand sForm = Prologue.getClearFormal(s, sidx);
	Prologue.setFormal(s2, s2idx++, sForm);
	if (sForm.type == VM_Type.LongType) {
	  Prologue.setFormal(s2, s2idx++, 
                             R(burs.ir.regpool.getSecondReg(sForm.register)));
	}
      }									     
      burs.append(s2);
    } else {
      burs.append(s);
    }
  }


  /**
   * Expand a call instruction.
   */
  void CALL(OPT_BURS burs, OPT_Instruction s) {
    OPT_Operand target = Call.getClearAddress(s);
    OPT_MethodOperand meth = Call.getClearMethod(s);

    // Step 1: Find out how many parameters we're going to have.
    int numParams = Call.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (Call.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be
    OPT_RegisterOperand result = Call.getClearResult(s);
    OPT_RegisterOperand result2 = null;
    if (result != null && result.type == VM_Type.LongType) {
      result2 = R(burs.ir.regpool.getSecondReg(result.register));
    }

    // Step 3: Figure out what the operator is going to be
    OPT_Operator callOp;
    if (target instanceof OPT_RegisterOperand) {
      // indirect call through target (contains code addr)
      OPT_Register ctr = burs.ir.regpool.getPhysicalRegisterSet().getCTR();
      burs.append(MIR_Move.create(PPC_MTSPR, R(ctr), 
				  (OPT_RegisterOperand)target));
      target = null;
      callOp = PPC_BCTRL;
    } else if (target instanceof OPT_BranchOperand) {
      // Earlier analysis has tagged this call as recursive, 
      // set up for a direct call.
      callOp = PPC_BL;
    } else {
      throw  new OPT_OptimizingCompilerException("Unexpected target operand "
						 + target + " to call " + s);
    }

    // Step 4: Mutate the Call to an MIR_Call.
    // Note MIR_Call and Call have a different number of fixed 
    // arguments, so some amount of copying is required. We'll hope the 
    // opt compiler can manage to make this more efficient than it looks.
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = Call.getClearParam(s, i);
    }
    burs.append(MIR_Call.mutate(s, callOp, result, result2, 
				(OPT_BranchOperand)target, meth, 
				numParams + longParams));
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      MIR_Call.setParam(s, mirCallIdx++, param);
      if (param instanceof OPT_RegisterOperand) {
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
			    L(burs.ir.regpool.getSecondReg(rparam.register)));
        }
      }
    }
  }

  /**
   * Expand a syscall instruction.
   */
  void SYSCALL(OPT_BURS burs, OPT_Instruction s) {
    burs.ir.setHasSysCall(true);
    OPT_Operand target = CallSpecial.getClearAddress(s);
    OPT_MethodOperand meth = (OPT_MethodOperand)CallSpecial.getClearMethod(s);

    // Step 1: Find out how many parameters we're going to have.
    int numParams = CallSpecial.getNumberOfParams(s);
    int longParams = 0;
    for (int pNum = 0; pNum < numParams; pNum++) {
      if (CallSpecial.getParam(s, pNum).getType() == VM_Type.LongType) {
        longParams++;
      }
    }

    // Step 2: Figure out what the result and result2 values will be
    OPT_RegisterOperand result = CallSpecial.getClearResult(s);
    OPT_RegisterOperand result2 = null;
    if (result != null && result.type == VM_Type.LongType) {
      result2 = R(burs.ir.regpool.getSecondReg(result.register));
    }

    // Step 3: Figure out what the operator is going to be
    OPT_Operator callOp;
    if (target instanceof OPT_RegisterOperand) {
      // indirect call through target (contains code addr)
      OPT_Register ctr = burs.ir.regpool.getPhysicalRegisterSet().getCTR();
      burs.append(MIR_Move.create(PPC_MTSPR, R(ctr), 
				  (OPT_RegisterOperand)target));
      target = null;
      callOp = PPC_BCTRL_SYS;
    } else if (target instanceof OPT_BranchOperand) {
      // Earlier analysis has tagged this call as recursive, 
      // set up for a direct call.
      callOp = PPC_BL_SYS;
    } else {
      throw  new OPT_OptimizingCompilerException("Unexpected target operand "
						 + target + " to call " + s);
    }

    // Step 4: Mutate the SysCall to an MIR_Call.
    // Note MIR_Call and Call have a different number of fixed 
    // arguments, so some amount of copying is required. We'll hope the 
    // opt compiler can manage to make this more efficient than it looks.
    OPT_Operand[] params = new OPT_Operand[numParams];
    for (int i = 0; i < numParams; i++) {
      params[i] = Call.getClearParam(s, i);
    }
    burs.append(MIR_Call.mutate(s, callOp, result, result2, 
				(OPT_BranchOperand)target, meth, 
				numParams + longParams));
    for (int paramIdx = 0, mirCallIdx = 0; paramIdx < numParams;) {
      OPT_Operand param = params[paramIdx++];
      MIR_Call.setParam(s, mirCallIdx++, param);
      if (param instanceof OPT_RegisterOperand) {
        OPT_RegisterOperand rparam = (OPT_RegisterOperand)param;
        if (rparam.type == VM_Type.LongType) {
          MIR_Call.setParam(s, mirCallIdx++, 
			    L(burs.ir.regpool.getSecondReg(rparam.register)));
        }
      }
    }
  }

  /**
   * Emit code for RETURN.
   * @param burs
   * @param s
   * @param value
   */
  void RETURN(OPT_BURS burs, OPT_Instruction s, OPT_Operand value) {
    if (value != null) {
      OPT_RegisterOperand rop = (OPT_RegisterOperand)value;
      if (value.getType() == VM_Type.LongType) {
	OPT_Register pair = burs.ir.regpool.getSecondReg(rop.register);
        burs.append(MIR_Return.mutate(s, PPC_BLR, rop.copyU2U(), R(pair)));
      } else {
        burs.append(MIR_Return.mutate(s, PPC_BLR, rop.copyU2U(), null));
      }
    } else {
      burs.append(MIR_Return.mutate(s, PPC_BLR, null, null));
    }
  }


  void SHL_USHR(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, 
		OPT_IntConstantOperand shift1, 
		OPT_IntConstantOperand shift2) {
    int x = shift1.value;
    int y = shift2.value;
    if (x < y) {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, def, left, 
					   I((32 - (y - x)) & 0x1f), 
					   I(y), I(31))); 
    } else {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, def, left, 
					   I(x - y), I(y), I(31 - (x - y))));
    }
  }

  void USHR_SHL(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, 
		OPT_IntConstantOperand shift1, 
		OPT_IntConstantOperand shift2) {
    int x = shift1.value;
    int y = shift2.value;
    if (x < y) {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, def, left, 
					   I(y - x), I(0), I(31 - y))); 
    } else {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, def, left, 
					   I((32 - (x - y)) & 0x1f), 
					   I(x - y), I(31 - y)));
    }
  }

  void USHR_AND(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand Def, 
		OPT_RegisterOperand left, 
		OPT_IntConstantOperand Mask, 
		OPT_IntConstantOperand Shift) {
    int shift = Shift.value;
    int mask = Mask.value;
    int MB = MaskBegin(mask);
    int ME = MaskEnd(mask);
    if (shift > ME) {           // result should be 0
      burs.append(MIR_Unary.create(PPC_LDI, Def, I(0)));
      return;
    } else if (shift > MB) {
      MB = shift;
    }
    burs.append(MIR_RotateAndMask.create(PPC_RLWINM, Def, left, 
					 I((32 - shift) & 0x1f), 
					 I(MB), I(ME)));
  }

  void AND_USHR(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand Def, 
		OPT_RegisterOperand left, 
		OPT_IntConstantOperand Mask, 
		OPT_IntConstantOperand Shift) {
    int shift = Shift.value;
    int mask = Mask.value;
    int MB = MaskBegin(mask);
    int ME = MaskEnd(mask);
    if ((MB + shift) >= 32) {                   // result should be 0
      burs.append(MIR_Unary.create(PPC_LDI, Def, I(0)));
      return;
    }
    MB += shift;
    ME += shift;
    if (ME >= 32) {
      ME = 31;
    }
    burs.append(MIR_RotateAndMask.create(PPC_RLWINM, Def, left, 
					 I((32 - shift) & 0x1f), 
					 I(MB), I(ME)));
  }

  void AND_MASK(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand Def, 
		OPT_RegisterOperand left, 
		OPT_IntConstantOperand Mask) {
    int mask = Mask.value;
    if (mask < 0) {
      mask = ~mask;
      int MB = MaskBegin(mask);
      int ME = MaskEnd(mask);
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, Def, left, I(0), 
					   I((ME + 1) & 0x1f), I(MB - 1)));
    } else {
      int MB = MaskBegin(mask);
      int ME = MaskEnd(mask);
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, Def, left, I(0), 
					   I(MB), I(ME)));
    }
  }

  void BOOLEAN_CMP_IMM(OPT_BURS burs, OPT_Instruction s, 
		       OPT_RegisterOperand def, 
		       OPT_RegisterOperand one, 
		       OPT_IntConstantOperand two) {
    OPT_ConditionOperand cmp = BooleanCmp.getCond(s);
    if (!boolean_cmp_imm(burs, def, one, two.value, cmp))
      burs.append(s);
  }

  /**
   * emit basic code to handle an INT_IFCMP when no folding
   * of the compare into some other computation is possible.
   */
  void CMP(OPT_BURS burs, OPT_Instruction s,
	   OPT_RegisterOperand val1, OPT_Operand val2,
	   OPT_ConditionOperand cond,
	   boolean immediate) {
    OPT_RegisterOperand cr = burs.ir.regpool.makeTempCondition();
    OPT_Operator op;
    if (immediate) {
      op = cond.isUNSIGNED() ? PPC_CMPLI : PPC_CMPI;
    } else { 
      op = cond.isUNSIGNED() ? PPC_CMPL : PPC_CMP;
    }
    burs.append(MIR_Binary.create(op, cr, val1, val2));
    burs.append(MIR_CondBranch.mutate(s, PPC_BCOND, cr.copyD2U(),
				      new OPT_PowerPCConditionOperand(cond),
				      IfCmp.getTarget(s),
				      IfCmp.getBranchProfile(s)));
  }

  /**
   * emit basic code to handle an INT_IFCMP2 when no folding
   * of the compare into some other computation is possible.
   */
  void CMP2(OPT_BURS burs, OPT_Instruction s,
	    OPT_RegisterOperand val1, OPT_Operand val2,
	    OPT_ConditionOperand cond1, OPT_ConditionOperand cond2,
	    boolean immediate) {
    OPT_Operator op1;
    OPT_Operator op2;
    if (immediate) {
      op1 = cond1.isUNSIGNED() ? PPC_CMPLI : PPC_CMPI;
      op2 = cond2.isUNSIGNED() ? PPC_CMPLI : PPC_CMPI;
    } else { 
      op1 = cond1.isUNSIGNED() ? PPC_CMPL : PPC_CMP;
      op2 = cond2.isUNSIGNED() ? PPC_CMPL : PPC_CMP;
    }
    if (op1 == op2) {
      OPT_RegisterOperand cr = burs.ir.regpool.makeTempCondition();
      burs.append(MIR_Binary.create(op1, cr, val1, val2));
      burs.append(MIR_CondBranch2.mutate(s, PPC_BCOND2, cr.copyD2U(),
					 new OPT_PowerPCConditionOperand(cond1),
					 IfCmp2.getTarget1(s),
					 IfCmp2.getBranchProfile1(s),
					 new OPT_PowerPCConditionOperand(cond2),
					 IfCmp2.getTarget2(s),
					 IfCmp2.getBranchProfile2(s)));
    } else {
      OPT_RegisterOperand cr1 = burs.ir.regpool.makeTempCondition();
      OPT_RegisterOperand cr2 = burs.ir.regpool.makeTempCondition();
      burs.append(MIR_Binary.create(op1, cr1, val1, val2));
      burs.append(MIR_Binary.create(op2, cr2, val1, val2));
      burs.append(MIR_CondBranch.create(PPC_BCOND, cr1.copyD2U(),
					new OPT_PowerPCConditionOperand(cond1),
					IfCmp2.getTarget1(s),
					IfCmp2.getBranchProfile1(s)));
      burs.append(MIR_CondBranch.mutate(s, PPC_BCOND, cr2.copyD2U(),
					new OPT_PowerPCConditionOperand(cond2),
					IfCmp2.getTarget2(s),
					IfCmp2.getBranchProfile2(s)));
    }
  }


  /**
   * Uses the record capability to avoid compare 
   */
  void CMP_ZERO(OPT_BURS burs, OPT_Instruction s, OPT_Operator op, 
		OPT_RegisterOperand def, OPT_Operand left,
		OPT_ConditionOperand cond) {
    if (VM.VerifyAssertions) VM.assert(!cond.isUNSIGNED());
    if (!def.register.spansBasicBlock()) {
      def.register = burs.ir.regpool.getPhysicalRegisterSet().getTemp();
    }
    burs.append(MIR_Unary.create(op, def, left));
    burs.append(MIR_CondBranch.mutate(s, PPC_BCOND, CR(0), 
				      new OPT_PowerPCConditionOperand(cond), 
				      IfCmp.getTarget(s),
				      IfCmp.getBranchProfile(s)));
  }

  void CMP_ZERO(OPT_BURS burs, OPT_Instruction s, OPT_Operator op, 
		OPT_RegisterOperand def, OPT_RegisterOperand left, 
		OPT_Operand right, OPT_ConditionOperand cond) {
    if (VM.VerifyAssertions) VM.assert(!cond.isUNSIGNED());
    if (!def.register.spansBasicBlock()) {
      def.register = burs.ir.regpool.getPhysicalRegisterSet().getTemp();
    }
    burs.append(MIR_Binary.create(op, def, left, right));
    burs.append(MIR_CondBranch.mutate(s, PPC_BCOND, CR(0), 
				      new OPT_PowerPCConditionOperand(cond), 
				      IfCmp.getTarget(s),
				      IfCmp.getBranchProfile(s)));
  }

  void CMP_ZERO_AND_MASK(OPT_BURS burs, OPT_Instruction s, 
			 OPT_RegisterOperand def, 
			 OPT_RegisterOperand left, 
			 OPT_IntConstantOperand Mask,
			 OPT_ConditionOperand cond) {
    if (VM.VerifyAssertions) VM.assert(!cond.isUNSIGNED());
    int mask = Mask.value;
    if (mask < 0) {
      mask = ~mask;
      int MB = MaskBegin(mask);
      int ME = MaskEnd(mask);
      burs.append(MIR_RotateAndMask.create(PPC_RLWINMr, def, left, I(0), 
					   I((ME + 1) & 0x1f), I(MB - 1)));
    } else {
      int MB = MaskBegin(mask);
      int ME = MaskEnd(mask);
      burs.append(MIR_RotateAndMask.create(PPC_RLWINMr, def, left, I(0), 
					   I(MB), I(ME)));
    }
    burs.append(MIR_CondBranch.mutate(s, PPC_BCOND, CR(0), 
				      new OPT_PowerPCConditionOperand(cond), 
				      IfCmp.getTarget(s),
				      IfCmp.getBranchProfile(s)));
  }

  // boolean compare
  /**
   * taken from: The PowerPC Compiler Writer's Guide, pp. 199 
   */
  boolean boolean_cmp_imm(OPT_BURS burs, OPT_RegisterOperand def, 
			  OPT_RegisterOperand one, 
			  int value, OPT_ConditionOperand cmp) {
    OPT_Register t1, t = burs.ir.regpool.getInteger();
    OPT_Register zero = burs.ir.regpool.getPhysicalRegisterSet().getTemp();
    boolean convert = true;
    switch (cmp.value) {
    case OPT_ConditionOperand.EQUAL:
      if (value == 0) {
	burs.append(MIR_Unary.create(PPC_CNTLZW, R(t), one)); 
      } else {
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(t), one, I(value)));
	burs.append(MIR_Unary.create(PPC_CNTLZW, R(t), R(t)));
      }
      burs.append(MIR_Binary.create(PPC_SRWI, def, R(t), I(5)));
      break;
    case OPT_ConditionOperand.NOT_EQUAL:
      if (value == 0) {
	burs.append(MIR_Binary.create(PPC_ADDIC, R(t), one, I(-1)));
	burs.append(MIR_Binary.create(PPC_SUBFE, def, R(t), one.copyRO()));
      } else {
	t1 = burs.ir.regpool.getInteger();
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(t1), one, I(value)));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(t), R(t1), I(-1)));
	burs.append(MIR_Binary.create(PPC_SUBFE, def, R(t), R(t1)));
      }
      break;
    case OPT_ConditionOperand.LESS:
      if (value == 0) {
	burs.append(MIR_Binary.create(PPC_SRWI, def, one, I(31)));
      } else if (value > 0) {
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(zero), one, 
				      I(value - 1)));
	burs.append(MIR_Unary.create(PPC_ADDZE, def, R(t)));
      } else if (value != 0xFFFF8000) {
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(zero), one.copyRO(), 
				      I(value - 1)));
	burs.append(MIR_Unary.create(PPC_ADDME, def, R(t)));
      } else {                  // value = 0xFFFF8000
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Unary.create(PPC_LDIS, R(zero), I(1)));
	burs.append(MIR_Binary.create(PPC_SUBFC, R(zero), one.copyRO(), R(zero)));
	burs.append(MIR_Unary.create(PPC_ADDME, def, R(t)));
      }
      break;
    case OPT_ConditionOperand.GREATER:
      if (value == 0) {
	burs.append(MIR_Unary.create(PPC_NEG, R(t), one));
	burs.append(MIR_Binary.create(PPC_ANDC, R(t), R(t), one.copyRO()));
	burs.append(MIR_Binary.create(PPC_SRWI, def, R(t), I(31)));
      } else if (value >= 0) {
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(zero), one.copyRO(), 
				      I(-value - 1)));
	burs.append(MIR_Unary.create(PPC_ADDZE, def, R(t)));
      } else {
	t1 = burs.ir.regpool.getInteger();
	burs.append(MIR_Unary.create(PPC_LDI, R(t1), I(1)));
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(zero), one.copyRO(), 
				      I(-value - 1)));
	burs.append(MIR_Binary.create(PPC_ADDE, def, R(t), R(t1)));
      }
      break;
    case OPT_ConditionOperand.LESS_EQUAL:
      if (value == 0) {
	burs.append(MIR_Binary.create(PPC_ADDI, R(t), one, I(-1)));
	burs.append(MIR_Binary.create(PPC_OR, R(t), R(t), one.copyRO()));
	burs.append(MIR_Binary.create(PPC_SRWI, def, R(t), I(31)));
      } else if (value >= 0) {
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(zero), one.copyRO(), I(value)));
	burs.append(MIR_Unary.create(PPC_ADDZE, def, R(t)));
      } else {
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFIC, R(zero), one.copyRO(), I(value)));
	burs.append(MIR_Unary.create(PPC_ADDME, def, R(t)));
      }
      break;
    case OPT_ConditionOperand.GREATER_EQUAL:
      if (value == 0) {
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_XORI, def, R(t), I(1)));
      } else if (value >= 0) {
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(zero), one.copyRO(), I(-value)));
	burs.append(MIR_Unary.create(PPC_ADDZE, def, R(t)));
      } else if (value != 0xFFFF8000) {
	t1 = burs.ir.regpool.getInteger();
	burs.append(MIR_Unary.create(PPC_LDI, R(t1), I(1)));
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(zero), one.copyRO(), I(-value)));
	burs.append(MIR_Binary.create(PPC_ADDE, def, R(t), R(t1)));
      } else {
	return false; // value == 0xFFFF8000; code sequence below does not work --dave 7/25/02
	/*
	t1 = burs.ir.regpool.getInteger();
	burs.append(MIR_Unary.create(PPC_LDI, R(t1), I(1)));
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t), one, I(31)));
	burs.append(MIR_Unary.create(PPC_LDIS, R(zero), I(1)));
	burs.append(MIR_Binary.create(PPC_ADDC, R(zero), one.copyRO(), R(zero)));
	burs.append(MIR_Binary.create(PPC_ADDE, def, R(t), R(t1)));
	*/
      }
      break;
    case OPT_ConditionOperand.HIGHER:
      return false; // todo
    case OPT_ConditionOperand.LOWER:
      return false; // todo
    case OPT_ConditionOperand.HIGHER_EQUAL:
      return false; // todo
    case OPT_ConditionOperand.LOWER_EQUAL:
      burs.append(MIR_Unary.create(PPC_LDI, R(t), I(-1)));
      burs.append(MIR_Binary.create(PPC_SUBFIC, R(zero), one, I(value)));
      burs.append(MIR_Unary.create(PPC_SUBFZE, def, R(t)));
      break;

    default:
      return false;
    }
    return true;
  }

  void BOOLEAN_CMP(OPT_BURS burs, OPT_Instruction s, 
		   OPT_RegisterOperand def, 
		   OPT_RegisterOperand one,
		   OPT_RegisterOperand two) {
    OPT_ConditionOperand cmp = BooleanCmp.getCond(s);
    if (!boolean_cmp(burs, def, one, two, cmp, s))
      burs.append(s);
  }

  boolean boolean_cmp (OPT_BURS burs, OPT_RegisterOperand def, 
		       OPT_RegisterOperand one, 
		       OPT_RegisterOperand two, OPT_ConditionOperand cmp,
		       OPT_Instruction inst) {
    OPT_Register t1, zero, t = burs.ir.regpool.getInteger();
    switch (cmp.value) {
    case OPT_ConditionOperand.EQUAL:
      {
	burs.append(MIR_Binary.create(PPC_SUBF, R(t), one, two));
	burs.append(MIR_Unary.create(PPC_CNTLZW, R(t), R(t)));
	burs.append(MIR_Binary.create(PPC_SRWI, def, R(t), I(5)));
      }
      break;
    case OPT_ConditionOperand.NOT_EQUAL:
      {
	t1 = burs.ir.regpool.getInteger();
	burs.append(MIR_Binary.create(PPC_SUBF, R(t), one, two));
	burs.append(MIR_Binary.create(PPC_ADDIC, R(t1), R(t), I(-1)));
	burs.append(MIR_Binary.create(PPC_SUBFE, def, R(t1), R(t)));
      }
      break;
    case OPT_ConditionOperand.LESS_EQUAL:
      {
	t1 = burs.ir.regpool.getInteger();
	zero = burs.ir.regpool.getPhysicalRegisterSet().getTemp();
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t1), two, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFC, R(zero), one.copyRO(), two.copyRO()));
	burs.append(MIR_Binary.create(PPC_ADDE, def, R(t1), R(t)));
      }
      break;
    case OPT_ConditionOperand.GREATER_EQUAL:
      {
	t1 = burs.ir.regpool.getInteger();
	zero = burs.ir.regpool.getPhysicalRegisterSet().getTemp();
	burs.append(MIR_Binary.create(PPC_SRWI, R(t), two, I(31)));
	burs.append(MIR_Binary.create(PPC_SRAWI, R(t1), one, I(31)));
	burs.append(MIR_Binary.create(PPC_SUBFC, R(zero), two.copyRO(), one.copyRO()));
	burs.append(MIR_Binary.create(PPC_ADDE, def, R(t1), R(t)));
      }
      break;
    default:
      return false;
    }
    return true;
  }

  void BYTE_LOAD(OPT_BURS burs, OPT_Instruction s, 
		 OPT_Operator opcode, 
		 OPT_RegisterOperand def, 
		 OPT_RegisterOperand left, 
		 OPT_Operand right, 
		 OPT_LocationOperand loc, 
		 OPT_Operand guard) {
    OPT_RegisterOperand reg1 = burs.ir.regpool.makeTempInt();
    burs.append(OPT_IRTools.nonPEIGC(MIR_Load.mutate(s, opcode, 
							reg1, left, right, 
							loc, guard)));
    burs.append(MIR_Unary.create(PPC_EXTSB, def, reg1.copyD2U()));
  }

  private int PowerOf2 (int v) {
    int i = 31;
    int power = -1;
    for (; v != 0; v = v << 1, i--)
      if (v < 0) {
        if (power == -1)
          power = i; 
        else 
          return  -1;
      }
    return  power;
  }

  void INT_DIV_IMM(OPT_BURS burs, OPT_Instruction s, 
		   OPT_RegisterOperand def, 
		   OPT_RegisterOperand left, 
		   OPT_RegisterOperand c, 
		   OPT_IntConstantOperand right) {
    int power = PowerOf2(right.value);
    if (power != -1) {
      burs.append(MIR_Binary.create(PPC_SRAWI, c, left, I(power)));
      burs.append(MIR_Unary.create(PPC_ADDZE, def, c.copyD2U()));
    } else {
      IntConstant(burs, c.register, right.value);
      burs.append(MIR_Binary.mutate(s, PPC_DIVW, def, left, c));
    }
  }

  void INT_REM(OPT_BURS burs, OPT_Instruction s, 
	       OPT_RegisterOperand def, 
	       OPT_RegisterOperand left, 
	       OPT_RegisterOperand right) {
    OPT_Register temp = burs.ir.regpool.getInteger();
    burs.append(MIR_Binary.mutate(s, PPC_DIVW, R(temp), left, right));
    burs.append(MIR_Binary.create(PPC_MULLW, R(temp), R(temp), 
				  right.copyU2U()));
    burs.append(MIR_Binary.create(PPC_SUBF, def, R(temp), left.copyU2U()));
  }

  void INT_REM_IMM(OPT_BURS burs, OPT_Instruction s, 
		   OPT_RegisterOperand def, 
		   OPT_RegisterOperand left, 
		   OPT_RegisterOperand c, 
		   OPT_IntConstantOperand right) {
    OPT_Register temp = burs.ir.regpool.getInteger();
    int power = PowerOf2(right.value);
    if (power != -1) {
      burs.append(MIR_Binary.mutate(s, PPC_SRAWI, R(temp), left, I(power)));
      burs.append(MIR_Unary.create(PPC_ADDZE, R(temp), R(temp)));
      burs.append(MIR_Binary.create(PPC_SLWI, R(temp), R(temp), I(power)));
      burs.append(MIR_Binary.create(PPC_SUBF, def, R(temp), left.copyU2U()));
    } else {
      IntConstant(burs, c.register, right.value);
      burs.append(MIR_Binary.mutate(s, PPC_DIVW, R(temp), left, c));
      burs.append(MIR_Binary.create(PPC_MULLW, R(temp), R(temp), c.copyU2U()));
      burs.append(MIR_Binary.create(PPC_SUBF, def, R(temp), left.copyU2U()));
    }
  }

  /**
   * Conversion
   */
  void INT_2LONG(OPT_BURS burs, OPT_Instruction s, 
		 OPT_RegisterOperand def, 
		 OPT_RegisterOperand left) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    burs.append(MIR_Move.mutate(s, PPC_MOVE, R(defLow), left));
    burs.append(MIR_Binary.create(PPC_SRAWI, R(defHigh), left.copyU2U(), I(31)));
  }

  /**
   * taken from: The PowerPC Compiler Writer's Guide, pp. 83 
   */
  void INT_2DOUBLE(OPT_BURS burs, OPT_Instruction s, 
		   OPT_RegisterOperand def, 
		   OPT_RegisterOperand left) {
    OPT_Register res = def.register;
    OPT_Register src = left.register;
    OPT_Register FP = burs.ir.regpool.getPhysicalRegisterSet().getFP();
    OPT_RegisterOperand temp = burs.ir.regpool.makeTempInt();
    int p = burs.ir.stackManager.allocateSpaceForConversion();
    burs.append(MIR_Unary.mutate(s, PPC_LDIS, temp, I(0x4330)));
    // TODO: valid location?
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Store.create(PPC_STW, R(temp.register), R(FP), I(p), 
                                  new OPT_TrueGuardOperand())));
    OPT_Register t1 = burs.ir.regpool.getInteger();
    burs.append(MIR_Binary.create(PPC_XORIS, R(t1), R(src), I(0x8000)));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Store.create(PPC_STW, R(t1), R(FP), I(p + 4), 
                                  new OPT_TrueGuardOperand())));
    burs.append(OPT_IRTools.nonPEIGC
                (MIR_Load.create(PPC_LFD, D(res), R(FP), I(p))));
    OPT_Register tempF = burs.ir.regpool.getDouble();
    emitLFtoc(burs, PPC_LFD, tempF, VM_Entrypoints.I2DconstantField);
    burs.append(MIR_Binary.create(PPC_FSUB, D(res), D(res), D(tempF)));
  }

  // LONG arithmetic:
  void LONG_2INT(OPT_BURS burs, OPT_Instruction s, 
		 OPT_RegisterOperand def, 
		 OPT_RegisterOperand left) {
    OPT_Register srcHigh = left.register;
    OPT_Register srcLow = burs.ir.regpool.getSecondReg(srcHigh);
    burs.append(MIR_Move.mutate(s, PPC_MOVE, def, R(srcLow)));
  }

  void LONG_MOVE(OPT_BURS burs, OPT_Instruction s, 
		 OPT_RegisterOperand def, 
		 OPT_RegisterOperand left) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = left.register;
    burs.append(MIR_Move.create(PPC_MOVE, R(defReg), R(leftReg)));
    burs.append(MIR_Move.create(PPC_MOVE, 
				R(burs.ir.regpool.getSecondReg(defReg)), 
				R(burs.ir.regpool.getSecondReg(leftReg))));
  }

  void LONG_CONSTANT(OPT_BURS burs, OPT_Instruction s, 
		     OPT_RegisterOperand def, 
		     OPT_LongConstantOperand left) {
    long value = left.value;
    int valueHigh = (int)(value >> 32);
    int valueLow = (int)(value & 0xffffffff);
    OPT_Register register = def.register;
    IntConstant(burs, register, valueHigh);
    IntConstant(burs, burs.ir.regpool.getSecondReg(register), valueLow);
  }

  void LONG_ADD(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, OPT_RegisterOperand right) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = left.register;
    OPT_Register rightReg = right.register;
    burs.append(MIR_Binary.create(PPC_ADDC, 
				  R(burs.ir.regpool.getSecondReg(defReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg)), 
				  R(burs.ir.regpool.getSecondReg(rightReg))));
    burs.append(MIR_Binary.create(PPC_ADDE, R(defReg), R(leftReg), 
				  R(rightReg)));
  }

  /* Notice: switching operands! */
  void LONG_SUB(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, 
		OPT_RegisterOperand right) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = right.register;
    OPT_Register rightReg = left.register;
    burs.append(MIR_Binary.create(PPC_SUBFC, 
				  R(burs.ir.regpool.getSecondReg(defReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg)), 
				  R(burs.ir.regpool.getSecondReg(rightReg))));
    burs.append(MIR_Binary.create(PPC_SUBFE, R(defReg), 
                                  R(leftReg), R(rightReg)));
  }

  void LONG_NEG(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = left.register;
    burs.append(MIR_Binary.create(PPC_SUBFIC, 
				  R(burs.ir.regpool.getSecondReg(defReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg)), 
				  I(0)));
    burs.append(MIR_Unary.create(PPC_SUBFZE, R(defReg), R(leftReg)));
  }

  void LONG_NOT(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = left.register;
    burs.append(MIR_Binary.create(PPC_NOR, R(defReg), R(leftReg), R(leftReg)));
    burs.append(MIR_Binary.create(PPC_NOR, 
				  R(burs.ir.regpool.getSecondReg(defReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg))));
  }

  void LONG_LOG(OPT_BURS burs, OPT_Instruction s, 
		OPT_Operator operator, 
		OPT_RegisterOperand def, OPT_RegisterOperand left, 
		OPT_RegisterOperand right) {
    OPT_Register defReg = def.register;
    OPT_Register leftReg = left.register;
    OPT_Register rightReg = right.register;
    burs.append(MIR_Binary.create(operator, R(defReg), 
				  R(leftReg), R(rightReg)));
    burs.append(MIR_Binary.create(operator, 
				  R(burs.ir.regpool.getSecondReg(defReg)), 
				  R(burs.ir.regpool.getSecondReg(leftReg)), 
				  R(burs.ir.regpool.getSecondReg(rightReg))));
  }

  /**
   * taken from "PowerPC Microprocessor Family, 
   * The Programming Environment for 32-bit Microprocessors 
   * */
  void LONG_SHL(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, OPT_RegisterOperand right) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = burs.ir.regpool.getSecondReg(leftHigh);
    OPT_Register shift = right.register;
    OPT_RegisterPool regpool = burs.ir.regpool;
    OPT_Register t0 = regpool.getInteger();
    OPT_Register t31 = regpool.getInteger();
    burs.append(MIR_Binary.create(PPC_SUBFIC, R(t31), R(shift), I(32)));
    burs.append(MIR_Binary.create(PPC_SLW, R(defHigh), R(leftHigh), R(shift)));
    burs.append(MIR_Binary.create(PPC_SRW, R(t0), R(leftLow), R(t31)));
    burs.append(MIR_Binary.create(PPC_OR, R(defHigh), R(defHigh), R(t0)));
    burs.append(MIR_Binary.create(PPC_ADDI, R(t31), R(shift), I(-32)));
    burs.append(MIR_Binary.create(PPC_SLW, R(t0), R(leftLow), R(t31)));
    burs.append(MIR_Binary.create(PPC_OR, R(defHigh), R(defHigh), R(t0)));
    burs.append(MIR_Binary.create(PPC_SLW, R(defLow), R(leftLow), R(shift)));
  }

  void LONG_SHL_IMM(OPT_BURS burs, OPT_Instruction s, 
		    OPT_RegisterOperand def, 
		    OPT_RegisterOperand left, 
		    OPT_IntConstantOperand right) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = burs.ir.regpool.getSecondReg(leftHigh);
    int shift = right.value & 0x3f;
    if (shift < 32) {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, R(defHigh), 
					   R(leftHigh), 
					   I(shift), I(0), I(31 - shift)));
      burs.append(MIR_RotateAndMask.create(PPC_RLWIMI, R(defHigh), R(defHigh), 
					   R(leftLow), I(shift), 
					   I(32 - shift), I(31)));
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, R(defLow), R(leftLow), 
					   I(shift), I(0), I(31 - shift)));
    } else if (shift == 32) {
      burs.append(MIR_Move.create(PPC_MOVE, R(defHigh), R(leftLow)));
      burs.append(MIR_Unary.create(PPC_LDI, R(defLow), I(0)));
    } else {
      shift = shift - 32;
      burs.append(MIR_Binary.create(PPC_SLWI, R(defHigh), R(leftLow), 
				    I(shift)));
      burs.append(MIR_Unary.create(PPC_LDI, R(defLow), I(0)));
    }
  }

  void LONG_USHR(OPT_BURS burs, OPT_Instruction s, 
		 OPT_RegisterOperand def, 
		 OPT_RegisterOperand left, OPT_RegisterOperand right) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = burs.ir.regpool.getSecondReg(leftHigh);
    OPT_Register shift = right.register;
    OPT_RegisterPool regpool = burs.ir.regpool;
    OPT_Register t0 = regpool.getInteger();
    OPT_Register t31 = regpool.getInteger();
    burs.append(MIR_Binary.create(PPC_SUBFIC, R(t31), R(shift), I(32)));
    burs.append(MIR_Binary.create(PPC_SRW, R(defLow), R(leftLow), R(shift)));
    burs.append(MIR_Binary.create(PPC_SLW, R(t0), R(leftHigh), R(t31)));
    burs.append(MIR_Binary.create(PPC_OR, R(defLow), R(defLow), R(t0)));
    burs.append(MIR_Binary.create(PPC_ADDI, R(t31), R(shift), I(-32)));
    burs.append(MIR_Binary.create(PPC_SRW, R(t0), R(leftHigh), R(t31)));
    burs.append(MIR_Binary.create(PPC_OR, R(defLow), R(defLow), R(t0)));
    burs.append(MIR_Binary.create(PPC_SRW, R(defHigh), R(leftHigh), R(shift)));
  }

  void LONG_USHR_IMM(OPT_BURS burs, OPT_Instruction s, 
		     OPT_RegisterOperand def, 
		     OPT_RegisterOperand left, 
		     OPT_IntConstantOperand right) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = burs.ir.regpool.getSecondReg(leftHigh);
    int shift = right.value & 0x3f;
    if (shift < 32) {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, R(defLow), R(leftLow), 
					   I(32 - shift), I(shift), I(31)));
      burs.append(MIR_RotateAndMask.create(PPC_RLWIMI, R(defLow), R(defLow), 
					   R(leftHigh), I(32 - shift), 
					   I(0), I(shift - 1)));
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, R(defHigh), 
					   R(leftHigh),
 					   I(32 - shift), I(shift), I(31)));
    } else if (shift == 32) {
      burs.append(MIR_Move.create(PPC_MOVE, R(defLow), R(leftHigh)));
      burs.append(MIR_Unary.create(PPC_LDI, R(defHigh), I(0)));
    } else {
      shift = shift - 32;
      burs.append(MIR_Binary.create(PPC_SRWI, R(defLow), R(leftHigh), 
				    I(shift)));
      burs.append(MIR_Unary.create(PPC_LDI, R(defHigh), I(0)));
    }
  }

  void LONG_SHR_IMM(OPT_BURS burs, OPT_Instruction s, 
		    OPT_RegisterOperand def, 
		    OPT_RegisterOperand left, 
		    OPT_IntConstantOperand right) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = burs.ir.regpool.getSecondReg(leftHigh);
    int shift = right.value & 0x3f;
    if (shift < 32) {
      burs.append(MIR_RotateAndMask.create(PPC_RLWINM, R(defLow), R(leftLow), 
					   I(32 - shift), I(shift), I(31)));
      burs.append(MIR_RotateAndMask.create(PPC_RLWIMI, R(defLow), R(defLow), 
					   R(leftHigh), I(32 - shift), I(0), 
					   I(shift - 1)));
      burs.append(MIR_Binary.create(PPC_SRAWI, R(defHigh), R(leftHigh), 
				    I(shift)));
    } else if (shift == 32) {
      burs.append(MIR_Move.create(PPC_MOVE, R(defLow), R(leftHigh)));
      burs.append(MIR_Binary.create(PPC_SRAWI, R(defHigh), R(leftHigh), 
				    I(31)));
    } else {
      shift = shift - 32;
      burs.append(MIR_Binary.create(PPC_SRAWI, R(defLow), R(leftHigh), 
				    I(shift)));
      burs.append(MIR_Binary.create(PPC_SRAWI, R(defHigh), R(leftHigh), 
				    I(31)));
    }
  }

  void LONG_MUL(OPT_BURS burs, OPT_Instruction s, 
		OPT_RegisterOperand def, 
		OPT_RegisterOperand left, OPT_RegisterOperand right) {
    OPT_Register dH = def.register;
    OPT_Register dL = burs.ir.regpool.getSecondReg(dH);
    OPT_Register lH = left.register;
    OPT_Register lL = burs.ir.regpool.getSecondReg(lH);
    OPT_Register rH = right.register;
    OPT_Register rL = burs.ir.regpool.getSecondReg(rH);
    OPT_RegisterPool regpool = burs.ir.regpool;
    OPT_Register tH = regpool.getInteger();
    OPT_Register t = regpool.getInteger();
    burs.append(MIR_Binary.create(PPC_MULHWU, R(tH), R(lL), R(rL)));
    burs.append(MIR_Binary.create(PPC_MULLW, R(t), R(lL), R(rH)));
    burs.append(MIR_Binary.create(PPC_ADD, R(tH), R(tH), R(t)));
    burs.append(MIR_Binary.create(PPC_MULLW, R(t), R(lH), R(rL)));
    burs.append(MIR_Binary.create(PPC_ADD, R(dH), R(tH), R(t)));
    burs.append(MIR_Binary.create(PPC_MULLW, R(dL), R(lL), R(rL)));
  }

  void LONG_LOAD_addi(OPT_BURS burs, OPT_Instruction s, 
		      OPT_RegisterOperand def, 
		      OPT_RegisterOperand left, 
		      OPT_IntConstantOperand right, 
		      OPT_LocationOperand loc, 
		      OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    int imm = right.value;
    if (VM.VerifyAssertions) VM.assert(imm < (0x8000 - 4));
    OPT_Instruction inst = OPT_IRTools.nonPEIGC (MIR_Load.create
						    (PPC_LWZ, 
						     R(defHigh), 
						     left, I(imm), loc, 
						     guard));
    inst.copyPosition(s);
    burs.append(inst);
    if (loc != null) {
      loc = (OPT_LocationOperand)loc.copy();
    }
    inst = OPT_IRTools.nonPEIGC (MIR_Load.create(PPC_LWZ, 
						    R(defLow), 
						    left.copyU2U(), 
						    I(imm + 4), loc, 
						    guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void LONG_LOAD_addis(OPT_BURS burs, OPT_Instruction s, 
		       OPT_RegisterOperand def, 
		       OPT_RegisterOperand left, 
		       OPT_RegisterOperand right, 
		       OPT_IntConstantOperand Value, 
		       OPT_LocationOperand loc, OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    int value = Value.value;
    burs.append(MIR_Binary.create(PPC_ADDIS, right, left, 
				  I(OPT_Bits.PPCMaskUpper16(value))));
    OPT_Instruction inst = OPT_IRTools.nonPEIGC(MIR_Load.create 
						   (PPC_LWZ, R(defHigh), 
						    right.copyD2U(), 
						    I(OPT_Bits.
						      PPCMaskLower16(value)), 
						    loc, guard));
    inst.copyPosition(s);
    burs.append(inst);
    if (loc != null) {
      loc = (OPT_LocationOperand)loc.copy();
    }
    inst = OPT_IRTools.nonPEIGC(MIR_Load.create(PPC_LWZ, R(defLow), 
						   right.copyD2U(), 
						   I(OPT_Bits.PPCMaskLower16
						     (value) + 4), loc, 
						   guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void LONG_LOAD_addx(OPT_BURS burs, OPT_Instruction s, 
		      OPT_RegisterOperand def, 
		      OPT_RegisterOperand left, 
		      OPT_RegisterOperand right, 
		      OPT_LocationOperand loc, 
		      OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Instruction inst = OPT_IRTools.nonPEIGC(MIR_Load.create
						   (PPC_LWZX, R(defHigh), 
						    left, right, loc, 
						    guard));
    inst.copyPosition(s);
    burs.append(inst);
    OPT_RegisterOperand kk = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Binary.create(PPC_ADDI, kk, right.copyU2U(), I(4)));
    if (loc != null)
      loc = (OPT_LocationOperand)loc.copy();
    inst = OPT_IRTools.nonPEIGC(MIR_Load.create(PPC_LWZX, R(defLow), 
						   left.copyU2U(), 
						   kk.copyD2U(), loc, 
						   guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void LONG_STORE_addi(OPT_BURS burs, OPT_Instruction s, 
		       OPT_RegisterOperand def, 
		       OPT_RegisterOperand left, 
		       OPT_IntConstantOperand right, 
		       OPT_LocationOperand loc, 
		       OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    int imm = right.value;
    if (VM.VerifyAssertions)
      VM.assert(imm < (0x8000 - 4));
    OPT_Instruction inst = 
      OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STW, R(defHigh), 
					       left, I(imm), loc, guard));
    inst.copyPosition(s);
    burs.append(inst);
    if (loc != null)
      loc = (OPT_LocationOperand)loc.copy();
    inst = OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STW, R(defLow), 
						    left.copyU2U(), 
						    I(imm + 4), loc, 
						    guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void LONG_STORE_addis(OPT_BURS burs, OPT_Instruction s, 
			OPT_RegisterOperand def, 
			OPT_RegisterOperand left, 
			OPT_RegisterOperand right, 
			OPT_IntConstantOperand Value, 
			OPT_LocationOperand loc, OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    int value = Value.value;
    burs.append(MIR_Binary.create(PPC_ADDIS, right, left, 
				  I(OPT_Bits.PPCMaskUpper16(value))));
    OPT_Instruction inst = 
      OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STW, R(defHigh), 
					       right.copyD2U(), 
					       I(OPT_Bits.PPCMaskLower16
						 (value)), 
					       loc, guard));
    inst.copyPosition(s);
    burs.append(inst);
    if (loc != null)
      loc = (OPT_LocationOperand)loc.copy();
    inst = OPT_IRTools.nonPEIGC
      (MIR_Store.create(PPC_STW, R(defLow), right.copyD2U(), 
                        I(OPT_Bits.PPCMaskLower16(value) + 4), loc, guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void LONG_STORE_addx(OPT_BURS burs, OPT_Instruction s, 
		       OPT_RegisterOperand def, 
		       OPT_RegisterOperand left, 
		       OPT_RegisterOperand right, 
		       OPT_LocationOperand loc, 
		       OPT_Operand guard) {
    OPT_Register defHigh = def.register;
    OPT_Register defLow = burs.ir.regpool.getSecondReg(defHigh);
    OPT_Instruction inst = 
      OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STWX, R(defHigh), left, 
					       right, loc, guard));
    inst.copyPosition(s);
    burs.append(inst);
    OPT_RegisterOperand kk = burs.ir.regpool.makeTempInt();
    burs.append(MIR_Binary.create(PPC_ADDI, kk, right.copyU2U(), I(4)));
    if (loc != null)
      loc = (OPT_LocationOperand)loc.copy();
    inst = OPT_IRTools.nonPEIGC(MIR_Store.create(PPC_STWX, R(defLow), 
						    left.copyU2U(), 
						    kk.copyD2U(), loc, 
						    guard));
    inst.copyPosition(s);
    burs.append(inst);
  }

  void DOUBLE_IFCMP(OPT_BURS burs, OPT_Instruction s, OPT_Operator op, 
		    OPT_RegisterOperand left, OPT_Operand right) {
    boolean UeqL = (op == DOUBLE_CMPL) || (op == FLOAT_CMPL);
    boolean UeqG = (op == DOUBLE_CMPG) || (op == FLOAT_CMPG);
    OPT_ConditionOperand c = IfCmp.getCond(s);
    OPT_BranchOperand target = IfCmp.getTarget(s);
    OPT_BranchOperand branch = null;
    OPT_BasicBlock bb = s.getBasicBlock();
    if (c.value == OPT_ConditionOperand.EQUAL || 
	(UeqL && (c.value == OPT_ConditionOperand.GREATER || 
		  c.value == OPT_ConditionOperand.GREATER_EQUAL)) || 
	(UeqG && (c.value == OPT_ConditionOperand.LESS || 
		  c.value == OPT_ConditionOperand.LESS_EQUAL))) {
      OPT_Instruction lastInstr = bb.lastRealInstruction();
      if (lastInstr.operator() == GOTO) {
        // We're in trouble if there is another instruction between 
        // s and lastInstr!
        if (VM.VerifyAssertions)
          VM.assert(s.nextInstructionInCodeOrder() == lastInstr);
        // Set branch = target of GOTO
        branch = (OPT_BranchOperand)Goto.getTarget(lastInstr);
      } else {
        // Set branch = label of next (fallthrough basic block)
        branch = bb.nextBasicBlockInCodeOrder().makeJumpTarget();
      }
    } else {
      branch = (OPT_BranchOperand)target.copy();
    }
    OPT_RegisterOperand cr = burs.ir.regpool.makeTempCondition();
    burs.append(MIR_Binary.create(PPC_FCMPU, cr, left, right));

    // Propagate branch probabilities as follows:  assume the
    // probability of overflow (first condition) is zero, and
    // propagate the original probability to the second condition.
    burs.append(MIR_CondBranch2.create(PPC_BCOND2, cr.copyD2U(), 
				       OPT_PowerPCConditionOperand.UNORDERED(),
				       branch, 
				       new OPT_BranchProfileOperand(0f),
				       new OPT_PowerPCConditionOperand(c), 
				       target,
				       IfCmp.getBranchProfile(s)));
  }


  /**
   * Expansion of LOWTABLESWITCH.  
   *
   * @param burs an OPT_BURS object
   * @param s the instruction to expand
   */
  final void LOWTABLESWITCH(OPT_BURS burs, OPT_Instruction s) {
    // (1) We're changing index from a U to a DU.
    //     Inject a fresh copy instruction to make sure we aren't
    //     going to get into trouble (if someone else was also using index).
    OPT_RegisterOperand newIndex = burs.ir.regpool.makeTempInt(); 
    burs.append(MIR_Move.create(PPC_MOVE, newIndex, LowTableSwitch.getIndex(s))); 
    int number = LowTableSwitch.getNumberOfTargets(s);
    OPT_Instruction s2 = CPOS(s,MIR_LowTableSwitch.create(MIR_LOWTABLESWITCH, newIndex, number*2));
    for (int i=0; i<number; i++) {
      MIR_LowTableSwitch.setTarget(s2,i,LowTableSwitch.getTarget(s,i));
      MIR_LowTableSwitch.setBranchProfile(s2,i,LowTableSwitch.getBranchProfile(s,i));
    }
    burs.append(s2);
  }




  // Take the generic LIR trap_if and coerce into the limited vocabulary
  // understand by C trap handler on PPC.  See VM_TrapConstants.java.
  // Also see OPT_ConvertToLowLevelIR.java which generates most of these TRAP_IFs.
  void TRAP_IF(OPT_BURS burs, OPT_Instruction s) {
    OPT_RegisterOperand gRes = TrapIf.getClearGuardResult(s);
    OPT_RegisterOperand v1 = (OPT_RegisterOperand)TrapIf.getClearVal1(s);
    OPT_RegisterOperand v2 = (OPT_RegisterOperand)TrapIf.getClearVal2(s);
    OPT_ConditionOperand cond = TrapIf.getClearCond(s);
    OPT_TrapCodeOperand tc = TrapIf.getClearTCode(s);
    
    switch(tc.getTrapCode()) {
    case VM_Runtime.TRAP_ARRAY_BOUNDS:
      {
	if (cond.isLOWER_EQUAL()) {
	  burs.append(MIR_Trap.mutate(s, PPC_TW, gRes, 
				      new OPT_PowerPCTrapOperand(cond),
				      v1, v2, tc));
	} else {
	  throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
	}
      }
      break;
    default:
      throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
    }
  }
  

  // Take the generic LIR trap_if and coerce into the limited vocabulary
  // understand by C trap handler on PPC.  See VM_TrapConstants.java.
  // Also see OPT_ConvertToLowLevelIR.java which generates most of these TRAP_IFs.
  void TRAP_IF_IMM(OPT_BURS burs, OPT_Instruction s) {
    OPT_RegisterOperand gRes = TrapIf.getClearGuardResult(s);
    OPT_RegisterOperand v1 =  (OPT_RegisterOperand)TrapIf.getClearVal1(s);
    OPT_IntConstantOperand v2 = (OPT_IntConstantOperand)TrapIf.getClearVal2(s);
    OPT_ConditionOperand cond = TrapIf.getClearCond(s);
    OPT_TrapCodeOperand tc = TrapIf.getClearTCode(s);

    switch(tc.getTrapCode()) {
    case VM_Runtime.TRAP_ARRAY_BOUNDS:
      {
	if (cond.isLOWER_EQUAL()) {
	  burs.append(MIR_Trap.mutate(s, PPC_TWI, gRes, 
				      new OPT_PowerPCTrapOperand(cond),
				      v1, v2, tc));
	} else if (cond.isHIGHER_EQUAL()) {
	  // have flip the operands and use non-immediate so trap handler can recognize.
	  OPT_RegisterOperand tmp = burs.ir.regpool.makeTempInt();
	  IntConstant(burs, tmp.register, v2.value);
	  burs.append(MIR_Trap.mutate(s, PPC_TW, gRes,
				      new OPT_PowerPCTrapOperand(cond.flipOperands()),
				      tmp, v1, tc)); 
	} else {
	  throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
	}
      }
      break;
    case VM_Runtime.TRAP_DIVIDE_BY_ZERO:
      {
	// A slightly ugly matter, but we need to deal with combining
	// the two pieces of a long register from a LONG_ZERO_CHECK.  
	// A little awkward, but probably the easiest workaround...
	if (v1.type == VM_Type.LongType) {
	  OPT_RegisterOperand rr = burs.ir.regpool.makeTempInt();
	  burs.append(MIR_Binary.create(PPC_OR, rr, v1, 
					R(burs.ir.regpool.getSecondReg(v1.register))));
	  v1 = rr.copyD2U();
	}
	
	if (cond.isEQUAL() && v2.value == 0) {
	  burs.append(MIR_Trap.mutate(s, PPC_TWI, gRes, 
				      new OPT_PowerPCTrapOperand(cond),
				      v1, v2, tc));
	} else {
	  throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
	}
      }
      break;

    default:
      throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
    }
  }



  // Take the generic LIR trap and coerce into the limited vocabulary
  // understand by C trap handler on PPC.  See VM_TrapConstants.java.
  void TRAP(OPT_BURS burs, OPT_Instruction s) {
    OPT_RegisterOperand gRes = Trap.getClearGuardResult(s);
    OPT_TrapCodeOperand tc = Trap.getClearTCode(s);
    switch(tc.getTrapCode()) {
    case VM_Runtime.TRAP_NULL_POINTER:
      {
	VM_Method target = VM_Entrypoints.raiseNullPointerException;
	mutateTrapToCall(burs, s, target);
      }
      break;
    case VM_Runtime.TRAP_ARRAY_BOUNDS:
      {
	VM_Method target = VM_Entrypoints.raiseArrayBoundsException;
	mutateTrapToCall(burs, s, target);
      }
      break;
    case VM_Runtime.TRAP_DIVIDE_BY_ZERO:
      {
	VM_Method target = VM_Entrypoints.raiseArithmeticException;
	mutateTrapToCall(burs, s, target);
      }
      break;
    case VM_Runtime.TRAP_CHECKCAST:
      {
	burs.append(MIR_Trap.mutate(s, PPC_TWI, gRes, 
				    OPT_PowerPCTrapOperand.ALWAYS(),
				    R(12), I(VM_TrapConstants.CHECKCAST_TRAP & 0xffff), tc));
      }
      break;
    case VM_Runtime.TRAP_MUST_IMPLEMENT:
      {
	burs.append(MIR_Trap.mutate(s, PPC_TWI, gRes, 
				    OPT_PowerPCTrapOperand.ALWAYS(),
				    R(12), I(VM_TrapConstants.MUST_IMPLEMENT_TRAP & 0xffff), tc));
      }
      break;
    case VM_Runtime.TRAP_STORE_CHECK:
      {
	burs.append(MIR_Trap.mutate(s, PPC_TWI, gRes, 
				    OPT_PowerPCTrapOperand.ALWAYS(),
				    R(12), I(VM_TrapConstants.STORE_CHECK_TRAP & 0xffff), tc));
      }
      break;
    default:
      throw new OPT_OptimizingCompilerException("Unexpected case of trap_if"+s);
    }
  }


  private void mutateTrapToCall(OPT_BURS burs, 
				OPT_Instruction s, 
				VM_Method target) {
    int offset = target.getOffset();
    OPT_RegisterOperand tmp = burs.ir.regpool.makeTemp(OPT_ClassLoaderProxy.JavaLangObjectArrayType);
    OPT_Register JTOC = burs.ir.regpool.getPhysicalRegisterSet().getJTOC();
    OPT_MethodOperand meth = OPT_MethodOperand.STATIC(target);
    meth.setIsNonReturningCall(true);
    if (SI16(offset)) {
      burs.append(OPT_IRTools.nonPEIGC(MIR_Load.create(PPC_LWZ, tmp, R(JTOC), I(offset))));
    } else {
      OPT_RegisterOperand tmp2 = burs.ir.regpool.makeTempInt();
      IntConstant(burs, tmp2.register, offset);
      burs.append(OPT_IRTools.nonPEIGC(MIR_Load.create(PPC_LWZX, tmp, R(JTOC), tmp2)));
    }
    burs.append(MIR_Move.create(PPC_MTSPR, R(CTR), tmp.copyD2U()));
    burs.append(MIR_Call.mutate0(s, PPC_BCTRL, null, null, meth));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Handles the conversion from LIR to MIR of operators whose 
 * expansion requires the introduction of new control flow (new basic blocks).
 * 
 * @author Dave Grove
 * @author Mauricio J. Serrano
 * @modified Vivek Sarkar
 * @modified Igor Pechtchanski
 * @modified Martin Trapp
 * @modified Stephen Fink
 */
abstract class OPT_ComplexLIR2MIRExpansion extends OPT_IRTools {

  /**
   * Converts the given IR to low level PowerPC IR.
   *
   * @param ir IR to convert
   */
  public static void convert (OPT_IR ir) {
    for (OPT_Instruction next = null, s = ir.firstInstructionInCodeOrder(); 
	 s != null; 
	 s = next) {
      next = s.nextInstructionInCodeOrder();
      switch (s.getOpcode()) {
      case DOUBLE_2INT_opcode:case FLOAT_2INT_opcode:
	double_2int(s, ir);
	break;
      case LONG_SHR_opcode:
	long_shr(s, ir);
	break;
      case LONG_IFCMP_opcode:
	long_ifcmp(s, ir);
	break;
      case BOOLEAN_CMP_opcode:
	boolean_cmp(s, ir);
	break;
      case DOUBLE_CMPL_opcode:
      case FLOAT_CMPL_opcode:
      case DOUBLE_CMPG_opcode:
      case FLOAT_CMPG_opcode:
	threeValueCmp(s, ir);
	break;
      case LONG_CMP_opcode:
	threeValueLongCmp(s, ir);
	break;
      case GET_TIME_BASE_opcode:
	get_time_base(s, ir);
	break;
      case ATTEMPT_opcode:
        attempt(s, ir);
        break;
      }
    }
    OPT_DefUse.recomputeSpansBasicBlock(ir);
  }

  private static void double_2int (OPT_Instruction s, OPT_IR ir) {
    OPT_Register res = Unary.getResult(s).register;
    OPT_Register src = ((OPT_RegisterOperand)Unary.getVal(s)).register;
    OPT_Register FP = ir.regpool.getPhysicalRegisterSet().getFP();
    int p = ir.stackManager.allocateSpaceForConversion();
    OPT_Register temp = ir.regpool.getDouble();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB3 = BB1.splitNodeAt(s, ir);
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_RegisterOperand cond = ir.regpool.makeTempCondition();
    BB1.appendInstruction(MIR_Binary.create(PPC_FCMPU, cond, D(src), D(src)));
    BB1.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(0)));
    BB1.appendInstruction(MIR_CondBranch.create(PPC_BCOND, cond.copyD2U(), 
						OPT_PowerPCConditionOperand.UNORDERED(), BB3.makeJumpTarget(),
						new OPT_BranchProfileOperand()));
    BB2.appendInstruction(MIR_Unary.create(PPC_FCTIWZ, D(temp), D(src)));
    BB2.appendInstruction(nonPEIGC(MIR_Store.create(PPC_STFD, D(temp), 
						    R(FP), I(p))));
    BB2.appendInstruction(nonPEIGC(MIR_Load.create(PPC_LWZ, R(res), R(FP), 
						   I(p + 4))));
    // fix up CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB3);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    s.remove();
  }

  private static void boolean_cmp (OPT_Instruction s, OPT_IR ir) {
    // undo the optimization because it cannot efficiently be generated
    OPT_Register res = BooleanCmp.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand)BooleanCmp.getClearVal1(s);
    OPT_Operand two = BooleanCmp.getClearVal2(s);
    OPT_ConditionOperand cond = BooleanCmp.getClearCond(s);
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB4 = BB1.splitNodeAt(s, ir);
    s = s.remove();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.createSubBlock(0, ir);
    OPT_RegisterOperand t = ir.regpool.makeTempInt();
    t.register.setCondition();
    OPT_Operator op;
    if (two instanceof OPT_IntConstantOperand) {
      op = cond.isUNSIGNED() ? PPC_CMPLI : PPC_CMPI;
    } else { 
      op = cond.isUNSIGNED() ? PPC_CMPL : PPC_CMP;
    }
    BB1.appendInstruction(MIR_Binary.create(op, t, one, two));
    BB1.appendInstruction(MIR_CondBranch.create(PPC_BCOND, t.copyD2U(), 
						OPT_PowerPCConditionOperand.get(cond), BB3.makeJumpTarget(),
						new OPT_BranchProfileOperand()));
    BB2.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(0)));
    BB2.appendInstruction(MIR_Branch.create(PPC_B, BB4.makeJumpTarget()));
    BB3.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(1)));
    // fix CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB4);
    BB3.insertOut(BB4);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);
  }

  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static void threeValueCmp (OPT_Instruction s, OPT_IR ir) {
    OPT_PowerPCConditionOperand
      firstCond = OPT_PowerPCConditionOperand.LESS_EQUAL();;
    int firstConst = 1;
      
    switch (s.operator.opcode) {
    case DOUBLE_CMPG_opcode:
    case FLOAT_CMPG_opcode:
      firstCond  = OPT_PowerPCConditionOperand.GREATER_EQUAL();
      firstConst = -1;
      break;
    case DOUBLE_CMPL_opcode:
    case FLOAT_CMPL_opcode:
      break;
    default: if (VM.VerifyAssertions) VM.assert (false);
      break;
    }
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand two = (OPT_RegisterOperand) Binary.getClearVal2(s);
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s = s.remove();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB4 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB5 = BB1.createSubBlock(0, ir);
    OPT_RegisterOperand t = ir.regpool.makeTempInt();
    t.register.setCondition();
    BB1.appendInstruction(MIR_Binary.create(PPC_FCMPU, t, one, two));
    BB1.appendInstruction
      (MIR_CondBranch.create(PPC_BCOND, t.copyD2U(), firstCond,
			     BB3.makeJumpTarget(),
			     new OPT_BranchProfileOperand(0.5f)));
    BB2.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(firstConst)));
    BB2.appendInstruction(MIR_Branch.create(PPC_B, BB6.makeJumpTarget()));
    BB3.appendInstruction
      (MIR_CondBranch.create(PPC_BCOND, t.copyD2U(),
			     OPT_PowerPCConditionOperand.EQUAL(),
			     BB5.makeJumpTarget(),
			     OPT_BranchProfileOperand.unlikely()));

    BB4.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(-firstConst)));
    BB4.appendInstruction(MIR_Branch.create(PPC_B, BB6.makeJumpTarget()));
    BB5.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(0)));
    // fix CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB6);
    BB3.insertOut(BB4);
    BB3.insertOut(BB5);
    BB4.insertOut(BB6);
    BB5.insertOut(BB6);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);
    ir.cfg.linkInCodeOrder(BB4, BB5);
    ir.cfg.linkInCodeOrder(BB5, BB6);
  }
  /**
   * compare to values and set result to -1, 0, 1 for <, =, >, respectively
   * @param s the compare instruction
   * @param ir the governing IR
   */
  private static void threeValueLongCmp (OPT_Instruction s, OPT_IR ir) {
    OPT_Register res = Binary.getClearResult(s).register;
    OPT_RegisterOperand one = (OPT_RegisterOperand) Binary.getClearVal1(s);
    OPT_RegisterOperand two = (OPT_RegisterOperand) Binary.getClearVal2(s);
    OPT_RegisterOperand lone = L(ir.regpool.getSecondReg(one.register));
    OPT_RegisterOperand ltwo = L(ir.regpool.getSecondReg(two.register));
    res.setSpansBasicBlock();
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB6 = BB1.splitNodeAt(s, ir);
    s = s.remove();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB4 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB5 = BB1.createSubBlock(0, ir);
    OPT_RegisterOperand t = ir.regpool.makeTempInt();
    t.register.setCondition();
    BB1.appendInstruction(MIR_Binary.create(PPC_CMP, t, one, two));
    BB1.appendInstruction
      (MIR_CondBranch2.create(PPC_BCOND2, t.copyD2U(),
			      OPT_PowerPCConditionOperand.LESS(),
			      BB4.makeJumpTarget(),
			      new OPT_BranchProfileOperand(0.49f),
			      OPT_PowerPCConditionOperand.GREATER(),
			      BB5.makeJumpTarget(),
			      new OPT_BranchProfileOperand(0.49f)));
    BB2.appendInstruction(MIR_Binary.create(PPC_CMPL, t.copyD2D(), lone, ltwo));
    BB2.appendInstruction
      (MIR_CondBranch2.create(PPC_BCOND2, t.copyD2U(),
			      OPT_PowerPCConditionOperand.LESS(),
			      BB4.makeJumpTarget(),
			      new OPT_BranchProfileOperand(0.49f),
			      OPT_PowerPCConditionOperand.GREATER(),
			      BB5.makeJumpTarget(),
			      new OPT_BranchProfileOperand(0.49f)));
    BB3.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(0)));
    BB3.appendInstruction(MIR_Branch.create(PPC_B, BB6.makeJumpTarget()));
    BB4.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(-1)));
    BB4.appendInstruction(MIR_Branch.create(PPC_B, BB6.makeJumpTarget()));
    BB5.appendInstruction(MIR_Unary.create(PPC_LDI, R(res), I(1)));
    // fix CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB4);
    BB1.insertOut(BB5);
    BB2.insertOut(BB3);
    BB2.insertOut(BB4);
    BB2.insertOut(BB5);
    BB3.insertOut(BB6);
    BB4.insertOut(BB6);
    BB5.insertOut(BB6);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);
    ir.cfg.linkInCodeOrder(BB4, BB5);
    ir.cfg.linkInCodeOrder(BB5, BB6);
  }

  
  private static void long_shr (OPT_Instruction s, OPT_IR ir) {
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB1.splitNodeAt(s, ir);
    OPT_Register defHigh = Binary.getResult(s).register;
    OPT_Register defLow = ir.regpool.getSecondReg(defHigh);
    OPT_RegisterOperand left = (OPT_RegisterOperand)Binary.getVal1(s);
    OPT_Register leftHigh = left.register;
    OPT_Register leftLow = ir.regpool.getSecondReg(leftHigh);
    OPT_RegisterOperand shiftOp = (OPT_RegisterOperand)Binary.getVal2(s);
    OPT_Register shift = shiftOp.register;
    OPT_Register t31 = ir.regpool.getInteger();
    OPT_Register t0 = ir.regpool.getInteger();
    OPT_Register cr = ir.regpool.getCondition();
    defLow.setSpansBasicBlock();
    defHigh.setSpansBasicBlock();
    s.insertBack(MIR_Binary.create(PPC_SUBFIC, R(t31), R(shift), I(32)));
    s.insertBack(MIR_Binary.create(PPC_SRW, R(defLow), R(leftLow), R(shift)));
    s.insertBack(MIR_Binary.create(PPC_SLW, R(t0), R(leftHigh), R(t31)));
    s.insertBack(MIR_Binary.create(PPC_OR, R(defLow), R(defLow), R(t0)));
    s.insertBack(MIR_Binary.create(PPC_ADDI, R(t31), R(shift), I(-32)));
    s.insertBack(MIR_Binary.create(PPC_SRAW, R(t0), R(leftHigh), R(t31)));
    s.insertBack(MIR_Binary.create(PPC_SRAW, R(defHigh), R(leftHigh), 
				   R(shift)));
    s.insertBack(MIR_Binary.create(PPC_CMPI, R(cr), R(t31), I(0)));
    MIR_CondBranch.mutate(s, PPC_BCOND, R(cr), 
			  OPT_PowerPCConditionOperand.LESS_EQUAL(), 
			  BB3.makeJumpTarget(),
			  new OPT_BranchProfileOperand());
    // insert the branch and second compare
    BB2.appendInstruction(MIR_Move.create(PPC_MOVE, R(defLow), R(t0)));
    // fix up CFG
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB3);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
  }


  private static void long_ifcmp(OPT_Instruction s, OPT_IR ir) {
    if (VM.VerifyAssertions) VM.assert(!IfCmp.getCond(s).isUNSIGNED());
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB3 = BB1.splitNodeAt(s, ir);
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB3);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);

    s.remove(); // s is in BB1, we'll mutate it and insert in BB3 below.

    OPT_RegisterOperand cr = ir.regpool.makeTempCondition();
    OPT_RegisterOperand val1 = (OPT_RegisterOperand)IfCmp.getClearVal1(s);
    OPT_RegisterOperand val2 = (OPT_RegisterOperand)IfCmp.getClearVal2(s);
    OPT_RegisterOperand lval1 = L(ir.regpool.getSecondReg(val1.register));
    OPT_RegisterOperand lval2 = L(ir.regpool.getSecondReg(val2.register));
    OPT_PowerPCConditionOperand cond = 
      new OPT_PowerPCConditionOperand(IfCmp.getCond(s));
    BB1.appendInstruction(MIR_Binary.create(PPC_CMP, cr, val1, val2));
    BB1.appendInstruction(MIR_CondBranch.create(PPC_BCOND, cr.copyD2U(),
						OPT_PowerPCConditionOperand.NOT_EQUAL(),
						BB3.makeJumpTarget(),
						new OPT_BranchProfileOperand()));
    BB2.appendInstruction(MIR_Binary.create(PPC_CMPL, cr.copyD2D(),
					    lval1, lval2));
    BB3.prependInstruction(MIR_CondBranch.mutate(s, PPC_BCOND, cr.copyD2U(),
						 cond,
						 IfCmp.getTarget(s),
						 IfCmp.getBranchProfile(s)));
  }

  private static void get_time_base (OPT_Instruction s, OPT_IR ir) {
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB2 = BB1.splitNodeAt(s, ir);
    OPT_Register defHigh = Nullary.getResult(s).register;
    OPT_Register defLow = ir.regpool.getSecondReg(defHigh);
    OPT_Register t0 = ir.regpool.getInteger();
    OPT_Register cr = ir.regpool.getCondition();
    defLow.setSpansBasicBlock();
    defHigh.setSpansBasicBlock();
    // Try to get the base
    OPT_Register TU = ir.regpool.getPhysicalRegisterSet().getTU();
    OPT_Register TL = ir.regpool.getPhysicalRegisterSet().getTL();
    s.insertBack(MIR_Move.create(PPC_MFTBU, R(defHigh), R(TU)));
    s.insertBack(MIR_Move.create(PPC_MFTB, R(defLow), R(TL)));
    // Try again to see if it changed
    s.insertBack(MIR_Move.create(PPC_MFTBU, R(t0), R(TU)));
    s.insertBack(MIR_Binary.create(PPC_CMP, R(cr), R(t0), R(defHigh)));
    MIR_CondBranch.mutate(s, PPC_BCOND, R(cr), 
			  OPT_PowerPCConditionOperand.NOT_EQUAL(), 
			  BB2.makeJumpTarget(),
			  new OPT_BranchProfileOperand());
    // fix up CFG
    BB1.insertOut(BB2);
    BB2.insertOut(BB2);
    ir.cfg.linkInCodeOrder(BB1, BB2);
  }

  private static void attempt(OPT_Instruction s, OPT_IR ir) {
    OPT_BasicBlock BB1 = s.getBasicBlock();
    OPT_BasicBlock BB4 = BB1.splitNodeAt(s, ir);
    OPT_BasicBlock BB2 = BB1.createSubBlock(0, ir);
    OPT_BasicBlock BB3 = BB2.createSubBlock(0, ir);
    BB1.insertOut(BB2);
    BB1.insertOut(BB3);
    BB2.insertOut(BB4);
    BB3.insertOut(BB4);
    ir.cfg.linkInCodeOrder(BB1, BB2);
    ir.cfg.linkInCodeOrder(BB2, BB3);
    ir.cfg.linkInCodeOrder(BB3, BB4);

    // mutate ATTEMPT into a STWCX
    OPT_RegisterOperand newValue = (OPT_RegisterOperand)Attempt.getNewValue(s);
    OPT_RegisterOperand address = (OPT_RegisterOperand)Attempt.getAddress(s);
    OPT_Operand offset = Attempt.getOffset(s);
    OPT_LocationOperand location = Attempt.getLocation(s);
    OPT_Operand guard = Attempt.getGuard(s);
    OPT_RegisterOperand result = Attempt.getResult(s);
    MIR_Store.mutate(s,PPC_STWCXr, newValue, address, offset, location,
                     guard);

    
    // Branch to BB3 iff the STWXC succeeds (CR(0) is EQUAL)
    // Else fall through to BB2
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    BB1.appendInstruction(MIR_CondBranch.create(PPC_BCOND,
                                                R(phys.getConditionRegister(0)),
						OPT_PowerPCConditionOperand.EQUAL(),
						BB3.makeJumpTarget(),
						new OPT_BranchProfileOperand()));
    // BB2 sets result to FALSE and jumps to BB4
    BB3.appendInstruction(MIR_Unary.create(PPC_LDI, result.copyRO(), I(0)));
    BB2.appendInstruction(MIR_Branch.create(PPC_B, BB4.makeJumpTarget()));
    
    // BB3 sets result to TRUE and falls through to BB4
    BB3.appendInstruction(MIR_Unary.create(PPC_LDI, result.copyRO(), I(1)));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Nothing to do on PowerPC.
 *
 * @author Dave Grove
 */
final class OPT_ConvertALUOperators extends OPT_CompilerPhase 
  implements OPT_Operators {

  final String getName() { return "ConvertALUOps"; }
  final OPT_CompilerPhase newExecution(OPT_IR ir) { return this; }
  final boolean printingEnabled (OPT_Options options, boolean before) {
    return false;
  }

  final void perform(OPT_IR ir) { 
    // Nothing to do on PPC
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Normalize the use of constants in the LIR
 * to match the patterns supported in LIR2MIR.rules
 *
 * @author Dave Grove, Mauricio J. Serrano, Martin Trapp
 */
abstract class OPT_NormalizeConstants extends OPT_IRTools {
  /**
   * lower bound on int immediate values in
   * an instruction (can use values down
   * to and including this immed)
   */
  static final int LOWER_IMMEDIATE = -(1 << 15);
  /** upper bound on int immediate values in
   * an instruction (can use values up
   * to and including this immed)
   */
  static final int UPPER_IMMEDIATE = (1 << 15) - 1;
  /** upper bound on unsigned int immediate values in
   * an instruction (can use values up
   * to and including this immed)
   * NOTE: used in INT_AND, INT_OR, INT_XOR
   */
  static final int UNSIGNED_UPPER_IMMEDIATE = (1 << 16) - 1;

  /**
   * Doit.
   *
   * @param ir IR to normalize
   */
  static void perform(OPT_IR ir) {
    // This code assumes that INT/LONG constant folding in OPT_Simplifier is enabled.
    // This greatly reduces the number of cases we have to worry about below.
    if (!(OPT_Simplifier.CF_INT && OPT_Simplifier.CF_LONG)) {
      throw  new OPT_OptimizingCompilerException("Unexpected config!");
    }
    for (OPT_Instruction s = ir.firstInstructionInCodeOrder(); 
	 s != null; 
	 s = s.nextInstructionInCodeOrder()) {

      // STEP ONE: Get 'large' constants into a form that the PPC BURS rules
      //           are prepared to deal with. 
      // Constants can't appear as defs, so only scan the uses.
      //
      int numUses = s.getNumberOfUses();
      if (numUses > 0) {
        int numDefs = s.getNumberOfDefs();
        for (int idx = numDefs; idx < numUses + numDefs; idx++) {
          OPT_Operand use = s.getOperand(idx);
          if (use != null) {
            if (use instanceof OPT_StringConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.JavaLangStringType);
	      OPT_RegisterOperand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_StringConstantOperand sc = (OPT_StringConstantOperand)use;
              int offset = sc.index << 2;
              if (offset == 0)
                throw  new OPT_OptimizingCompilerException("String constant w/o valid JTOC offset");
              OPT_LocationOperand loc = new OPT_LocationOperand(offset);
	      s.insertBefore(Load.create(INT_LOAD, rop, jtoc, asImmediateOrReg(I(offset), s, ir), loc));
	      s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_LongConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.LongType);
	      use.clear();
	      s.insertBefore(Move.create(LONG_MOVE, rop, use));
	      s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_DoubleConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.DoubleType);
	      OPT_RegisterOperand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_DoubleConstantOperand dc = (OPT_DoubleConstantOperand)use;
              int index = dc.index;
              if (index == 0) {
                index = VM_Statics.findOrCreateDoubleLiteral(VM_Magic.doubleAsLongBits(dc.value));
              }
	      int offset = index << 2;
	      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
	      s.insertBefore(Load.create(DOUBLE_LOAD, rop, jtoc, asImmediateOrReg(I(offset), s, ir), loc));
              s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_FloatConstantOperand) {
              OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.FloatType);
	      OPT_RegisterOperand jtoc = ir.regpool.makeJTOCOp(ir,s);
              OPT_FloatConstantOperand fc = (OPT_FloatConstantOperand)use;
              int index = fc.index;
              if (index == 0) {
                index = VM_Statics.findOrCreateFloatLiteral(VM_Magic.floatAsIntBits(fc.value));
              }
	      int offset = index << 2;
	      OPT_LocationOperand loc = new OPT_LocationOperand(offset);
	      s.insertBefore(Load.create(FLOAT_LOAD, rop, jtoc, asImmediateOrReg(I(offset), s, ir), loc));
              s.putOperand(idx, rop.copyD2U());
            } else if (use instanceof OPT_NullConstantOperand) {
              s.putOperand(idx, I(0));
            }
          }
        }
      }
      
      // Calling OPT_Simplifier.simplify ensures that the instruction is 
      // in normalized form. This reduces the number of cases we have to 
      // worry about (and does last minute constant folding on the off chance
      // we've missed an opportunity...)
      OPT_Simplifier.simplify(s);
      
      switch (s.getOpcode()) {
	//////////
	// LOAD/STORE
	//////////
      case REF_STORE_opcode:
	s.operator = INT_STORE;
	// fallthrough!
      case BYTE_STORE_opcode:case SHORT_STORE_opcode:case INT_STORE_opcode:
	// On PowerPC, the value being stored must be in a register
	Store.setValue(s, asReg(Store.getClearValue(s), s, ir));
	// Supported addressing modes are quite limited.
	Store.setAddress(s, asReg(Store.getClearAddress(s), s, ir));
	Store.setOffset(s, asImmediateOrReg(Store.getClearOffset(s), s, ir));
      break;

      case ATTEMPT_opcode:
	// On PowerPC, the value being stored must be in a register
	Attempt.setNewValue(s, asReg(Attempt.getClearNewValue(s), s, ir));
	Attempt.setOldValue(s, null);       // not used on powerpc.
	// Supported addressing modes are quite limited.
	Attempt.setAddress(s, asReg(Attempt.getClearAddress(s),s,ir));
	Attempt.setOffset(s, asReg(Attempt.getClearOffset(s),s,ir));
	break;

      case LONG_STORE_opcode:case FLOAT_STORE_opcode:case DOUBLE_STORE_opcode:
	// Supported addressing modes are quite limited.
	Store.setAddress(s, asImmediateOrReg(Store.getClearAddress(s), s, ir));
	Store.setOffset(s, asImmediateOrReg(Store.getClearOffset(s), s, ir));
	break;

      case REF_LOAD_opcode:
	s.operator = INT_LOAD;
	// fallthrough!
      case BYTE_LOAD_opcode:case UBYTE_LOAD_opcode:
      case SHORT_LOAD_opcode:case USHORT_LOAD_opcode:case INT_LOAD_opcode:
      case LONG_LOAD_opcode:case FLOAT_LOAD_opcode:case DOUBLE_LOAD_opcode:
	// Supported addressing modes are quite limited.
	Load.setAddress(s, asReg(Load.getClearAddress(s), s, ir));
	Load.setOffset(s, asImmediateOrReg(Load.getClearOffset(s), s, ir));
	break;

      case PREPARE_opcode:
	// Supported addressing modes are quite limited.
	Prepare.setAddress(s, asReg(Prepare.getAddress(s), s, ir));
	Prepare.setOffset(s, asReg(Prepare.getOffset(s), s, ir));
	break;

      case REF_MOVE_opcode:
	s.operator = INT_MOVE;
	break;

      case REF_COND_MOVE_opcode:
	s.operator = INT_COND_MOVE;
	break;
	
	//////////
	// INT ALU OPS
	//////////
	// There are some instructions for which LIR2MIR.rules doesn't
	// seem to expect constant operands at all. 
      case INT_REM_opcode:case INT_DIV_opcode:
	GuardedBinary.setVal1(s, asReg(GuardedBinary.getClearVal1(s), s, ir));
	GuardedBinary.setVal2(s, asReg(GuardedBinary.getClearVal2(s), s, ir));
	break;

      case REF_IFCMP_opcode:
	s.operator = INT_IFCMP;
	// fallthrough!
      case INT_IFCMP_opcode:
	// val1 can't be a constant, val2 must be small enough.
	IfCmp.setVal1(s, asReg(IfCmp.getClearVal1(s), s, ir));
	IfCmp.setVal2(s, asImmediateOrReg(IfCmp.getClearVal2(s), s, ir));
	break;

      case INT_IFCMP2_opcode:
	// val1 can't be a constant, val2 must be small enough.
	IfCmp2.setVal1(s, asReg(IfCmp2.getClearVal1(s), s, ir));
	IfCmp2.setVal2(s, asImmediateOrReg(IfCmp2.getClearVal2(s), s, ir));
	break;

      case BOOLEAN_CMP_opcode:
	// val2 must be small enough.
	BooleanCmp.setVal2(s, asImmediateOrReg(BooleanCmp.getClearVal2(s),s,ir));
	break;

      case INT_SUB_opcode:
	// val1 can't be a constant
	Binary.setVal1(s, asReg(Binary.getClearVal1(s), s, ir));
	// val2 isn't be constant (if it were, OPT_Simplifier would have
	// converted this into an ADD of -Val2).
	break;

      case INT_SHL_opcode:case INT_SHR_opcode:case INT_USHR_opcode:
	// Val2 could be a constant, but Val1 apparently can't be.
	Binary.setVal1(s, asReg(Binary.getClearVal1(s), s, ir));
	break;

      // There are other instructions for which LIR2MIR.rules may not
      // handle constant operands that won't fit in the immediate field.
      // TODO: Some of these actually might be ok, but for now we'll expand them all. 
      case INT_ADD_opcode:case INT_MUL_opcode:
	Binary.setVal2(s, asImmediateOrReg(Binary.getVal2(s), s, ir));
	break;

      case INT_AND_opcode:case INT_OR_opcode:case INT_XOR_opcode:
	{
	  OPT_Operand val = Binary.getVal2(s);
	  if (val instanceof OPT_IntConstantOperand) {
	    OPT_IntConstantOperand ival = (OPT_IntConstantOperand)val;
	    if ((ival.value < 0) || (ival.value > UNSIGNED_UPPER_IMMEDIATE)) {
	      val.instruction = null;
	      OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.IntType);
	      s.insertBefore(Move.create(INT_MOVE, rop, val));
	      Binary.setVal2(s, rop.copyD2U());
	    }
	  }
	}
      break;

      // Deal with OPT_Simplifier.CF_FLOAT or OPT_Simplifier.CF_DOUBLE being false
      case INT_2DOUBLE_opcode:case INT_2FLOAT_opcode:
      case INT_BITS_AS_FLOAT_opcode:
	Unary.setVal(s, asReg(Unary.getVal(s), s, ir));
	break;

      case NULL_CHECK_opcode:
	NullCheck.setRef(s, asReg(NullCheck.getClearRef(s), s, ir));
	break;

      // Force all call parameters to be in registers
      case CALL_opcode:
	{
	  int numArgs = Call.getNumberOfParams(s);
	  for (int i = 0; i < numArgs; i++) {
	    Call.setParam(s, i, asReg(Call.getClearParam(s, i), s, ir));
	  }
	}
	break;

      case RETURN_opcode:
	if (Return.hasVal(s)) {
	  Return.setVal(s, asReg(Return.getClearVal(s), s, ir));
	}
	break;
      } 
    }
  }

  public static boolean canBeImmediate(int val) {
    return (val >= LOWER_IMMEDIATE) && (val <= UPPER_IMMEDIATE);
  }

  static OPT_Operand asImmediateOrReg(OPT_Operand addr, 
				      OPT_Instruction s, 
				      OPT_IR ir) {
    if (addr instanceof OPT_IntConstantOperand) {
      if (!canBeImmediate(((OPT_IntConstantOperand)addr).value)) {
        OPT_RegisterOperand rop = ir.regpool.makeTempInt();
        s.insertBefore(Move.create(INT_MOVE, rop, addr));
        return rop.copyD2U();
      }
    }
    // Operand was OK as is.
    return addr;
  }

  /**
   * Force addr to be a register operand
   * @param addr
   * @param s
   * @param ir
   */
  static OPT_Operand asReg(OPT_Operand addr,
			   OPT_Instruction s, 
			   OPT_IR ir) {
    if (addr instanceof OPT_IntConstantOperand) {
      OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.IntType);
      s.insertBefore(Move.create(INT_MOVE, rop, addr));
      return rop.copyD2U();
    }
    // Operand was OK as is.
    return addr;
  }

  
  /**
   * Replace LongConstant uses by materializeConstants
   * @param inst
   * @param ir
   */
  static void exterminateLongConstants (OPT_Instruction s, OPT_IR ir) {
    
    int numUses = s.getNumberOfUses();
    if (numUses > 0) {
      int numDefs = s.getNumberOfDefs();
      for (int idx = numDefs; idx < numUses + numDefs; idx++) {
	OPT_Operand use = s.getOperand(idx);
	if (use != null) {
	  if (use instanceof OPT_LongConstantOperand) {
	    OPT_RegisterOperand rop = ir.regpool.makeTemp(VM_Type.LongType);
	    use.clear();
	    s.insertBack(Move.create(LONG_MOVE, rop, use));
	    s.putOperand(idx, rop.copyD2U());
	  }
	}
      }
    }
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Assemble PowerPC MIR into binary code.
 *
 * @author Jong-Deok Choi
 * @author Dave Grove
 * @author Igor Pechtchanski
 * @author Mauricio Serrano
 */
public final class OPT_Assembler implements OPT_Operators, VM_Constants {

  private static final boolean DEBUG = false;
  private static final boolean DEBUG_CODE_PATCH = false;

  // PowerPC specific constants/masks
  private static final int REG_MASK = 0x1F;             // for 32 registers
  private static final int SHORT_MASK = 0xFFFF;         // for 16-bit integer

  private static final int LI_MASK = 0x3FFFFFC;         // for 24-bit integer (shifted by 2)
  private static final int BD_MASK = 0xFFFC;            // for 14-bit integer (shifted by 2)
  static final int MAX_24_BITS = 0x7FFFFF;              // for 24-bit signed positive integer
  private static final int MIN_24_BITS = -0x800000;     // for 24-bit signed positive integer
  private static final int MAX_14_BITS = 0x1FFF;        // for 14-bit signed positive integer
  private static final int MIN_14_BITS = -0x2000;       // for 14-bit signed positive integer
  static final int MAX_COND_DISPL = MAX_14_BITS;        // max conditional displacement
  private static final int MIN_COND_DISPL = MIN_14_BITS;// min conditional displacement
  private static final int MAX_DISPL = MAX_24_BITS;     // max unconditional displacement
  private static final int MIN_DISPL = MIN_24_BITS;     // min unconditional displacement

  private static final int CFLIP_MASK = 0x14 << 21;     // used to flip BO by XOR
  private static final int NOPtemplate = (24 << 26);
  private static final int Btemplate = (18 << 26);

  private int unresolvedBranches = 0;

  /**
   * Generate machine code into ir.MIRInfo.machinecode.
   * 
   * @param ir the IR to generate
   * @return   the number of machinecode instructions generated
   */
  public static final int generateCode (OPT_IR ir, boolean shouldPrint) {
    return new OPT_Assembler().genCode(ir, shouldPrint);
  }

  private final int genCode(OPT_IR ir, boolean shouldPrint) {
    int mi = 0;
    INSTRUCTION[] machinecodes = ir.MIRInfo.machinecode;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    boolean unsafeCondDispl = machinecodes.length > MAX_COND_DISPL;
    boolean unsafeDispl = machinecodes.length > MAX_DISPL;
    for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	 p != null; 
	 p = p.nextInstructionInCodeOrder()) {
      int inst = p.operator().instTemplate;
      switch (p.getOpcode()) {
      case LABEL_opcode:
	// Back-patch any forward branches to it.
	// The Label instructions scratchObject holds the head of a 
	// linked list of the (forward) branch instructions with this 
	// label as their target.
	for (BranchSrcElement bSrc = (BranchSrcElement)p.scratchObject; 
	     bSrc != null; 
	     bSrc = bSrc.next) {
	  OPT_Instruction branchStmt = bSrc.source;
	  int bo = branchStmt.getmcOffset() - (1 << LG_INSTRUCTION_WIDTH);
	  int bi = bo >> LG_INSTRUCTION_WIDTH;
	  int targetOffset = (mi - bi) << LG_INSTRUCTION_WIDTH;
	  boolean setLink = false;

	  if (targetOffset > MAX_DISPL << LG_INSTRUCTION_WIDTH) {
	    if (branchStmt.getOpcode() == IG_PATCH_POINT_opcode) {
	      // IG_PATCH_POINT throws OPT_PatchPointGuardedInliningException
	      // if the targetOffset is not in the range.
	      // The lower bound was checked in resolveBranch
	      // see also VM_RuntimeOptCompilerInfrastructure.optCompile     
	      throw new OPT_PatchPointGuardedInliningException(targetOffset);
	    } else {
	      // back to normal cases 
	      throw  new OPT_OptimizingCompilerException("CodeGen", 
				   "Branch positive offset too large: ", 
						       targetOffset);
	    }
	  }

	  switch (branchStmt.getOpcode()) {
	  case PPC_B_opcode:case PPC_BL_opcode:
	    machinecodes[bi] |= targetOffset & LI_MASK;
	    break;
	  case PPC_DATA_LABEL_opcode:
	    machinecodes[bi] = targetOffset & LI_MASK;
	    break;
	  // Since resolveBranch and patch already check the range
	  // of target offset, and will fail if it is out of range
	  case IG_PATCH_POINT_opcode:
	    // do nothing
	    break;
	  case PPC_BCL_opcode:
	    setLink = true;
	    // fall through!
	  default:          // conditional branches
	    if (targetOffset <= MAX_COND_DISPL << 2) {// one word is enough
	      machinecodes[bi] |= targetOffset & BD_MASK;
	      if (DEBUG) {
		VM.sysWrite("**** Forward Short Cond. Branch ****\n");
		VM.sysWrite(disasm(machinecodes[bi], 0)+"\n");
	      }
	    } else {          // one word is not enough
	      // we're moving the "real" branch ahead 1 instruction
	      // if it's a GC point (eg BCL for yieldpoint) then we must 
	      // make sure the GCMap is generated at the correct mc offset.
	      branchStmt.setmcOffset(branchStmt.getmcOffset() + 
				     (1 <<  LG_INSTRUCTION_WIDTH));
	      // flip the condition and skip the next branch instruction
	      machinecodes[bi] = flipCondition(machinecodes[bi]);
	      machinecodes[bi] |= (2 << LG_INSTRUCTION_WIDTH);
	      machinecodes[bi] &= 0xfffffffe;       // turn off link bit.
	      // make a long branch
	      machinecodes[bi + 1] = Btemplate | ((targetOffset-4) & LI_MASK);
	      if (setLink)
		machinecodes[bi + 1] |= 1;          // turn on link bit.
	      if (DEBUG) {
		VM.sysWrite("**** Forward Long Cond. Branch ****\n");
		VM.sysWrite(disasm(machinecodes[bi], 0)+"\n");
		VM.sysWrite(disasm(machinecodes[bi + 1], 0)+"\n");
	      }
	    }
	    break;
	  }
	  unresolvedBranches--;
	}
	p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	break;

      case BBEND_opcode:case UNINT_BEGIN_opcode:case UNINT_END_opcode:
      case GUARD_MOVE_opcode:case GUARD_COMBINE_opcode:
	p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	break;

      case PPC_DATA_INT_opcode:
	{
	  int value = MIR_DataInt.getValue(p).value;
	  machinecodes[mi++] = value;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_DATA_LABEL_opcode:
	{
	  OPT_Instruction target = MIR_DataLabel.getTarget(p).target;
	  int targetOffset = resolveBranch(p, target, mi);
	  machinecodes[mi++] = targetOffset;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_CRAND_opcode: case PPC_CRANDC_opcode:
      case PPC_CROR_opcode: case PPC_CRORC_opcode:
	{
	  int op0 = MIR_Condition.getResultBit(p).value & REG_MASK;
	  int op1 = MIR_Condition.getValue1Bit(p).value & REG_MASK;
	  int op2 = MIR_Condition.getValue2Bit(p).value & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_ADD_opcode:
      case PPC_ADDr_opcode:case PPC_ADDC_opcode:
      case PPC_ADDE_opcode:
      case PPC_SUBF_opcode:
      case PPC_SUBFr_opcode:case PPC_SUBFC_opcode:
      case PPC_SUBFCr_opcode:
      case PPC_SUBFE_opcode:
      case PPC_FADD_opcode:
      case PPC_FADDS_opcode:
      case PPC_FDIV_opcode:
      case PPC_FDIVS_opcode:
      case PPC_DIVW_opcode:
      case PPC_DIVWU_opcode:
      case PPC_MULLW_opcode:
      case PPC_MULHW_opcode:
      case PPC_MULHWU_opcode:
      case PPC_FSUB_opcode:
      case PPC_FSUBS_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_LWZX_opcode:
      case PPC_LWARX_opcode:
      case PPC_LBZX_opcode:
      case PPC_LHAX_opcode:
      case PPC_LHZX_opcode:
      case PPC_LFDX_opcode:
      case PPC_LFSX_opcode:
	{
	  int op0 = MIR_Load.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Load.getAddress(p).register.number & REG_MASK;
	  int op2 = MIR_Load.getOffset(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_STWX_opcode:
      case PPC_STWCXr_opcode:
      case PPC_STBX_opcode:
      case PPC_STHX_opcode:
      case PPC_STFDX_opcode:
      case PPC_STFSX_opcode:
	{
	  int op0 = MIR_Store.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_Store.getAddress(p).register.number & REG_MASK;
	  int op2 = MIR_Store.getOffset(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_LWZUX_opcode:
      case PPC_LBZUX_opcode:
	{
	  int op0 = MIR_LoadUpdate.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_LoadUpdate.getAddress(p).register.number & REG_MASK;
	  int op2 = MIR_LoadUpdate.getOffset(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_LWZU_opcode:
	{
	  int op0 = MIR_LoadUpdate.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_LoadUpdate.getAddress(p).register.number & REG_MASK;
	  int op2 = MIR_LoadUpdate.getOffset(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;


      case PPC_TW_opcode:
	{
	  int op0 = MIR_Trap.getCond(p).value;
	  int op1 = MIR_Trap.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Trap.getValue2(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_TWI_opcode:
	{
	  int op0 = MIR_Trap.getCond(p).value;
	  int op1 = MIR_Trap.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Trap.getValue2(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case NULL_CHECK_opcode:
	/* Just a nicer name for a twi <ref> lessthan 1 */
	{
	  int op0 = OPT_PowerPCTrapOperand.LOWER;
	  int op1 = ((OPT_RegisterOperand)NullCheck.getRef(p)).register.number & REG_MASK;
	  int op2 = 1;
	  inst = PPC_TWI.instTemplate;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_LDI_opcode:case PPC_LDIS_opcode:
	// D_Form. pseudo instructions derived from PPC_ADDI and PPC_ADDIS
	{
	  int op0 = MIR_Unary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Unary.getValue(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | op1);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_ADDIC_opcode:
      case PPC_ADDICr_opcode:case PPC_SUBFIC_opcode:
      case PPC_MULLI_opcode:
      case PPC_ADDI_opcode:
      case PPC_ADDIS_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_CNTLZW_opcode:
      case PPC_EXTSB_opcode:
      case PPC_EXTSBr_opcode:case PPC_EXTSH_opcode:
      case PPC_EXTSHr_opcode:
	{
	  int op0 = MIR_Unary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Unary.getValue(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_ADDZE_opcode:case PPC_SUBFZE_opcode:
      case PPC_NEG_opcode:
      case PPC_NEGr_opcode:
      case PPC_ADDME_opcode:
	{
	  int op0 = MIR_Unary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Unary.getValue(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

	// Bit positions of op1 and op2 are reversed.
      case PPC_XORI_opcode:
      case PPC_XORIS_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

	// Bit positions of op1 and op2 are reversed.
      case PPC_AND_opcode:
      case PPC_ANDr_opcode:case PPC_NAND_opcode:
      case PPC_NANDr_opcode:case PPC_ANDC_opcode:
      case PPC_ANDCr_opcode:case PPC_OR_opcode:
      case PPC_ORr_opcode:case PPC_NOR_opcode:
      case PPC_NORr_opcode:case PPC_ORC_opcode:
      case PPC_ORCr_opcode:case PPC_XOR_opcode:
      case PPC_XORr_opcode:case PPC_EQV_opcode:
      case PPC_EQVr_opcode:case PPC_SLW_opcode:
      case PPC_SLWr_opcode:case PPC_SRW_opcode:
      case PPC_SRWr_opcode:case PPC_SRAW_opcode:
      case PPC_SRAWr_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MOVE_opcode:
	/* pseudo opcode, equal to PPC_ORI with 0 */
	{
	  int op0 = MIR_Move.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Move.getValue(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_SRWI_opcode:
	/* pseudo opcode, equal to rlwinm Rx,Ry,32-n,n,31 */
      case PPC_SRWIr_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int shift = MIR_Binary.getValue2(p).asIntConstant().value & REG_MASK;
	  int op2 = (32 - shift);
	  int op3 = shift;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11) | (op3 << 6));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

	// Bit positions of op1 and op2 are reversed.
      case PPC_SLWI_opcode:
      case PPC_SLWIr_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int shift = MIR_Binary.getValue2(p).asIntConstant().value & REG_MASK;
	  int op2 = shift;
	  int op3 = (31 - shift);
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11) | (op3 << 1));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_SRAWI_opcode:
      case PPC_SRAWIr_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asIntConstant().value & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

	// Bit positions of op1 and op2 are reversed.
      case PPC_ANDIr_opcode:
      case PPC_ANDISr_opcode:
      case PPC_ORI_opcode:
      case PPC_ORIS_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_RLWINM_opcode:
      case PPC_RLWINMr_opcode:
	{  
	  int op0 = MIR_RotateAndMask.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_RotateAndMask.getValue(p).register.number & REG_MASK;
	  int op2 = MIR_RotateAndMask.getShift(p).asIntConstant().value & REG_MASK;
	  int op3 = MIR_RotateAndMask.getMaskBegin(p).value & REG_MASK;
	  int op4 = MIR_RotateAndMask.getMaskEnd(p).value & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11) | (op3 << 6) | (op4 << 1));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_RLWIMI_opcode:
      case PPC_RLWIMIr_opcode:
	{
	  int op0 = MIR_RotateAndMask.getResult(p).register.number & REG_MASK;
	  int op0f = MIR_RotateAndMask.getSource(p).register.number & REG_MASK;
	  if (op0 != op0f)
	    throw  new OPT_OptimizingCompilerException("CodeGen", 
						       "format for RLWIMI is incorrect");
	  int op1 = MIR_RotateAndMask.getValue(p).register.number & REG_MASK;
	  int op2 = MIR_RotateAndMask.getShift(p).asIntConstant().value & REG_MASK;
	  int op3 = MIR_RotateAndMask.getMaskBegin(p).value & REG_MASK;
	  int op4 = MIR_RotateAndMask.getMaskEnd(p).value & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11) | (op3 << 6) | (op4 << 1));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_RLWNM_opcode:
      case PPC_RLWNMr_opcode:
	{ 
	  int op0 = MIR_RotateAndMask.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_RotateAndMask.getValue(p).register.number & REG_MASK;
	  int op2 = MIR_RotateAndMask.getShift(p).asRegister().register.number & REG_MASK;
	  int op3 = MIR_RotateAndMask.getMaskBegin(p).value & REG_MASK;
	  int op4 = MIR_RotateAndMask.getMaskEnd(p).value & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21) | (op2 << 11) | (op3 << 6) | (op4 << 1));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_B_opcode:
	{
	  OPT_BranchOperand o = MIR_Branch.getTarget(p);
	  int targetOffset = resolveBranch(p, o.target, mi);
	  machinecodes[mi++] = inst | (targetOffset & LI_MASK);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_BLR_opcode:
      case PPC_BCTR_opcode:
	/* p   , == bcctr  0x14,BI */
	{                     // INDIRECT BRANCH (Target == null)
	  machinecodes[mi++] = inst;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_BC_opcode:
      case PPC_BCOND_opcode:
	/* p 38, BO == 001zy or 011zy */
      case PPC_BCC_opcode:
	/* p 38, BO == 0000y, 0001y, 0100y or 0101y */
	{                     // COND BRANCH
	  int op0 = MIR_CondBranch.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_CondBranch.getCond(p).value;
	  // Add (CR field)<<2 to make BI represent the correct
	  // condition bit (0..3) in the correct condition field (0..7).
	  // 1 <= op <= 7
	  int bo_bi = op0 << 2 | op1;
	  OPT_BranchOperand o = MIR_CondBranch.getTarget(p);
	  int targetOffset = resolveBranch(p, o.target, mi);
	  if (targetOffset == 0) {            // unresolved branch
	    if (DEBUG) VM.sysWrite("**** Forward Cond. Branch ****\n");
	    machinecodes[mi++] = inst | (bo_bi << 16);
	    p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    if (unsafeCondDispl) {            // assume we might need two words
	      machinecodes[mi++] = NOPtemplate;   // for now fill with NOP
	      if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    }
	  } else if (targetOffset < MIN_COND_DISPL << 2) {
	    // one word is not enough
	    if (DEBUG) VM.sysWrite("**** Backward Long Cond. Branch ****\n");
	    // flip the condition and skip the following branch instruction
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    machinecodes[mi++] = inst | flipCondition(bo_bi << 16) | (2 << 2);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    // make a long branch to the target
	    machinecodes[mi++] = Btemplate | ((targetOffset - 4) & LI_MASK);
	    p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	  } else {              // one word is enough
	    if (DEBUG) VM.sysWrite("**** Backward Short Cond. Branch ****\n");
	    machinecodes[mi++] = inst | (bo_bi << 16) | (targetOffset & BD_MASK);
	    p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	  }
	}
	break;

      case PPC_BCLR_opcode:
      case PPC_BCCTR_opcode:
	/* p   , BO == 0z10y or 0z11y */
	{                     // INDIRECT COND BRANCH
	  int op0 = MIR_CondBranch.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_CondBranch.getCond(p).value;
	  // Add (CR field)<<2 to make BI represent the correct
	  // condition bit (0..3) in the correct condition field (0..7).
	  // 1 <= op <= 7
	  int bo_bi = op0 << 2 | op1;
	  machinecodes[mi++] = inst | (bo_bi << 16);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	  if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0));
	}
	break;

      case PPC_BL_opcode:
	{                     // CALL
	  OPT_BranchOperand o = (OPT_BranchOperand)MIR_Call.getTarget(p);
	  int targetOffset = resolveBranch(p, o.target, mi);
	  machinecodes[mi++] = inst | (targetOffset & LI_MASK);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_BLRL_opcode:
	/* p 39, == bclrl  0x14,BI */
      case PPC_BCTRL_opcode:
	/* p   , == bcctrl 0x14,BI */
	{                     // INDIRECT CALL (Target == null)
	  machinecodes[mi++] = inst;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_BCL_opcode:
	{                     // COND CALL
	  int op0 = MIR_CondCall.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_CondCall.getCond(p).value;
	  // Add (CR field)<<2 to make BI represent the correct
	  // condition bit (0..3) in the correct condition field (0..7).
	  // 1 <= op <= 7
	  int bo_bi = op0 << 2 | op1;
	  OPT_BranchOperand o = (OPT_BranchOperand)MIR_CondCall.getTarget(p);
	  int targetOffset = resolveBranch(p, o.target, mi);
	  if (targetOffset == 0) {            // unresolved branch
	    if (DEBUG) VM.sysWrite("**** Forward Cond. Branch ****\n");
	    machinecodes[mi++] = inst | (bo_bi << 16);
	    p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    if (unsafeCondDispl) {            // assume we need two words
	      machinecodes[mi++] = NOPtemplate;    // for now fill with NOP
	      if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	    }
	  } else if (targetOffset < MIN_COND_DISPL << 2) {      
	    // one instruction is not enough
	    throw  new OPT_OperationNotImplementedException( "Support for long backwards conditional branch and link is incorrect.");          //--dave
	    /*
	      -- we have to branch (and not link) around an 
	      unconditional branch and link. 
	      -- the code below generates a conditional branch and 
	      link around an unconditional branch.
	      if (DEBUG) VM.sysWrite("**** Backward Long Cond. Branch ****\n");
	      // flip the condition and skip the following branch instruction
	      machinecodes[mi++] = inst | flipCondition(bo_bi<<16) | (2<<2);
	      if (DEBUG) printInstruction(mi-1, inst, 
	      flipCondition(bo_bi<<16), 2<<2);
	      // make a long branch to the target
	      machinecodes[mi++] = Btemplate | ((targetOffset-4) & LI_MASK);
	      p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	      if (DEBUG) printInstruction(mi-1, Btemplate, targetOffset-4);
	    */
	  } else {              // one instruction is enough
	    if (DEBUG) VM.sysWrite("**** Backward Short Cond. Branch ****\n");
	    machinecodes[mi++] = inst | (bo_bi << 16) | (targetOffset & BD_MASK);
	    p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	    if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0)+"\n");
	  }
	}
	break;

      case PPC_BCLRL_opcode:
	{                     // INDIRECT COND CALL
	  int op0 = MIR_CondCall.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_CondCall.getCond(p).value;
	  // Add (CR field)<<2 to make BI represent the correct
	  // condition bit (0..3) in the correct condition field (0..7).
	  // 1 <= op <= 7
	  int bo_bi = op0 << 2 | op1;
	  machinecodes[mi++] = inst | (bo_bi << 16);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	  if (DEBUG) VM.sysWrite(disasm(machinecodes[mi-1], 0));
	}
	break;

      case PPC_CMP_opcode:
      case PPC_CMPL_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 23) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_CMPI_opcode:
      case PPC_CMPLI_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 23) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_FMR_opcode:
	{
	  int op0 = MIR_Move.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Move.getValue(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_FABS_opcode:
      case PPC_FCFID_opcode:
      case PPC_FNEG_opcode:
      case PPC_FRSP_opcode:
      case PPC_FCTIW_opcode:case PPC_FCTIWZ_opcode:
	{
	  int op0 = MIR_Unary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Unary.getValue(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_FCMPO_opcode:
      case PPC_FCMPU_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asRegister().register.number 
	    & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 23) | (op1 << 16) | (op2 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_FMUL_opcode:
      case PPC_FMULS_opcode:
	{
	  int op0 = MIR_Binary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Binary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Binary.getValue2(p).asRegister().register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 6));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_FMADD_opcode:
      case PPC_FMADDS_opcode:
      case PPC_FMSUB_opcode:
      case PPC_FMSUBS_opcode:
      case PPC_FNMADD_opcode:
      case PPC_FNMADDS_opcode:
      case PPC_FNMSUB_opcode:
      case PPC_FNMSUBS_opcode:
      case PPC_FSEL_opcode:
	{
	  int op0 = MIR_Ternary.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Ternary.getValue1(p).register.number & REG_MASK;
	  int op2 = MIR_Ternary.getValue2(p).register.number & REG_MASK;
	  int op3 = MIR_Ternary.getValue3(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | (op2 << 6) | (op3 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_LWZ_opcode:
      case PPC_LBZ_opcode:
      case PPC_LHA_opcode:
      case PPC_LHZ_opcode:
      case PPC_LFD_opcode:
      case PPC_LFS_opcode:
      case PPC_LMW_opcode:
	{
	  int op0 = MIR_Load.getResult(p).register.number & REG_MASK;
	  int op1 = MIR_Load.getOffset(p).asIntConstant().value & SHORT_MASK;
	  int op2 = MIR_Load.getAddress(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | op1 | (op2 << 16));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_STW_opcode:
      case PPC_STB_opcode:
      case PPC_STH_opcode:
      case PPC_STFD_opcode:
      case PPC_STFS_opcode:
      case PPC_STMW_opcode:
	{
	  int op0 = MIR_Store.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_Store.getOffset(p).asIntConstant().value & SHORT_MASK;
	  int op2 = MIR_Store.getAddress(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | op1 | (op2 << 16));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_STWU_opcode:
      case PPC_STFDU_opcode:
      case PPC_STFSU_opcode:
	{
	  int op0 = MIR_StoreUpdate.getValue(p).register.number & REG_MASK;
	  int op1 = MIR_StoreUpdate.getAddress(p).register.number & REG_MASK;
	  int op2 = MIR_StoreUpdate.getOffset(p).asIntConstant().value & SHORT_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16) | op2);
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MFSPR_opcode:
	{
	  int op0 = MIR_Move.getResult(p).register.number & REG_MASK;
	  int op1 = phys.getSPR(MIR_Move.getValue(p).register);
	  machinecodes[mi++] = (inst | (op0 << 21) | (op1 << 16));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MTSPR_opcode:
	{
	  int op0 = phys.getSPR(MIR_Move.getResult(p).register);
	  int op1 = MIR_Move.getValue(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MFTB_opcode:
      case PPC_MFTBU_opcode:
	{
	  int op0 = MIR_Move.getResult(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MFCR_opcode:
	{
	  int op0 = MIR_Move.getResult(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_MTCR_opcode:
	{
	  int op0 = MIR_Move.getValue(p).register.number & REG_MASK;
	  // exclude THREAD_SWITCH_REGISTER
	  int mask = 0xff & ~(0x80 >> THREAD_SWITCH_REGISTER);
	  machinecodes[mi++] = (inst | (mask << 12) | (op0 << 21));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_SYNC_opcode:
      case PPC_ISYNC_opcode:
	{
	  machinecodes[mi++] = inst;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;

      case PPC_DCBST_opcode:
      case PPC_DCBF_opcode:
      case PPC_ICBI_opcode:
	{
	  int op0 = MIR_CacheOp.getAddress(p).register.number & REG_MASK;
	  int op1 = MIR_CacheOp.getOffset(p).register.number & REG_MASK;
	  machinecodes[mi++] = (inst | (op0 << 16) | (op1 << 11));
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);
	}
	break;
	
      case IG_PATCH_POINT_opcode:
	{
	  /* Two options here:
	   * 1. generate one nop:
	   *   if the target offset is in the range of 2^25-1 ~ -2^25, 
	   *      do nothing;
	   *   otherwise, fail the compilation
	   *
	   * 2. generate three nops:
	   *   do nothing, code patching will stop the world and handle this;
	   */

	  /* currently I am using option 1 which should be the common case, 
	   * and patch back when resolving the label.
	   */
	  // verify the target is a label
	  OPT_BranchOperand bop = InlineGuard.getTarget(p);
	  OPT_Instruction target = bop.target;
	  
	  if (VM.VerifyAssertions) {
	    VM.assert(target.getOpcode() == LABEL_opcode);
	  }

	  // resolve the target instruction, in LABEL_opcode, 
	  // add one case for IG_PATCH_POINT
	  int targetOffset = resolveBranch(p, target, mi);

	  machinecodes[mi++] = NOPtemplate;
	  p.setmcOffset(mi << LG_INSTRUCTION_WIDTH);

	  if (DEBUG_CODE_PATCH) {
	    VM.sysWrite("to be patched at ");
	    VM.sysWrite(mi-1, false);
	    VM.sysWrite(" inst ");
	    VM.sysWriteHex(machinecodes[mi-1]);
	    VM.sysWrite("\n");
	  }
	}
	break;
      default:
	throw new OPT_OptimizingCompilerException("CodeGen", 
						  "OPCODE not implemented:", 
						  p);
      }
    }
    if (unresolvedBranches != 0)
      throw new OPT_OptimizingCompilerException("CodeGen", 
						" !!! Unresolved Branch Targets Exist!!! \n");
    
    if (shouldPrint) {
      OPT_Compiler.header("Final machine code", ir.method);
      for (int i = 0; i < machinecodes.length; i++) {
	System.out.print(VM_Services.getHexString(i << LG_INSTRUCTION_WIDTH, true) + 
			 " : " + 
			 VM_Services.getHexString(machinecodes[i], false));
	System.out.print("  ");
	System.out.print(disasm(machinecodes[i], i << LG_INSTRUCTION_WIDTH));
	System.out.println();
      }
    }

    return mi;
  }


  // used to build a link list of unresolved forward branches 
  // on the target label instr.
  private static final class BranchSrcElement {
    OPT_Instruction source;
    BranchSrcElement next;

    BranchSrcElement (OPT_Instruction src, BranchSrcElement Next) {
      source = src;
      next = Next;
    }
  }

  /**
   * Resolve a branch instruction to a machine code offset.
   * @param src
   * @param tgt
   * @param mi
   */
  private int resolveBranch(OPT_Instruction src, 
			    OPT_Instruction tgt, 
			    int mi) {
    if (tgt.getmcOffset() < 0) {
      unresolvedBranches++;
      // forward branch target, which has not been fixed yet.
      // Unresolved forward branch stmts will form a linked list
      // via the scratchObject of the label instruction.
      // These branch stmts will be back-patched as part of assembly of LABEL
      tgt.scratchObject = 
	new BranchSrcElement(src, (BranchSrcElement)tgt.scratchObject);
      return 0;
    } else {
      // backward branch target, which has been fixed.
      int targetOffset = tgt.getmcOffset() - (mi << LG_INSTRUCTION_WIDTH);

      if (targetOffset < (MIN_DISPL << LG_INSTRUCTION_WIDTH)) {

	if (src.getOpcode() == IG_PATCH_POINT_opcode) {
	  // IG_PATCH_POINT throws OPT_PatchPointGuardedInliningException
	  // if the targetOffset is not in the range.
	  // The upper bound is checked in 'case LABEL_opcode' block.
	  // see also VM_RuntimeOptCompilerInfrastructure.java
	  throw new OPT_PatchPointGuardedInliningException(targetOffset);
	} else {
	  throw new OPT_OptimizingCompilerException("CodeGen", 
				  " Branch negative offset too large: ", 
						    targetOffset);
	}
      }
      return targetOffset;
    }
  }


  // flip the condition field of a conditional branch (p 38)
  private int flipCondition (int inst) {
    // structure of BO field: UTCZy, where U=unconditional branch?
    //                                     T=condition true?
    //                                     C=ignore counter?
    //                                     Z=counter==0?
    //                                     y=branch predicted taken?
    // flip the condition:
    //    after the flip:         _            _
    //                        T = U ^ T;   Z = C ^ Z
    //    i.e. flip the condition if the branch is conditional;
    //         flip the zero test if the counter is tested.
    // WARNING: may not be correct when both condition and counter
    //          are tested, since, by DeMorgan's law,
    //          ~(A & B) == ~A | ~B, and the flip will produce ~A & ~B
   int flip = (~inst & CFLIP_MASK) >> 1;
   return (inst ^ flip);
  }


  /**
   * Debugging support (return a printable representation of the machine code).
   *
   * @param instr, an integer to be interpreted as a PowerPC instruction
   * @param offset the mcoffset (in bytes) of the instruction
   */
  private String disasm (int instr, int offset) {
    return PPC_Disassembler.disasm(instr, offset);
  }

  /** Apply a patch.
   * The instruction at patchOffset should be a NOP instruction.
   * It is replaced by a "B rel32" instruction. 
   *  
   * @param code        the code intructions to be patched
   * @param patchOffset the offset of the last byte of the patch point
   * @param rel32       the new immediate to use in the branch instruction
   * 
   */
  final static void patchCode(INSTRUCTION[] code,
			      int patchOffset,
			      int rel32) {

    /* The expecting instruction at patchOffset should be a NOP.
     */    
    if (DEBUG_CODE_PATCH) {
      VM.sysWrite("patching at ");
      VM.sysWrite(patchOffset, false);
      VM.sysWrite(" inst ");
      VM.sysWriteHex(code[patchOffset]);
      VM.sysWrite(" offset ");
      VM.sysWrite(rel32, false);
      VM.sysWrite("\n");
    }
     
    // turn this into VM.VerifyAssertions later
    if (VM.VerifyAssertions) {
      VM.assert(code[patchOffset] == NOPtemplate);
      VM.assert(rel32 <= (MAX_DISPL << LG_INSTRUCTION_WIDTH));
      VM.assert(rel32 >= (MIN_DISPL << LG_INSTRUCTION_WIDTH));
    }
    /* the rel32 has to be in the range from -2^25 to 2^25-1, 
     * is is guaranteed when generating code for IG_PATCH_POINT.
     */  
    // make a B IMM instruction
    code[patchOffset] = (18 << 26) | (rel32 & LI_MASK);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * Final acts of MIR expansion for the PowerPC architecture.
 * Things that are expanded here (immediately before final assembly)
 * should only be those sequences that cannot be expanded earlier
 * due to difficulty in keeping optimizations from interfering with them.
 *
 * @author Mauricio J. Serrano
 * @author Jong-Deok Choi
 * @author Dave Grove
 * @author Igor Pechtchanski
 */
abstract class OPT_FinalMIRExpansion extends OPT_IRTools
  implements VM_BytecodeConstants {

  /**
   * @param ir the IR to expand
   * @return upperbound on number of machine code instructions 
   * that will be generated for this IR
   */
  public final static int expand (OPT_IR ir) {
    int instructionCount = 0;
    int conditionalBranchCount = 0;
    int machinecodeLength = 0;
    boolean frameCreated = false;

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    for (OPT_Instruction p = ir.firstInstructionInCodeOrder(); 
	 p != null; 
	 p = p.nextInstructionInCodeOrder()) {
      p.setmcOffset(-1);
      p.scratchObject = null;
      switch (p.getOpcode()) {
      case MIR_LOWTABLESWITCH_opcode:
	{
	  
	  OPT_BasicBlock tableBlock = p.getBasicBlock();
	  OPT_BasicBlock nextBlock = 
	    tableBlock.splitNodeWithLinksAt(p.prevInstructionInCodeOrder(), ir);
	  nextBlock.firstInstruction().setmcOffset(-1);
	  OPT_Register regI = MIR_LowTableSwitch.getIndex(p).register;
	  int NumTargets = MIR_LowTableSwitch.getNumberOfTargets(p);
	  tableBlock.appendInstruction(MIR_Call.create0(PPC_BL, null, null, 
							nextBlock.makeJumpTarget()));

	  for (int i = 0; i < NumTargets; i++) {
	    tableBlock.appendInstruction(MIR_DataLabel.create(PPC_DATA_LABEL, 
							      MIR_LowTableSwitch.getClearTarget(p, i)));
	  }
	  OPT_Register temp = phys.getGPR(0);
	  p.insertBack(MIR_Move.create(PPC_MFSPR, R(temp), R(phys.getLR())));
	  p.insertBack(MIR_Binary.create(PPC_SLWI, R(regI), R(regI), I(2)));
	  p.insertBack(nonPEIGC(MIR_LoadUpdate.create(PPC_LWZUX, R(temp), R(regI), R(temp))));
	  p.insertBack(MIR_Binary.create(PPC_ADD, R(regI), R(regI), R(temp)));
	  p.insertBack(MIR_Move.create(PPC_MTSPR, R(phys.getCTR()), R(regI)));
	  MIR_Branch.mutate(p, PPC_BCTR);
	  instructionCount += NumTargets + 7;
	}
	break;
      case PPC_BCOND2_opcode:
	{
	  OPT_RegisterOperand cond = MIR_CondBranch2.getClearValue(p);
	  p.insertFront(MIR_CondBranch.create(PPC_BCOND, cond.copyU2U(), 
					      MIR_CondBranch2.getClearCond2(p), 
					      MIR_CondBranch2.getClearTarget2(p),
					      MIR_CondBranch2.getClearBranchProfile2(p)));
	  MIR_CondBranch.mutate(p, PPC_BCOND, cond, 
				MIR_CondBranch2.getClearCond1(p), 
				MIR_CondBranch2.getClearTarget1(p),
				MIR_CondBranch2.getClearBranchProfile1(p));
	  conditionalBranchCount++;
	}
	break;
      case PPC_BLRL_opcode:case PPC_BCTRL_opcode:
	{
	  // indirect calls.  Second part of fast interface invoker expansion
	  // See also OPT_ConvertToLowlevelIR.java
	  if (VM.BuildForIMTInterfaceInvocation) {
	    if (MIR_Call.hasMethod(p)) {
	      OPT_MethodOperand mo = MIR_Call.getMethod(p);
	      if (mo.isInterface()) {
		int signatureId = VM_ClassLoader.
		  findOrCreateInterfaceMethodSignatureId(mo.method.getName(), 
							 mo.method.getDescriptor());
		OPT_Instruction s;
		if (OPT_Bits.fits(signatureId, 16)) {
		  s = MIR_Unary.create(PPC_LDI, 
				       R(phys.getGPR(LAST_SCRATCH_GPR)), 
				       I(signatureId)); p.insertBack(s);
		  instructionCount++;
		} else {
		  s = MIR_Unary.create(PPC_LDIS, 
				       R(phys.getGPR(LAST_SCRATCH_GPR)), 
				       I(OPT_Bits.PPCMaskUpper16(signatureId)));
		  p.insertBack(s);
		  s = MIR_Binary.create(PPC_ADDI, 
					R(phys.getGPR(LAST_SCRATCH_GPR)), 
					R(phys.getGPR(LAST_SCRATCH_GPR)), 
					I(OPT_Bits.PPCMaskLower16(signatureId)));
		  p.insertBack(s);
		  instructionCount += 2;
		}
	      }
	    }
	  }
	  instructionCount++;

	  //-#if RVM_WITH_SPECIALIZATION
	  if (MIR_Call.hasMethod(p)) {
	    GNO_InstructionLocation loc = new GNO_InstructionLocation(p);
	    if (loc.isCallInstruction()) {
	      int callSiteNumber = 0;

	      if (VM_SpecializationSentry.isValid()) {
		OPT_SpecializationGraphNode c = ir.context;
		callSiteNumber =
		  VM_SpecializationCallSites.getCallSiteNumber(c, loc);
	      }
		
	      OPT_Instruction ss = MIR_Unary.create(PPC_LDI, 
						    R(phys.getGPR(0)), 
						    I(callSiteNumber<<2));
	      p.insertBack(ss);
	      instructionCount++;
	    }
	  }
	  //-#endif
	}
	break;
      case LABEL_opcode:case BBEND_opcode:case UNINT_BEGIN_opcode:
      case UNINT_END_opcode:
	// These generate no code, so don't count them.
	break;
      case RESOLVE_opcode:
	{
	  OPT_Register zero = phys.getGPR(0);
	  OPT_Register JTOC = phys.getJTOC();
	  OPT_Register CTR = phys.getCTR();
	  if (VM.VerifyAssertions) 
	    VM.assert(p.bcIndex >= 0 && p.position != null);
	  int offset = VM_Entrypoints.optResolveMethod.getOffset();
	  if (OPT_Bits.fits(offset, 16)) {
	    p.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(zero), 
						    R(JTOC), I(offset))));
	  } else {
	    p.insertBefore(MIR_Unary.create(PPC_LDIS, R(zero), 
					    I(offset >>> 16)));
	    p.insertBefore(MIR_Binary.create(PPC_ORI, R(zero), R(zero), 
					     I(offset & 0xffff)));
	    p.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZX, R(zero), 
						    R(JTOC), R(zero))));
	    instructionCount += 2;
	  }
	  p.insertBefore(MIR_Move.create(PPC_MTSPR, R(CTR), R(zero)));
	  instructionCount += 3;
	  // Because the GC Map code holds a reference to the original
	  // instruction, it is important that we mutate the last instruction
	  // because this will be the GC point.
	  MIR_Call.mutate0(p, PPC_BCTRL, null, null);
	  break;
	}
        case YIELDPOINT_PROLOGUE_opcode:
          {
            OPT_Register TSR = phys.getTSR();
            OPT_BasicBlock yieldpoint = findOrCreateYieldpointBlock(ir,
                                        VM_Thread.PROLOGUE);                    
            // Because the GC Map code holds a reference to the original
            // instruction, it is important that we mutate the last instruction
            // because this will be the GC point.
            MIR_CondCall.mutate0(p, PPC_BCL, null, null, R(TSR), 
                                 OPT_PowerPCConditionOperand.THREAD_SWITCH(), 
                                 yieldpoint.makeJumpTarget());
            conditionalBranchCount++;
          }
          break;
        case YIELDPOINT_BACKEDGE_opcode:
          {
            OPT_BasicBlock yieldpoint = findOrCreateYieldpointBlock(ir,
                                        VM_Thread.BACKEDGE);                    
            OPT_Register zero = phys.getGPR(0);
            OPT_Register TSR = phys.getTSR();
            if (!VM.BuildForThreadSwitchUsingControlRegisterBit) {
              OPT_Register PR = phys.getPR();
              p.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(zero), 
                             R(PR), 
                             I(VM_Entrypoints.threadSwitchRequestedField.getOffset()))));
              p.insertBefore(MIR_Binary.create(PPC_CMPI, R(TSR), R(zero), 
                             I(0)));
              instructionCount += 2;
            }
            // Because the GC Map code holds a reference to the original
            // instruction, it is important that we mutate the last instruction
            // because this will be the GC point.
            MIR_CondCall.mutate0(p, PPC_BCL, null, null, R(TSR), 
                                 OPT_PowerPCConditionOperand.THREAD_SWITCH(), 
                                 yieldpoint.makeJumpTarget());
            conditionalBranchCount++;
          }
          break;
        case YIELDPOINT_EPILOGUE_opcode:
          {
            OPT_BasicBlock yieldpoint = findOrCreateYieldpointBlock(ir,
                                        VM_Thread.EPILOGUE);                    
            OPT_Register zero = phys.getGPR(0);
            OPT_Register TSR = phys.getTSR();
            if (!VM.BuildForThreadSwitchUsingControlRegisterBit) {
              OPT_Register PR = phys.getPR();
              p.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(zero), 
                             R(PR), 
                             I(VM_Entrypoints.threadSwitchRequestedField.getOffset()))));
              p.insertBefore(MIR_Binary.create(PPC_CMPI, R(TSR), R(zero), 
                             I(0)));
              instructionCount += 2;
            }
            // Because the GC Map code holds a reference to the original
            // instruction, it is important that we mutate the last instruction
            // because this will be the GC point.
            MIR_CondCall.mutate0(p, PPC_BCL, null, null, R(TSR), 
                OPT_PowerPCConditionOperand.THREAD_SWITCH(), 
                yieldpoint.makeJumpTarget());
            conditionalBranchCount++;
          }
          break;
        case IR_ENDPROLOGUE_opcode:
          {
	    // Remember where the end of prologue is for jdp
	    OPT_Instruction next = p.nextInstructionInCodeOrder();
	    ir.MIRInfo.instAfterPrologue = next;
	    p.remove();
	    p = next.prevInstructionInCodeOrder();
	  }
	  break;

      default:
	if (p.operator().isConditionalBranch())
	  conditionalBranchCount++; 
	else 
	  instructionCount++;
	break;
      }
    }
    
    // this is conservative but pretty close, especially for 
    // reasonably sized methods 
    if ((instructionCount + conditionalBranchCount) > OPT_Assembler.MAX_COND_DISPL)
      machinecodeLength = instructionCount + 2*conditionalBranchCount; 
    else 
      machinecodeLength = instructionCount + conditionalBranchCount;
    if ((machinecodeLength & ~OPT_Assembler.MAX_24_BITS) != 0)
      throw new OPT_OptimizingCompilerException("CodeGen", 
						"method too large to compile:", 
						OPT_Assembler.MAX_24_BITS);
    return machinecodeLength;
  }

  /**
   * Return a basic block holding the call to a runtime yield service.
   * Create a new basic block at the end of the code order if necessary.
   *
   * @param ir the governing IR
   * @param whereFrom is this yieldpoint from the PROLOGUE, EPILOGUE, or a 
   * BACKEDGE?
   */
  final static OPT_BasicBlock findOrCreateYieldpointBlock(OPT_IR ir, 
							  int whereFrom) {
    VM_Method meth = null;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register zero = phys.getGPR(0);
    
    // first see if the requested block exists. If not, set up some
    // state for creating the block
    if (whereFrom == VM_Thread.PROLOGUE) {
      if (ir.MIRInfo.prologueYieldpointBlock != null) 
        return ir.MIRInfo.prologueYieldpointBlock;
      else 
        meth = VM_Entrypoints.optThreadSwitchFromPrologueMethod;
    } else if (whereFrom == VM_Thread.BACKEDGE) {
      if (ir.MIRInfo.backedgeYieldpointBlock != null) 
        return ir.MIRInfo.backedgeYieldpointBlock;
      else 
        meth = VM_Entrypoints.optThreadSwitchFromBackedgeMethod;
    } else if (whereFrom == VM_Thread.EPILOGUE) {
      if (ir.MIRInfo.epilogueYieldpointBlock != null) 
        return ir.MIRInfo.epilogueYieldpointBlock;
      else 
        meth = VM_Entrypoints.optThreadSwitchFromEpilogueMethod;
    }

    // Not found.  create new basic block holding the requested yieldpoint
    // method
    OPT_BasicBlock result = new OPT_BasicBlock(-1, null, ir.cfg);
    ir.cfg.addLastInCodeOrder(result);
    OPT_Register JTOC = phys.getJTOC();
    OPT_Register CTR = phys.getCTR();
    int offset = meth.getOffset();
    if (OPT_Bits.fits(offset, 16)) {
      result.appendInstruction(nonPEIGC(MIR_Load.create(PPC_LWZ, 
                               R(zero), R(JTOC), I(offset))));
    } else {
      result.appendInstruction(MIR_Unary.create(PPC_LDIS, 
                               R(zero), I(offset >>> 16)));
      result.appendInstruction(MIR_Binary.create(PPC_ORI, 
                               R(zero), R(zero), I(offset & 0xffff)));
      result.appendInstruction(nonPEIGC(MIR_Load.create(PPC_LWZX, 
                               R(zero), R(JTOC), R(zero))));
    }
    result.appendInstruction(MIR_Move.create(PPC_MTSPR, R(CTR), R(zero)));
    result.appendInstruction(MIR_Branch.create(PPC_BCTR));

    // cache the create block and then return it
    if (whereFrom == VM_Thread.PROLOGUE)      
      ir.MIRInfo.prologueYieldpointBlock = result;
    else if (whereFrom == VM_Thread.BACKEDGE) 
      ir.MIRInfo.backedgeYieldpointBlock = result;
    else                                      
      ir.MIRInfo.epilogueYieldpointBlock = result;

    return result;
  }
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Encodes the BO & BI condition fields for PowerPC
 * 
 * @see OPT_Operand
 * @author by Mauricio Serrano
 */
public final class OPT_PowerPCConditionOperand extends OPT_Operand {
  /**
   * Value of this operand.
   */
  int value;
  static final int ALWAYS = (20 << 5);
  static final int EQUAL = (12 << 5) | 2;
  static final int NOT_EQUAL = (4 << 5) | 2;
  static final int LESS = (12 << 5) | 0;
  static final int GREATER_EQUAL = (4 << 5) | 0;
  static final int GREATER = (12 << 5) | 1;
  static final int LESS_EQUAL = (4 << 5) | 1;
  static final int OVERFLOW = (12 << 5) | 3;
  static final int NOT_OVERFLOW = (4 << 5) | 3;

  /* interpretation for floating-point values */
  static final int UNORDERED = (12 << 5) | 3;
  static final int NOT_UNORDERED = (4 << 5) | 3;

  /* special RVM value */
  static final int NO_THREAD_SWITCH = (4 << 5) | 0;             // same as !geq
  static final int THREAD_SWITCH = (12 << 5) | 0;               // same as less
  // --CTR == 0
  static final int CTRZ = (17 << 5) | 0;
  // --CTR != 0
  static final int CTRNZ = (16 << 5) | 0;
  // (--CTR == 0) & condition
  static final int CTRZ_EQUAL = (10 << 5) | 2;
  static final int CTRZ_NOT_EQUAL = (2 << 5) | 2;
  static final int CTRZ_LESS = (10 << 5) | 0;
  static final int CTRZ_GREATER_EQUAL = (2 << 5) | 0;
  static final int CTRZ_GREATER = (10 << 5) | 1;
  static final int CTRZ_LESS_EQUAL = (2 << 5) | 1;
  static final int CTRZ_OVERFLOW = (10 << 5) | 3;
  static final int CTRZ_NOT_OVERFLOW = (2 << 5) | 3;
  // (--CTR != 0) & condition
  static final int CTRNZ_EQUAL = (8 << 5) | 2;
  static final int CTRNZ_NOT_EQUAL = (0 << 5) | 2;
  static final int CTRNZ_LESS = (8 << 5) | 0;
  static final int CTRNZ_GREATER_EQUAL = (0 << 5) | 0;
  static final int CTRNZ_GREATER = (8 << 5) | 1;
  static final int CTRNZ_LESS_EQUAL = (0 << 5) | 1;
  static final int CTRNZ_OVERFLOW = (8 << 5) | 3;
  static final int CTRNZ_NOT_OVERFLOW = (0 << 5) | 3;

  // TODO: add the things with the CTR register also.
  OPT_PowerPCConditionOperand (int Code) {
    value = Code;
  }

  static OPT_PowerPCConditionOperand EQUAL () {
    return  new OPT_PowerPCConditionOperand(EQUAL);
  }

  static OPT_PowerPCConditionOperand NOT_EQUAL () {
    return  new OPT_PowerPCConditionOperand(NOT_EQUAL);
  }

  static OPT_PowerPCConditionOperand LESS () {
    return  new OPT_PowerPCConditionOperand(LESS);
  }

  static OPT_PowerPCConditionOperand LESS_EQUAL () {
    return  new OPT_PowerPCConditionOperand(LESS_EQUAL);
  }

  static OPT_PowerPCConditionOperand GREATER () {
    return  new OPT_PowerPCConditionOperand(GREATER);
  }

  static OPT_PowerPCConditionOperand GREATER_EQUAL () {
    return  new OPT_PowerPCConditionOperand(GREATER_EQUAL);
  }

  static OPT_PowerPCConditionOperand UNORDERED () {
    return  new OPT_PowerPCConditionOperand(UNORDERED);
  }

  static OPT_PowerPCConditionOperand NO_THREAD_SWITCH () {
    return  new OPT_PowerPCConditionOperand(NO_THREAD_SWITCH);
  }

  static OPT_PowerPCConditionOperand THREAD_SWITCH () {
    return  new OPT_PowerPCConditionOperand(THREAD_SWITCH);
  }

  static OPT_PowerPCConditionOperand get (OPT_ConditionOperand cond) {
    return  new OPT_PowerPCConditionOperand(cond);
  }

  OPT_Operand copy () {
    return  new OPT_PowerPCConditionOperand(value);
  }

  boolean similar (OPT_Operand op) {
    return  (op instanceof OPT_PowerPCConditionOperand) 
        && (((OPT_PowerPCConditionOperand)op).value == value);
  }

  /**
   * flips the direction of the condition
   */
  OPT_PowerPCConditionOperand flipCode () {
    switch (value) {
      case EQUAL:
        value = NOT_EQUAL;
        break;
      case NOT_EQUAL:
        value = EQUAL;
        break;
      case LESS:
        value = GREATER_EQUAL;
        break;
      case LESS_EQUAL:
        value = GREATER;
        break;
      case GREATER:
        value = LESS_EQUAL;
        break;
      case GREATER_EQUAL:
        value = LESS;
        break;
      case OVERFLOW:
        value = NOT_OVERFLOW;
        break;
      case NOT_OVERFLOW:
        value = OVERFLOW;
        break;
      case CTRZ:
        value = CTRNZ;
        break;
      case CTRNZ:
        value = CTRZ;
        break;
    default:
      throw new OPT_OptimizingCompilerException("Unhandled case in flipCode");
    }
    return  this;
  }

  /**
   * this could be used if you want to flip the order of the operands
   * you will notice that there are some differences
   */
  OPT_PowerPCConditionOperand flipOperands () {
    switch (value) {
      case EQUAL:
        value = NOT_EQUAL;
        break;
      case NOT_EQUAL:
        value = EQUAL;
        break;
      case LESS:
        value = GREATER;
        break;
      case LESS_EQUAL:
        value = GREATER_EQUAL;
        break;
      case GREATER:
        value = LESS;
        break;
      case GREATER_EQUAL:
        value = LESS_EQUAL;
        break;
      case OVERFLOW:
        value = NOT_OVERFLOW;
        break;
      case NOT_OVERFLOW:
        value = OVERFLOW;
        break;
        // TODO remaining
    }
    return  this;
  }

  OPT_PowerPCConditionOperand (OPT_ConditionOperand c) {
    translate(c);
  }

  /**
   * translate from OPT_ConditionOperand: used by BURS
   */
  void translate (OPT_ConditionOperand c) {
    switch (c.value) {
      case OPT_ConditionOperand.EQUAL:
        value = EQUAL;
        break;
      case OPT_ConditionOperand.NOT_EQUAL:
        value = NOT_EQUAL;
        break;
      case OPT_ConditionOperand.LESS:
        value = LESS;
        break;
      case OPT_ConditionOperand.LESS_EQUAL:
        value = LESS_EQUAL;
        break;
      case OPT_ConditionOperand.GREATER:
        value = GREATER;
        break;
      case OPT_ConditionOperand.GREATER_EQUAL:
        value = GREATER_EQUAL;
        break;
      case OPT_ConditionOperand.NULL:
        value = EQUAL;
        break;
      case OPT_ConditionOperand.NONNULL:
        value = NOT_EQUAL;
        break;
      case OPT_ConditionOperand.OVERFLOW:
        value = OVERFLOW;
        break;
      case OPT_ConditionOperand.NOT_OVERFLOW:
        value = NOT_OVERFLOW;
        break;
      case OPT_ConditionOperand.HIGHER:
        value = GREATER;
        break;
      case OPT_ConditionOperand.LOWER:
        value = LESS;
        break;
      case OPT_ConditionOperand.HIGHER_EQUAL:
        value = GREATER_EQUAL;
        break;
      case OPT_ConditionOperand.LOWER_EQUAL:
        value = LESS_EQUAL;
        break;
      case OPT_ConditionOperand.CARRY:
        value = OVERFLOW;
        break;
      case OPT_ConditionOperand.NOT_CARRY:
        value = NOT_OVERFLOW;
        break;
      case OPT_ConditionOperand.UNORDERED:
        value = OVERFLOW;
        break;
      case OPT_ConditionOperand.NOT_UNORDERED:
        value = NOT_OVERFLOW;
        break;
    }
  }

  /**
   * Returns the string representation of this operand.
   */
  public String toString () {
    String result = "ppc ";
    if ((value & 0x1C0) == 0)
      result = result + "--ctr!=0 && ";
    if ((value & 0x1C0) == 0x40)
      result = result + "--ctr==0 && ";
    String temp = null;
    if ((value & 0x300) == 0x100) {             // true
      switch (value & 0x3) {
        case 0:
          temp = "<";
          break;
        case 1:
          temp = ">";
          break;
        case 2:
          temp = "==";
          break;
        case 3:
          temp = "overflow";
          break;
      }
    }
    if ((value & 0x300) == 0x000) {             // false
      switch (value & 0x3) {
        case 0:
          temp = ">=";
          break;
        case 1:
          temp = "<=";
          break;
        case 2:
          temp = "!=";
          break;
        case 3:
          temp = "not_overflow";
          break;
      }
    }
    return  result + temp;
  }
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Encodes the T0 field for trap operations 
 * 
 * @see OPT_Operand
 * @author by Mauricio Serrano
 */
public final class OPT_PowerPCTrapOperand extends OPT_Operand {
  /**
   * Value of this operand.
   */
  int value;
  // see PowerPC BOOK
  static final int ALWAYS = 31;
  static final int EQUAL = 4;
  static final int NOT_EQUAL = 24;
  static final int LESS = 16;
  static final int GREATER_EQUAL = 12;
  static final int GREATER = 8;
  static final int LESS_EQUAL = 20;
  static final int HIGHER = 1;
  static final int LOWER = 2;
  static final int HIGHER_EQUAL = 5;
  static final int LOWER_EQUAL = 6;
  static final int NOT_SAME = 3;
  static final int SAME = 4;

  private OPT_PowerPCTrapOperand (int Code) {
    value = Code;
  }

  static OPT_PowerPCTrapOperand LESS () {
    return  new OPT_PowerPCTrapOperand(LESS);
  }

  static OPT_PowerPCTrapOperand LOWER () {
    return  new OPT_PowerPCTrapOperand(LOWER);
  }

  static OPT_PowerPCTrapOperand ALWAYS () {
    return  new OPT_PowerPCTrapOperand(ALWAYS);
  }

  OPT_Operand copy () {
    return  new OPT_PowerPCTrapOperand(value);
  }

  boolean similar (OPT_Operand op) {
    return  (op instanceof OPT_PowerPCTrapOperand) && 
        (((OPT_ConditionOperand)op).value == value);
  }

  /**
   * flips the direction of the condition
   */
  OPT_PowerPCTrapOperand flipCode () {
    switch (value) {
      case EQUAL:
        value = NOT_EQUAL;
        break;
      case NOT_EQUAL:
        value = EQUAL;
        break;
      case LESS:
        value = GREATER_EQUAL;
        break;
      case LESS_EQUAL:
        value = GREATER;
        break;
      case GREATER:
        value = LESS_EQUAL;
        break;
      case GREATER_EQUAL:
        value = LESS;
        break;
      case HIGHER:
        value = LOWER_EQUAL;
        break;
      case LOWER:
        value = HIGHER_EQUAL;
        break;
      case HIGHER_EQUAL:
        value = LOWER;
        break;
      case LOWER_EQUAL:
        value = HIGHER;
        break;
      case NOT_SAME:
        value = SAME;
        break;
    }
    return  this;
  }

  /**
   * this could be used if you want to flip the order of the operands
   * you will notice that there are some differences
   */
  OPT_PowerPCTrapOperand flipOperands () {
    switch (value) {
      case EQUAL:
        value = NOT_EQUAL;
        break;
      case NOT_EQUAL:
        value = EQUAL;
        break;
      case LESS:
        value = GREATER;
        break;
      case LESS_EQUAL:
        value = GREATER_EQUAL;
        break;
      case GREATER:
        value = LESS;
        break;
      case GREATER_EQUAL:
        value = LESS_EQUAL;
        break;
      case HIGHER:
        value = LOWER;
        break;
      case LOWER:
        value = HIGHER;
        break;
      case HIGHER_EQUAL:
        value = LOWER_EQUAL;
        break;
      case LOWER_EQUAL:
        value = HIGHER_EQUAL;
        break;
      case NOT_SAME:
        value = SAME;
        break;
    }
    return  this;
  }

  OPT_PowerPCTrapOperand (OPT_ConditionOperand c) {
    translate(c);
  }

  /**
   * translate from OPT_ConditionOperand: used by BURS
   */
  void translate (OPT_ConditionOperand c) {
    switch (c.value) {
      case OPT_ConditionOperand.EQUAL:
        value = EQUAL;
        break;
      case OPT_ConditionOperand.NOT_EQUAL:
        value = NOT_EQUAL;
        break;
      case OPT_ConditionOperand.LESS:
        value = LESS;
        break;
      case OPT_ConditionOperand.LESS_EQUAL:
        value = LESS_EQUAL;
        break;
      case OPT_ConditionOperand.GREATER:
        value = GREATER;
        break;
      case OPT_ConditionOperand.GREATER_EQUAL:
        value = GREATER_EQUAL;
        break;
      case OPT_ConditionOperand.HIGHER:
        value = HIGHER;
        break;
      case OPT_ConditionOperand.LOWER:
        value = LOWER;
        break;
      case OPT_ConditionOperand.HIGHER_EQUAL:
        value = HIGHER_EQUAL;
        break;
      case OPT_ConditionOperand.LOWER_EQUAL:
        value = LOWER_EQUAL;
        break;
      case OPT_ConditionOperand.SAME:
        value = SAME;
        break;
      case OPT_ConditionOperand.NOT_SAME:
        value = NOT_SAME;
        break;
    }
  }

  /**
   * Returns the string representation of this operand.
   */
  public String toString () {
    String result = "ppc trap ";
    switch (value) {
      case EQUAL:
        return  result + "==";
      case NOT_EQUAL:
        return  result + "!=";
      case LESS:
        return  result + "<";
      case LESS_EQUAL:
        return  result + "<=";
      case GREATER:
        return  result + ">";
      case GREATER_EQUAL:
        return  result + ">=";
      case HIGHER:
        return  result + ">U";
      case LOWER:
        return  result + "<U";
      case HIGHER_EQUAL:
        return  result + ">=U";
      case LOWER_EQUAL:
        return  result + "<=U";
      case NOT_SAME:
        return  result + "U!=";
      case ALWAYS:
        return  result + "always";
    }
    return  "UNKNOWN";
  }
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Pool of symbolic registers.
 * powerPC specific implementation where JTOC is stored in a reserved register.
 * Each IR contains has exactly one register pool object associated with it.
 * 
 * @see OPT_Register
 * 
 * @author Dave Grove
 * @author Mauricio J. Serrano
 * @author John Whaley
 * @modified Vivek Sarkar
 * @author Peter Sweeney
 */
class OPT_RegisterPool extends OPT_GenericRegisterPool {

  /**
   * Initializes a new register pool for the method meth.
   * 
   * @param meth the VM_Method of the outermost method
   */
  OPT_RegisterPool(VM_Method meth) {
    super(meth);
  }

  /**
   * Get the JTOC register
   * 
   * @return the JTOC register
   */ 
  public OPT_Register getJTOC() {
    return physical.getJTOC();
  }

  /**
   * Get a temporary that represents the JTOC register (as an INT)
   * 
   * @param ir  
   * @param s  
   * @return the temp
   */ 
  public OPT_RegisterOperand makeJTOCOp(OPT_IR ir, OPT_Instruction s) {
    return new OPT_RegisterOperand(getJTOC(),VM_Type.AddressType);
  }

  /**
   * Get a temporary that represents the JTOC register (as an Object)
   * 
   * @return the temp
   */ 
  public OPT_RegisterOperand makeTocOp() {
    return new OPT_RegisterOperand(getJTOC(),VM_Type.JavaLangObjectType);
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This class implements the machine-specific magics for the opt compiler. 
 *
 * @see OPT_GenerateMagic for the machine-independent magics.
 * 
 * @author Dave Grove
 * @author Mauricio Serrano
 */
class OPT_GenerateMachineSpecificMagic 
  implements OPT_Operators, VM_StackframeLayoutConstants {

  /**
   * "Semantic inlining" of methods of the VM_Magic class
   * Based on the methodName, generate a sequence of opt instructions
   * that implement the magic, updating the stack as neccessary
   *
   * @param bc2ir the bc2ir object that is generating the 
   *              ir containing this magic
   * @param gc == bc2ir.gc
   * @param meth the VM_Method that is the magic method
   */
  static void generateMagic (OPT_BC2IR bc2ir, 
			     OPT_GenerationContext gc, 
			     VM_Method meth) 
    throws OPT_MagicNotImplementedException {

    OPT_PhysicalRegisterSet phys = gc.temps.getPhysicalRegisterSet();

    VM_Atom methodName = meth.getName();
    if (methodName == VM_MagicNames.getFramePointer) {
      bc2ir.push(gc.temps.makeFPOp());
      gc.allocFrame = true;
    } else if (methodName == VM_MagicNames.getTocPointer) {
      bc2ir.push(gc.temps.makeJTOCOp(null,null));
    } else if (methodName == VM_MagicNames.getJTOC) {
      bc2ir.push(gc.temps.makeTocOp());
    } else if (methodName == VM_MagicNames.getThreadId) {
      OPT_RegisterOperand TIOp = 
	new OPT_RegisterOperand(phys.getTI(),VM_Type.IntType);
      bc2ir.push(TIOp);
    } else if (methodName == VM_MagicNames.setThreadSwitchBit) {
      bc2ir.appendInstruction(Empty.create(SET_THREAD_SWITCH_BIT));
    } else if (methodName == VM_MagicNames.clearThreadSwitchBit) {
      bc2ir.appendInstruction(Empty.create(CLEAR_THREAD_SWITCH_BIT));
    } else if (methodName == VM_MagicNames.getCallerFramePointer) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCallerFramePointer) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getCompiledMethodID) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTempInt();
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setCompiledMethodID) {
      OPT_Operand val = bc2ir.popInt();
      OPT_Operand fp = bc2ir.popInt();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_METHOD_ID_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getNextInstructionAddress) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_NEXT_INSTRUCTION_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setNextInstructionAddress) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   fp, 
					   new OPT_IntConstantOperand(STACKFRAME_NEXT_INSTRUCTION_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getReturnAddress) {
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand callerFP = gc.temps.makeTemp(VM_Type.AddressType);
      OPT_RegisterOperand val = gc.temps.makeTemp(VM_Type.AddressType);
      bc2ir.appendInstruction(Load.create(INT_LOAD, callerFP, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					  null));
      bc2ir.appendInstruction(Load.create(INT_LOAD, val, 
					  callerFP,
					  new OPT_IntConstantOperand(STACKFRAME_NEXT_INSTRUCTION_OFFSET),
					  null));
      bc2ir.push(val.copyD2U());
    } else if (methodName == VM_MagicNames.setReturnAddress) {
      OPT_Operand val = bc2ir.popAddress();
      OPT_Operand fp = bc2ir.popAddress();
      OPT_RegisterOperand callerFP = gc.temps.makeTempInt();
      bc2ir.appendInstruction(Load.create(INT_LOAD, callerFP, 
					  fp,
					  new OPT_IntConstantOperand(STACKFRAME_FRAME_POINTER_OFFSET),
					  null));
      bc2ir.appendInstruction(Store.create(INT_STORE, val, 
					   callerFP, 
					   new OPT_IntConstantOperand(STACKFRAME_NEXT_INSTRUCTION_OFFSET),
					   null));
    } else if (methodName == VM_MagicNames.getTime) {
      OPT_RegisterOperand val = gc.temps.makeTempDouble();
      OPT_MethodOperand mo = 
	new OPT_MethodOperand(VM_Entrypoints.getTimeInstructionsField,
			      OPT_MethodOperand.STATIC, 
			      VM_Entrypoints.getTimeInstructionsField.getOffset());
      bc2ir.appendInstruction(Call.create1(CALL, val, null, mo, bc2ir.popRef()));
      bc2ir.push(val.copyD2U(), VM_Type.DoubleType);
    } else if (methodName == VM_MagicNames.sysCall0) {
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, toc));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_0) {
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create0(SYSCALL, op0, ip, toc));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall_L_I) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, 
						  toc, p1));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall1) {
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create1(SYSCALL, op0, ip, 
                                                  toc, p1));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall2) {
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, 
						  toc, p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCallAD) {
      OPT_Operand p2 = bc2ir.popDouble();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create2(SYSCALL, op0, ip, 
                                                  toc, p1, p2));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall3) {
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create3(SYSCALL, op0, ip, 
						  toc, p1, p2, p3));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.sysCall4) {
      OPT_Operand p4 = bc2ir.popInt();
      OPT_Operand p3 = bc2ir.popInt();
      OPT_Operand p2 = bc2ir.popInt();
      OPT_Operand p1 = bc2ir.popInt();
      OPT_Operand toc = bc2ir.popInt();
      OPT_Operand ip = bc2ir.popInt();
      OPT_RegisterOperand op0 = gc.temps.makeTempInt();
      bc2ir.appendInstruction(CallSpecial.create4(SYSCALL, op0, ip, 
						  toc, p1, p2, p3, p4));
      bc2ir.push(op0.copyD2U());
    } else if (methodName == VM_MagicNames.getTimeBase) {
      OPT_RegisterOperand op0 = gc.temps.makeTempLong();
      bc2ir.appendInstruction(Nullary.create(GET_TIME_BASE, op0));
      bc2ir.pushDual(op0.copyD2U());
    } else if (methodName == VM_MagicNames.isync) {
      if (!gc.options.NO_CACHE_FLUSH)
        bc2ir.appendInstruction(Empty.create(READ_CEILING));
    } else if (methodName == VM_MagicNames.sync) {
      if (!gc.options.NO_CACHE_FLUSH)
        bc2ir.appendInstruction(Empty.create(WRITE_FLOOR));
    } else if (methodName == VM_MagicNames.dcbst) {
      bc2ir.appendInstruction(CacheOp.create(DCBST, bc2ir.popInt()));
    } else if (methodName == VM_MagicNames.icbi) {
      bc2ir.appendInstruction(CacheOp.create(ICBI, bc2ir.popInt()));
    } else {
      // Distinguish between magics that we know we don't implement
      // (and never plan to implement) and those (usually new ones) 
      // that we want to be warned that we don't implement.
      String msg = "Magic method not implemented: " + meth;
      if (methodName == VM_MagicNames.returnToNewStack || 
	  methodName == VM_MagicNames.pragmaNoOptCompile) {
	throw OPT_MagicNotImplementedException.EXPECTED(msg);
      } else {
	throw OPT_MagicNotImplementedException.UNEXPECTED(msg);
      }
    }
  }
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import  java.util.Vector;

/**
 * This class specifies the order in which OPT_CompilerPhases are
 * executed in the target-specific backend of the optimzing compiler.
 * The methods LIR2MIR, MIROptimizations, and MIR2MC each specify the
 * elements that make up the main compilation stages.
 *
 * @author Stephen Fink
 * @author Dave Grove
 * @author Michael Hind 
 */
class OPT_MIROptimizationPlanner extends OPT_OptimizationPlanner {


  /**
   * Initialize the "master plan" for the PowerPC backend of the opt compiler.
   */
  static void intializeMasterPlan(Vector temp) {
    LIR2MIR(temp);
    MIROptimizations(temp);
    MIR2MC(temp);
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert LIR to PowerPC MIR.
   *
   * @param p the plan under construction
   */
  private static void LIR2MIR(Vector p) {
    composeComponents(p, "Convert LIR to MIR", new Object[] {
      // Optional printing of final LIR
      new OPT_IRPrinter("Final LIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_FINAL_LIR;
        }
      }, 
      // Split very large basic blocks into smaller ones.
      new OPT_SplitBasicBlock(), 
      // Change operations that split live ranges to moves
      new OPT_MutateSplits(),
      // Instruction selection
      new OPT_ConvertLIRtoMIR(), 
      // Optional printing of initial MIR
      new OPT_IRPrinter("Initial MIR") {
        boolean shouldPerform(OPT_Options options) {
          return options.PRINT_MIR;
        }
      }
    });
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed on PowerPC MIR.
   *
   * @param p the plan under construction
   */
  private static void MIROptimizations(Vector p) {
    ////////////////////
    // MIR OPTS(1) (before register allocation)
    ////////////////////
    // INSTRUCTION SCHEDULING (PRE-PASS --- PRIOR TO REGISTER ALLOCATION)
    addComponent(p, new OPT_PrePassScheduler());
    // NullCheck combining and validation operand removal.
    addComponent(p, new OPT_NullCheckCombining());
    ////////////////////
    // GCMapping part1 and RegisterAllocation
    ////////////////////

    composeComponents(p, "Register Mapping", new Object[] {
      // MANDATORY: Expand calling convention
      new OPT_ExpandCallingConvention(),
      // MANDATORY: Perform Live analysis and create GC maps
      new OPT_LiveAnalysis(true, false),
      // MANDATORY: Perform register allocation
      new OPT_RegisterAllocator(),
      // MANDATORY: Add prologue and epilogue
      new OPT_PrologueEpilogueCreator(),
    });
    ////////////////////
    // MIR OPTS(2) (after register allocation)
    // NOTE: GCMapping part 1 has created the GC maps already.
    //       From now until the end of compilation, we cannot change
    //       the set of live references at a GC point 
    //       without updating the GCMaps.
    //       Effectively this means that we can only do the 
    //       most trivial optimizations from
    //       here on out without having to some potentially complex bookkeeping.
    ////////////////////
    // Peephole branch optimizations
    addComponent(p, new OPT_MIRBranchOptimizations(1));
  }

  /** 
   * This method defines the optimization plan elements that
   * are to be performed to convert PowerPC MIR into
   * ready-to-execute machinecode (and associated mapping tables).
   * 
   * @param p the plan under construction
   */
  private static void MIR2MC(Vector p) {
    // MANDATORY: Final assembly
    addComponent(p, new OPT_IRPrinter("Final MIR") {
	boolean shouldPerform(OPT_Options options) {
	  return options.PRINT_FINAL_MIR; } 
      });
    addComponent(p, new OPT_ConvertMIRtoMC());
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;

/**
 * This class contains PowerPC Calling conventions.
 * The two public methods are:
 * <ul> 
 *  <li> expandCallingConventions(OPT_IR) which is called by 
 *      the register allocator immediately before allocation to make 
 *      manifest the use of registers by the calling convention.
 *  <li> expandSysCall(OPT_Instruction, OPT_IR) which is called to 
 *      expand a SYSCALL HIR instruction into the appropriate 
 *      sequence of LIR instructions.
 *  </ul>
 *
 * @author Mauricio J. Serrano
 * @author Dave Grove
 * @author Stephen Fink
 */
final class OPT_CallingConvention extends OPT_IRTools 
implements OPT_PhysicalRegisterConstants {

  /**
   * Expand calls, returns, and add initialize code for arguments/parms.
   * @param ir
   */
  public static void expandCallingConventions(OPT_IR ir) {
    for (OPT_Instruction inst = ir.firstInstructionInCodeOrder(); 
         inst != null; inst = inst.nextInstructionInCodeOrder()) {
      if (inst.isCall()) {
        callExpand(inst, ir);
      } else if (inst.isReturn()) {
        returnExpand(inst, ir);
      }
    }
    prologueExpand(ir);
  }


  /**
   * This is just called for instructions that were added by 
   * instrumentation during register allocation
   */
  public static void expandCallingConventionsForInstrumentation(OPT_IR ir, 
                                                                OPT_Instruction 
                                                                from, 
                                                                OPT_Instruction 
                                                                to) {
    for (OPT_Instruction inst = from; inst != to; 
         inst = inst.nextInstructionInCodeOrder()) {
      if (inst.isCall()) {
        callExpand(inst, ir);
      } else if (inst.isReturn()) {
        returnExpand(inst, ir);
      }
    }
  }


  /**
   * Calling convention to implement calls to 
   * native (C) routines using the AIX linkage conventions
   */
  public static void expandSysCall(OPT_Instruction s, OPT_IR ir) {
    OPT_RegisterOperand ip, toc = null;
    if (CallSpecial.getMethod(s) instanceof OPT_SysMethodOperand) {
      OPT_SysMethodOperand nat = (OPT_SysMethodOperand)CallSpecial.getClearMethod(s);
      OPT_RegisterOperand t1 = 
	OPT_ConvertToLowLevelIR.getStatic(s, ir, VM_Entrypoints.the_boot_recordField);
      ip = OPT_ConvertToLowLevelIR.getField(s, ir, t1, nat.ip);
      toc = OPT_ConvertToLowLevelIR.getField(s, ir, t1.copyRO(), VM_Entrypoints.sysTOCField);
    } else {
      ip = (OPT_RegisterOperand)CallSpecial.getClearAddress(s);
      toc = (OPT_RegisterOperand)CallSpecial.getClearMethod(s);
    }

    /* compute the parameter space */
    int numberParams = CallSpecial.getNumberOfParams(s);
    int parameterWords = 0;
    for (int i = 0; i < numberParams; i++) {
      parameterWords++;
      OPT_Operand op = CallSpecial.getParam(s, i);
      if (op instanceof OPT_RegisterOperand) {
        OPT_RegisterOperand reg = (OPT_RegisterOperand)op;
        if ((reg.type == VM_Type.LongType) || (reg.type == VM_Type.DoubleType))
          parameterWords++;
      } else if ((op instanceof OPT_LongConstantOperand) || 
                 (op instanceof OPT_DoubleConstantOperand))
        parameterWords++;
    }
    // see PowerPC Compiler Writer's Guide, pp. 162
    ir.stackManager.allocateParameterSpace((6 + parameterWords)*4);
    // IMPORTANT WARNING: as the callee C routine may destroy the cmid field
    // (it is the saved CR field of the callee in C convention) 
    // we are restoring the methodID after a sysCall. 
    OPT_Instruction s2;
    if (toc != null) {
      s2 = Store.create(INT_STORE, ir.regpool.makeJTOCOp(ir,s), 
                        ir.regpool.makeFPOp(), 
                        I(20), null);         // TODO: valid location?
      s.insertBack(s2);
      s.insertBack(Move.create(INT_MOVE, ir.regpool.makeJTOCOp(ir,s), toc));
    }
    Call.mutate0(s, CALL, CallSpecial.getClearResult(s), ip, null);
    if (toc != null) {
      s2 = Load.create(INT_LOAD, ir.regpool.makeJTOCOp(ir,s), ir.regpool.makeFPOp(),
                       I(20), null);         // TODO: valid location?
      s.insertFront(s2);
      OPT_RegisterOperand temp = ir.regpool.makeTempInt();
      s2 = Move.create(INT_MOVE, temp, I(ir.compiledMethod.getId()));
      OPT_Instruction s3 = Store.create(INT_STORE, temp.copy(), 
                                        ir.regpool.makeFPOp(), 
                                        I(STACKFRAME_METHOD_ID_OFFSET), null);  // TODO: valid location?
      s.insertFront(s3);
      s.insertFront(s2);
    }
  }


  /////////////////////
  // Implementation
  /////////////////////

  /**
   * Expand the prologue instruction to make calling convention explicit.
   */
  private static void prologueExpand(OPT_IR ir) {
    
  // set up register lists for dead code elimination.
    if (ir.options.SIMPLE_OPT) {
      OPT_DefUse.computeDU(ir);
    }

    OPT_Instruction prologueInstr = 
      ir.firstInstructionInCodeOrder().nextInstructionInCodeOrder();
    if (VM.VerifyAssertions) VM.assert(prologueInstr.operator == IR_PROLOGUE);
    OPT_Instruction start = prologueInstr.nextInstructionInCodeOrder();

    int int_index = 0;
    int double_index = 0;
    int spilledArgumentCounter = 
      (-256 - VM_Constants.STACKFRAME_HEADER_SIZE) >> 2;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register FP = phys.getFP();
    for (OPT_OperandEnumeration symParams = prologueInstr.getDefs();
         symParams.hasMoreElements(); ) {
      OPT_RegisterOperand symParamOp = (OPT_RegisterOperand)symParams.next();
      OPT_Register symParam = symParamOp.register;
      VM_Type t = symParamOp.type;
      if (t.isFloatType()) {
        // if optimizing, skip dead parameters
        // SJF: This optimization current breaks the paranoid sanity test.
        // Why? TODO: figure this out and remove the 'true' case below
        if (true || !ir.options.SIMPLE_OPT || symParam.useList != null) {

          if (double_index < NUMBER_DOUBLE_PARAM) {
            OPT_Register param = phys.get(FIRST_DOUBLE_PARAM + (double_index));
            start.insertBack(MIR_Move.create(PPC_FMR, F(symParam), F(param)));
          } else {                  // spilled parameter
            start.insertBack(nonPEIGC(MIR_Load.create(PPC_LFS, F(symParam), 
                                                      R(FP), 
                                                      I(spilledArgumentCounter 
                                                        << 2))));
            spilledArgumentCounter--;
          }
        }
        double_index++;
      } else if (t.isDoubleType()) {
        // if optimizing, skip dead parameters
        // SJF: This optimization current breaks the paranoid sanity test.
        // Why? TODO: figure this out and remove the 'true' case below
        if (true || !ir.options.SIMPLE_OPT || symParam.useList != null) {
          if (double_index < NUMBER_DOUBLE_PARAM) {
            OPT_Register param = phys.get(FIRST_DOUBLE_PARAM + (double_index));
            start.insertBack(MIR_Move.create(PPC_FMR, D(symParam), D(param)));
          } else {                  // spilled parameter
            start.insertBack(nonPEIGC(MIR_Load.create(PPC_LFD, D(symParam), 
                                                      R(FP), 
                                                      I(spilledArgumentCounter 
                                                        << 2))));
            spilledArgumentCounter -= 2;
          }
        }
        double_index++;
      } else { // t is object, 1/2 of a long, int, short, char, byte, or boolean
        // if optimizing, skip dead parameters
        // SJF: This optimization current breaks the paranoid sanity test.
        // Why? TODO: figure this out and remove the 'true' case below
        if (true || !ir.options.SIMPLE_OPT || symParam.useList != null) {
          if (int_index < NUMBER_INT_PARAM) {
            OPT_Register param = phys.get(FIRST_INT_PARAM + (int_index));
            start.insertBack(MIR_Move.create(PPC_MOVE, 
                                             new OPT_RegisterOperand
                                             (symParam, t),
                                             R(param)));
          } else {                  // spilled parameter
            start.insertBack(nonPEIGC(MIR_Load.create
                                      (PPC_LWZ, new OPT_RegisterOperand
                                       (symParam, t), R(FP), 
                                       I(spilledArgumentCounter << 2))));
            spilledArgumentCounter--;
          }
        }
        int_index++;
      }
    }

    // Now that we've made the calling convention explicit in the prologue,
    // set IR_PROLOGUE to have no defs.
    prologueInstr.replace(Prologue.create(IR_PROLOGUE, 0));
  }


  /**
   * Expand the call as appropriate
   * @param s the call instruction
   * @param ir the ir
   */
  private static void callExpand(OPT_Instruction s, OPT_IR ir) {
    int NumberParams = MIR_Call.getNumberOfParams(s);
    int int_index = 0;          // points to the first integer volatile
    int double_index = 0;       // poinst to the first f.p.    volatile
    int callSpillLoc = STACKFRAME_HEADER_SIZE;
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Instruction prev = s.prevInstructionInCodeOrder();
    OPT_Register FP = phys.getFP();
    OPT_Register JTOC = phys.getJTOC();
    // (1) Expand parameters
    for (int opNum = 0; opNum < NumberParams; opNum++) {
      OPT_Operand param = MIR_Call.getClearParam(s, opNum);
      OPT_RegisterOperand Reg = (OPT_RegisterOperand)param;
      // as part of getting into MIR, we make sure all params are in registers.
      OPT_Register reg = Reg.register;
      if (Reg.type == VM_Type.FloatType) {
        if (double_index < NUMBER_DOUBLE_PARAM) {       // register copy
          OPT_Register real = phys.get(FIRST_DOUBLE_PARAM + (double_index++));
          s.insertBack(MIR_Move.create(PPC_FMR, F(real), Reg));
          Reg = F(real);
          // Record that the call now has a use of the real reg
          // This is to ensure liveness is correct
          MIR_Call.setParam(s, opNum, Reg);
        } else {                  // spill to memory
          OPT_Instruction p = prev.nextInstructionInCodeOrder();
          p.insertBack(nonPEIGC(MIR_Store.create(PPC_STFS, F(reg), R(FP), 
                                                 I(callSpillLoc))));
          callSpillLoc += 4;
          // We don't have uses of the heap at MIR, so null it out
          MIR_Call.setParam(s, opNum, null);
        }
      } else if (Reg.type == VM_Type.DoubleType) {
        if (double_index < NUMBER_DOUBLE_PARAM) {     // register copy
          OPT_Register real = phys.get(FIRST_DOUBLE_PARAM + (double_index++));
          s.insertBack(MIR_Move.create(PPC_FMR, D(real), Reg));
          Reg = D(real);
          // Record that the call now has a use of the real reg
          // This is to ensure liveness is correct
          MIR_Call.setParam(s, opNum, Reg);
        } else {                  // spill to memory
          OPT_Instruction p = prev.nextInstructionInCodeOrder();
          p.insertBack(nonPEIGC(MIR_Store.create(PPC_STFD, D(reg), R(FP), 
                                                 I(callSpillLoc))));
          callSpillLoc += 8;
          // We don't have uses of the heap at MIR, so null it out
          MIR_Call.setParam(s, opNum, null);
        }
      } else {                    // IntType (or half of long) or reference
        if (int_index < NUMBER_INT_PARAM) {             // register copy
          OPT_Register real = phys.get(FIRST_INT_PARAM + (int_index++));
          OPT_RegisterOperand Real = new OPT_RegisterOperand(real, Reg.type);
          s.insertBack(MIR_Move.create(PPC_MOVE, Real, Reg));
          Reg = new OPT_RegisterOperand(real, Reg.type);
          // Record that the call now has a use of the real reg
          // This is to ensure liveness is correct
          MIR_Call.setParam(s, opNum, Reg);
        } else {                  // spill to memory
          OPT_Instruction p = prev.nextInstructionInCodeOrder();
          p.insertBack(nonPEIGC(MIR_Store.create
                                (PPC_STW, 
                                 new OPT_RegisterOperand(reg, Reg.type), 
                                 R(FP), I(callSpillLoc))));
          callSpillLoc += 4;
          // We don't have uses of the heap at MIR, so null it out
          MIR_Call.setParam(s, opNum, null);
        }
      }
    }
    // If we needed to pass arguments on the stack, 
    // then make sure we have a big enough stack 
    if (callSpillLoc != STACKFRAME_HEADER_SIZE)
      ir.stackManager.allocateParameterSpace(callSpillLoc);
    // (2) expand result
    OPT_RegisterOperand callResult = null;
    OPT_Instruction lastCallSeqInstr = s;
    if (MIR_Call.hasResult2(s)) {
      OPT_RegisterOperand result2 = MIR_Call.getClearResult2(s);
      OPT_RegisterOperand physical = new OPT_RegisterOperand(phys.get
                                                             (FIRST_INT_RETURN 
                                                              + 1), 
                                                             result2.type);
      OPT_Instruction tmp = MIR_Move.create(PPC_MOVE, result2, physical);
      lastCallSeqInstr.insertFront(tmp);
      lastCallSeqInstr = tmp;
      MIR_Call.setResult2(s, null);
    }
    if (MIR_Call.hasResult(s)) {
      OPT_RegisterOperand result1 = MIR_Call.getClearResult(s);
      callResult = result1;
      if (result1.type == VM_Type.DoubleType 
          || result1.type == VM_Type.FloatType) {
        OPT_RegisterOperand physical = new 
          OPT_RegisterOperand(phys.get(FIRST_DOUBLE_RETURN), 
                              result1.type);
        OPT_Instruction tmp = MIR_Move.create(PPC_FMR, result1, physical);
        lastCallSeqInstr.insertFront(tmp);
        lastCallSeqInstr = tmp;
        MIR_Call.setResult(s, null);
      } else {
        OPT_RegisterOperand physical = new 
          OPT_RegisterOperand(phys.get(FIRST_INT_RETURN), 
                              result1.type);
        OPT_Instruction tmp = MIR_Move.create(PPC_MOVE, result1, physical);
        lastCallSeqInstr.insertFront(tmp);
        lastCallSeqInstr = tmp;
        MIR_Call.setResult(s, null);
      }
    }
  }

  /**
   * Expand return statements.
   * @param s the return instruction
   * @param ir the ir
   */
  private static void returnExpand (OPT_Instruction s, OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    if (MIR_Return.hasVal(s)) {
      OPT_RegisterOperand symb1 = MIR_Return.getClearVal(s);
      OPT_RegisterOperand phys1;
      if ((symb1.type == VM_Type.FloatType) 
          || (symb1.type == VM_Type.DoubleType)) {
        phys1 = D(phys.get(FIRST_DOUBLE_RETURN));
        s.insertBack(MIR_Move.create(PPC_FMR, phys1, symb1));
      } else {
        phys1 = new OPT_RegisterOperand(phys.get(FIRST_INT_RETURN), symb1.type);
        s.insertBack(MIR_Move.create(PPC_MOVE, phys1, symb1));
      }
      MIR_Return.setVal(s, phys1.copyD2U());
    }
    if (MIR_Return.hasVal2(s)) {
      OPT_RegisterOperand symb2 = MIR_Return.getClearVal2(s);
      OPT_RegisterOperand phys2 = R(phys.get(FIRST_INT_RETURN + 1));
      s.insertBack(MIR_Move.create(PPC_MOVE, phys2, symb2));
      MIR_Return.setVal2(s, phys2.copyD2U());
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class splits live ranges for certain special cases before register
 * allocation.
 *
 * On PPC, this phase is currently a No-op.
 *
 * @author Stephen Fink
 */
class OPT_MIRSplitRanges extends OPT_CompilerPhase {

  /**
   * Should this phase be performed?
   * @param options controlling compiler options
   * @return true or false
   */
  final boolean shouldPerform(OPT_Options options) {
    return false;
  }

  /**
   * Return the name of this phase
   * @return "Live Range Splitting"
   */
  final String getName() {
    return "MIR Range Splitting"; 
  }

  /**
   * The main method.
   * 
   * @param ir the governing IR
   */
  final public void perform(OPT_IR ir) {
  }
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class provides utilities to record defs and uses of physical
 * registers by IR operators.
 *
 * @author Stephen Fink
 */
class OPT_PhysicalDefUse {

  // constants used to encode defs/uses of physical registers
  final static int mask         = 0x00;  // empty mask
  final static int maskC0       = 0x01;
  final static int maskXER      = 0x02;
  final static int maskLR       = 0x04;
  final static int maskJTOC     = 0x08;
  final static int maskCTR      = 0x10;
  final static int maskPR       = 0x20;

  // Meta mask for the enumeration.
  private final static int maskHIGH = 0x20;
  private final static int maskALL  = 0x3F;

  final static int maskC0_XER   = maskC0 | maskXER;
  final static int maskJTOC_LR  = maskJTOC | maskLR;
  final static int maskJTOC_CTR = maskJTOC | maskCTR;
  final static int maskcallDefs = maskLR;
  final static int maskcallUses = maskJTOC;
  final static int maskIEEEMagicUses = maskJTOC;
  final static int maskTSPDefs  = maskPR;
  final static int maskTSPUses  = maskJTOC;

  /**
   * @return a string representation of the physical registers encoded by
   * an integer
   */
  static String getString(int code) {
    if (code == mask) return "";
    // Not a common case, construct it...
    String s = "";
    if ((code & maskC0) != 0) s += " C0";
    if ((code & maskXER) != 0) s += " XER";
    if ((code & maskLR) != 0) s += " LR";
    if ((code & maskJTOC) != 0) s += " JTOC";
    if ((code & maskCTR) != 0) s += " CTR";
    if ((code & maskPR) != 0) s += " PR";
    return s;
  }

  /**
   * @param code an integer that encodes a set of physical registers
   * @param ir the governing IR
   * @return an enumeration of the physical registers embodied by a code
   */
  static PDUEnumeration enumerate(int code, OPT_IR ir) {
    return new PDUEnumeration(code,ir);
  }

  /**
   * @param ir the governing IR
   * @return an enumeration of all physical registers that code be 
   *         implicitly defed/used
   */
  static PDUEnumeration enumerateAllImplicitDefUses(OPT_IR ir) {
    return new PDUEnumeration(maskALL,ir);
  }

  /**
   * A class to enumerate physical registers based on a code.
   */
  static final class PDUEnumeration implements Enumeration {
    private int code;
    private int curMask;
    private OPT_PhysicalRegisterSet phys;
    
    PDUEnumeration(int c, OPT_IR ir) {
      phys = ir.regpool.getPhysicalRegisterSet();
      code = c;
      curMask = maskHIGH;
    }

    public boolean hasMoreElements() {
      return code != 0;
    }

    public Object nextElement() {
      while (true) {
	int curBit = code & curMask;
	code -= curBit;
	curMask = curMask >> 1;
	if (curBit != 0) return getReg(curBit, phys);
      }
    }

    // artifically make static to enable scalar replacement of 
    // enumeration object without requiring this method to be inlined.
    private static OPT_Register getReg(int m, OPT_PhysicalRegisterSet phys) {
      switch(m) {
      case maskC0: return phys.getConditionRegister(0);
      case maskXER: return phys.getXER();
      case maskLR: return phys.getLR();
      case maskJTOC: return phys.getJTOC();
      case maskCTR: return phys.getCTR();
      case maskPR: return phys.getPR();
      }
      OPT_OptimizingCompilerException.UNREACHABLE();
      return null; // placate jikes.
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * This class holds constants that describe PowerPC register set.
 *
 * @author Mauricio J. Serrano
 * @author Stephen Fink
 * @modified Vivek Sarkar
 * @see OPT_RegisterAllocator
 */
interface OPT_PhysicalRegisterConstants extends VM_RegisterConstants {
  
  // Types of values stored in physical registers; 
  // These affect instruction selection for accessing
  // the data
  static final byte INT_VALUE= 0;
  static final byte DOUBLE_VALUE = 1;
  static final byte FLOAT_VALUE = 2;
  static final byte CONDITION_VALUE = 3;
  
  // There are different types of hardware registers, so we define
  // the following register classes:
  // NOTE: they must be in consecutive ordering
  // TODO: Kill this?
  static final byte INT_REG = 0;
  static final byte DOUBLE_REG = 1;
  static final byte CONDITION_REG = 2;
  static final byte SPECIAL_REG = 3;
  static final byte NUMBER_TYPE = 4;

  // Derived constants for use by the register pool.
  // In the register pool, the physical registers are assigned integers
  // based on these constants.
  static final int FIRST_INT = 0;
  static final int FIRST_DOUBLE = NUM_GPRS;
  static final int FIRST_CONDITION = NUM_GPRS + NUM_FPRS;
  static final int FIRST_SPECIAL = NUM_GPRS + NUM_FPRS + NUM_CRS;

  // Derived constants for use by the register pool.
  static final int NUMBER_INT_NONVOLAT = LAST_NONVOLATILE_GPR 
                                         - FIRST_NONVOLATILE_GPR + 1;
  static final int NUMBER_DOUBLE_NONVOLAT = LAST_NONVOLATILE_FPR 
                                            - FIRST_NONVOLATILE_FPR + 1;
                                           
  
  // Derived constants for use by the register pool.
  // These constants give the register pool numbers for parameters
  static final int FIRST_INT_PARAM = FIRST_VOLATILE_GPR + FIRST_INT;
  static final int NUMBER_INT_PARAM = LAST_VOLATILE_GPR - FIRST_VOLATILE_GPR
                                        + 1;
  static final int FIRST_DOUBLE_PARAM = FIRST_VOLATILE_FPR + FIRST_DOUBLE;
  static final int NUMBER_DOUBLE_PARAM = LAST_VOLATILE_FPR - FIRST_VOLATILE_FPR
                                        + 1;
  
  // Derived constants for use by the register pool.
  // These constants give the register pool numbers for caller saved registers 
  // (or volatile registers, preserved across function calls).
  // NOTE: the order is used by the register allocator 
  // TODO: fix this.
  static final int FIRST_INT_RETURN = FIRST_VOLATILE_GPR + FIRST_INT;
  static final int NUMBER_INT_RETURN = 2;
  static final int FIRST_DOUBLE_RETURN = FIRST_VOLATILE_FPR + FIRST_DOUBLE;
  static final int NUMBER_DOUBLE_RETURN = 1;

  // special PowerPC registers 
  static final int XER = FIRST_SPECIAL + 0;     // extended register
  static final int LR = FIRST_SPECIAL + 1;      // link register
  static final int CTR = FIRST_SPECIAL + 2;     // count register
  static final int CR = FIRST_SPECIAL + 3;      // condition register
  static final int TU = FIRST_SPECIAL + 4;      // time upper
  static final int TL = FIRST_SPECIAL + 5;      // time lower
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;

/**
 * This class represents a set of OPT_Registers corresponding to the
 * PowerPC register set.
 *
 * <P> Implementation Notes:
 * <P> Due to some historical ugliness not yet cleaned up, the register
 * allocator depend on properties cached in the 
 * <code>next</code> field of OPT_Register.  The constructor sets the
 * following properties:
 * <ul>
 * <li> The volatile GPRs form a linked list, starting with
 * FIRST_VOLATILE_GPR and ending with LAST_SCRATCH_GPR
 * <li> The volatile FPRs form a linked list, starting with
 * FIRST_VOLATILE_FPR and ending with LAST_SCRATCH_FPR
 * <li> The non-volatile GPRs form a linked list, starting with
 * FIRST_NONVOLATILE_GPR and ending with LAST_NONVOLATILE_GPR
 * <li> The non-volatile FPRs form a linked list, starting with
 * FIRST_NONVOLATILE_FPR and ending with LAST_NONVOLATILE_FPR
 * <li> The condition registers from a linked list, starting with
 * FIRST_CONDITION and ending with LAST_CONDITION, but excluding
 * THREAD_SWITCH_REGISTER
 * </ul>
 * <P> The register allocator allocates registers according to the order
 * in these lists.  For volatile registers, it traverses the lists in
 * order, starting with getFirstVolatile() and traversing with getNext().  
 * For non-volatiles, it traverses the lists
 * <STRONG> Backwards </STRONG>, starting with getLastNonvolatile() and 
 * using getPrev().
 * <P> TODO; clean up all this and provide appropriate enumerators
 * 
 * @author Stephen Fink
 * @author Mauricio J. Serrano
 */
final class OPT_PhysicalRegisterSet extends OPT_GenericPhysicalRegisterSet 
implements VM_RegisterConstants, OPT_PhysicalRegisterConstants{

  /**
   * This array holds a pool of objects representing physical registers
   */
  private OPT_Register[] reg = new OPT_Register[getSize()];

  /**
   * The set of volatile registers; cached for efficiency
   */
  private OPT_BitSet volatileSet;

  /**
   * Return the total number of physical registers.
   */
  static final int getSize() {
    return NUM_GPRS + NUM_FPRS + NUM_CRS + NUM_SPECIALS;
  }

  /**
   * Return the total number of physical registers.
   */
  final int getNumberOfPhysicalRegisters() {
    return getSize();
  }

  /**
   * Return the total number of nonvolatile GPRs.
   */
  static final int getNumberOfNonvolatileGPRs() {
    return NUM_NONVOLATILE_GPRS;
  }

  /**
   * Return the total number of nonvolatile FPRs.
   */
  static final int getNumberOfNonvolatileFPRs() {
    return NUM_NONVOLATILE_FPRS;
  }

  /**
   * Constructor: set up a pool of physical registers.
   */
  OPT_PhysicalRegisterSet() {
    
    // 1. Create all the physical registers in the pool.
    for (int i = 0; i < reg.length ; i++) {
      OPT_Register r = new OPT_Register(i);
      r.setPhysical();
      reg[i] = r;
    }
    
    // 2. Set the 'integer' attribute on each GPR
    for (int i = FIRST_INT; i < FIRST_DOUBLE; i++) {
      reg[i].setInteger();
    }
    
    // 3. Set the 'double' attribute on each FPR
    for (int i = FIRST_DOUBLE; i < FIRST_CONDITION; i++) {
      reg[i].setDouble();
    }

    // 4. Set the 'condition' attribute on each CR
    for (int i = FIRST_CONDITION; i < FIRST_SPECIAL; i++) {
      reg[i].setCondition();
    }

    // 5. set up the volatile GPRs
    for (int i = FIRST_VOLATILE_GPR; i < LAST_VOLATILE_GPR; i++) {
      OPT_Register r= reg[i];
      r.setVolatile();
      r.linkWithNext(reg[i + 1]);
    }
    reg[LAST_VOLATILE_GPR].setVolatile();
    reg[LAST_VOLATILE_GPR].linkWithNext(reg[FIRST_SCRATCH_GPR]);
    for (int i = FIRST_SCRATCH_GPR; i < LAST_SCRATCH_GPR; i++) {
      OPT_Register r = reg[i];
      r.setVolatile();
      r.linkWithNext(reg[i + 1]);
    }
    reg[LAST_SCRATCH_GPR].setVolatile();
    
    // 6. set up the non-volatile GPRs
    for (int i = FIRST_NONVOLATILE_GPR; i < LAST_NONVOLATILE_GPR; i++) {
      OPT_Register r = reg[i];
      r.setNonVolatile();
      r.linkWithNext(reg[i + 1]);
    }

    // 7. set properties on some special registers
    reg[PROCESSOR_REGISTER].setSpansBasicBlock();
    reg[FRAME_POINTER].setSpansBasicBlock();
    reg[JTOC_POINTER].setSpansBasicBlock();
    reg[THREAD_ID_REGISTER].setSpansBasicBlock();

    // 8. set up the volatile FPRs
    for (int i = FIRST_DOUBLE + FIRST_VOLATILE_FPR; i < FIRST_DOUBLE + 
        LAST_VOLATILE_FPR; i++) {
      OPT_Register r = reg[i];
      r.setVolatile();
      r.linkWithNext(reg[i + 1]);
    }
    reg[FIRST_DOUBLE + LAST_VOLATILE_FPR].linkWithNext(reg[FIRST_DOUBLE
        + FIRST_SCRATCH_FPR]);
    reg[FIRST_DOUBLE + LAST_VOLATILE_FPR].setVolatile();
    if (FIRST_SCRATCH_FPR != LAST_SCRATCH_FPR)
      for (int i = FIRST_DOUBLE + FIRST_SCRATCH_FPR; i < FIRST_DOUBLE + 
          LAST_SCRATCH_FPR; i++) {
        OPT_Register r = reg[i];
        r.setVolatile();
        r.linkWithNext(reg[i + 1]);
      }
    reg[FIRST_DOUBLE + LAST_SCRATCH_FPR].setVolatile();


    // 9. set up the non-volatile FPRs
    for (int i = FIRST_DOUBLE + FIRST_NONVOLATILE_FPR; i < FIRST_DOUBLE
        + LAST_NONVOLATILE_FPR; i++) {
      OPT_Register r = reg[i];
      r.setNonVolatile();
      r.linkWithNext(reg[i + 1]);
    }

    // 10. set up the condition registers
    int firstCR = -1;
    int prevCR = -1;
    for (int i = 0; i < NUM_CRS; i++) {
      // TSR is non-allocatable
      if (i == THREAD_SWITCH_REGISTER)
        continue;
      reg[FIRST_CONDITION + i].setVolatile();
      if (prevCR != -1)
        reg[FIRST_CONDITION + prevCR].linkWithNext(reg[FIRST_CONDITION 
                                                   + i]);
      prevCR = i;
      if (firstCR == -1)
        firstCR = i;
    }

    // 11. cache the volatiles for efficiency
    volatileSet = new OPT_BitSet(this);
    for (Enumeration e = enumerateVolatiles(); e.hasMoreElements(); ) {
      OPT_Register r = (OPT_Register)e.nextElement();
      volatileSet.add(r);
    }

    // 12. Show which registers should be excluded from live analysis
    reg[CTR].setExcludedLiveA();
    reg[CR].setExcludedLiveA();
    reg[TU].setExcludedLiveA();
    reg[TL].setExcludedLiveA();
    reg[XER].setExcludedLiveA();
    reg[FRAME_POINTER].setExcludedLiveA();
    reg[JTOC_POINTER].setExcludedLiveA();
    reg[LR].setExcludedLiveA();

  }

  /**
   * Is a certain physical register allocatable?
   */
  boolean isAllocatable(OPT_Register r) {
    switch(r.number) {
      case PROCESSOR_REGISTER:
      case FRAME_POINTER:
      case JTOC_POINTER:
      case THREAD_ID_REGISTER:
        return false;
      default:
        return  (r.number < FIRST_SPECIAL);
    }
  }

  /**
   * @return the XER register.
   */
  OPT_Register getXER() {
    return  reg[XER];
  }

  /**
   * @return the LR register;.
   */
  OPT_Register getLR() {
    return  reg[LR];
  }

  /**
   * @return the CTR register
   */
  OPT_Register getCTR() {
    return  reg[CTR];
  }

  /**
   * @return the TU register
   */
  OPT_Register getTU() {
    return  reg[TU];
  }

  /**
   * @return the TL register
   */
  OPT_Register getTL() {
    return  reg[TL];
  }

  /**
   * @return the CR register
   */
  OPT_Register getCR() {
    return  reg[CR];
  }

  /**
   * @return the JTOC register
   */
  OPT_Register getJTOC() {
    return  reg[JTOC_POINTER];
  }

  /**
   * @return the FP registers
   */
  OPT_Register getFP() {
    return  reg[FRAME_POINTER];
  }

  /**
   * @return the TI register
   */
  OPT_Register getTI() {
    return  reg[THREAD_ID_REGISTER];
  }

  /**
   * @return the processor register
   */
  OPT_Register getPR() {
    return  reg[PROCESSOR_REGISTER];
  }

  /**
   * @return the thread-switch register
   */
  OPT_Register getTSR() {
    return  reg[FIRST_CONDITION + THREAD_SWITCH_REGISTER];
  }

  /**
   * @return the nth physical GPR 
   */
  OPT_Register getGPR(int n) {
    return  reg[FIRST_INT + n];
  }

  /**
   * @return the first scratch GPR
   */
  OPT_Register getFirstScratchGPR() {
    return  reg[FIRST_SCRATCH_GPR];
  }

  /**
   * @return the last scratch GPR
   */
  OPT_Register getLastScratchGPR() {
    return  reg[LAST_SCRATCH_GPR];
  }

  /**
   * @return the first volatile GPR
   */
  OPT_Register getFirstVolatileGPR() {
    return  reg[FIRST_INT + FIRST_VOLATILE_GPR];
  }

  /**
   * @return the first nonvolatile GPR
   */
  OPT_Register getFirstNonvolatileGPR() {
    return  reg[FIRST_INT + FIRST_NONVOLATILE_GPR];
  }

  /**
   * @return the last nonvolatile GPR
   */
  OPT_Register getLastNonvolatileGPR() {
    return  reg[FIRST_INT + LAST_NONVOLATILE_GPR];
  }

  /**
   * @return the first GPR return
   */
  OPT_Register getFirstReturnGPR() {
    return  reg[FIRST_INT_RETURN];
  }

  /**
   * @return the nth physical FPR 
   */
  OPT_Register getFPR(int n) {
    return  reg[FIRST_DOUBLE + n];
  }

  /**
   * @return the first scratch FPR
   */
  OPT_Register getFirstScratchFPR() {
    return  reg[FIRST_DOUBLE + FIRST_SCRATCH_FPR];
  }

  /**
   * @return the first volatile FPR
   */
  OPT_Register getFirstVolatileFPR() {
    return  reg[FIRST_DOUBLE + FIRST_VOLATILE_FPR];
  }

  /**
   * @return the last scratch FPR
   */
  OPT_Register getLastScratchFPR() {
    return  reg[FIRST_DOUBLE + LAST_SCRATCH_FPR];
  }

  /**
   * @return the first nonvolatile FPR
   */
  OPT_Register getFirstNonvolatileFPR() {
    return  reg[FIRST_DOUBLE + FIRST_NONVOLATILE_FPR];
  }

  /**
   * @return the last nonvolatile FPR
   */
  OPT_Register getLastNonvolatileFPR() {
    return  reg[FIRST_DOUBLE + LAST_NONVOLATILE_FPR];
  }


  /**
   * @return the nth physical condition register 
   */
  OPT_Register getConditionRegister(int n) {
    return  reg[FIRST_CONDITION + n];
  }

  /**
   * @return the first condition
   */
  OPT_Register getFirstConditionRegister() {
    return  reg[FIRST_CONDITION];
  }

  /**
   * @return the first volatile CR
   */
  OPT_Register getFirstVolatileConditionRegister() {
    if (VM.VerifyAssertions) {
      VM.assert(getFirstConditionRegister() != getTSR());
    }
    return  getFirstConditionRegister();
  }
  /**
   * @return the nth physical register in the pool. 
   */
  OPT_Register get(int n) {
    return  reg[n];
  }

  /**
   * @return the first volatile physical register of a given class
   * @param regClass one of INT_REG, DOUBLE_REG, CONDITION_REG, or
   * SPECIAL_REG
   */
  OPT_Register getFirstVolatile(int regClass) {
    switch (regClass) {
      case INT_REG:
        return getFirstVolatileGPR();
      case DOUBLE_REG:
        return getFirstVolatileFPR();
      case CONDITION_REG:
        return getFirstVolatileConditionRegister();
      case SPECIAL_REG:
        return null;
      default:
        throw new OPT_OptimizingCompilerException("Unknown register class");
    }
  }

  /**
   * @return the first nonvolatile physical register of a given class
   * @param regClass one of INT_REG, DOUBLE_REG, CONDITION_REG, or
   * SPECIAL_REG
   */
  OPT_Register getLastNonvolatile(int regClass) {
    switch (regClass) {
      case INT_REG:
        return getLastNonvolatileGPR();
      case DOUBLE_REG:
        return getLastNonvolatileFPR();
      case CONDITION_REG:
        return null;
      case SPECIAL_REG:
        return null;
      default:
        throw new OPT_OptimizingCompilerException("Unknown register class");
    }
  }

  /**
   * Given a symbolic register, return a cdoe that gives the physical
   * register type to hold the value of the symbolic register.
   * @param r a symbolic register
   * @return one of INT_REG, DOUBLE_REG, or CONDITION_REG
   */
  static final int getPhysicalRegisterType(OPT_Register r) {
    if (r.isInteger() || r.isLong()) {
      return INT_REG;
    } else if (r.isFloatingPoint()) {
      return DOUBLE_REG;
    } else if (r.isCondition()) {
      return CONDITION_REG;
    } else {
      throw new OPT_OptimizingCompilerException("getPhysicalRegisterType "
                                                + " unexpected " + r);
    }
  }

  /**
   * Given a physical register (XER, LR, or CTR), return the integer that
   * denotes the PowerPC Special Purpose Register (SPR) in the PPC
   * instruction set.  See p.129 of PPC ISA book
   */
  final byte getSPR(OPT_Register r) {
    if (VM.VerifyAssertions) {
      VM.assert ( (r == getXER()) || (r == getLR()) || (r == getCTR()) ); 
    } 
    if (r == getXER()) {
      return 1;
    } else if (r == getLR()) {
      return 8;
    } else if (r == getCTR()) {
      return 9;
    } else {
      throw new OPT_OptimizingCompilerException("Invalid SPR");
    }
  };
  /**
   * register names for each class. used in printing the IR
   * The indices for "FP" and "JTOC" should always match the
   * final static values of int FP and int JTOC defined below.
   */
  private static final String registerName[] = new String[getSize()];
  static {
    String regName[] = registerName;
    for (int i = 0; i < NUM_GPRS; i++)
      regName[i + FIRST_INT] = "R" + i;
    for (int i = 0; i < NUM_FPRS; i++)
      regName[i + FIRST_DOUBLE] = "F" + i;
    for (int i = 0; i < NUM_CRS; i++)
      regName[i + FIRST_CONDITION] = "C" + i;
    regName[JTOC_POINTER] = "JTOC";
    regName[FRAME_POINTER] = "FP";
    regName[THREAD_ID_REGISTER] = "TI";
    regName[PROCESSOR_REGISTER] = "PR";
    regName[XER] = "XER";
    regName[LR] = "LR";
    regName[CTR] = "CTR";
    regName[TU] = "TU";
    regName[TL] = "TL";
    regName[CR] = "CR";
  }
  /**
   * @return R0, a temporary register
   */
  static final int TEMP = FIRST_INT;   // temporary register (currently r0)
  OPT_Register getTemp() {
    return  reg[TEMP];
  }
  /**
   * Get the register name for a register with a particular number in the
   * pool
   */
  static String getName(int number) {
    return  registerName[number];
  }
  /**
   * Get the spill size for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG, CONDITION_REG, SPECIAL_REG
   */
  static int getSpillSize(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == CONDITION_REG) || (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }
  /**
   * Get the required spill alignment for a register with a particular type
   * @param type one of INT_REG, DOUBLE_REG, CONDITION_REG, SPECIAL_REG
   */
  static int getSpillAlignment(int type) {
    if (VM.VerifyAssertions) {
      VM.assert( (type == INT_REG) || (type == DOUBLE_REG) ||
                 (type == CONDITION_REG) || (type == SPECIAL_REG));
    }
    if (type == DOUBLE_REG) {
      return 8;
    } else {
      return 4;
    }
  }

  /**
   * Enumerate all the physical registers in this set.
   */
  Enumeration enumerateAll() {
    return new PhysicalRegisterEnumeration(0,getSize()-1);
  }

  /**
   * Enumerate all the GPRs in this set.
   */
  Enumeration enumerateGPRs() {
    return new PhysicalRegisterEnumeration(FIRST_INT,FIRST_DOUBLE-1);
  }

  /**
   * Enumerate all the volatile GPRs in this set.
   * NOTE: This assumes the scratch GPRs are numbered immediately 
   * <em> after </em> the volatile GPRs
   */
  Enumeration enumerateVolatileGPRs() {
    return new PhysicalRegisterEnumeration(FIRST_INT+FIRST_VOLATILE_GPR,
                                           FIRST_INT+LAST_SCRATCH_GPR);
  }

  /**
   * Enumerate the first n GPR parameters.
   */
  Enumeration enumerateGPRParameters(int n) {
    if (VM.VerifyAssertions)
      VM.assert(n <= NUMBER_INT_PARAM);
    return new PhysicalRegisterEnumeration(FIRST_INT_PARAM,
                                           FIRST_INT_PARAM + n - 1);
  }

  /**
   * Enumerate all the nonvolatile GPRs in this set.
   */
  Enumeration enumerateNonvolatileGPRs() {
    return new
      PhysicalRegisterEnumeration(FIRST_INT+FIRST_NONVOLATILE_GPR,
                                  FIRST_INT+LAST_NONVOLATILE_GPR);
  }


  /**
   * Enumerate all the volatile FPRs in this set.
   * NOTE: This assumes the scratch FPRs are numbered immediately 
   * <em> before</em> the volatile FPRs
   */
  Enumeration enumerateVolatileFPRs() {
    return new PhysicalRegisterEnumeration(FIRST_DOUBLE+FIRST_SCRATCH_FPR,
                                           FIRST_DOUBLE+LAST_VOLATILE_FPR);
  }

  /**
   * Enumerate the first n FPR parameters.
   */
  Enumeration enumerateFPRParameters(int n) {
    if (VM.VerifyAssertions)
      VM.assert(n <= NUMBER_DOUBLE_PARAM);
    return new PhysicalRegisterEnumeration(FIRST_DOUBLE_PARAM,
                                           FIRST_DOUBLE_PARAM + n - 1);
  }

  /**
   * Enumerate all the nonvolatile FPRs in this set.
   */
  Enumeration enumerateNonvolatileFPRs() {
    return new
      PhysicalRegisterEnumeration(FIRST_DOUBLE+FIRST_NONVOLATILE_FPR,
                                  FIRST_DOUBLE+LAST_NONVOLATILE_FPR);
  }

  /**
   * Enumerate the volatile physical condition registers.
   * Note that the TSR is non-volatile.
   */
  Enumeration enumerateVolatileConditionRegisters() {
    return new
      PhysicalRegisterEnumeration(FIRST_CONDITION, FIRST_SPECIAL-1,
                                  FIRST_CONDITION+THREAD_SWITCH_REGISTER);
  }

  /**
   * Enumerate the non-volatile physical condition registers.
   * Note that only the TSR is non-volatile.
   */
  Enumeration enumerateNonvolatileConditionRegisters() {
    return new PhysicalRegisterEnumeration(0, -1);
  }
  /** 
   * Enumerate the volatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, CONDITION_REG
   */
  Enumeration enumerateVolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateVolatileGPRs();
      case DOUBLE_REG:
        return enumerateVolatileFPRs();
      case CONDITION_REG:
        return enumerateVolatileConditionRegisters();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException("Unsupported volatile type");
    }
  }

  /**
   * Enumerate all the volatile physical registers
   */
  Enumeration enumerateVolatiles() {
    Enumeration e1 = enumerateVolatileGPRs();
    Enumeration e2 = enumerateVolatileFPRs();
    Enumeration e3 = enumerateVolatileConditionRegisters();
    return new OPT_CompoundEnumerator(e1, new
                                      OPT_CompoundEnumerator(e2,e3));
  }

  /**
   * Return a set of all the volatile registers.
   */
  OPT_BitSet getVolatiles() {
    return volatileSet;
  }

  /** 
   * Enumerate the nonvolatile physical registers of a given class.
   * @param regClass one of INT_REG, DOUBLE_REG, CONDITION_REG
   */
  Enumeration enumerateNonvolatiles(int regClass) {
    switch (regClass) {
      case INT_REG:
        return enumerateNonvolatileGPRs();
      case DOUBLE_REG:
        return enumerateNonvolatileFPRs();
      case CONDITION_REG:
        return enumerateNonvolatileConditionRegisters();
      case SPECIAL_REG:
        return OPT_EmptyEnumerator.EMPTY;
      default:
        throw new OPT_OptimizingCompilerException
          ("Unsupported non-volatile type");
    }
  }

  /** 
   * Enumerate the nonvolatile physical registers of a given class,
   * backwards.
   */
  Enumeration enumerateNonvolatilesBackwards(int regClass) {
    return new OPT_ReverseEnumerator(enumerateNonvolatiles(regClass));
  }


  /**
   * If the passed in physical register r is used as a GPR parameter register,
   * return the index into the GPR parameters for r.  Otherwise, return -1;
   */
  int getGPRParamIndex(OPT_Register r) {
    if ( (r.number < FIRST_INT_PARAM) || (r.number > LAST_VOLATILE_GPR)) {
      return -1;
    } else {
      return r.number - FIRST_INT_PARAM;
    }
  }

  /**
   * If the passed in physical register r is used as an FPR parameter register,
   * return the index into the FPR parameters for r.  Otherwise, return -1;
   */
  int getFPRParamIndex(OPT_Register r) {
    if ( (r.number < FIRST_DOUBLE_PARAM) || (r.number > LAST_VOLATILE_FPR)) {
      return -1;
    } else {
      return r.number - FIRST_DOUBLE_PARAM;
    }
  }

  /** 
   * If r is used as the first half of a (long) register pair, return 
   * the second half of the pair.
   */
  OPT_Register getSecondHalf(OPT_Register r) {
    int n = r.number;
    return get(n+1);
  }

  /**
   * An enumerator for use by the physical register utilities.
   */
  final class PhysicalRegisterEnumeration implements Enumeration {
    private int start;
    private int end;
    private int index;
    private int exclude = -1; // an index in the register range to exclude
    PhysicalRegisterEnumeration(int start, int end) {
      this.start = start;
      this.end = end;
      this.index = start;
    }
    PhysicalRegisterEnumeration(int start, int end, int exclude) {
      this.start = start;
      this.end = end;
      this.exclude = exclude;
      this.index = start;
    }
    public Object nextElement() {
      if (index == exclude) index++;
      return reg[index++];
    }
    public boolean hasMoreElements() {
      if (index == exclude) index++;
      return (index <= end);
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Enumeration;

/**
 * This abstract class provides a set of useful methods for
 * manipulating physical registers for an IR.
 *
 * @author Jong-Deok Choi
 * @author Dave Grove
 * @author Mauricio Serrano
 * @author John Whaley
 * @author Stephen Fink
 */
abstract class OPT_PhysicalRegisterTools extends
OPT_GenericPhysicalRegisterTools {

  /**
   * Create a condition register operand for a given register number.
   * To be used in passthrough expressions like
   * <pre>
   *    ... Binary.create(INT_CMP, CR(2), R(1), I(4)) ...
   * </pre>
   *
   * @param regnum the given condition register number
   * @return condition register operand
   */
  final OPT_RegisterOperand CR(int regnum) {
    OPT_PhysicalRegisterSet phys = getIR().regpool.getPhysicalRegisterSet();
    return CR(phys.getConditionRegister(regnum));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.Enumeration;
import instructionFormats.*;

/**
 * An instance of this class provides a mapping from symbolic register to
 * physical register, representing a preferred register assignment.
 * 
 * @author Stephen Fink
 */
final class OPT_RegisterPreferences extends OPT_GenericRegisterPreferences
implements OPT_Operators {

  /**
   * If the following is set, we use a heuristic optimization as follows:
   * weight a  
   *                    MOVE symbolic = symbolic 
   * TWICE as much as either of:
   *                    MOVE symbolic = physical, 
   *                    MOVE physical = symbolic.
   *
   * Rationale: At this point (before register allocation), the second
   * class of moves appear only due to calling conventions, parameters,
   * and return values.  We posit that the dynamic frequency of these
   * MOVES will be smaller than the frequency of an average move.
   */
  private final boolean SYMBOLIC_SYMBOLIC_HEURISTIC = true;

  /**
   * Set up register preferences based on instructions in an IR.
   */
  void initialize(OPT_IR ir) {
    for (Enumeration e = ir.forwardInstrEnumerator(); 
         e.hasMoreElements();) {
      OPT_Instruction s = (OPT_Instruction)e.nextElement();
      switch (s.operator.opcode) {
        case PPC_MOVE_opcode:
          // add affinities produced by MOVE instructions
          OPT_Operand result = MIR_Move.getResult(s);
          OPT_Operand value = MIR_Move.getValue(s);
          if (result.isRegister() && value.isRegister()) {
            OPT_Register r1 = result.asRegister().register;
            OPT_Register r2 = value.asRegister().register;
            addAffinity(1,r2,r1);

            // double the affinities if using the heuristic described
            // above.
            if (SYMBOLIC_SYMBOLIC_HEURISTIC && r1.isSymbolic() && 
                r2.isSymbolic()) {
              addAffinity(1,r2,r1);
            }
          }
          break;
        default:
          break;
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * An instance of this class encapsulates restrictions on register
 * allocation.
 * 
 * @author Stephen Fink
 */
final class OPT_RegisterRestrictions extends OPT_GenericRegisterRestrictions {
  /**
   * Default Constructor
   */
  OPT_RegisterRestrictions(OPT_PhysicalRegisterSet phys) {
    super(phys);
  }
  /**
   * Is it forbidden to assign symbolic register symb to physical register r
   * in instruction s?
   */
  boolean isForbidden(OPT_Register symb, OPT_Register r,
                             OPT_Instruction s) {
    return false;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import instructionFormats.*;
import java.util.Enumeration;
import java.util.Iterator;

/**
 * Class to manage the allocation of the "compiler-specific" portion of 
 * the stackframe.  This class holds only the architecture-specific
 * functions.
 * <p>
 *
 * @author Dave Grove
 * @author Mauricio J. Serrano
 * @author Stephen Fink
 */
final class OPT_StackManager extends OPT_GenericStackManager
  implements OPT_Operators {
  
  /**
   * stack locaiton to save the CR register
   */
  private int saveCRLocation;
  /**
   * stack locaiton to save the XER register
   */
  private int saveXERLocation;
  /**
   * stack locaiton to save the CTR register
   */
  private int saveCTRLocation;
  /**
   * Return the size of the fixed portion of the stack.
   * @return size in bytes of the fixed portion of the stackframe
   */
  final int getFrameFixedSize() {
    return frameSize;
  }

  /**
   * Allocate a new spill location and grow the
   * frame size to reflect the new layout.
   *
   * @param type the type to spill
   * @return the spill location
   */
  final int allocateNewSpillLocation(int type) {

    // increment by the spill size
    spillPointer += OPT_PhysicalRegisterSet.getSpillSize(type);

    if (spillPointer > frameSize) {
      frameSize = spillPointer;
    }
    return spillPointer - OPT_PhysicalRegisterSet.getSpillSize(type);
  }

  /**
   * Clean up some junk that's left in the IR after register allocation,
   * and add epilogue code.
   */ 
  void cleanUpAndInsertEpilogue() {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    OPT_Instruction inst = ir.firstInstructionInCodeOrder().getNext();
    for (; inst != null; inst = inst.nextInstructionInCodeOrder()) {
      switch (inst.getOpcode()) {
	case PPC_MOVE_opcode:
	case PPC_FMR_opcode:
	  // remove frivolous moves
	  if (MIR_Move.getResult(inst).register.number ==
	      MIR_Move.getValue(inst).register.number)
	    inst = inst.remove();
	  break;
	case PPC_BLR_opcode:
	  if (frameIsRequired()) 
	    insertEpilogue(inst);
	  break;
      	case PPC_LFD_opcode:
	case PPC_LFS_opcode:
	case PPC_LWZ_opcode:
	  // the following to handle spilled parameters
          // SJF: this is ugly.  clean it up someday.
	  if (MIR_Load.getAddress(inst).register ==
              ir.regpool.getPhysicalRegisterSet().getFP()) {
	    OPT_Operand one = MIR_Load.getOffset(inst);
	    if (one instanceof OPT_IntConstantOperand) {
	      int offset = ((OPT_IntConstantOperand) one).value;
	      if (offset <= -256) {
		MIR_Load.setOffset(inst, I(frameSize - offset - 256));
	      }
	    }
	  }
        default:
          break;
      }
    }
  }

  /**
   * Insert a spill of a physical register before instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location
   */
  final void insertSpillBefore(OPT_Instruction s, OPT_Register r,
                               byte type, int location) {

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register FP = phys.getFP();
    if (type == CONDITION_VALUE) {
      s.insertBack(MIR_Move.create(PPC_MFCR,
                                   R(phys.getTemp()),
                                   R(phys.getCR())));
      s.insertBack(nonPEIGC(MIR_Store.create(PPC_STW,
                                             R(phys.getTemp()), R(FP),
                                             I(location))));
    } else if (type == FLOAT_VALUE) {
      s.insertBack(nonPEIGC(MIR_Store.create(PPC_STFS, F(r), R(FP),
                                             I(location))));
    } else if (type == DOUBLE_VALUE) {
      s.insertBack(nonPEIGC(MIR_Store.create(PPC_STFD, D(r), R(FP),
                                             I(location))));
    } else if (type == INT_VALUE) {      // integer or half of long
      s.insertBack(nonPEIGC(MIR_Store.create(PPC_STW, R(r), R(FP),
                                             I(location))));
    } else
      throw new OPT_OptimizingCompilerException("insertSpillBefore", 
                                                "unsupported type " +
                                                type);
  }
  
  /**
   * Create an MIR instruction to move rhs into lhs
   */
  final OPT_Instruction makeMoveInstruction(OPT_Register lhs, 
                                            OPT_Register rhs) {
    if (rhs.isFloatingPoint() && lhs.isFloatingPoint()) {
      return MIR_Move.create(PPC_FMR, D(lhs), D(rhs));
    } else if (rhs.isInteger() && lhs.isInteger()) { // integer
      return MIR_Move.create(PPC_MOVE, R(lhs), R(rhs));
    } else
      throw new OPT_OptimizingCompilerException("RegAlloc", 
                                                "unknown register:", 
                                                lhs.toString());
  }
  
  /**
   * Insert a load of a physical register from a spill location before 
   * instruction s.
   *
   * @param s the instruction before which the spill should occur
   * @param r the register (should be physical) to spill
   * @param type one of INT_VALUE, FLOAT_VALUE, DOUBLE_VALUE, or
   *                    CONDITION_VALUE
   * @param location the spill location
   */
  final void insertUnspillBefore(OPT_Instruction s, OPT_Register r, 
                                 byte type, int location) {
    
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register FP = phys.getFP();
    if (type == CONDITION_VALUE) {
      OPT_Register temp = phys.getTemp();
      s.insertBack(nonPEIGC(MIR_Load.create(PPC_LWZ, R(temp), R(FP),
                                            I(location))));
      // CR2 is used by the thread scheduler
      s.insertBack(MIR_Move.create(PPC_MTCR,
                                   R(phys.getCR()), R(temp)));
    } else if (type == DOUBLE_VALUE) {
	s.insertBack(nonPEIGC(MIR_Load.create(PPC_LFD, D(r), R(FP),
                                              I(location))));
    } else if (type == FLOAT_VALUE) {
      s.insertBack(nonPEIGC(MIR_Load.create(PPC_LFS, F(r), R(FP),
                                            I(location))));
    } else if (type == INT_VALUE) { // integer or half of long
      s.insertBack(nonPEIGC(MIR_Load.create(PPC_LWZ, R(r), R(FP),
                                            I(location))));
    } else {
      throw new OPT_OptimizingCompilerException("insertUnspillBefore", 
						"unknown type:" + type);
    }
  }
  
  /**
   * Insert the epilogue before a particular return instruction.
   *
   * @param ret the return instruction.
   */
  final private void insertEpilogue(OPT_Instruction ret) {

    // 1. Restore any saved registers
    if (ir.compiledMethod.isSaveVolatile()) {
      restoreVolatileRegisters(ret);
    }
    restoreNonVolatiles(ret);

    // 2. Restore return address
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register temp = phys.getTemp();
    OPT_Register FP = phys.getFP();
    ret.insertBack(nonPEIGC(MIR_Load.create(PPC_LWZ, R(temp), R(FP),
                 I(STACKFRAME_NEXT_INSTRUCTION_OFFSET + frameSize))));

    // 3. Load return address into LR
    ret.insertBack(MIR_Move.create(PPC_MTSPR, R(phys.getLR()),
                                   R(phys.getTemp())));

    // 4. Restore old FP
    ret.insertBack(MIR_Binary.create(PPC_ADDI, R(FP), R(FP), I(frameSize)));

  }
  
  /**
   * Insert code in the prologue to save the 
   * volatile registers.
   *
   * @param inst 
   */
  private void saveVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    // 1. save the volatile GPRs
    OPT_Register FP = phys.getFP();
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      inst.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(r), R(FP),
					      I(location))));
    }
    // 2. save the volatile FPRs
    i = 0;
    for (Enumeration e = phys.enumerateVolatileFPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileFPRLocation[i];
      inst.insertBefore(nonPEIGC(MIR_Store.create(PPC_STFD, D(r), R(FP),
					      I(location))));
    }
    
    // 3. Save some special registers
    OPT_Register temp = phys.getTemp();
    
    // cr2 is used by the thread scheduler
    inst.insertBack(MIR_Move.create(PPC_MFCR, R(temp), R(phys.getCR())));
    inst.insertBack(nonPEIGC(MIR_Store.create(PPC_STW, R(temp), R(FP),
                                              I(saveCRLocation))));

    inst.insertBack(MIR_Move.create(PPC_MFSPR, R(temp), R(phys.getXER()) ));
    inst.insertBack(nonPEIGC(MIR_Store.create(PPC_STW, R(temp), R(FP),
                                              I(saveXERLocation))));

    inst.insertBack(MIR_Move.create(PPC_MFSPR, R(temp), R(phys.getCTR())));
    inst.insertBack(nonPEIGC(MIR_Store.create(PPC_STW, R(temp), R(FP), 
                                             I(saveCTRLocation))));
  }
  
  /**
   * Insert code into the prologue to save any used non-volatile
   * registers.  
   *
   * @param inst the first instruction after the prologue.  
   */
  private void saveNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();
    if (ir.compiledMethod.isSaveVolatile()) {
      // pretend we use all non-volatiles
      nNonvolatileGPRS = phys.getNumberOfNonvolatileGPRs();
    }

    // 1. save the nonvolatile GPRs
    int n = nNonvolatileGPRS - 1;
    OPT_Register FP = phys.getFP();
    if (n <= MULTIPLE_CUTOFF) {
      // use a sequence of load instructions
      for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
           e.hasMoreElements() && n >= 0 ; n--) {
        OPT_Register nv = (OPT_Register)e.nextElement();
        int offset = getNonvolatileGPROffset(n);
	inst.insertBack(nonPEIGC(MIR_Store.create (PPC_STW, R(nv), R(FP),
                                               I(offset))));
      }
    } else {
      // use a stm
      OPT_Register nv = null;
      for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
           e.hasMoreElements() && n >= 0 ; n--) {
        nv = (OPT_Register)e.nextElement();
      }
      n++;
      OPT_RegisterOperand range = R(nv);
      // YUCK!!! Why is this crap in register operand??
      range.setRange(FIRST_INT + LAST_NONVOLATILE_GPR - nv.number);
      int offset = getNonvolatileGPROffset(n);
      inst.insertBack(nonPEIGC(MIR_Store.create
                           (PPC_STMW, range, R(FP), 
                            I(offset))));
    }
    // 1. save the nonvolatile FPRs
    if (ir.compiledMethod.isSaveVolatile()) {
      // pretend we use all non-volatiles
      // DANGER: as an optimization, we assert that a SaveVolatile method
      // will never use non-volatile FPRs.
    } else {
      int nNonvolatileFPRS = ir.compiledMethod.getNumberOfNonvolatileFPRs();
      n = nNonvolatileFPRS - 1;
      // use a sequence of load instructions
      for (Enumeration e = phys.enumerateNonvolatileFPRsBackwards(); 
         e.hasMoreElements() && n >= 0 ; n--) {
        OPT_Register nv = (OPT_Register)e.nextElement();
        int offset = getNonvolatileFPROffset(n);
        inst.insertBack(nonPEIGC(MIR_Store.create(PPC_STFD, D(nv), R(FP),
                                                I(offset))));
      }
    }
  }

  /**
   * Insert code before a return instruction to restore the nonvolatile 
   * registers.
   *
   * @param inst the return instruction
   */
  private void restoreNonVolatiles(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    // 1. restore the nonvolatile GPRs
    int n = nNonvolatileGPRS - 1;
    OPT_Register FP = phys.getFP();
    if (n <= MULTIPLE_CUTOFF) {
      // use a sequence of load instructions
      for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
           e.hasMoreElements() && n >= 0 ; n--) {
        OPT_Register nv = (OPT_Register)e.nextElement();
        int offset = getNonvolatileGPROffset(n);
	inst.insertBack(nonPEIGC(MIR_Load.create (PPC_LWZ, R(nv), R(FP),
                                               I(offset))));
      }
    } else {
      // use an lm
      OPT_Register nv = null;
      for (Enumeration e = phys.enumerateNonvolatileGPRsBackwards(); 
           e.hasMoreElements() && n >= 0 ; n--) {
        nv = (OPT_Register)e.nextElement();
      }
      n++;
      OPT_RegisterOperand range = R(nv);
      // YUCK!!! Why is this crap in register operand??
      range.setRange(FIRST_INT + LAST_NONVOLATILE_GPR - nv.number);
      int offset = getNonvolatileGPROffset(n);
      inst.insertBack(nonPEIGC(MIR_Load.create
                           (PPC_LMW, range, R(FP), 
                            I(offset))));
    }
    // Note that save-volatiles are forbidden from using nonvolatile FPRs.
    if (!ir.compiledMethod.isSaveVolatile()) {
      // 1. restore the nonvolatile FPRs
      int nNonvolatileFPRS = ir.compiledMethod.getNumberOfNonvolatileFPRs();
      n = nNonvolatileFPRS - 1;
      // use a sequence of load instructions
      for (Enumeration e = phys.enumerateNonvolatileFPRsBackwards(); 
           e.hasMoreElements() && n >= 0 ; n--) {
        OPT_Register nv = (OPT_Register)e.nextElement();
        int offset = getNonvolatileFPROffset(n);
        inst.insertBack(nonPEIGC(MIR_Load.create (PPC_LFD, D(nv), R(FP),
                                                  I(offset))));
      }
    }
  }

  /**
   * Insert code before a return instruction to restore the 
   * volatile registers.
   *
   * @param inst the return instruction
   */
  private void restoreVolatileRegisters(OPT_Instruction inst) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    int nNonvolatileGPRS = ir.compiledMethod.getNumberOfNonvolatileGPRs();

    // 1. restore the volatile GPRs
    OPT_Register FP = phys.getFP();
    int i = 0;
    for (Enumeration e = phys.enumerateVolatileGPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileGPRLocation[i];
      inst.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(r), R(FP),
					      I(location))));
    }
    // 2. restore the volatile FPRs
    i = 0;
    for (Enumeration e = phys.enumerateVolatileFPRs();
         e.hasMoreElements(); i++) {
      OPT_Register r = (OPT_Register)e.nextElement();
      int location = saveVolatileFPRLocation[i];
      inst.insertBefore(nonPEIGC(MIR_Load.create(PPC_LFD, D(r), R(FP),
					      I(location))));
    }
    // 3. Restore some special registers
    OPT_Register temp = phys.getTemp();
    // cr2 is used by the thread scheduler
    inst.insertBack(nonPEIGC(MIR_Load.create
                          (PPC_LWZ, R(temp), R(FP), I(saveCRLocation))));
    inst.insertBack(MIR_Move.create(PPC_MTCR,
                                 R(phys.getCR()), R(temp)));

    inst.insertBack(nonPEIGC(MIR_Load.create
                          (PPC_LWZ, R(temp), R(FP), I(saveXERLocation))));
    inst.insertBack(MIR_Move.create(PPC_MTSPR,
                                 R(phys.getXER()), R(temp)));

    inst.insertBack(nonPEIGC(MIR_Load.create
                          (PPC_LWZ, R(temp), R(FP), 
                           I(saveCTRLocation))));
    inst.insertBack(MIR_Move.create(PPC_MTSPR,
                                 R(phys.getCTR()), R(temp)));
  }
  
  /**
   * Insert the prologue.
   * The available scratch registers are normally: R0, S0, S1
   * However, if this is the prologue for a 'save volatile' frame, 
   * then R0 is the only available scratch register.
   * The "normal" prologue must perform the following tasks:
   *    stack overflow check         
   *    set cr2 for the yieldpoint   
   *      (when !VM.BuildForThreadSwitchUsingControlRegisterBit && 
   *                            there is a prologue yieldpoint instruction)
   *    save lr
   *    store cmid
   *    buy stack frame
   *    store any used non volatiles
   * We schedule the prologue for this combination of operations, 
   * since it is currently the common case.
   * When this changes, this code should be modifed accordingly.
   * The desired sequence is:
   *  1    mflr    00  # return addr
   *  2    l       S1 threadSwitchRequestedOffset(PR)         # setting cr2 for yield point
   *  3    stu     FP -frameSize(FP)                          # buy frame, save caller's fp
   *  4    l       S0 stackLimitOffset(S0)                    # stack overflow check
   *  5    <save used non volatiles>
   *  6    cmpi    cr2 S1 0x0                                 # setting cr2 for yield point (S1 is now free)
   *  7    lil     S1 CMID                                    # cmid
   *  8    st      00 STACKFRAME_NEXT_INSTRUCTION_OFFSET(FP)  # return addr (00 is now free)
   *  9    st      S1 STACKFRAME_METHOD_ID_OFFSET(FP)         # cmid
   *  10   tlt     FP, S0                                     # stack overflow check
   */

  /**
   * Schedule prologue for 'normal' case (see above)
   */
  final void insertNormalPrologue() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register FP = phys.getFP();
    OPT_Register PR = phys.getPR();
    OPT_Register TSR = phys.getTSR();
    OPT_Register R0 = phys.getTemp();
    OPT_Register S0 = phys.getGPR(FIRST_SCRATCH_GPR);
    OPT_Register S1 = phys.getGPR(LAST_SCRATCH_GPR);
    boolean interruptible = ir.method.isInterruptible();
    boolean stackOverflow = interruptible;
    boolean yp = !VM.BuildForThreadSwitchUsingControlRegisterBit && 
      ir.stackManager.hasPrologueYieldpoint();

    int frameFixedSize = getFrameFixedSize();
    ir.compiledMethod.setFrameFixedSize(frameFixedSize);
    
    if (frameFixedSize >= STACK_SIZE_GUARD || ir.compiledMethod.isSaveVolatile()) {
      insertExceptionalPrologue();
      return;
    }

    OPT_Instruction ptr = ir.firstInstructionInCodeOrder().getNext();
    if (VM.VerifyAssertions) VM.assert(ptr.getOpcode() == IR_PROLOGUE_opcode);

    ptr.insertBefore(MIR_Move.create(PPC_MFSPR, R(R0),
                                     R(phys.getLR()))); // 1
    if (yp) {
      ptr.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(S1), R(PR),
			I(VM_Entrypoints.threadSwitchRequestedField.getOffset())))); // 2
    }

    ptr.insertBefore(nonPEIGC(MIR_StoreUpdate.create(PPC_STWU, R(FP), R(FP),
  		        I(-frameSize)))); // 3

    if (stackOverflow) {
      ptr.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(S0),
                                                R(phys.getPR()), 
			I(VM_Entrypoints.activeThreadStackLimitField.getOffset())))); // 4
    }

    // Now add any instructions to save the volatiles and nonvolatiles (5)
    saveNonVolatiles(ptr);
    
    if (yp) {
      ptr.insertBefore(MIR_Binary.create(PPC_CMPI, R(TSR), R(S1), I(0))); // 6
    }
    int cmid = ir.compiledMethod.getId();
    if (cmid <= 0x7fff) {
      ptr.insertBefore(MIR_Unary.create(PPC_LDI, R(S1), I(cmid))); // 7
    } else {
      ptr.insertBefore(MIR_Unary.create(PPC_LDIS, R(S1),I(cmid>>>16))); // 7 (a)
      ptr.insertBefore(MIR_Binary.create(PPC_ORI, R(S1), R(S1),
					 I(cmid&0xffff))); // 7 (b)
    }
    ptr.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(R0), R(FP), 
		 I(frameSize + STACKFRAME_NEXT_INSTRUCTION_OFFSET)))); // 8
    ptr.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(S1), R(FP), 
		       I(STACKFRAME_METHOD_ID_OFFSET)))); // 9

    ptr.insertBefore(Empty.create(IR_ENDPROLOGUE));

    if (stackOverflow) {
      // Mutate the Prologue instruction into the trap
      MIR_Trap.mutate(ptr, PPC_TW, OPT_PowerPCTrapOperand.LESS(), R(FP), R(S0),
		      OPT_TrapCodeOperand.StackOverflow()); // 10
    } else {
      // no stack overflow test, so we remove the IR_Prologue instruction
      ptr.remove();
    }
  }
  
  
  /**
   * prologue for the exceptional case.
   * (1) R0 is the only available scratch register.
   * (2) stack overflow check has to come first.
   */
  final void insertExceptionalPrologue () {
    if (frameSize >= 0x7ff0) {
      throw new OPT_OptimizingCompilerException("Stackframe size exceeded!");
    }

    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    OPT_Register FP = phys.getFP();
    OPT_Register PR = phys.getPR();
    OPT_Register TSR= phys.getTSR();
    OPT_Register R0 = phys.getTemp();
    OPT_Register S1 = phys.getGPR(LAST_SCRATCH_GPR);
    boolean interruptible = ir.method.isInterruptible();
    boolean stackOverflow = interruptible;
    boolean yp = !VM.BuildForThreadSwitchUsingControlRegisterBit && 
      ir.stackManager.hasPrologueYieldpoint();

    OPT_Instruction ptr = ir.firstInstructionInCodeOrder().getNext();
    if (VM.VerifyAssertions) VM.assert(ptr.getOpcode() == IR_PROLOGUE_opcode);

    // Stack overflow check
    if (stackOverflow) {
      // R0 is fairly useless (can't be operand 1 of an addi or the base ptr
      // of a load) so, free up S1 for use by briefly saving its contents in the
      // return address slot of my caller's frame
      ptr.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(S1), R(FP), 
			I(STACKFRAME_NEXT_INSTRUCTION_OFFSET))));
      ptr.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(S1),
                                                R(phys.getPR()), 
			I(VM_Entrypoints.activeThreadStackLimitField.getOffset()))));
      ptr.insertBefore(MIR_Binary.create(PPC_ADDI, R(R0), R(S1), 
			I(frameSize)));
      ptr.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(S1), R(FP), 
			I(STACKFRAME_NEXT_INSTRUCTION_OFFSET))));

      // Mutate the Prologue holder instruction into the trap
      MIR_Trap.mutate(ptr, PPC_TW, OPT_PowerPCTrapOperand.LESS(), R(FP), R(R0),
		      OPT_TrapCodeOperand.StackOverflow()); 

      // advance ptr because we want the remaining instructions to come after
      // the trap
      ptr = ptr.getNext();

    } else {
      // no stack overflow test, so we must remove the IR_Prologue instruction
      OPT_Instruction next = ptr.getNext();
      ptr.remove();
      ptr = next;
    }
    
    // Buy stack frame, save LR, caller's FP 
    ptr.insertBefore(MIR_Move.create(PPC_MFSPR, R(R0),
                                     R(phys.getLR())));
    ptr.insertBefore(nonPEIGC(MIR_StoreUpdate.create(PPC_STWU, R(FP), R(FP),
						     I(-frameSize))));
    ptr.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(R0), R(FP), 
			I(frameSize+STACKFRAME_NEXT_INSTRUCTION_OFFSET))));
    
    // Store cmid
    int cmid = ir.compiledMethod.getId();
    if (cmid <= 0x7fff) {
      ptr.insertBefore(MIR_Unary.create(PPC_LDI, R(R0), I(cmid)));
    } else {
      ptr.insertBefore(MIR_Unary.create(PPC_LDIS, R(R0),I(cmid>>>16)));
      ptr.insertBefore(MIR_Binary.create(PPC_ORI, R(R0), R(R0),I(cmid&0xffff)));
    }
    ptr.insertBefore(nonPEIGC(MIR_Store.create(PPC_STW, R(R0), R(FP), 
		       I(STACKFRAME_METHOD_ID_OFFSET))));

    // Now add the non volatile save instructions
    if (ir.compiledMethod.isSaveVolatile()) {
      saveVolatiles(ptr);
    }
    saveNonVolatiles(ptr);
    
    // Threadswitch
    if (yp) {
      ptr.insertBefore(nonPEIGC(MIR_Load.create(PPC_LWZ, R(R0), R(PR), 
			I(VM_Entrypoints.threadSwitchRequestedField.getOffset()))));
      ptr.insertBefore(MIR_Binary.create(PPC_CMPI, R(TSR), R(R0), I(0)));
    }
    ptr.insertBefore(Empty.create(IR_ENDPROLOGUE));
  }

  /**
   * Compute the number of stack words needed to hold nonvolatile
   * registers.
   *
   * Side effects: 
   * <ul>
   * <li> updates the VM_OptCompiler structure 
   * <li> updates the <code>frameSize</code> field of this object
   * <li> updates the <code>frameRequired</code> field of this object
   * </ul>
   */
  void computeNonVolatileArea() {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();

    if (ir.compiledMethod.isSaveVolatile()) {
      // Record that we use every nonvolatile GPR
      int numGprNv = phys.getNumberOfNonvolatileGPRs();
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);

      // set the frame size
      frameSize += numGprNv * WORDSIZE;

      int numFprNv = phys.getNumberOfNonvolatileFPRs();
      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)numFprNv);
      frameSize += numFprNv * WORDSIZE * 2;

      frameSize = align(frameSize, STACKFRAME_ALIGNMENT);

      // Record that we need a stack frame.
      setFrameRequired();

      // Map each volatile GPR to a spill location.
      int i = 0;
      for (Enumeration e = phys.enumerateVolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        saveVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }

      // Allocate two dummy slots in the stack frame between the volatile
      // GPRs and the non-volatile GPRs.  This is expected by the Save
      // Volatile GC map iterator.  Also, these two slots will allow us to
      // save all GPRs with a stm if we choose
      allocateNewSpillLocation(INT_REG);     // empty slot for R15 
      allocateNewSpillLocation(INT_REG);     // emptly slot for R16
      
      
      // Map each non-volatile GPR register to a spill location.
      i=0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        nonVolatileGPRLocation[i] = allocateNewSpillLocation(INT_REG);      
      }
      
      
      // Map some special registers to spill locations.
      saveCRLocation = allocateNewSpillLocation(INT_REG);
      saveXERLocation = allocateNewSpillLocation(INT_REG);
      saveCTRLocation = allocateNewSpillLocation(INT_REG);
      i=0;

      for (Enumeration e = phys.enumerateVolatileFPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        saveVolatileFPRLocation[i] = allocateNewSpillLocation(DOUBLE_REG);      
      }

      // Map each non-volatile FPR register to a spill location.
      // DANGER: We forbid save volatile frames from using non-volatile
      // FPRs.  Hence, the following is unnecessary.
      /*
      i=0;
      for (Enumeration e = phys.enumerateNonvolatileFPRs(); 
           e.hasMoreElements(); i++)  {
        OPT_Register r = (OPT_Register)e.nextElement();
        // Note that as a side effect, the following call bumps up the
        // frame size.
        nonVolatileFPRLocation[i] = allocateNewSpillLocation(DOUBLE_REG);      
      }
      */

      // Set the offset to find non-volatiles.
      int gprOffset = getNonvolatileGPROffset(0);
      ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);

    } else {
      // Count the number of nonvolatiles used. 
      int numGprNv = 0;
      int i = 0;
      for (Enumeration e = phys.enumerateNonvolatileGPRs();
           e.hasMoreElements(); ) {
        OPT_Register r = (OPT_Register)e.nextElement();
        if (r.isTouched() ) {
          // Note that as a side effect, the following call bumps up the
          // frame size.
          nonVolatileGPRLocation[i++] = allocateNewSpillLocation(INT_REG);
          numGprNv++;
        }
      }
      i = 0;
      int numFprNv = 0;
      for (Enumeration e = phys.enumerateNonvolatileFPRs();
           e.hasMoreElements(); ) {
        OPT_Register r = (OPT_Register)e.nextElement();
        if (r.isTouched() ) {
          // Note that as a side effect, the following call bumps up the
          // frame size.
          nonVolatileFPRLocation[i++] = allocateNewSpillLocation(DOUBLE_REG);
          numFprNv++;
        }
      }
      // Update the VM_OptCompiledMethod object.
      ir.compiledMethod.setNumberOfNonvolatileGPRs((short)numGprNv);
      ir.compiledMethod.setNumberOfNonvolatileFPRs((short)numFprNv);
      if (numGprNv > 0 || numFprNv > 0) {
        int gprOffset = getNonvolatileGPROffset(0);
        ir.compiledMethod.setUnsignedNonVolatileOffset(gprOffset);
        // record that we need a stack frame
        setFrameRequired();
      } else {
        ir.compiledMethod.setUnsignedNonVolatileOffset(0);
      }
      frameSize = align(frameSize, STACKFRAME_ALIGNMENT);
    }
  }

  /**
   * Walk over the currently available scratch registers. 
   *
   * <p>For any scratch register r which is def'ed by instruction s, 
   * spill r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which is used by instruction s, 
   * restore r before s and remove r from the pool of available scratch 
   * registers.  
   *
   * <p>For any scratch register r which has current contents symb, and 
   * symb is spilled to location M, and s defs M: the old value of symb is
   * dead.  Mark this.
   *
   * <p>Invalidate any scratch register assignments that are illegal in s.
   */
  void restoreScratchRegistersBefore(OPT_Instruction s) {
    for (Iterator i = scratchInUse.iterator(); i.hasNext(); ) {
      ScratchRegister scratch = (ScratchRegister)i.next();

      if (scratch.currentContents == null) continue;
      if (verboseDebug) {
        System.out.println("RESTORE: consider " + scratch);
      }
      boolean removed = false;
      boolean unloaded = false;
      if (definedIn(scratch.scratch,s) 
          || (s.isCall() && s.operator != CALL_SAVE_VOLATILE 
              && scratch.scratch.isVolatile())) {
        // s defines the scratch register, so save its contents before they
        // are killed.
        if (verboseDebug) {
          System.out.println("RESTORE : unload because defined " + scratch);
        }
        unloadScratchRegisterBefore(s,scratch);

        // update mapping information
        if (verboseDebug) System.out.println("RSRB: End scratch interval " + 
                                             scratch.scratch + " " + s);
        scratchMap.endScratchInterval(scratch.scratch,s);
        OPT_Register scratchContents = scratch.currentContents;
        if (scratchContents != null) {
          if (verboseDebug) System.out.println("RSRB: End symbolic interval " + 
                                               scratch.currentContents + " " 
                                               + s);
          scratchMap.endSymbolicInterval(scratch.currentContents,s);
        } 

        i.remove();
        removed = true;
        unloaded = true;
      }

      if (usedIn(scratch.scratch,s) ||
          !isLegal(scratch.currentContents,scratch.scratch,s)) {
        // first spill the currents contents of the scratch register to 
        // memory 
        if (!unloaded) {
          if (verboseDebug) {
            System.out.println("RESTORE : unload because used " + scratch);
          }
          unloadScratchRegisterBefore(s,scratch);

          // update mapping information
          if (verboseDebug) System.out.println("RSRB2: End scratch interval " + 
                                               scratch.scratch + " " + s);
          scratchMap.endScratchInterval(scratch.scratch,s);
          OPT_Register scratchContents = scratch.currentContents;
          if (scratchContents != null) {
            if (verboseDebug) System.out.println("RSRB2: End symbolic interval " + 
                                                 scratch.currentContents + " " 
                                                 + s);
            scratchMap.endSymbolicInterval(scratch.currentContents,s);
          } 

        }
        // s or some future instruction uses the scratch register, 
	// so restore the correct contents.
        if (verboseDebug) {
          System.out.println("RESTORE : reload because used " + scratch);
        }
        reloadScratchRegisterBefore(s,scratch);

        if (!removed) {
          i.remove();
          removed=true;
        }
      }
    }
  }
  protected static final int MULTIPLE_CUTOFF = 4;

  /**
   * Initializes the "tmp" regs for this object
   * @param ir the governing ir
   */
  final void initForArch(OPT_IR ir) {
    OPT_PhysicalRegisterSet phys = ir.regpool.getPhysicalRegisterSet();
    phys.getJTOC().reserveRegister();
    phys.getFirstConditionRegister().reserveRegister();
  }

  /**
   * Is a particular instruction a system call?
   */
  boolean isSysCall(OPT_Instruction s) {
    return s.operator == PPC_BCTRL_SYS || s.operator == PPC_BL_SYS;
  } 
  /**
   * Given symbolic register r in instruction s, do we need to ensure that
   * r is in a scratch register is s (as opposed to a memory operand)
   */
  boolean needScratch(OPT_Register r, OPT_Instruction s) {
    // PowerPC does not support memory operands.
    return true;
  }
  /**
   * In instruction s, replace all appearances of a symbolic register 
   * operand with uses of the appropriate spill location, as cached by the
   * register allocator.
   *
   * @param s the instruction to mutate.
   * @param symb the symbolic register operand to replace
   */
  void replaceOperandWithSpillLocation(OPT_Instruction s, 
                                               OPT_RegisterOperand symb) {
    // PowerPC does not support memory operands.
    VM.assert(NOT_REACHED);
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/** 
 * Handle exception delivery and stack unwinding for 
 * opt compiled methods.
 * 
 * @author Dave Grove
 * @author Mauricio J. Serrano 
 */
final class VM_OptExceptionDeliverer extends VM_ExceptionDeliverer
  implements VM_Constants, VM_BytecodeConstants {

  /** 
   * Pass control to a catch block.
   */
  void deliverException(VM_CompiledMethod cm, 
			VM_Address catchBlockInstructionAddress, 
			Throwable exceptionObject, 
			VM_Registers registers) {

    // store exception object for later retrieval by catch block
    VM_OptCompiledMethod compiledMethod = (VM_OptCompiledMethod)cm;
    int offset = compiledMethod.getUnsignedExceptionOffset();
    if (offset != 0) {
      // only put the exception object in the stackframe if the catch block is expecting it.
      // (if the method hasn't allocated a stack slot for caught exceptions, then we can safely
      //  drop the exceptionObject on the floor).
      VM_Address fp = registers.getInnermostFramePointer();
      VM_Magic.setObjectAtOffset(VM_Magic.addressAsObject(fp), offset, exceptionObject);
    }

    // set address at which to resume executing frame
    registers.ip = catchBlockInstructionAddress;
    VM.enableGC(); // disabled right before VM_Runtime.deliverException was called

    if (VM.VerifyAssertions) VM.assert(registers.inuse == true);
    registers.inuse = false;

    // "branches" to catchBlockInstructionAddress
    VM_Magic.restoreHardwareExceptionState(registers);
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
  }

  /**
   * Unwind a stackframe.
   */ 
  void unwindStackFrame(VM_CompiledMethod cm, VM_Registers registers) {
    VM_Address fp = registers.getInnermostFramePointer();
    VM_OptCompiledMethod compiledMethod = (VM_OptCompiledMethod)cm;

    // restore non-volatile registers
    int frameOffset = compiledMethod.getUnsignedNonVolatileOffset();
    int firstInteger = compiledMethod.getFirstNonVolatileGPR();
    if (firstInteger >= 0) {
      for (int i = firstInteger; i < 32; i++) {
	registers.gprs[i] = VM_Magic.getMemoryWord(fp.add(frameOffset));
	frameOffset += 4;
      }
    }
    int firstFloat = compiledMethod.getFirstNonVolatileFPR();
    if (firstFloat >= 0) {
      frameOffset = (frameOffset + 7) & ~7;  // align pointer for doubles
      for (int i = firstFloat; i < 32; i++) {
	long temp = VM_Magic.getLongAtOffset(VM_Magic.addressAsObject(fp), frameOffset);
	registers.fprs[i] = VM_Magic.longBitsAsDouble(temp);
	frameOffset += 8;
      }
    }

    registers.unwindStackFrame();
  }
}



/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * An instance of this class provides iteration across the references 
 * represented by a frame built by the OPT compiler.
 *
 * The architecture-specific version of the GC Map iterator.  It inherits
 * its architecture-independent code from VM_OptGenericGCMapIterator.
 * This version is for the PowerPC
 *
 * @author Michael Hind
 */
public final class VM_OptGCMapIterator extends VM_OptGenericGCMapIterator {

  private final static boolean DEBUG = false;

  public VM_OptGCMapIterator(int[] registerLocations) {
    super(registerLocations);
  }

  /** 
   * If any non-volatile gprs were saved by the method being processed
   * then update the registerLocations array with the locations where the
   * registers were saved.
   *
   */
  void updateLocateRegisters() {

    //  HIGH MEMORY
    //
    //    +------------------+                                           
    //    |  NVolGPR[n]      |
    //    |     ...          |   k == info.getFirstNonVolatileGPR()      
    //    |  NVolGPR[k]      |  <-- info.getUnsignedNonVolatileOffset()      
    //    +------------------+                                           
    //    |  ScratchGPR[LAST]|                                           
    //    |     ...          |     only SaveVolatile Frames              
    //    | ScratchGPR[FIRST]|                                           
    //    +------------------+                                           
    //    |  VolGPR[LAST]    |                                           
    //    |     ...          |     only SaveVolatile Frames              
    //    |  VolGPR[FIRST]   |                                           
    //    +------------------+                                           
    //
    //  LOW MEMORY
    

    int frameOffset = compiledMethod.getUnsignedNonVolatileOffset();
    if (frameOffset >= 0) {

      // get to the nonVol area
      VM_Address nonVolArea = framePtr.add(frameOffset);
      
      // update non-volatiles that were saved
      int first = compiledMethod.getFirstNonVolatileGPR();
      if (first >= 0) {
	// move to the beginning of the save area for nonvolatiles
	VM_Address location = nonVolArea;
	for (int i = first; i <= LAST_GCMAP_REG; i++) {
	  registerLocations[i] = location.toInt();
	  location = location.add(4);
	}
      }
      
      // update volatiles if needed
      if (compiledMethod.isSaveVolatile()) {
	// move to the beginning of the save area for volatiles
	int numSlotsToSkip = FIRST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR;
	VM_Address location = nonVolArea.sub(4 * numSlotsToSkip);
	
	// Walk the saved volatiles, updating registerLocations array
	for (int i = FIRST_VOLATILE_GPR; i <= LAST_VOLATILE_GPR; i++) {
	  registerLocations[i] = location.toInt();
	  location = location.add(4);
	}
	
	// Walk the saved scratch, updating registerLocations array
	for (int i = FIRST_SCRATCH_GPR; i <= LAST_SCRATCH_GPR; i++) {
	  registerLocations[i] = location.toInt();
	  location = location.add(4);
	}
      }
    }
  }

  /** 
   *  Determine the stack location given the frame ptr and spill offset.
   *  (The offset direction varies among architectures.)
   *  @param framePtr the frame pointer
   *  @param offset  the offset 
   *  @return the resulting stack location
   */
  VM_Address getStackLocation(VM_Address framePtr, int offset) {
    return framePtr.add(offset);
  }

  /** 
   *  Get address of the first spill location for the given frame ptr
   *  @param the frame pointer
   *  @return the first spill location
   */
  VM_Address getFirstSpillLoc() {
    return framePtr.add(SPILL_DISTANCE_FROM_FP);
  }

  /** 
   *  Get address of the last spill location for the given frame ptr
   *
   *
   *  @param the frame pointer
   *  @return the last spill location, if no spills occur, we return the
   *      first spill location
   */
  VM_Address getLastSpillLoc() {
    if (DEBUG) {
      VM.sysWrite("\n unsigendNVOffset: ");
      VM.sysWrite(compiledMethod.getUnsignedNonVolatileOffset());
      VM.sysWrite("\t isSaveVolatile: ");
      if (compiledMethod.isSaveVolatile()) {
	VM.sysWrite("true");
      } else {
	VM.sysWrite("false");
      }
      VM.sysWrite("\nLAST_VOLATILE_GPR: ");
      VM.sysWrite(LAST_VOLATILE_GPR);
      VM.sysWrite("\tFIRST_VOLATILE_GPR: ");
      VM.sysWrite(LAST_VOLATILE_GPR);
      VM.sysWrite("\nLAST_SCRATCH_GPR: ");
      VM.sysWrite(LAST_SCRATCH_GPR);
      VM.sysWrite("\tFIRST_SCRATCH_GPR: ");
      VM.sysWrite(LAST_SCRATCH_GPR);
      VM.sysWrite("\nSAVE_VOL_SIZE: ");
      VM.sysWrite(SAVE_VOL_SIZE);
      VM.sysWrite("\n");
    }

    // This computation will include some locations that are not technically
    // spill locations.  We do this because we currently do not record 
    // enough info in the VM_OptCompiledMethod object (the one that is available
    // at GC time) to distinguish the lower part of the spill.

    VM_Address firstSpill = getFirstSpillLoc();
    VM_Address lastSpill;
    int nonVolOffset = compiledMethod.getUnsignedNonVolatileOffset();
    if (nonVolOffset != 0) {
      if (compiledMethod.isSaveVolatile()) {
	lastSpill = framePtr.add(nonVolOffset - 4 - SAVE_VOL_SIZE);
      } else {
	lastSpill = framePtr.add(nonVolOffset - 4);
      }
      // If the above computation is less than firstSpill, there are no spills
      if (lastSpill.LT(firstSpill)) {
	lastSpill = firstSpill;
      }
    } else {
      // If nonVolOffset = 0, there are no spills
      lastSpill = firstSpill;
    }
    return lastSpill;
  }

  final static int SPILL_DISTANCE_FROM_FP = 12;
  final static int SAVE_VOL_SIZE = 4 *
    ((LAST_VOLATILE_GPR - FIRST_VOLATILE_GPR + 1) + 
     (LAST_SCRATCH_GPR - FIRST_SCRATCH_GPR + 1)); 

}
/*
 * (C) Copyright IBM Corp. 2001
 */
// $Id$

/**
 * This interface holds constants for the Opt GC map code specific to PowerPC
 *
 * @author Michael Hind
 */
interface VM_OptGCMapIteratorConstants extends OPT_PhysicalRegisterConstants {
  
  // NOTE: The following two constants seem to imply that registers 
  //       that can hold references are contiguous.  This is not true,
  //       in general, however, for the GC map code we only need to make
  //       sure that all such registers are included in the range defined
  //       below these contants.

  /*
   * The first register that may hold a reference
   */
  static final int FIRST_GCMAP_REG = 1;

  /*
   * The last register that may hold a reference
   */
  static final int LAST_GCMAP_REG = NUM_GPRS - 1;
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * @author Julian Dolby
 */

class OPT_SpecializationCodePatching implements VM_BaselineConstants {

    private static final int HIDDEN_STORE_OFFSET = -1;
    private static boolean DO_CACHE_SYNCS = true;

    static private void insertCallSiteNumber(INSTRUCTION[] codeArray,
					     int callInstructionIndex,
					     int callSiteNumber) {
	int hiddenStoreIndex = callInstructionIndex + HIDDEN_STORE_OFFSET;
	codeArray[hiddenStoreIndex] = VM_Assembler.LIL(0, callSiteNumber);
	if (DO_CACHE_SYNCS && VM.runningVM) {
	    int byteOffset = hiddenStoreIndex << LG_INSTRUCTION_WIDTH;
	    int address = VM_Magic.objectAsAddress(codeArray) + byteOffset;
	    VM_Memory.sync(address, LG_INSTRUCTION_WIDTH);
	}
    }
    
    static public void insertCallSiteNumber(INSTRUCTION[] codeArray,
					    VM_CompiledMethod codeInfo,
					    GNO_InstructionLocation call,
					    int callSiteNumber)
    {
	if ( codeInfo.getCompilerType() == VM_CompiledMethod.BASELINE ) {
	    VM_BaselineCompiledMethod info = (VM_BaselineCompiledMethod)codeInfo;
	    int bcIndex = call.getByteCodeOffset();
	    int mcOffset = info.findInstructionForBytecodeIndex( bcIndex );
	    insertCallSiteNumber( codeArray, mcOffset, callSiteNumber );

	} else if ( codeInfo.getCompilerType() == VM_CompiledMethod.OPT ) {
	    VM_OptCompiledMethod info = (VM_OptCompiledMethod) codeInfo;
	    VM_OptMachineCodeMap map = info.getMCMap();
	    int[] callSiteEncoding = map.inlineEncoding;
	    int bcIndex = call.getByteCodeOffset();
	    int mcOffset = map.getMCoffsetForBCindex( bcIndex );
	    while ( mcOffset != -1 ) {
		GNO_InstructionLocation pos =
		    new GNO_InstructionLocation(info, mcOffset);
		if (pos.equals(call)) {
		    insertCallSiteNumber(codeArray, mcOffset, callSiteNumber);
		    return;
		} else 
		    mcOffset = 
			map.getNextMCoffsetForBCindex( bcIndex, mcOffset );
	    }
	} 
    }
    
    static public void insertCallSiteNumber(VM_CompiledMethod code,
					    GNO_InstructionLocation call,
					    int callSiteNumber)
    {
	INSTRUCTION[] codeArray = code.getInstructions();

	insertCallSiteNumber(codeArray, code, call, callSiteNumber);
    }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * @author Julian Dolby
 */
class OPT_SpecializationDispatchTrampolines implements VM_BaselineConstants {
    
    static private INSTRUCTION[] generate(OPT_ConcreteMethodKey key) {
	int codeMapOffset =  OPT_SpecializationManager.getJTOCoffset(key);
	if (OPT_SpecializationManager.DEBUG)
	    VM.sysWrite("replacing " + key.getMethod() + " of " + key.getType() + " with table in JTOC entry " + codeMapOffset + "\n");

	VM_Assembler code = new VM_Assembler(5);
	code.emitLtoc(S0, codeMapOffset << 2);
	code.emitLX(SP, S0, 0);
	code.emitMTCTR(SP);       
	code.emitBCTR();
	code.mc.finish();
	return code.mc.getInstructions();
    }

    static void insert(OPT_ConcreteMethodKey key) {
	INSTRUCTION[] code = generate(key);
	VM_Method method = key.getMethod();
	// the code in VM_Method does this >>> 2.  Why?
	int offset = method.getOffset() >>> 2;
	if ( method.isStatic() || 
	     method.isObjectInitializer() || 
	     method.isClassInitializer()) 
	{
	    VM_Statics.setSlotContents( offset, code );
	} else if (key.getType() instanceof VM_Class)
	    ((VM_Class)key.getType()).resetTIBEntry(method, code);
	else {
	    // arrays
	    VM_Type receiver = key.getType();
	    Object[] tib = receiver.getTypeInformationBlock();
	    tib[ offset ] = code;
	}
    }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * @author Julian Dolby
 */

class VM_LazySpecializationTrampoline 
implements VM_BaselineConstants, VM_DynamicBridge 
{
  /**
   *  The DynamicBridge protocol saves all registers on the stack,
   * so the local variables are below all the registers on the
   * stack.  The offsets are subtracted rather than added because
   * the stack grows downwards.  The `thisArgOffset' is the first
   * local because it is the first argument, and arguments are
   * (initially) put into the first local slots.  
   */
  private final static int thisArgOffset =  0
    - (LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1)*8  
    - (LAST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR + 1)*4;

  static void lazySpecializationTrampoline() throws VM_ResolutionException, VM_PragmaNoInline {

    // get top of calling stack frame:
    // this is the method that performed the call we are handling
    int topOfCallerFrame =
      VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());

    // stacks grow downward and thisArgOffset is negative,
    // so this is this frame's `this', just below register saves
    Object thisObject =
      VM_Magic.addressAsObject(
                               VM_Magic.getMemoryWord(topOfCallerFrame + thisArgOffset));

    // this identifies which method called us
    int callingCompiledMethodId = 
      VM_Magic.getCompiledMethodID(topOfCallerFrame);

    // this is which call instruction within that method called us
    int callingInstruction = 
      VM_Magic.getNextInstructionAddress(topOfCallerFrame);

    //
    // A comment above notes that the dynamic bridge protocol
    // saves all the registers.  Actually, the dynamic bridge
    // protocol saves _almost_ all registers, not including register
    // 0.  So we cannot find our hidden parameter in the saved
    // registers.  Damn.  We extract the call site number from the
    // call site machine code instead.   The instruction before
    // the call instruction itself is the store of the hidden
    // argument, so we extract the offset by masking the immediate
    // from that instruction, which we know is an LIL.
    //
    // 0xffff is the bitmask needed to extract the constant from
    // an LIL instruction, and it is shifted over 2 to convert it
    // from a byte index as needed for an offset load into a logical
    // call site number.
    // 
    // If the store is one instruction before the call, why do we
    // subtract 8, which is two instructions?  Because the call
    // instruction, being obtained from the return address, is
    // really the instruction after the call. (I think)
    //
    int callSiteStore = VM_Magic.getMemoryWord(callingInstruction-8);
    int callSiteNumber = (callSiteStore & 0xffff)>>>2;

    // compiled method info of method that called us
    VM_CompiledMethod callingCompiledMethod = 
      VM_CompiledMethods.getCompiledMethod(callingCompiledMethodId);

    // call instruction address relative to beginning of code array
    int callingInstructionOffset = 
      callingInstruction - 
      VM_Magic.objectAsAddress(callingCompiledMethod.getInstructions());

    // obtain symbolic method reference
    VM_DynamicLink link = new VM_DynamicLink();
    callingCompiledMethod.getDynamicLink(link, callingInstructionOffset);
    VM_Method invokedMethod = link.methodRef();

    // get the class of the receiver object
    VM_Type invokedClass;
    if (invokedMethod.isStatic() || invokedMethod.isObjectInitializer())
      invokedClass = invokedMethod.getDeclaringClass();
    else
      invokedClass = VM_Magic.getObjectType(thisObject).asClass();

    if (! invokedMethod.getDeclaringClass().isInstantiated())
      invokedMethod.getDeclaringClass().instantiate();

    // static method calls may be the first ``real'' use of a class,
    // in which case they trigger class initialization.
    if (! invokedMethod.getDeclaringClass().isInitialized())
      invokedMethod.getDeclaringClass().initialize();

    if (OPT_SpecializationManager.DEBUG)
      VM.sysWrite("called from " + callingCompiledMethod.getMethod() + "\n");

    if (callSiteNumber != 0) {
      // find specialization info given class, method and call site
      INSTRUCTION[] instructions =
        OPT_SpecializationManager.specializeAndInstall(invokedClass,
                                                       invokedMethod.getName(), 
                                                       invokedMethod.getDescriptor(),
                                                       callSiteNumber);

      // restore parameters and branch
      VM_Magic.dynamicBridgeTo(instructions);           
    }

    else {
      // a call site of 0 indicates a use of the generic method.

      // if we get here, it means that we are making an unspecialized
      // call to a method with specialized versions for which the
      // generic code has not yet been compiled.  so compile it.

      // first, find out where we are going...
      if (link.isInvokeSpecial()) {
        invokedMethod = VM_Class.findSpecialMethod(invokedMethod);
      } else if (link.isInvokeStatic()) { 
        // do nothing.
      } else { // invokevirtual or invokeinterface
        invokedMethod = 
          ((VM_Class)invokedClass).findVirtualMethod(
                                                     invokedMethod.getName(), 
                                                     invokedMethod.getDescriptor());
        if (invokedMethod == null)
          throw new VM_ResolutionException(
                                           invokedClass.getDescriptor(), 
                                           new IncompatibleClassChangeError(
                                                                            invokedClass.getDescriptor().classNameFromDescriptor()));
      }
      invokedMethod = invokedMethod.resolve();

      // ...then, compile the appropriate method
      VM_CompiledMethod code = 
        VM_RuntimeOptCompilerInfrastructure.
        optCompileWithFallBack(invokedMethod);

      VM_CompiledMethods.setCompiledMethod(code.getId(), code);

      int tableIndex = 
        OPT_SpecializationManager.getJTOCoffset(
                                                new OPT_ConcreteMethodKey(invokedClass, invokedMethod));

      INSTRUCTION[][] table = (INSTRUCTION[][])
        VM_Statics.getSlotContentsAsObject( tableIndex );

      table[0] = code.getInstructions();

      VM_Magic.dynamicBridgeTo( table[0] );
    }	    

    // does not return here
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);  
  }

  static private INSTRUCTION[] lazySpecializerInstructions = null;

  static public INSTRUCTION[] getLazySpecializerInstructions() {
    if (lazySpecializerInstructions == null) {
      VM_Member member = 
        VM.getMember("LVM_LazySpecializationTrampoline;",
                     "lazySpecializationTrampoline", 
                     "()V");

      lazySpecializerInstructions = ((VM_Method)member).compile();
    }

    return lazySpecializerInstructions;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Register Usage Conventions
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_RegisterConstants {
  //--------------------------------------------------------------------------------------------//
  //                              Register usage conventions.                                   //
  //--------------------------------------------------------------------------------------------//

   // Machine instructions.
   //
  static final int    LG_INSTRUCTION_WIDTH = 2;           // log2 of instruction width in bytes, powerPC
  static final String INSTRUCTION_ARRAY_SIGNATURE = "[I"; // for powerPC

   // Condition register thread switch bit (must be a field of a non-volatile condition register).
   //
  static final int THREAD_SWITCH_BIT = 8; // field 0 of condition register 2 [ I think. Bowen, is this right? --DL ]
  // TODO: change to define THREAD_SWITCH_BIT in terms of the following
  static final int THREAD_SWITCH_REGISTER = THREAD_SWITCH_BIT >> 2;
  static final int THREAD_SWITCH_FIELD = THREAD_SWITCH_BIT & 3;

   // General purpose register usage. (GPR's are 32 bits wide).
   //
  static final int REGISTER_ZERO              =  0; // special instruction semantics on this register
  static final int FRAME_POINTER              =  1; // AIX is 1
  static final int JTOC_POINTER               =  2; // AIX is 2
  static final int FIRST_VOLATILE_GPR         =  3; // AIX is 3
  //                                            ...
  static final int LAST_VOLATILE_GPR          = 12; // AIX is 10
  static final int FIRST_SCRATCH_GPR          = 13; // AIX is 11
  static final int LAST_SCRATCH_GPR           = 14; // AIX is 12
  static final int THREAD_ID_REGISTER         = 15;
  static final int PROCESSOR_REGISTER         = 16;
  static final int FIRST_NONVOLATILE_GPR      = 17; // AIX is 14
  //                                            ...
  static final int LAST_NONVOLATILE_GPR       = 31; // AIX is 31
  static final int NUM_GPRS                   = 32;

   // Floating point register usage. (FPR's are 64 bits wide).
   //
  static final int FIRST_SCRATCH_FPR          =  0; // AIX is 0
  static final int LAST_SCRATCH_FPR           =  0; // AIX is 0
  static final int FIRST_VOLATILE_FPR         =  1; // AIX is 1
  //                                            ...
  static final int LAST_VOLATILE_FPR          = 15; // AIX is 13
  static final int FIRST_NONVOLATILE_FPR      = 16; // AIX is 14
  //                                            ...
  static final int LAST_NONVOLATILE_FPR       = 31; // AIX is 31
  static final int NUM_FPRS                   = 32;

  static final int NUM_NONVOLATILE_GPRS = LAST_NONVOLATILE_GPR - FIRST_NONVOLATILE_GPR + 1;
  static final int NUM_NONVOLATILE_FPRS = LAST_NONVOLATILE_FPR - FIRST_NONVOLATILE_FPR + 1;

  // condition registers
  // TODO: fill table
  static final int NUM_CRS                    = 8;
   
   // special   registers (user visible)
  static final int NUM_SPECIALS               = 8;


  // AIX register convention (for mapping parameters in JNI calls)
  static final int FIRST_AIX_VOLATILE_GPR         =  3; 
  static final int LAST_AIX_VOLATILE_GPR          = 10; 
  static final int FIRST_AIX_VOLATILE_FPR         =  1; 
  static final int LAST_AIX_VOLATILE_FPR          = 13; 
  static final int AIX_FRAME_HEADER_SIZE          = 24;  // fp + cr + lr + res + res + toc = 6 * 4

  // Native code to JNI Function (Java) glue frame
  //
  //   RVM link area    -  STACKFRAME_HEADER_SIZE
  //   Volatile GPR 3-10 save area  -  8 words
  //   Volatile FPR 1-6  save area  - 12 words
  //   Non-Volatile GPR 13-16 save area  4 words   for AIX non-vol GPR not restored by RVM
  //   Non-Volatile FPR 14-15 save area  4 words   for AIX non-vol FPR not restored by RVM
  //   padding                           1 word
  //   offset to previous to java frame  1 word    the preceeding java to native transition frame
  //
  static final int JNI_GLUE_FRAME_SIZE = 
    VM_StackframeLayoutConstants.STACKFRAME_HEADER_SIZE + ((8+12+4+4+1+1)*4);

  // offset into the vararg save area within the native to Java glue frame
  // to saved regs GPR 6-10 & FPR 1-6, the volatile regs containing vararg arguments
  //
  static final int VARARG_AREA_OFFSET = 
    VM_StackframeLayoutConstants.STACKFRAME_HEADER_SIZE + (3*4);    // the RVM link area and saved GPR 3-5

  // number of volatile registers that may carry parameters and that need to be saved
  // and restored for the thread reschedule from Java VM_Processor to native VM_Processor
  // GPR4-10 = 7 words  (does not include R3)
  // FPR1-6  = 12 words
  static final int JNI_AIX_VOLATILE_REGISTER_SIZE   =  
    ((LAST_AIX_VOLATILE_GPR - (FIRST_AIX_VOLATILE_GPR + 1) + 1 + 12) * 4) ;   


  // offset into the Java to Native glue frame, relative to the Java caller frame
  // the definitions are chained to the first one, JNI_JTOC_OFFSET
  // saved R17-R31 + R16 + GCflag + affinity + saved JTOC + saved SP
  static final int JNI_JTOC_OFFSET                  = 4;
  static final int JNI_SP_OFFSET                    = JNI_JTOC_OFFSET + 4;  // at 8
  static final int JNI_RVM_NONVOLATILE_OFFSET       = JNI_SP_OFFSET + 4;    // at 12
  static final int JNI_PR_OFFSET                    = JNI_RVM_NONVOLATILE_OFFSET + 
    ((LAST_NONVOLATILE_GPR - FIRST_NONVOLATILE_GPR + 1) * 4);             // at 72
  static final int JNI_AIX_VOLATILE_REGISTER_OFFSET = JNI_PR_OFFSET + 4;    // at 76: save 7 register 4-10
  static final int JNI_AFFINITY_OFFSET = JNI_AIX_VOLATILE_REGISTER_OFFSET + JNI_AIX_VOLATILE_REGISTER_SIZE; // at 104
  static final int JNI_PROLOG_RETURN_ADDRESS_OFFSET  = JNI_AFFINITY_OFFSET + 4;          // 108
  static final int JNI_GC_FLAG_OFFSET  = JNI_PROLOG_RETURN_ADDRESS_OFFSET  + 4;          // 112

  // size in byte of the whole save area for the Java to C glue frame
  // static final int JNI_SAVE_AREA_OFFSET = (4*(LAST_NONVOLATILE_GPR-FIRST_NONVOLATILE_GPR+6)
  //                                             +JNI_AIX_VOLATILE_REGISTER_SIZE);
  static final int JNI_SAVE_AREA_SIZE = JNI_GC_FLAG_OFFSET;

  // Register mnemonics (for use by debugger).
  //
  static final String [] GPR_NAMES = {
    "R0", "FP", "JT", "R3", "R4", "R5", "R6", "R7",
    "R8", "R9", "R10", "R11", "R12", "R13", "SP", "TI",
    "PR", "R17", "R18", "R19", "R20", "R21", "R22", "R23",
    "R24", "R25", "R26", "R27", "R28", "R29", "R30", "R31"
  };

  static final String [] FPR_NAMES = {
    "F0",  "F1",  "F2",  "F3",  "F4",  "F5",  "F6", "F7",
    "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15",
    "F16",  "F17",  "F18",  "F19",  "F20",  "F21",  "F22",  "F23",
    "F24",  "F25",  "F26",  "F27",  "F28",  "F29",  "F30",  "F31"
  };

}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * The machine state comprising a thread's execution context.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
public class VM_Registers implements VM_Constants, VM_Uninterruptible {
  // The following are used both for thread context switching
  // and for hardware exception reporting/delivery.
  //
  public int    gprs[]; // 32-bit general purpose registers
  public double fprs[]; // 64-bit floating point registers
  public VM_Address ip; // instruction address register
  
  // The following are used by exception delivery.
  // They are set by either VM_Runtime.athrow or the C hardware exception 
  // handler and restored by "VM_Magic.restoreHardwareExceptionState".
  // They are not used for context switching.
  //
  public VM_Address lr;     // link register
  public boolean inuse; // do exception registers currently contain live values?

  static VM_Address invalidIP = VM_Address.fromInt(-1);
  
  VM_Registers() {
    gprs = new int[NUM_GPRS];
    fprs = new double[NUM_FPRS];
    ip = invalidIP;
  }

  // Return framepointer for the deepest stackframe
  //
  public final VM_Address getInnermostFramePointer () {
      return VM_Address.fromInt(gprs[FRAME_POINTER]);
  }

  // Return next instruction address for the deepest stackframe
  //
  public final VM_Address getInnermostInstructionAddress () {
    if (ip.NE(invalidIP)) return ip; // ip set by hardware exception handler or VM_Magic.threadSwitch
    return VM_Magic.getNextInstructionAddress(getInnermostFramePointer()); // ip set to -1 because we're unwinding
  }

  // update the machine state to unwind the deepest stackframe.
  // 
  final void unwindStackFrame() {
    ip = invalidIP; // if there was a valid value in ip, it ain't valid anymore
    gprs[FRAME_POINTER] = VM_Magic.getCallerFramePointer(getInnermostFramePointer()).toInt();
  }

  // set ip & fp. used to control the stack frame at which a scan of
  // the stack during GC will start, for ex., the top java frame for
  // a thread that is blocked in native code during GC.
  //
  public final void setInnermost( VM_Address newip, VM_Address newfp ) {
    ip = newip;
    gprs[FRAME_POINTER] = newfp.toInt();
  }

  // set ip and fp values to those of the caller. used just prior to entering
  // sigwait to set fp & ip so that GC will scan the threads stack
  // starting at the frame of the method that called sigwait.
  //
  public final void setInnermost() {
    VM_Address fp = VM_Magic.getFramePointer();
    ip = VM_Magic.getReturnAddress(fp);
    gprs[FRAME_POINTER] = VM_Magic.getCallerFramePointer(fp).toInt();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */

//$Id$

/**
 *--------------------------------------------------------------------------
 *                     Stackframe layout conventions           
 *---------------------------------------------------------------------------
 *
 * A stack is an array of "slots", declared formally as integers, each slot
 * containing either a primitive (byte, int, float, etc), an object pointer,
 * a machine code pointer (a return address pointer), or a pointer to another
 * slot in the same stack (a frame pointer). The interpretation of a slot's 
 * contents depends on the current value of IP, the machine instruction 
 * address register.
 * Each machine code generator provides maps, for use by the garbage collector,
 * that tell how to interpret the stack slots at "safe points" in the
 * program's execution.
 *
 * Here's a picture of what a stack might look like in memory.
 *
 * Note: this (array) object is drawn upside down compared to other objects
 * because the hardware stack grows from high memory to low memory, but
 * array objects are layed out from low memory to high (header first).
 * <pre>
 *  hi-memory
 *                 +===============+
 *                 |  LR save area |
 *                 +---------------+
 *                 |     MI=-1     |   <-- "invisible method" id
 *                 +---------------+
 *             +-> |     FP=0      |   <-- "end of vm stack" sentinal
 *             |   +===============+ . . . . . . . . . . . . . . . . . . . . . . . . . . .
 *             |   |   saved FPRs  |  \                                                  .
 *             |   +---------------+   \_nonvolatile register save area                  .
 *             |   |   saved GPRS  |   /                                                 .
 *             |   +---------------+  /                                                  .
 *             |   |   (padding)   |  <--optional padding so frame size is multiple of 8 .
 *             |   +---------------+                                                     .
 *             |   |     SP        |  <-- caller save/restore area (++)                  .
 *             |   +---------------+                                                     .
 *             |   |   local0      |  \                                                  .
 *             |   +---------------+   \_local variables (++)                            .
 *             |   |   local1      |   /                                                 .
 *             |   +---------------+  /                                                  .
 *             |   |   operand0    |  \                                                  .
 *             |   +---------------+   \_operand stack (++)                              .
 *  (++)SP ->  |   |   operand1    |   /                                                 .
 *             |   +---------------+  /                                                  ..frame
 *             |   |     ...       |                                                     .
 *             |   +---------------+                                                     .
 *             |   |    spill1     |  \                                                  .
 *             |   +---------------+   \_parameter spill area                            .
 *             |   |    spill0     |   /                                                 .
 *             |   +===============+  /                                                  .
 *             |   |               |   <-- spot for this frame's callee's return address .
 *             |   +---------------+                                                     .
 *             |   |     MI        |   <-- this frame's method id                        .
 *             \   +---------------+                                                     .
 *      FP ->      |   saved FP    |   <-- this frame's caller's frame                   .
 *                 +===============+ . . . . . . . . . . . . . . . . . . . . . . . . . . .
 * th.stackLimit-> |     ...       | \
 *                 +---------------+  \_guard region for detecting & processing stack overflow
 *                 |     ...       |  /
 *                 +---------------+ /
 *                 |(object header)|
 *  low-memory     +---------------+              
 *
 * note: (++) means "baseline compiler frame layout and register 
 * usage conventions"
 * 
 * </pre>
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_StackframeLayoutConstants  {

   static final int STACKFRAME_HEADER_SIZE             = 12; // size of frame header, in bytes
   static final int STACKFRAME_NEXT_INSTRUCTION_OFFSET =  8; // spot for this frame's callee to put return address
   static final int STACKFRAME_METHOD_ID_OFFSET        =  4; // spot for this frame's method id
   static final int STACKFRAME_FRAME_POINTER_OFFSET    =  0; // base of this frame

   static final int STACKFRAME_SENTINAL_FP = -2; // fp value indicating end of stack walkback
   static final int INVISIBLE_METHOD_ID    = -1; // marker for "assembler" frames that have no associated VM_Method

   // Stackframe alignment.
   // Align to 8 byte boundary for good floating point save/restore performance (on powerPC, anyway).
   //
   static final int STACKFRAME_ALIGNMENT = 8;
   static final int STACKFRAME_ALIGNMENT_MASK = STACKFRAME_ALIGNMENT - 1; // roundedUpSize = (size + STACKFRAME_ALIGNMENT_MASK) & ~STACKFRAME_ALIGNMENT_MASK
   
   // Sizes for stacks and subregions thereof.
   // Values are in bytes and must be a multiple of 4 (size of a stack slot).
   //
   static final int STACK_SIZE_GROW      = 8*1024; // how much to grow normal stack when overflow detected
   static final int STACK_SIZE_GUARD     = 8*1024; // max space needed for stack overflow trap processing
   static final int STACK_SIZE_NATIVE    = 4*1024; // max space needed for entry to sysCall# via VM_Magic
   static final int STACK_SIZE_JNINATIVE      = 180*1024; // max space needed for first entry to native code via JNI
   static final int STACK_LOG_JNINATIVE      = 6 + 10; // large constants are a pain.  This is log2 of STACK_SIZE_JNINATIVE
   static final int STACK_SIZE_DLOPEN    = 30*1024; // max space needed for dlopen sys call 
   static final int STACK_SIZE_JNINATIVE_GROW = 184*1024; // size to grow once for native on first entry via JNI
   static final int STACK_SIZE_GCDISABLED= 4*1024; // max space needed while running with gc disabled
   
   // Complications:
   // - STACK_SIZE_GUARD must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that frames allocated by stack growing code will fit within guard region.
   // - STACK_SIZE_GROW must be greater than STACK_SIZE_NATIVE or STACK_SIZE_GCDISABLED
   //   to ensure that, if stack is grown prior to disabling gc or calling native code,
   //   the new stack will accomodate that code without generating a stack overflow trap.
   // - Values chosen for STACK_SIZE_NATIVE and STACK_SIZE_GCDISABLED are pure guesswork
   //   selected by trial and error.
   
   // Stacks for "normal" threads grow as needed by trapping on guard region.
   // Stacks for "boot" and "collector" threads are fixed in size and cannot grow.
   //
   static final int STACK_SIZE_NORMAL    = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +   8*1024; // initial stack space to allocate for normal    thread (includes guard region)
   static final int STACK_SIZE_BOOT      = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for boot      thread (includes guard region)
   static final int STACK_SIZE_COLLECTOR = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED +  20*1024; // total   stack space to allocate for collector thread (includes guard region)
   static final int STACK_SIZE_MAX       = STACK_SIZE_GUARD + STACK_SIZE_GCDISABLED + 244*1024; // upper limit on stack size (includes guard region)
   }



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Trap Conventions
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_TrapConstants {
  //--------------------------------------------------------------------------------------------//
  //                              Trap Conventions.                                             //
  //--------------------------------------------------------------------------------------------//
 
  // Compilers should generate trap instructions that conform to the following
  // values in order for traps to be correctly recognized by the trap handler
  // in libvm.C
  //
  static final int DIVIDE_BY_ZERO_MASK        = 0xFFE0FFFF; // opcode, condition mask, & immediate
  static final int DIVIDE_BY_ZERO_TRAP        = 0x0C800000; // teqi, divisor, 0
  static final int ARRAY_INDEX_MASK           = 0xFFE007FE; // extended opcode and condition mask
  static final int ARRAY_INDEX_TRAP           = 0x7CC00008; // tlle arraySize, arrayIndex
  static final int ARRAY_INDEX_REG_MASK       = 0x0000f800;
  static final int ARRAY_INDEX_REG_SHIFT      = 11;
  static final int CONSTANT_ARRAY_INDEX_MASK  = 0xFFE00000; // opcode and condition mask
  static final int CONSTANT_ARRAY_INDEX_TRAP  = 0x0CC00000; // tllei arraySize, arrayIndexConstant
  static final int CONSTANT_ARRAY_INDEX_INFO  = 0x0000ffff;
  static final int STACK_OVERFLOW_MASK        = 0xFFE007FE; // opcode and condition mask
  static final int STACK_OVERFLOW_TRAP        = 0x7E000008; // tlt stackPointer, stackLimit
  static final int WRITE_BUFFER_OVERFLOW_MASK = 0xFFE007FE; // opcode and condition mask
  static final int WRITE_BUFFER_OVERFLOW_TRAP = 0x7E800008; // tle modifiedOldObjectMax, modifiedOldObjectAddr

  /* JNI stack size checking */
  static final int JNI_STACK_TRAP_MASK        = 0x0FECFFFF; // tALWAYSi, 12, 0x0001 
  static final int JNI_STACK_TRAP             = 0x0FEC0001; 

  /* USED BY THE OPT_COMPILER */
  static final int CHECKCAST_MASK             = 0x0FECFFFF; // tALWAYSi, 12, 0x0000 
  static final int CHECKCAST_TRAP             = 0x0FEC0000; 
  static final int MUST_IMPLEMENT_MASK        = 0x0FECFFFF; // tALWAYSi, 12, 0x0002 
  static final int MUST_IMPLEMENT_TRAP        = 0x0FEC0002; 
  static final int STORE_CHECK_MASK           = 0x0FECFFFF; // tALWAYSi, 12, 0x0003 
  static final int STORE_CHECK_TRAP           = 0x0FEC0003; 
  static final int REGENERATE_MASK            = 0xFFE007FE;
  static final int REGENERATE_TRAP            = 0x7C600008; // tlne
  static final int NULLCHECK_MASK             = 0xFFE0FFFF;
  static final int NULLCHECK_TRAP             = 0x0C400001; // tllt 1
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Disassembler for Rios instruction set.
 * @author Ton Ngo
 *
 * Defined: disasm(inst, addr, buf, reg)
 *          INSTRUCTION inst; ADDRESS addr;  CHAR *buf; CHAR reg[4][10];
 *
 * 31 Jul 1990 Derek Lieber.
 *      Borrowed from libdbx (opcode.c, decode.c).
 * 
 * 30 Jan 1998 Ton Ngo:
 *      adapted for Java debugger jdp
 *
 *  7 Aug 1998 John Whaley:
 *      rewritten in Java
 *
 *  26 Jan 1999 Ton Ngo:
 *    New instruction for PowerPC that are not in the POWER instruction set
 *    opcode is bit 0-5 of the instruction
 *    extop is the extended opcode as listed in the manual
 *    key is extop plus additional bits for variation on an opcode (. or o)
 *    form is the instruction format: I, B, SC, D, DS, X, XL, XFX, XFL, XO, A, M 
 *    format is a local enumeration to specify how to print the instruction 
 *    (this is specific to each decode_?? method).
 *  
 * 18 Dec 2000 Dave Grove:
 *    Changed the mnemonics for POWER instructions that have 
 *    different names on the PowerPC to use the PowerPC mnemonic.
 *    (Applied changes from Appendix F of PPC Architecture book).
 *    
 *  mnemonic   	opcode  extop   key    	form    format  Example  
 *  --------   	------  ---    	--- 	----    ------  --------
 *  dcbf	31	86	172	X	6	7C0218AC
 *  dcbi	31	470	940	X	6	7C021BAC
 *  dcbst	31	54	108	X	6	7C02186C
 *  dcbt	31	278	556	X	6	7C021A2C
 *  dcbtst	31	246	492	X	6	7C0219EC
 *  dcbz	31	1014	2028	X	6	7C021FEC
 *  divw	31	491	982	XO	1	7C221BD6
 *  divw.	31	491	983	XO	1	7C221BD7
 *  divwo	31	491	2006	XO	1	7C221FD6
 *  divwo.	31	491	2007	XO	1	7C221FD7
 *  divwu	31	459	918	XO	1	7C221B96
 *  divwu.	31	459	919	XO	1	7C221B97
 *  divwuo	31	459	1942	XO	1	7C221F96
 *  divwuo.	31	459	1943	XO	1	7C221F97
 *  eieio	31	854	1708	X	1	7C0006AC
 *  extsb	31	954	1908	X	0	7C220774
 *  extsb.	31	954	1909	X	0	7C220775
 *  icbi	31	982	1964	X	6	7C021FAC
 *  lwarx	31	20	40	X	7	7C221828
 *  mfsrin	31	659	1318	X	4	7C201526
 *  mulhw	31	75	150	XO	1	7C221896
 *  mulhw.	31	75	151	XO	1	7C221897
 *  mulhwu	31	11	22	XO	1	7C221816
 *  mulhwu.	31	11	23	XO	1	7C221817
 *  stwcx.	31	150	301	X	7	7C22192D
 *  subf	31	40	80	XO	1	7C221850
 *  subf.	31	40	81	XO	1	7C221851
 *  subfo	31	40	1104	XO	1	7C221C50
 *  subfo.	31	40	1105	XO	1	7C221C51
 *  							    
 *  fadds	59	21	42	A	0	EC22182A
 *  fadds.	59	21	43	A	0	EC22182B
 *  fdivs	59	18	36	A	0	EC221824
 *  fdivs.	59	18	37	A	0	EC221825
 *  fmadds	59	29	58	A	2	EC22193A
 *  fmadds.	59	29	59	A	2	EC22193B
 *  fmsubs	59	28	56	A	2	EC221938
 *  fmsubs.	59	28	57	A	2	EC221939
 *  fmuls	59	25	50	A	1	EC2200F2
 *  fmuls.	59	25	51	A	1	EC2200F3
 *  fnmadds	59	31	62	A	2	EC22193E
 *  fnmadds.	59	31	63	A	2	EC22193F
 *  fnmsubs	59	30	60	A	2	EC22193C
 *  fnmsubs.	59	30	61	A	2	EC22193D
 *  fsubs	59	20	40	A	0	EC221828
 *  fsubs.	59	20	41	A	0	EC221829
 *  mfear					
 *  mfpvw					
 *  mfsprg					
 *  mtear					
 *  mtsprg					
 *  mfdbatl		
 *  mfdbatu		
 *  mtdbatl		
 *  mtdbatu		
 *  mttb		
 *  mttbu		
 *  mftb		
 *  mftbu		
 *  mfibatl		
 *  mfibatu		
 *  mtibatl		
 *  mtibatu		
 */
class PPC_Disassembler implements VM_Constants {
  // special register name copied from /usr/include/sys/reg.h
  static final int IAR = 128;
  static final int MSR = 129;
  static final int CR = 130;
  static final int LR = 131;
  static final int CTR = 132;
  static final int XER = 133;
  static final int MQ = 134;
  static final int TID = 135;
  static final int FPSCR = 136;
  static final int FPINFO = 138;
  static final int FPSCRX = 148;

  // for testing purposes
  public static void main(String[] args) {

    int instr = Integer.parseInt(args[0],16);
    int addr = Integer.parseInt(args[1],16);
    System.out.println("instr = "+intAsHexString(instr)+" addr = "+
       intAsHexString(addr));
    System.out.println("result --> "+disasm(instr, addr));
    
  }
  
  static final int bits (int x, int n, int m) {
    return ((x >> (31-m)) & ((1 << (m-n+1)) - 1));
  }
  
  static final int signed (int x, int n, int m) {
    return ((x << n) >> 31-m+n);
  }

  static String rname(int n) {
    String rvmName;
    if (n>=0 && n<=31)
      rvmName = GPR_NAMES[n];
    else if (n>=128 && n<=135)
      rvmName = FPR_NAMES[n-128];
    else 
      switch (n) {
      case IAR:  rvmName = "IAR"; break;
      case MSR:  rvmName = "MSR"; break;
      case CR:   rvmName = "CR"; break;
      case LR:   rvmName = "LR"; break;
      case CTR:  rvmName = "CTR"; break;
      case XER:  rvmName = "XER"; break;
      case MQ:   rvmName = "MQ"; break;
      case TID:  rvmName = "TID"; break;
      case FPSCR:   rvmName = "FPSCR"; break;
      case FPINFO:  rvmName = "FPINFO"; break;
      case FPSCRX:  rvmName = "FPSCRX"; break;
      default: rvmName = "r" + n; break;
      }
    return rvmName;
  }

  
  static final int INST_SZ = 4;
  
  /* Special register fields */
  static final int SPR_MQ  = 0;
  static final int SPR_XER  = 1;
  static final int SPR_LR  = 8;
  static final int SPR_CTR  = 9;
  static final int SPR_TID  = 17;
  static final int SPR_DSISR= 18;
  static final int SPR_DAR  = 19;
  static final int SPR_RTCU = 20;
  static final int SPR_RTCL = 21;
  static final int SPR_DEC  = 22;
  static final int SPR_SDR0 = 24;
  static final int SPR_SDR1 = 25;
  static final int SPR_SRR0 = 26;
  static final int SPR_SRR1 = 27;
  
  /* Trap Options */
  static final int TO_LT=16;
  static final int TO_GT= 8;
  static final int TO_EQ= 4;
  static final int TO_LLT=   2;
  static final int TO_LGT= 1;
  
  /* Different instruction formats */
  
  static final int  D_FORM    = 0;
  static final int  B_FORM    = 1;
  static final int  I_FORM=     2;
  static final int  SC_FORM =   3;
  static final int  X_FORM   =  4;
  static final int  XL_FORM   = 5;
  static final int  XFX_FORM  = 6;
  static final int  XFL_FORM  = 7;
  static final int  XO_FORM   = 8;
  static final int  A_FORM    = 9;
  static final int  M_FORM    =10;
  static final int  EXTENDED  =11;
  static final int  INVALID_OP=12;

  static final int OPCODE31_SZ  =  187;
  static final int OPCODE63_SZ  =  23;
  static final int AFORM_SZ     =  16;
  
  /* Condition register fields */
  static final int CR_LT=8;
  static final int CR_GT=4;
  static final int CR_EQ=2;
  static final int CR_SO=1;
  
  static final int X=9; // place holder
  
  /*
   * Instruction decoder.
   */
  static int destination;

  static
  int opcode_to_form[] = {
    /* table to convert opcode into */
    /* instruction formats          */

    /* FORM */
    INVALID_OP, /* OPCODE 00 */
    INVALID_OP, /* OPCODE 01 */
    INVALID_OP, /* OPCODE 02 */
    D_FORM,     /* OPCODE 03 */
    D_FORM,     /* OPCODE 04 */
    D_FORM,     /* OPCODE 05 */
    D_FORM,     /* OPCODE 06 */
    D_FORM,     /* OPCODE 07 */
    D_FORM,     /* OPCODE 08 */
    D_FORM,     /* OPCODE 09 */
    D_FORM,     /* OPCODE 10 */
    D_FORM,     /* OPCODE 11 */
    D_FORM,     /* OPCODE 12 */
    D_FORM,     /* OPCODE 13 */
    D_FORM,     /* OPCODE 14 */
    D_FORM,     /* OPCODE 15 */
    B_FORM,     /* OPCODE 16 */
    SC_FORM,    /* OPCODE 17 */
    I_FORM,     /* OPCODE 18 */
    XL_FORM,    /* OPCODE 19 */
    M_FORM,     /* OPCODE 20 */
    M_FORM,     /* OPCODE 21 */
    M_FORM,     /* OPCODE 22 */
    M_FORM,     /* OPCODE 23 */
    D_FORM,     /* OPCODE 24 */
    D_FORM,     /* OPCODE 25 */
    D_FORM,     /* OPCODE 26 */
    D_FORM,     /* OPCODE 27 */
    D_FORM,     /* OPCODE 28 */
    D_FORM,     /* OPCODE 29 */
    D_FORM,     /* OPCODE 30 */
    EXTENDED,   /* OPCODE 31 */
    D_FORM,     /* OPCODE 32 */
    D_FORM,     /* OPCODE 33 */
    D_FORM,     /* OPCODE 34 */
    D_FORM,     /* OPCODE 35 */
    D_FORM,     /* OPCODE 36 */
    D_FORM,     /* OPCODE 37 */
    D_FORM,     /* OPCODE 38 */
    D_FORM,     /* OPCODE 39 */
    D_FORM,     /* OPCODE 40 */
    D_FORM,     /* OPCODE 41 */
    D_FORM,     /* OPCODE 42 */
    D_FORM,     /* OPCODE 43 */
    D_FORM,     /* OPCODE 44 */
    D_FORM,     /* OPCODE 45 */
    D_FORM,     /* OPCODE 46 */
    D_FORM,     /* OPCODE 47 */
    D_FORM,     /* OPCODE 48 */
    D_FORM,     /* OPCODE 49 */
    D_FORM,     /* OPCODE 50 */
    D_FORM,     /* OPCODE 51 */
    D_FORM,     /* OPCODE 52 */
    D_FORM,     /* OPCODE 53 */
    D_FORM,     /* OPCODE 54 */
    D_FORM,     /* OPCODE 55 */
    INVALID_OP, /* OPCODE 56 */
    INVALID_OP, /* OPCODE 57 */
    INVALID_OP, /* OPCODE 58 */
    A_FORM,     /* OPCODE 59 */
    INVALID_OP, /* OPCODE 60 */
    INVALID_OP, /* OPCODE 61 */
    INVALID_OP, /* OPCODE 62 */
    EXTENDED    /* OPCODE 63 */
  };
  
  static opcode_tab Dform[] = {
    
    /* Table for the D instruction format */
    
    /*   OPCD     EO                                format      mnemonic   */
    /*   ----     --                                ------      --------   */
    /*    0,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    1,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    2,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    3,      XXX,     */  new opcode_tab (      3,      "twi"     ), 
    /*    4,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
       
    /*    5,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    6,      XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    7,      XXX,     */  new opcode_tab (      0,      "mulli"   ),
    /*    8,      XXX,     */  new opcode_tab (      0,      "subfic"  ),
    /*    9,      XXX,     */  new opcode_tab (      0,      "dozi"    ),
    
    /*    10,     XXX,     */  new opcode_tab (      4,      "cmpli"   ),
    /*    11,     XXX,     */  new opcode_tab (      4,      "cmpi"    ),
    /*    12,     XXX,     */  new opcode_tab (      0,      "addic"   ),
    /*    13,     XXX,     */  new opcode_tab (      0,      "addic."  ),
    /*    14,     XXX,     */  new opcode_tab (      2,      "addi"    ),
       
    /*    15,     XXX,     */  new opcode_tab (      0,      "addis"   ),
    /*    16,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    17,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    18,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    19,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    
    /*    20,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    21,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    22,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    23,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    24,     XXX,     */  new opcode_tab (      1,      "ori"     ),
    
    /*    25,     XXX,     */  new opcode_tab (      1,      "oris"    ),
    /*    26,     XXX,     */  new opcode_tab (      1,      "xori"    ),
    /*    27,     XXX,     */  new opcode_tab (      1,      "xoris"   ),
    /*    28,     XXX,     */  new opcode_tab (      1,      "andi. "  ),
    /*    29,     XXX,     */  new opcode_tab (      1,      "andis."  ),
    
    /*    30,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    31,     XXX,     */  new opcode_tab (      X,      "RESERVED"),
    /*    32,     XXX,     */  new opcode_tab (      2,      "lwz"     ),
    /*    33,     XXX,     */  new opcode_tab (      2,      "lwzu"    ),
    /*    34,     XXX,     */  new opcode_tab (      2,      "lbz"     ),
    
    /*    35,     XXX,     */  new opcode_tab (      2,      "lbzu"    ),
    /*    36,     XXX,     */  new opcode_tab (      2,      "stw"     ),
    /*    37,     XXX,     */  new opcode_tab (      2,      "stwu"    ),
    /*    38,     XXX,     */  new opcode_tab (      2,      "stb"     ),
    /*    39,     XXX,     */  new opcode_tab (      2,      "stbu"    ),
    
    /*    40,     XXX,     */  new opcode_tab (      2,      "lhz"     ),
    /*    41,     XXX,     */  new opcode_tab (      2,      "lhzu"    ),
    /*    42,     XXX,     */  new opcode_tab (      2,      "lha"     ),
    /*    43,     XXX,     */  new opcode_tab (      2,      "lhau"    ),
    /*    44,     XXX,     */  new opcode_tab (      2,      "sth"     ),
    
    /*    45,     XXX,     */  new opcode_tab (      2,      "sthu"    ),
    /*    46,     XXX,     */  new opcode_tab (      2,      "lmw"     ),
    /*    47,     XXX,     */  new opcode_tab (      2,      "stmw"    ),
    /*    48,     XXX,     */  new opcode_tab (      5,      "lfs"     ),
    /*    49,     XXX,     */  new opcode_tab (      5,      "lfsu"    ),
    
    /*    50,     XXX,     */  new opcode_tab (      5,      "lfd"     ),
    /*    51,     XXX,     */  new opcode_tab (      5,      "lfdu"    ),
    /*    52,     XXX,     */  new opcode_tab (      5,      "stfs"    ),
    /*    53,     XXX,     */  new opcode_tab (      5,      "stfsu"   ),
    /*    54,     XXX,     */  new opcode_tab (      5,      "stfd"    ),
    
    /*    55,     XXX,     */  new opcode_tab (      5,      "stfdu"   )
  };
  
  
  static
  opcode_tab XLform[] = {
    
    /* Table for the XL instruction format */
    
    /*   OPCD      EO                 format     mnemonic      */
    /*   ----      --                 ------     --------      */ 
    /*    19,      0,      */  new opcode_tab(        2,        "mcrf"   ),
    /*    19,      16,     */  new opcode_tab(        1,        "bclr or bclrl"),
    /*    19,      33,     */  new opcode_tab(        3,        "crnor"   ),
    /*    19,      50,     */  new opcode_tab(        0,        "rfi"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    
    /*    19,      82,     */  new opcode_tab(        0,        "rfsvc"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      129,    */  new opcode_tab(        3,        "crandc"   ),
    /*    19,      150,    */  new opcode_tab(        0,        "isync"   ),
    
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      193,    */  new opcode_tab(        3,        "crxor"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      225,    */  new opcode_tab(        3,        "crnand"   ),
    
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      257,    */  new opcode_tab(        3,        "crand"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      289,    */  new opcode_tab(        3,        "creqv"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      417,    */  new opcode_tab(        3,        "crorc"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      449,    */  new opcode_tab(        3,        "cror"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      XXX,    */  new opcode_tab(        X,        "RESERVED"   ),
    /*    19,      528,    */  new opcode_tab(        1,        "bcctr or bcctrl")
  };
  
  
  
  /**
   *  Opcode 31 table: 
   *  The key is bits 21 through 31 of the instruction. 
   *  "Form" is the instruction format: 
   *      I, B, SC, D, DS, X, XL, XFX, XFL, XO, A, M
   *  "format" is how the instruction should be printed (specific to the disassembler) 
   */
  
  
  
  static
  opcodeXX opcode31[] = {
    
    /*                  key        form     format       mnemonic       */
    /*                  ---        ----     ------       --------       */
    new opcodeXX(        0,         4,        22,         "cmp"), 
    new opcodeXX(        8,         4,        24,         "tw"),
    new opcodeXX(       16,         8,         1,         "subfc"),
    new opcodeXX(       17,         8,         1,         "subfc."),
    new opcodeXX(       20,         8,         1,         "addc"),
    new opcodeXX(       21,         8,         1,         "addc."),
    new opcodeXX(       38,         4,         2,         "mfcr"),
    new opcodeXX(       46,         4,         7,         "lwzx"),
    new opcodeXX(       48,         4,         8,         "slw"),
    new opcodeXX(       49,         4,         8,         "slw."),
    new opcodeXX(       52,         4,         9,         "cntlzw"),
    new opcodeXX(       53,         4,         9,         "cntlzw."),
    new opcodeXX(       56,         4,         8,         "and"),
    new opcodeXX(       57,         4,         8,         "and."),
    new opcodeXX(       58,         4,         8,         "maskg"),
    new opcodeXX(       59,         4,         8,         "maskg."),
    new opcodeXX(       64,         4,        22,         "cmpl"),
    new opcodeXX(      110,         4,         7,         "lwzux"),
    new opcodeXX(      120,         4,         8,         "andc"),
    new opcodeXX(      121,         4,         8,         "andc."),
    new opcodeXX(      166,         4,         2,         "mfmsr"),
    new opcodeXX(      174,         4,         7,         "lbzx"),
    new opcodeXX(      208,         8,         0,         "neg"),
    new opcodeXX(      209,         8,         0,         "neg."),
    new opcodeXX(      214,         8,         1,         "mul"),
    new opcodeXX(      215,         8,         1,         "mul."),
    new opcodeXX(      236,         4,         6,         "clf"),
    new opcodeXX(      238,         4,         7,         "lbzux"),
    new opcodeXX(      248,         4,         8,         "nor"),
    new opcodeXX(      249,         4,         8,         "nor."),
    new opcodeXX(      272,         8,         1,         "subfe"),
    new opcodeXX(      273,         8,         1,         "subfe."),
    new opcodeXX(      276,         8,         1,         "adde"),
    new opcodeXX(      277,         8,         1,         "adde."),
    new opcodeXX(      288,         6,         9,         "mtcrf"),
    new opcodeXX(      292,         4,         2,         "mtmsr"),
    new opcodeXX(      302,         4,         7,         "stwx"),
    new opcodeXX(      304,         4,         8,         "slq"),
    new opcodeXX(      305,         4,         8,         "slq."),
    new opcodeXX(      306,         4,         8,         "sle"),
    new opcodeXX(      307,         4,         8,         "sle."),
    new opcodeXX(      366,         4,         7,         "stwux"),
    new opcodeXX(      368,         4,        11,         "sliq"),
    new opcodeXX(      369,         4,        11,         "sliq."),
    new opcodeXX(      400,         8,         0,         "subfze"),
    new opcodeXX(      401,         8,         0,         "subfze."),
    new opcodeXX(      404,         8,         0,         "addze"),
    new opcodeXX(      405,         8,         0,         "addze."),
    new opcodeXX(      420,         4,        17,         "mtsr"),
    new opcodeXX(      430,         4,         7,         "stbx"),
    new opcodeXX(      432,         4,         8,         "sllq"),
    new opcodeXX(      433,         4,         8,         "sllq."),
    new opcodeXX(      434,         4,         8,         "sleq"),
    new opcodeXX(      435,         4,         8,         "sleq."),
    new opcodeXX(      464,         8,         0,         "subfme"),
    new opcodeXX(      465,         8,         0,         "subfme."),
    new opcodeXX(      468,         8,         0,         "addme"),
    new opcodeXX(      469,         8,         0,         "addme."),
    new opcodeXX(      470,         8,         1,         "mullw"),
    new opcodeXX(      471,         8,         1,         "mullw."),
    new opcodeXX(      484,         4,         7,         "mtsrin"),
    new opcodeXX(      494,         4,         7,         "stbux"),
    new opcodeXX(      496,         4,        11,         "slliq"),
    new opcodeXX(      497,         4,        11,         "slliq"),
    new opcodeXX(      528,         8,         1,         "doz"),
    new opcodeXX(      529,         8,         1,         "doz."),
    new opcodeXX(      532,         8,         1,         "add"),
    new opcodeXX(      533,         8,         1,         "add."),
    new opcodeXX(      554,         4,        25,         "lscbx"),
    new opcodeXX(      555,         4,        25,         "lscbx."),
    new opcodeXX(      558,         4,         7,         "lhzx"),
    new opcodeXX(      568,         4,         8,         "eqv"),
    new opcodeXX(      569,         4,         8,         "eqv."),
    new opcodeXX(      612,         4,         6,         "tlbie"),
    new opcodeXX(      622,         4,         7,         "lhzux"),
    new opcodeXX(      632,         4,         8,         "xor"),
    new opcodeXX(      633,         4,         8,         "xor."),
    new opcodeXX(      662,         8,         1,         "div"),
    new opcodeXX(      663,         8,         1,         "div."),
    new opcodeXX(      678,         4,         3,         "mfspr"),
    new opcodeXX(      686,         4,         7,         "lhax"),
    new opcodeXX(      720,         8,         0,         "abs"),
    new opcodeXX(      721,         8,         0,         "abs."),
    new opcodeXX(      726,         8,         1,         "divs"),
    new opcodeXX(      727,         8,         1,         "divs."),
    new opcodeXX(      750,         4,         7,         "lhaux"),
    new opcodeXX(      814,         4,         7,         "sthx"),
    new opcodeXX(      824,         4,         8,         "orc"),
    new opcodeXX(      825,         4,         8,         "orc."),
    new opcodeXX(      878,         4,         7,         "sthux"),
    new opcodeXX(      888,         4,         8,         "or"),
    new opcodeXX(      889,         4,         8,         "or."),
    new opcodeXX(      934,         4,         3,         "mtspr"),
    new opcodeXX(      952,         4,         8,         "nand"),
    new opcodeXX(      953,         4,         8,         "nand."),
    new opcodeXX(      976,         8,         0,         "nabs"),
    new opcodeXX(      977,         8,         0,         "nabs."),
    new opcodeXX(     1004,         4,         6,         "cli"),
    new opcodeXX(     1024,         4,        23,         "mcrxr"),
    new opcodeXX(     1040,         8,         1,         "subfco"),
    new opcodeXX(     1041,         8,         1,         "subfco."),
    new opcodeXX(     1044,         8,         1,         "addco"),
    new opcodeXX(     1045,         8,         1,         "addco."),
    new opcodeXX(     1062,         4,         5,         "clcs"),
    new opcodeXX(     1066,         4,         7,         "lswx"),
    new opcodeXX(     1068,         4,         7,         "lwbrx"),
    new opcodeXX(     1070,         4,        12,         "lfsx"),
    new opcodeXX(     1072,         4,         8,         "srw"),
    new opcodeXX(     1073,         4,         8,         "srw."),
    new opcodeXX(     1074,         4,         8,         "rrib"),
    new opcodeXX(     1075,         4,         8,         "rrib."),
    new opcodeXX(     1082,         4,         8,         "maskir"),
    new opcodeXX(     1083,         4,         8,        "maskir."),
    new opcodeXX(     1134,         4,        12,         "lfsux"),
    new opcodeXX(     1190,         4,        18,         "mfsr"),
    new opcodeXX(     1194,         4,        10,         "lswi"),
    new opcodeXX(     1196,         4,         1,         "sync"),
    new opcodeXX(     1198,         4,        12,         "lfdx"),
    new opcodeXX(     1232,         8,         0,         "nego"),
    new opcodeXX(     1233,         8,         0,         "nego."),
    new opcodeXX(     1238,         8,         1,         "mulo"),
    new opcodeXX(     1239,         8,         1,         "mulo."),
    new opcodeXX(     1254,         4,         7,         "mfsri"),
    new opcodeXX(     1260,         4,         6,         "dclst"),
    new opcodeXX(     1262,         4,        12,         "lfdux"),
    new opcodeXX(     1296,         8,         1,         "subfeo"),
    new opcodeXX(     1297,         8,         1,         "subfeo."),
    new opcodeXX(     1300,         8,         1,         "addeo"),
    new opcodeXX(     1301,         8,         1,         "addeo."),
    new opcodeXX(     1322,         4,         7,         "stswx"),
    new opcodeXX(     1324,         4,         7,         "stwbrx"),
    new opcodeXX(     1326,         4,        12,         "stfsx"),
    new opcodeXX(     1328,         4,         8,         "srq"),
    new opcodeXX(     1329,         4,         8,         "srq."),
    new opcodeXX(     1330,         4,         8,         "sre"),
    new opcodeXX(     1331,         4,         8,         "sre."),
    new opcodeXX(     1390,         4,        12,         "stfsux"),
    new opcodeXX(     1392,         4,        11,         "sriq"),
    new opcodeXX(     1393,         4,        11,         "sriq."),
    new opcodeXX(     1424,         8,         0,         "subfzeo"),
    new opcodeXX(     1425,         8,         0,         "subfzeo."),
    new opcodeXX(     1428,         8,         0,         "addzeo."),
    new opcodeXX(     1429,         8,         0,         "addzeo."),
    new opcodeXX(     1450,         4,        10,         "stswi"), 
    new opcodeXX(     1454,         4,        12,         "stfdx"),
    new opcodeXX(     1456,         4,         8,         "srlq"),
    new opcodeXX(     1457,         4,         8,         "srlq."),
    new opcodeXX(     1458,         4,         8,         "sreq"),
    new opcodeXX(     1459,         4,         8,         "sreq."),
    new opcodeXX(     1488,         8,         0,         "subfmeo."),
    new opcodeXX(     1489,         8,         0,         "subfmeo."),
    new opcodeXX(     1492,         8,         0,         "addmeo"),
    new opcodeXX(     1493,         8,         0,         "addmeo."),
    new opcodeXX(     1494,         8,         1,         "mullwo."),
    new opcodeXX(     1495,         8,         1,         "mullwo."),
    new opcodeXX(     1518,         4,        12,         "stfdux"),
    new opcodeXX(     1520,         4,        11,         "srliq"),
    new opcodeXX(     1521,         4,        11,         "srliq."),
    new opcodeXX(     1552,         8,         1,         "dozo."),
    new opcodeXX(     1553,         8,         1,         "dozo."),
    new opcodeXX(     1556,         8,         1,         "addo"),
    new opcodeXX(     1557,         8,         1,         "addo."),
    new opcodeXX(     1580,         4,         7,         "lhbrx"),
    new opcodeXX(     1584,         4,         8,         "sraw"),
    new opcodeXX(     1585,         4,         8,         "sraw."),
    new opcodeXX(     1636,         4,        25,         "rac"),
    new opcodeXX(     1637,         4,        25,         "rac."),
    new opcodeXX(     1648,         4,        11,         "srawi"),
    new opcodeXX(     1649,         4,        11,         "srawi."),
    new opcodeXX(     1686,         8,         1,         "divo"),
    new opcodeXX(     1687,         8,         1,         "divo."),
    new opcodeXX(     1744,         8,         0,         "abso"),
    new opcodeXX(     1745,         8,         0,         "abso."),
    new opcodeXX(     1750,         8,         1,         "divso"),
    new opcodeXX(     1751,         8,         1,         "divso."),
    new opcodeXX(     1836,         4,         7,         "sthbrx"),
    new opcodeXX(     1840,         4,         8,         "sraq"),
    new opcodeXX(     1841,         4,         8,         "sraq."),
    new opcodeXX(     1842,         4,         8,         "srea"),
    new opcodeXX(     1843,         4,         8,         "srea."),
    new opcodeXX(     1844,         4,         9,         "extsh"),
    new opcodeXX(     1845,         4,         9,         "extsh."),
    new opcodeXX(     1904,         4,        11,         "sraiq"),
    new opcodeXX(     1905,         4,        11,         "sraiq."),
    new opcodeXX(     2000,         8,         0,         "nabso"),
    new opcodeXX(     2001,         8,         0,         "nabso."),
    new opcodeXX(     2028,         4,         6,         "dcbz"),

    // these are the addition for the PowerPC
    new opcodeXX(	172,	4,	6,	 "dcbf"),    
    new opcodeXX(	940,	4,	6,	 "dcbi"),    
    new opcodeXX(	108,	4,	6,	 "dcbst"),   
    new opcodeXX(	556,	4,	6,	 "dcbt"),    
    new opcodeXX(	492,	4,	6,	 "dcbtst"),  
    new opcodeXX(	2028,	4,	6,	 "dcbz"),    
    new opcodeXX(	982,	8,	1,	 "divw"),   
    new opcodeXX(	983,	8,	1,	 "divw."),   
    new opcodeXX(	2006,	8,	1,	 "divwo"),   
    new opcodeXX(	2007,	8,	1,	 "divwo."),  
    new opcodeXX(	918,	8,	1,	 "divwu"),   
    new opcodeXX(	919,	8,	1,	 "divwu."),  
    new opcodeXX(	1942,	8,	1,	 "divwuo"),  
    new opcodeXX(	1943,	8,	1,	 "divwuo."), 
    new opcodeXX(	1708,	4,	1,	 "eieio"),  
    new opcodeXX(	1908,	4,	0,	 "extsb"),  
    new opcodeXX(	1909,	4,	0,	 "extsb."),  
    new opcodeXX(	1964,	4,	6,	 "icbi"),    
    new opcodeXX(	40,	4,	7,	 "lwarx"),   
    new opcodeXX(	1318,	4,	4,	 "mfsrin"),  
    new opcodeXX(	150,	8,	1,	 "mulhw"),   
    new opcodeXX(	151,	8,	1,	 "mulhw."),  
    new opcodeXX(	22,	8,	1,	 "mulhwu"),  
    new opcodeXX(	23,	8,	1,	 "mulhwu."), 
    new opcodeXX(	301,	4,	7,	 "stwcx."), 
    new opcodeXX(	80,	8,	1,	 "subf"),  
    new opcodeXX(	81,	8,	1,	 "subf."),  
    new opcodeXX(	1104,	8,	1,	 "subfo"),   
    new opcodeXX(	1105,	8,	1,	 "subfo.")
  };
  
  
/*  Opcode 63 table: The key is computed by taking 
 *  bits 21 through 31 of the instruction. "Form" is
 *  the instruction format and "format" is how the
 *  instruction should be printed.  */

  static
  opcodeXX opcode63[] = {

    /*                  key        form     format       mnemonic       */
    /*                  ---        ----     ------       --------       */
    new opcodeXX(        0,         4,        19,         "fcmpu"),
    new opcodeXX(       24,         4,        21,         "frsp"),
    new opcodeXX(       25,         4,        21,         "frsp."),
    new opcodeXX(       30,         4,        21,         "fctiwz"),  // PowerPC only
    new opcodeXX(       31,         4,        21,         "fctiwz."), // PowerPC only
    new opcodeXX(       64,         4,        19,         "fcmpo"),
    new opcodeXX(       76,         4,        16,         "mtfsb1"),
    new opcodeXX(       77,         4,        16,         "mtfsb1."),
    new opcodeXX(       80,         4,        21,         "fneg"),
    new opcodeXX(       81,         4,        21,         "fneg."),
    new opcodeXX(      128,         4,        14,         "mcrfs"),
    new opcodeXX(      140,         4,        16,         "mtfsb0"),
    new opcodeXX(      141,         4,        16,         "mtfsb0."),
    new opcodeXX(      144,         4,        21,         "fmr"),
    new opcodeXX(      145,         4,        21,         "fmr."),
    new opcodeXX(      268,         4,        15,         "mtfsfi"),
    new opcodeXX(      269,         4,        15,         "mtfsfi."),
    new opcodeXX(      272,         4,        21,         "fnabs"),
    new opcodeXX(      273,         4,        21,         "fnabs."),
    new opcodeXX(      528,         4,        21,         "fabs"),
    new opcodeXX(      529,         4,        21,         "fabs."),
    new opcodeXX(     1166,         4,        13,         "mffs"),
    new opcodeXX(     1167,         4,        13,         "mffs."),
    new opcodeXX(     1422,         7,         9,         "mtfsf"),
    new opcodeXX(     1423,         7,         9,         "mtfsf.")
  };

  static
  opcodeXX opcode59[] = {
    
    /*  opcode59 table: These are the addition for the PowerPC set
     *  Key is  bits 26 through 31 of the instruction. 
     *  "Form" is the instruction format and "format" is how the
     *  instruction should be printed (the enumeration is specific 
     *  to each decode method)
     */
    
    /*                  key        form     format       mnemonic       */
    new opcodeXX(       42,         9,          0,      "fadds"),
    new opcodeXX(       43,         9,          0,      "fadds."),
    new opcodeXX(       36,	    9,	  	0,	"fdivs"),   
    new opcodeXX(       37,	    9,	  	0,	"fdivs."),  
    new opcodeXX(       58,	    9,	  	2,	"fmadds"),  
    new opcodeXX(       59,	    9,	  	2,	"fmadds."), 
    new opcodeXX(       56,	    9,	  	2,	"fmsubs"),  
    new opcodeXX(       57,	    9,	  	2,	"fmsubs."), 
    new opcodeXX(       50,	    9,	  	1,	"fmuls"),  
    new opcodeXX(       51,	    9,	  	1,	"fmuls."),  
    new opcodeXX(       62,	    9,	  	2,	"fnmadds"), 
    new opcodeXX(       63,	    9,	  	2,	"fnmadds."),
    new opcodeXX(       60,	    9,	  	2,	"fnmsubs"),
    new opcodeXX(       61,	    9,	  	2,	"fnmsubs."),
    new opcodeXX(       40,	    9,	  	0,	"fsubs"),  
    new opcodeXX(       41,	    9,	  	0,	"fsubs.")  
  };

  static
  opcodeXX Aform[] = {
    
    /*  Aform table: The key is computed by taking 
     *  bits 26 through 31 of the instruction. "Form" is
     *  the instruction format and "format" is how the
     *  instruction should be printed.  */
    
    /*                  key        form     format       mnemonic       */
    new opcodeXX(       36,         9,         0,         "fdiv"),
    new opcodeXX(       37,         9,         0,         "fdiv."),
    new opcodeXX(       40,         9,         0,         "fsub"),
    new opcodeXX(       41,         9,         0,         "fsub."),
    new opcodeXX(       42,         9,         0,         "fadd"),
    new opcodeXX(       43,         9,         0,         "fadd."),
    new opcodeXX(       50,         9,         1,         "fm"),
    new opcodeXX(       51,         9,         1,         "fm."),
    new opcodeXX(       56,         9,         2,         "fmsub"),
    new opcodeXX(       57,         9,         2,         "fmsub."),
    new opcodeXX(       58,         9,         2,         "fmadd"),
    new opcodeXX(       59,         9,         2,         "fmadd."),
    new opcodeXX(       60,         9,         2,         "fnmsub"),
    new opcodeXX(       61,         9,         2,         "fnmsub."),
    new opcodeXX(       62,         9,         2,         "fnmadd"),
    new opcodeXX(       63,         9,         2,         "fnmadd.")
  };
  
  /* 
   *  SPR_name - common special purpose register names options   
   */
  static String SPR_name(int SPR)
  {
    switch (SPR) {
    case SPR_MQ:      return("mq");
    case SPR_XER:     return("xer");
    case SPR_LR:      return("lr");
    case SPR_CTR:     return("ctr");
    case SPR_TID:     return("tid");
    case SPR_DSISR:   return("dsisr");
    case SPR_DAR:     return("dar");
    case SPR_RTCU:    return("rtcu");
    case SPR_RTCL:    return("rtcu");
    case SPR_DEC:     return("dec");
    case SPR_SDR0:    return("sdr0");
    case SPR_SDR1:    return("sdr1");
    case SPR_SRR0:    return("srr0");
    case SPR_SRR1:    return("srr1");
    default: return null;
    }
  }
  
  /* 
   *  TO_ext - common trap options   
   */
  static String TO_ext(int TO)
  {
    switch (TO) {
    case TO_LT:             return("lt");
    case (TO_LT | TO_EQ):   return("le");
    case (TO_LT | TO_GT):   return("ne");
    case TO_GT:             return("gt");
    case (TO_GT | TO_EQ):   return("ge");
    case TO_LLT:            return("llt");
    case (TO_LLT | TO_EQ):  return("lle");
    case (TO_LLT | TO_LGT): return("lne");
    case TO_LGT:            return("lgt");
    case (TO_LGT | TO_EQ):  return("lge");
    case TO_EQ:             return("eq");
    default:return null;
    }
  }
  
  /*
   *  Translate an instruction from its
   *  numeric form into something more 
   *  readable.
   */
  
  static String disasm(int inst, int addr)
  {
    int opcode;
    int form;
    
    opcode = bits(inst,0,5);                /* Determine opcode */
    form = opcode_to_form[opcode];           /* Determine instruction format */
    
    switch(form)     /* decode known instruction format */
      {
      case D_FORM:
	return decode_Dform(inst, opcode);
      case B_FORM:
	return decode_Bform(addr, inst, opcode);
      case I_FORM:
	return decode_Iform(addr,inst, opcode);
      case SC_FORM:
	return decode_SCform(inst, opcode);
      case XL_FORM:
	return decode_XLform(inst, opcode);
      case M_FORM:
	return decode_Mform(inst, opcode);
      case A_FORM:
	return decode_opcode59(inst);
      case EXTENDED:      /* More work to do... */
	switch(opcode)   /* Switch off of opcode and process from there */
	  {
	  case 31:
	    return decode_opcode31(inst);
	  case 63:
	    return decode_opcode63(inst);
	  default:
	    return "    Invalid opcode";
	  }
      case INVALID_OP:      /* More work to do... */
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode the D instruction format */
  
  static String decode_Dform(int inst, int opcode)
  {
    int rt, RA, TO, BF, FRT,ufield;
    int sfield;
    opcode_tab opcode_info;
    String datafield, mnemonic, asm_mnemonic, common_opt;
    
    rt = TO = FRT = bits(inst, 6, 10);
    RA = bits(inst, 11, 15);
    BF = bits(inst,6,8);
    ufield = inst & 0xffff;
    sfield = (inst << 16) >> 16;
    if (sfield < 0) {
      datafield = ""+sfield;
    } else {
      datafield = intAsHexString(sfield);
    }
    opcode_info = Dform[opcode];
    mnemonic = opcode_info.mnemonic;
    
    switch(opcode_info.format)
      {
      case 0:
	if (opcode != 15) {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+rname(rt)+","+rname(RA)+","+datafield;
	} else {
	  if (RA != 0) {
	    return "        ".substring(mnemonic.length()) + mnemonic +
	      "   "+rname(rt)+","+rname(RA)+","+intAsHexString(ufield);
	  } else {
	    return "     liu   "+
	      rname(rt)+","+intAsHexString(ufield);
	  }
	}
      case 1:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(rt)+","+intAsHexString(ufield);
      case 2:
	if ((opcode == 14) && (RA == 0)) {
	  return "     lil" +
	    "   "+rname(rt)+","+datafield;
	} else {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+rname(rt)+","+datafield+"("+rname(RA)+")";
	}
      case 3: /* Trap immediate */
	common_opt = TO_ext(TO);
	asm_mnemonic = "t"+common_opt+"i";
	if (common_opt!=null) {
	  return "        ".substring(asm_mnemonic.length()) + asm_mnemonic +
	    "   "+rname(RA)+","+datafield;
	} else {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+TO+","+rname(RA)+","+datafield;
	}
      case 4:
	if (opcode == 11) {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   cr"+BF+","+rname(RA)+","+datafield;
	} else {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   cr"+BF+","+rname(RA)+","+ufield;
	}
      case 5:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT+","+rname(RA)+","+datafield;
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode the B instruction format */
  
  static String decode_Bform(int addr, int inst, int opcode)
  {
    int AA, LK, BO, BI; 
    String mnemonic;
    int cr_field;
    int target;
    
    AA = bits(inst,30,30);
    LK = bits(inst,31,31);
    BO = bits(inst,6,10);
    BI = bits(inst,11,15);
    target = (((inst) & 0xfffc) << 16) >> 16;
    if (AA == 0)
      target += addr;
    mnemonic = build_branch_op(BO,1 << (3 - (BI&3)),LK,AA,0);
    destination = target;
    cr_field = (BI>>2);
    /* Build disassembly without target, added on later... */
    if (cr_field != 0) {/* Not CR 0 ? */
      return "        ".substring(mnemonic.length()) + mnemonic +
	"   "+cr_field + " " + Integer.toHexString(destination);
    } else {
      return "        ".substring(mnemonic.length()) + mnemonic +
	"   " + Integer.toHexString(destination);
    }
  }
  
  /* Decode the I instruction format */
  
  static String decode_Iform(int addr, int inst, int opcode)
  {
    int target;
    int AA, LK; 
    String mnemonic;
    
    AA = bits(inst,30,30);
    LK = bits(inst,31,31);
    target = (((inst) & ~3) << 6) >> 6;
    if (AA!=0) {
      mnemonic = (LK!=0) ? "bla" : "ba";
    } else {
      target += addr;
      mnemonic = (LK!=0) ? "bl" : "b";
    }
    destination = target;
    return "        ".substring(mnemonic.length()) + mnemonic + "   " + Integer.toHexString(destination);
  }
  
  /* Decode the SC instruction format */
  
  static String decode_SCform(int inst, int opcode)
  {
    int SA, LK, LEV, FL1, FL2, SV; 
    
    SA = bits(inst,30,30);
    LK = bits(inst,31,31);
    if (SA != 0)
      {
	SV = bits(inst,16,29);
	String mnemonic = (LK!=0) ? "svcla" : "svca";
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+intAsHexString(SV);
      }
    else
      {
	LEV = bits(inst,20,26);
	FL1 = bits(inst,16,19);
	FL2 = bits(inst,27,29);
	String mnemonic = (LK!=0) ? "svcl" : "svc";
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+intAsHexString(LEV)+","+intAsHexString(FL1)+","+
	  intAsHexString(FL2);
      }
  }
  
  /* Decode the XL instruction format */
  
  static String decode_XLform(int inst, int opcode)
  {
    String mnemonic;
    int ext_opcode;
    int LK, BO,  BI, BB; 
    int BF, BFA, BT, BA; 
    opcode_tab opcode_info;
    int cr_field;
    String branch_name;
    
    ext_opcode = bits(inst,21,30);
    opcode_info = XLform[ext_opcode >> 4]; /* shift to get XL table index */
    mnemonic = opcode_info.mnemonic;
    switch(opcode_info.format)
      {
      case 0:
	return "        ".substring(mnemonic.length()) + mnemonic;
      case 1:
	BO = bits(inst,6,10);
	BI = bits(inst,11,15);
	LK = bits(inst,31,31);
	cr_field = (byte) (BI>>2);
	branch_name = build_branch_op(BO,1 << (3 - (BI&3)),LK,0,ext_opcode);
	if (cr_field!=0) {/* Not CR 0 ? */
	  return "        ".substring(branch_name.length()) + branch_name +
	    "   "+cr_field;
	} else {
	  return "        ".substring(branch_name.length()) + branch_name;
	}
      case 2:
	BF  = bits(inst,6,10);
	BFA = bits(inst,11,13);
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+","+intAsHexString(BFA);
      case 3:
	BT = bits(inst,6,10);
	BA = bits(inst,11,15);
	BB = bits(inst,16,20);
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+intAsHexString(BT)+","+intAsHexString(BA)+","+
	  intAsHexString(BB);
      default:
	return "    Invalid opcode";
      }
  }
  
  static String Mforms[]  = { "rlimi", "rlimi.", "rlinm", "rlinm.",
      "rlmi", "rlmi.", "rlnm", "rlnm."};
  
  /* Decode the M instruction format */
  
  static String decode_Mform(int inst, int opcode)
  {
    int RS, RA, RB, SH;
    int MB, ME, Rc;
    int SH_RB;
    String asm_mnemonic;
    
    RS = bits(inst,6,10);
    RA = bits(inst,11,15);
    SH_RB = RB = SH = bits(inst,16,20);
    MB = bits(inst,21,25);
    ME = bits(inst,26,30);
    Rc = inst & 1;
    
    asm_mnemonic = Mforms[(opcode - 20) * 2 + Rc];
    if ((opcode < 20) || ((opcode >>> 23)!=0)) {
      return "    Invalid opcode";
    } else {
      if (opcode == 21) { /* sri and sli are special forms of rlmni */
	if ((ME == 32) && (MB == (31-SH_RB))) {
	  String mnemonic = (Rc!=0) ? "sri." : "sri";
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+rname(RA)+","+rname(RS)+","+intAsHexString(MB);
	} else if ((MB == 0) && (SH_RB == (31-ME))) {
	  String mnemonic = (Rc!=0) ? "sli." : "sli";
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+rname(RA)+","+rname(RS)+","+intAsHexString(SH_RB);
	} else {
	  return "        ".substring(asm_mnemonic.length()) + asm_mnemonic +
	    "   "+rname(RA)+","+rname(RS)+","+intAsHexString(SH_RB)+","+
	    intAsHexString(MB)+","+intAsHexString(ME) ;
	}
      } else {
	return "        ".substring(asm_mnemonic.length()) + asm_mnemonic +
	  "   "+rname(RA)+","+rname(RS)+","+intAsHexString(SH_RB)+","+
	  intAsHexString(MB)+","+intAsHexString(ME) ;
      }
    }
  }
  
  static opcodeXX searchXX(int key, opcodeXX[] where) {
    
    for (int i=0; i<where.length; ++i) {
      opcodeXX opxx = where[i];
      if (opxx.key == key) return opxx;
    }
    
    return null;
    
  }
  
  /* Decode opcode 31 and then the relevent format */
  
  static String decode_opcode31(int inst)
  {
    opcodeXX search_results;
    int format;
    String mnemonic;
    
    
    int testkey = bits(inst,21,31);
    search_results = searchXX(testkey, opcode31);
    
    if (search_results == null) {
      return "    Invalid opcode";
    }
    
    mnemonic = search_results.mnemonic;
    format = search_results.format;
    switch(search_results.form) 
      {
      case 4:
	return decode_Xform(inst,mnemonic,format,testkey);
      case 6:
	return decode_XFXform(inst);
      case 8:
	return decode_XOform(inst,mnemonic,format);
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode the X instruction format */
  
  static String decode_Xform(int inst, String mnemonic, int format, int ext_op)
  {
    int rt,RA,RB,NB,SH,FRS,SPR,FRT,FXM;
    int BF,BFA,I,BT,SR,FRA,FRB,TO;
    String asm_mnemonic, common_opt, mn;
    
    FRS = FRT = TO = BT = rt = bits(inst,6,10);
    FRB = NB  = SH = RB = I = bits(inst,16,20);
    FRA = SPR = SR = RA = bits(inst,11,15);
    BFA = bits(inst,11,13);
    I = bits(inst,16,19);
    BF = bits(inst,6,8);
    
    switch(format) 
      {
      case 0:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(rt);
      case 1:
	return "        ".substring(mnemonic.length()) + mnemonic;
      case 2:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt);
      case 3:
	common_opt = SPR_name(SPR);
	if (common_opt != null) {
	  asm_mnemonic = mnemonic.substring(0, 2) + common_opt;
	  return "        ".substring(asm_mnemonic.length()) + asm_mnemonic +
	    "   "+rname(rt);
	} else {/* reserved register? */
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+intAsHexString(SPR)+","+rname(rt);
	}
      case 4:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RB);
      case 5:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA);
      case 6:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(RB);
      case 7:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA)+","+rname(RB);
      case 8:
	if ((ext_op == 888) || (ext_op == 889)) {
	  if (rt == RB) {
	    String mne = (ext_op == 889) ? "mr." : "mr";
	    return "        ".substring(mne.length()) + mne +
	      "   "+rname(RA)+","+rname(rt);
	  }
	}
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(rt)+","+rname(RB);
      case 9:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(rt);
      case 10:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA)+","+intAsHexString(NB);
      case 11:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(RA)+","+rname(rt)+","+intAsHexString(SH);
      case 12:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRS+","+rname(RA)+","+rname(RB);
      case 13:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT;
      case 14:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+","+intAsHexString(BFA);
      case 15:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+","+intAsHexString(I);
      case 16:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+intAsHexString(BT);
      case 17:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+intAsHexString(SR)+","+rname(rt);
      case 18:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+intAsHexString(SR);
      case 19:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+",fr"+FRA+",fr"+FRB;
      case 20:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+",fr"+FRB;
      case 21:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT+",fr"+FRB;
      case 22:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF+","+rname(RA)+","+rname(RB);
      case 23:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   cr"+BF;
      case 24:
	common_opt = TO_ext(TO);
	if (common_opt != null) {
	  asm_mnemonic = "t" + common_opt;
	  return "        ".substring(asm_mnemonic.length()) + asm_mnemonic +
	    "   "+rname(RA)+","+rname(RB);
	} else {
	  return "        ".substring(mnemonic.length()) + mnemonic +
	    "   "+TO+","+rname(RA)+","+rname(RB);
	}
      case 25:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA)+","+rname(RB);
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode the XFX instruction format */
  
  static String decode_XFXform(int inst)
  {
    int rt,FXM;
    
    rt = bits(inst,6,10);
    FXM = bits(inst,12,19);
    if (FXM == 0xff) {
      return "    mtcr   "+rname(rt);
    } else {
      return "   mtcrf   "+intAsHexString(FXM)+","+rname(rt);
    }
  }
  
  /* Decode the XO instruction format */
  
  static String decode_XOform(int inst, String mnemonic, int format)
  {
    int rt,RA,RB;
    
    rt = bits(inst,6,10);
    RA = bits(inst,11,15);
    switch(format)
      {
      case 0:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA);
      case 1:
	RB = bits(inst,16,20);
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   "+rname(rt)+","+rname(RA)+","+rname(RB);
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode opcode 59 and then the relevent format */
  
  static String decode_opcode59(int inst)
  {
    opcodeXX opcode, search_results;
    String mnemonic;
    
    int testkey = bits(inst,26,31);
    
    search_results = searchXX(testkey, opcode59);

    if (search_results == null) {
      return "    Invalid opcode";
    }
    
    mnemonic = search_results.mnemonic;
    int format = search_results.format;

    // All opcode 59 are in A form
    switch(search_results.form) 
      {
      case 9:
	return decode_Aform(inst,mnemonic,format);
      default:
	return "    Invalid opcode";
      }
    }

  /* Decode opcode 63 and then the relevent format */
  
  static String decode_opcode63(int inst)
  {
    opcodeXX opcode, search_results;
    String mnemonic;
    
    int testkey = bits(inst,21,31);
    
    search_results = searchXX(testkey, opcode63);
    
    if (search_results == null)
      {
	testkey = bits(inst,26,31);
	search_results = searchXX(testkey, Aform);
      }
    
    if (search_results == null) {
      return "    Invalid opcode";
    }
    
    mnemonic = search_results.mnemonic;
    int format = search_results.format;
    switch(search_results.form) 
      {
      case 4:
	return decode_Xform(inst,mnemonic,format,testkey);
      case 7:
	return decode_XFLform(inst);
      case 9:
	return decode_Aform(inst,mnemonic,format);
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Decode the XFL instruction format */
  
  static String decode_XFLform(int inst)
  {
    int FLM, FRB, Rc;
    
    Rc  = bits(inst,31,31);
    FLM = bits(inst,7,14);
    FRB = bits(inst,16,20);
    if (FLM == 0xff) {
      String mnemonic = (Rc!=0) ? "mtfs." : "mtfs";
      return "        ".substring(mnemonic.length()) + mnemonic +
	"   fr"+FRB;
    } else {
      String mnemonic = (Rc!=0) ? "mtfsf." : "mtfsf";
      return "        ".substring(mnemonic.length()) + mnemonic +
	"   "+intAsHexString(FLM)+",fr"+FRB;
    }
  }
  
  /* Decode the A instruction format */
  
  static String decode_Aform(int inst,String mnemonic,int format)
  {
    int FRT,FRA,FRB,FRC;
    
    FRT = bits(inst, 6,10);
    FRA = bits(inst,11,15);
    FRB = bits(inst,16,20);
    FRC = bits(inst,21,25);
    switch(format) 
      {

      case 0:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT+",fr"+FRA+",fr"+FRB;
      case 1:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT+",fr"+FRA+",fr"+FRC;
      case 2:
	return "        ".substring(mnemonic.length()) + mnemonic +
	  "   fr"+FRT+",fr"+FRA+",fr"+FRC+",fr"+FRB;
      default:
	return "    Invalid opcode";
      }
  }
  
  /* Construct an assembler-like branch instruction */
  static
  String build_branch_op(int br_opt, int cr_bit, int update_link, int absolute, int ext_op)
  {
    String c;
    int uncond = 0;/* Unconditional br to count reg */
    int pos_cond = 0;/* Branch if condition is positive */
    int ctr_zero = 0;/* Branch if count register = 0 */
    int dec_ctr = 0;/* Decrement count register */
    
    c = "b";
    if ((br_opt & 4) != 0) {/* Don't decrement count register */
      if ((br_opt & 16) != 0) {
	uncond = 1;
      } else if ((br_opt & 8) != 0) {
	pos_cond = 1;
      } 
    } else {/* Decrement count register */
      dec_ctr = 1;
      if ((br_opt & 2)!=0) {
	ctr_zero = 1;
      } else if ((br_opt & 8) != 0) {
	pos_cond = 1;
      } 
    }
    if (dec_ctr!=0) {
      c += 'd';
      c += (ctr_zero!=0)?'z' : 'n';
    }
    if (uncond==0) {
      if (pos_cond!=0) {
	switch(cr_bit) {
	case CR_LT:  c += "lt"; break;
	case CR_GT:  c += "gt"; break;
	case CR_EQ:  c += "eq"; break;
	case CR_SO:  c += "so"; break;
	}
      } else {
	switch(cr_bit) {
	case CR_LT:  c += "ge"; break;
	case CR_GT:  c += "le"; break;
	case CR_EQ:  c += "ne"; break;
	case CR_SO:  c += "ns"; break;
	}
      }
    }
    if (ext_op == 16) {
      c += 'r';
    } else if (ext_op == 528)  {
      c += 'c';
      if (uncond!=0) {/* Can't confuse with br conditional */
	c += "tr";
      }
    }
    if (update_link!=0)
      c += "l";
    if (absolute!=0)
      c += "a";
    
    return c;
  }
  
  /*
   * Simply return whether an instruction is a branch_with_link
   */
  /* static int branch_link(inst) */
  static boolean isBranchAndLink(int inst)
  {
    int opcode, ext_op;
    int link;
    
    opcode = bits(inst,0,5);
    link = bits(inst,31,31);
    switch (opcode) {
    case 16: /* unconditional branch */
    case 18: /* conditional branch */
      break;
    case 19: /* possibly branch register */
      ext_op = bits(inst,21,30);
      if ((ext_op != 16) && (ext_op != 528)) {
	link = 0;
      } 
      break;
    default: /* definitely not a branch */
      link = 0;
    }
    
    if (link==0)
      return false;
    else
      return true;
  }

  /*
   * Return whether an instruction is a branch for yieldpoint
   *  used by OPT compiler
   */
  /* static int branch_for_yieldpoint(inst) */
  static boolean isBranchForYieldpoint(int inst)
  {
    int opcode, ext_op;
    int link;
    
    opcode = bits(inst,0,5);
    link = bits(inst,31,31);
    switch (opcode) {
    case 16: /* unconditional branch */
      if (link==1)
	return true;
      else
	return false;
    }
    
    return false;
  }
  
  
  static String intAsHexString(int x) {
    return "0x"+Integer.toHexString(x);
  }
  
}



/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Structure for opcode 31 and 63, PowerPC instruction set: 
 * these include the X, XO, XFL, XFX and A form
 * @author John Waley
 * @see PPC_Disassembler 
 */

class opcodeXX {

  int key;
  int form;
  int format;
  String mnemonic;

  opcodeXX(int key, int form, int format, String mnemonic) {
    this.key = key;
    this.form = form;
    this.format = format;
    this.mnemonic = mnemonic;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Structure for the D and XL forms, PowerPC instruction set 
 *
 * @author John Waley
 * @see PPC_Disassembler 
 */
class opcode_tab {  

  int format;
  String mnemonic;

  opcode_tab(int format, String mnemonic) {
    this.format = format;
    this.mnemonic = mnemonic;
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Constants for JNI support
 *
 * @author Ton Ngo
 * @author Steve Smith
 */
interface VM_JNIAIXConstants extends VM_JNIConstants {
  // index of IP in the AIX linkage triplet
  static final int IP = 0;                    

  // index of TOC in the AIX linage triplet
  static final int TOC = 1;                   
}

/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * @author Ton Ngo 
 * @author Steve Smith
 */
public class VM_JNICompiler implements VM_BaselineConstants,
				       VM_AssemblerConstants {

  /**
   * This method creates the stub to link native method.  It will be called
   * from the lazy linker the first time a native method is invoked.  The stub
   * generated will be patched by the lazy linker to link to the native method
   * for all future calls. <p>
   * <pre>
   * The stub performs the following tasks in the prologue:
   *   -Allocate the glue frame
   *   -Save the TI and PR registers in the JNI Environment for reentering Java later
   *   -Shuffle the parameters in the registers to conform to the AIX convention
   *   -Save the nonvolatile registers in a known space in the frame to be used 
   *    for the GC stack map
   *   -Push a new JREF frame on the JNIRefs stack
   *   -Set the VM_Processor affinity so that this thread won't migrate while in native
   *   -Supply the first JNI argument:  the JNI environment pointer
   *   -Supply the second JNI argument:  class object if static, "this" if virtual
   *   -Hardcode the TOC and IP to the corresponding native code
   *
   * The stub performs the following tasks in the epilogue:
   *   -TI and PR registers are AIX nonvolatile, so they should be restored already
   *   -Restore the nonvolatile registers if GC has occurred
   *   -Pop the JREF frame off the JNIRefs stack
   *   -Check for pending exception and deliver to Java caller if present
   *   -Process the return value from native:  push onto caller's Java stack  
   *  
   * The stack frame created by this stub conforms to the AIX convention:
   *   -6-word frame header
   *   -parameter save/spill area
   *   -one word flag to indicate whether GC has occurred during the native execution
   *   -16-word save area for nonvolatile registers
   *  
   *   | fp       | <- native frame
   *   | cr       |
   *   | lr       |
   *   | resv     |
   *   | resv     |
   *   + toc      +
   *   |          |
   *   |          |
   *   |----------|  
   *   | fp       | <- Java to C glue frame
   *   | cr/mid   |
   *   | lr       |
   *   | resv     |
   *   | resv     |
   *   + toc      +
   *   |   0      | spill area (at least 8 words reserved)
   *   |   1      | (also used for saving volatile regs during calls in prolog)
   *   |   2      |
   *   |   3      |
   *   |   4      |
   *   |   5      |
   *   |   6      |
   *   |   7      |
   *   |  ...     | 
   *   |          |
   *   |GC flag   | offset = JNI_SAVE_AREA_OFFSET           <- JNI_GC_FLAG_OFFSET
   *   |affinity  | saved VM_Thread.processorAffinity       <- JNI_AFFINITY_OFFSET
   *   |vol fpr1  | saved AIX volatile fpr during becomeNativeThread
   *   | ...      | 
   *   |vol fpr6  | saved AIX volatile fpr during becomeNativeThread
   *   |vol r4    | saved AIX volatile regs during Yield (to be removed when code moved to Java)   
   *   | ...      | 
   *   |vol r10   | saved AIX volatile regs during Yield    <- JNI_AIX_VOLATILE_REGISTER_OFFSET
   *   |Proc reg  | processor register R16                  <- JNI_PR_OFFSET
   *   |nonvol 17 | save 15 nonvolatile registers for stack mapper
   *   | ...      |
   *   |nonvol 31 |                                         <- JNI_RVM_NONVOLATILE_OFFSET
   *   |savedSP   | SP is no longer saved & restored (11/21/00)  <- JNI_SP_OFFSET XXX
   *   |savedJTOC | save RVM JTOC for return                 <- JNI_JTOC_OFFSET
   *   |----------|   
   *   |  fp   	  | <- Java caller frame
   *   | mid   	  |
   *   | xxx   	  |
   *   |       	  |
   *   |       	  |
   *   |       	  |
   *   |----------|
   *   |       	  |
   * </pre>
   */
  static VM_MachineCode generateGlueCodeForNative (VM_CompiledMethod cm) {
    int compiledMethodId = cm.getId();
    VM_Method method     = cm.getMethod();
    VM_Assembler asm	= new VM_Assembler(0);
    int frameSize	= VM_Compiler.getFrameSize(method);
    VM_Class klass	= method.getDeclaringClass();

    /* initialization */
    if (VM.VerifyAssertions) VM.assert(T3 <= LAST_VOLATILE_GPR);           // need 4 gp temps
    if (VM.VerifyAssertions) VM.assert(F3 <= LAST_VOLATILE_FPR);           // need 4 fp temps
    if (VM.VerifyAssertions) VM.assert(S0 < SP && SP <= LAST_SCRATCH_GPR); // need 2 scratch

    VM_Address bootRecordAddress = VM_Magic.objectAsAddress(VM_BootRecord.the_boot_record);
    int lockoutLockOffset = VM_Entrypoints.lockoutProcessorField.getOffset();
    VM_Address lockoutLockAddress = bootRecordAddress.add(lockoutLockOffset);
    int sysTOCOffset      = VM_Entrypoints.sysTOCField.getOffset();
    int sysYieldIPOffset  = VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset();

    VM_Address gCFlagAddress = VM_Magic.objectAsAddress(VM_BootRecord.the_boot_record).add(VM_Entrypoints.globalGCInProgressFlagField.getOffset());

    int nativeIP  = method.getNativeIP();
    int nativeTOC = method.getNativeTOC();

    // NOTE:  this must be done before the condition VM_Thread.hasNativeStackFrame() become true
    // so that the first Java to C transition will be allowed to resize the stack
    // (currently, this is true when the JNIRefsTop index has been incremented from 0)
    asm.emitNativeStackOverflowCheck(frameSize + 14);   // add at least 14 for C frame (header + spill)

    int numValues = method.getParameterWords();     // number of arguments for this method
    int parameterAreaSize = (numValues) * 4;

    asm.emitMFLR(0);                                // save return address in caller frame
    asm.emitST(0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP);	// this was set up the first time by DynamicBridgeTo()
    
    asm.emitSTU (FP,  -frameSize, FP);                      // get transition frame on stack
    asm.emitST  (JTOC, frameSize - JNI_JTOC_OFFSET, FP);    // save RVM JTOC in frame

    asm.emitST  (PROCESSOR_REGISTER, frameSize - JNI_PR_OFFSET, FP);  // save PR in frame

    // store method ID for JNI frame, occupies AIX saved CR slot, which we don't use
    // hardcoded in the glue routine
    asm.emitLVAL (S0, compiledMethodId); 
    asm.emitST   (S0, STACKFRAME_METHOD_ID_OFFSET, FP);

    // establish SP -> VM_Thread, S0 -> threads JNIEnv structure      
    asm.emitL(SP, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);
    asm.emitL(S0, VM_Entrypoints.jniEnvField.getOffset(), SP);  

    // save the TI & PR registers in the JNIEnvironment object for possible calls back into Java
    asm.emitST(TI, VM_Entrypoints.JNIEnvSavedTIField.getOffset(), S0);           
    asm.emitST(PROCESSOR_REGISTER, VM_Entrypoints.JNIEnvSavedPRField.getOffset(), S0);   

    // save current frame pointer in JNIEnv, JNITopJavaFP, which will be the frame
    // to start scanning this stack during GC, if top of stack is still executing in C
    asm.emitST(FP, VM_Entrypoints.JNITopJavaFPField.getOffset(), S0);           

    // save the RVM nonvolatile registers, to be scanned by GC stack mapper
    // remember to skip past the saved JTOC and SP by starting with offset=-12
    //
    for (int i = LAST_NONVOLATILE_GPR, offset = JNI_RVM_NONVOLATILE_OFFSET;
	 i >= FIRST_NONVOLATILE_GPR; --i, offset+=4) {
      asm.emitST (i, frameSize - offset, FP);
    }

    // clear the GC flag on entry to native code
    asm.emitCAL(PROCESSOR_REGISTER,0,0);          // use PR as scratch
    asm.emitST(PROCESSOR_REGISTER, frameSize-JNI_GC_FLAG_OFFSET, FP);

    // generate the code to map the parameters to AIX convention and add the
    // second parameter 2 (either the "this" ptr or class if a static method).
    // The JNI Function ptr first parameter is set before making the call.
    // Opens a new frame in the JNIRefs table to register the references
    // Assumes S0 set to JNIEnv, kills TI, SP & PROCESSOR_REGISTER
    // On return, S0 is still valid.
    //
    storeParametersForAIX(asm, frameSize, method, klass);

    // Get address of out_of_line prolog into SP, before setting TOC reg.
    asm.emitL   (SP, VM_Entrypoints.invokeNativeFunctionInstructionsField.getOffset(), JTOC);
    asm.emitMTLR(SP);

    // set the TOC and IP for branch to out_of_line code
    asm.emitLVAL (JTOC,  nativeTOC);         // load TOC for native function into TOC reg
    asm.emitLVAL (TI,    nativeIP);	      // load TI with address of native code

    // go to part of prologue that is in the boot image. It will change the Processor
    // status to "in_native" and transfer to the native code.  On return it will
    // change the state back to "in_java" (waiting if blocked).
    //
    // The native address entrypoint is in register TI
    // The native TOC has been loaded into the TOC register
    // S0 still points to threads JNIEnvironment
    //
    asm.emitBLRL  ();

    // restore PR, saved in this glue frame before the call. If GC occurred, this saved
    // VM_Processor reference was relocated while scanning this stack. If while in native,
    // the thread became "stuck" and was moved to a Native Processor, then this saved
    // PR was reset to point to that native processor
    //
    asm.emitL     (PROCESSOR_REGISTER, frameSize - JNI_PR_OFFSET, FP);  

    // at this point, test if in blue processor: if yes, return to
    // Java caller; by picking up at "check if GC..." below
    // if not, must transfer to blue processor by using
    // become RVM thread.

    asm.emitL     (S0, VM_Entrypoints.processorModeField.getOffset(), PROCESSOR_REGISTER); // get processorMode
    asm.emitCMPI  (S0, VM_Processor.RVM)     ;           // are we still on blue processor
    VM_ForwardReference frBlue = asm.emitForwardBC(EQ);

    asm.emitL(T0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);  // get activeThread from Processor
    asm.emitL(S0, VM_Entrypoints.jniEnvField.getOffset(), T0);             // get JNIEnvironment from activeThread
    asm.emitL (TI, VM_Entrypoints.JNIEnvSavedTIField.getOffset(), S0);     // and restore TI from JNIEnvironment  

    // restore RVM JTOC
    asm.emitL   (JTOC, frameSize - 4, FP);          

    // Branch to becomeRVMThread:  we will yield and get rescheduled to execute 
    // on a regular VM_Processor for Java code
    asm.emitL   (S0, VM_Entrypoints.becomeRVMThreadMethod.getOffset(), JTOC);
    asm.emitMTLR(S0);
    asm.emitBLRL  ();
     
    // At this point, we have resumed execution in the regular Java VM_Processor
    // so the PR contains the correct values for this VM_Processor.  TI is also correct

    // branch to here if on blue processor	
    //
    // check if GC has occurred, If GC did not occur, then 
    // VM NON_VOLATILE regs were restored by AIX and are valid.  If GC did occur
    // objects referenced by these restored regs may have moved, in this case we
    // restore the nonvolatile registers from our savearea,
    // where any object references would have been relocated during GC.
    // use T2 as scratch (not needed any more on return from call)
    //
    frBlue.resolve(asm);
    asm.emitL(T2, frameSize - JNI_GC_FLAG_OFFSET, FP);
    asm.emitCMPI(T2,0);
    VM_ForwardReference fr1 = asm.emitForwardBC(EQ);
    for (int i = LAST_NONVOLATILE_GPR, offset = JNI_RVM_NONVOLATILE_OFFSET;
	 i >= FIRST_NONVOLATILE_GPR; --i, offset+=4) {
      asm.emitL (i, frameSize - offset, FP);
    }
    fr1.resolve(asm);
    asm.emitL(S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);  // S0 holds thread pointer

    // reset threads processor affinity if it was previously zero, before the call.
    // ...again, rethink if the setting & resetting of processor affinity is still necessary
    //
    asm.emitL(T2, frameSize - JNI_AFFINITY_OFFSET, FP);          // saved affinity in glue frame
    asm.emitCMPI(T2,0);
    VM_ForwardReference fr2 = asm.emitForwardBC(NE);
    asm.emitST(T2, VM_Entrypoints.processorAffinityField.getOffset(), S0);  // store 0 into affinity field
    fr2.resolve(asm);
		 
    // reestablish S0 to hold jniEnv pointer
    asm.emitL(S0, VM_Entrypoints.jniEnvField.getOffset(), S0);       

    // pop jrefs frame off the JNIRefs stack, "reopen" the previous top jref frame
    // use SP as scratch before it's restored, also use T2, T3 for scratch which are no longer needed
    asm.emitL  (SP, VM_Entrypoints.JNIRefsField.getOffset(), S0);          // load base of JNIRefs array
    asm.emitL  (T2, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), S0);   // get saved offset for JNIRefs frame ptr previously pushed onto JNIRefs array
    asm.emitCAL(T3, -4, T2);                                    // compute offset for new TOP
    asm.emitST (T3, VM_Entrypoints.JNIRefsTopField.getOffset(), S0);       // store new offset for TOP into JNIEnv
    asm.emitLX (T2, SP, T2);                                    // retrieve the previous frame ptr
    asm.emitST (T2, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), S0);   // store new offset for JNIRefs frame ptr into JNIEnv

    // Restore the return value R3-R4 saved in the glue frame spill area before the migration
    asm.emitL (T0, AIX_FRAME_HEADER_SIZE, FP);
    asm.emitL (T1, AIX_FRAME_HEADER_SIZE+4, FP);
      
    // if the the return type is a reference, the native C is returning a jref
    // which is a byte offset from the beginning of the threads JNIRefs stack/array
    // of the corresponding ref.  In this case, emit code to replace the returned
    // offset (in R3) with the ref from the JNIRefs array

    VM_Type returnType = method.getReturnType();
    if ( returnType.isReferenceType() ) {
      // use returned offset to load ref from JNIRefs into R3
      asm.emitLX (3, SP, 3);         // SP is still the base of the JNIRefs array
    }

    // reload TI ref saved in JNIEnv
    asm.emitL (TI, VM_Entrypoints.JNIEnvSavedTIField.getOffset(), S0);           


    // pop the glue stack frame, restore the Java caller frame
    asm.emitCAL (FP,  +frameSize, FP);              // remove linkage area

    // C return value is already where caller expected it (R3, R4 or F0)
    // and SP is a scratch register, so we actually don't have to 
    // restore it and/or pop arguments off it.
    // So, just restore the return address to the link register.

    asm.emitL(0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP); 
    asm.emitMTLR(0);                           // restore return address

    // CHECK EXCEPTION AND BRANCH TO ATHROW CODE OR RETURN NORMALLY

    asm.emitL  (T2, VM_Entrypoints.JNIPendingExceptionField.getOffset(), S0);   // get pending exception from JNIEnv
    asm.emitCAL (T3,  0, 0);             // get a null value to compare
    asm.emitST  (T3, VM_Entrypoints.JNIPendingExceptionField.getOffset(), S0); // clear the current pending exception
    asm.emitCMP (T2, T3);                      // check for pending exception on return from native
    VM_ForwardReference fr3 = asm.emitForwardBC(NE);
    asm.emitBLR();                             // if no pending exception, proceed to return to caller
    fr3.resolve(asm);

    // An exception is pending, deliver the exception to the caller as if executing an athrow in the caller
    // at the location of the call to the native method
    asm.emitLtoc(T3, VM_Entrypoints.athrowMethod.getOffset());
    asm.emitMTCTR(T3);                         // point LR to the exception delivery code
    asm.emitCAL (T0, 0, T2);                   // copy the saved exception to T0

    asm.emitBCTR();                            // then branch to the exception delivery code, does not return

    return asm.makeMachineCode();
  } 


  // Map the arguments from RVM convention to AIX convention,
  // and replace all references with indexes into JNIRefs array.
  // Assumption on entry:
  // -TI, PROCESSOR_REGISTER and SP are available for use as scratch register
  // -the frame has been created, FP points to the new callee frame
  // Also update the JNIRefs arra
  private static void storeParametersForAIX(VM_Assembler asm, 
					    int frameSize, 
					    VM_Method method, VM_Class klass) {

    int nextAIXArgReg, nextAIXArgFloatReg, nextVMArgReg, nextVMArgFloatReg; 
    
    // offset to the spill area in the callee (AIX frame):
    // skip past the 2 arguments to be added in front:  JNIenv and class or object pointer
    int spillOffsetAIX = AIX_FRAME_HEADER_SIZE + 8;

    // offset to the spill area in the caller (RVM frame), relative to the callee's FP
    int spillOffsetVM = frameSize + STACKFRAME_HEADER_SIZE;

    VM_Type[] types = method.getParameterTypes();   // does NOT include implicit this or class ptr
    int numArguments = types.length;                // number of arguments for this method
    
    // Set up the Reference table for GC
    // PR <- JREFS array base
    asm.emitL(PROCESSOR_REGISTER, VM_Entrypoints.JNIRefsField.getOffset(), S0);
    // TI <- JREFS current top 
    asm.emitL(TI, VM_Entrypoints.JNIRefsTopField.getOffset(), S0);   // JREFS offset for current TOP 
    asm.emitA(TI, PROCESSOR_REGISTER, TI);                // convert into address

    // TODO - count number of refs
    // TODO - emit overflow check for JNIRefs array
    
    // start a new JNIRefs frame on each transition from Java to native C
    // push current SavedFP ptr onto top of JNIRefs stack (use available SP reg as a temp)
    // and make current TOP the new savedFP
    //
    asm.emitL   ( SP, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), S0);
    asm.emitSTU ( SP, 4, TI );                           // push prev frame ptr onto JNIRefs array	
    asm.emitSF  ( SP, PROCESSOR_REGISTER, TI);           // compute offset for new TOP
    asm.emitST  ( SP, VM_Entrypoints.JNIRefsSavedFPField.getOffset(), S0);  // save new TOP as new frame ptr in JNIEnv


    // for static methods: caller has placed args in r3,r4,... 
    // for non-static methods:"this" ptr is in r3, and args start in r4,r5,...
    // 
    // for static methods:                for nonstatic methods:       
    //  Java caller     AIX callee         Java caller     AIX callee    
    //  -----------     ----------	    -----------     ----------  
    //  spill = arg11 -> new spill	    spill = arg11 -> new spill  
    //  spill = arg10 -> new spill	    spill = arg10 -> new spill  
    // 				            spill = arg9  -> new spill  
    //    R12 = arg9  -> new spill	                                
    //    R11 = arg8  -> new spill	      R12 = arg8  -> new spill  
    //    R10 = arg7  -> new spill	      R11 = arg7  -> new spill  
    //    R9  = arg6  -> new spill	      R10 = arg6  -> new spill  
    // 								   
    //    R8  = arg5  -> R10                  R9  = arg5  -> R10         
    //    R7  = arg4  -> R9		      R8  = arg4  -> R9          
    //    R6  = arg3  -> R8		      R7  = arg3  -> R8          
    //    R5  = arg2  -> R7		      R6  = arg2  -> R7          
    //    R4  = arg1  -> R6		      R5  = arg1  -> R6          
    //    R3  = arg0  -> R5		      R4  = arg0  -> R5          
    //                   R4 = class           R3  = this  -> R4         
    // 	                 R3 = JNIenv                         R3 = JNIenv
    //
    // if the number of args in GPR does not exceed R11, then we can use R12 as scratch 
    //   to move the args
    // if the number of args in GPR exceed R12, then we need to save R12 first to make 
    //   room for a scratch register
    // if the number of args in FPR does not exceed F12, then we can use F13 as scratch

    nextAIXArgFloatReg = FIRST_AIX_VOLATILE_FPR;
    nextVMArgFloatReg = FIRST_VOLATILE_FPR;
    nextAIXArgReg      = FIRST_AIX_VOLATILE_GPR + 2;   // 1st reg = JNIEnv, 2nd reg = class
    if ( method.isStatic() ) {
      nextVMArgReg = FIRST_VOLATILE_GPR;              
    } else {
      nextVMArgReg = FIRST_VOLATILE_GPR+1;            // 1st reg = this, to be processed separately
    }

    // The loop below assumes the following relationship:
    if (VM.VerifyAssertions) VM.assert(FIRST_AIX_VOLATILE_FPR==FIRST_VOLATILE_FPR);
    if (VM.VerifyAssertions) VM.assert(LAST_AIX_VOLATILE_FPR<=LAST_VOLATILE_FPR);
    if (VM.VerifyAssertions) VM.assert(FIRST_AIX_VOLATILE_GPR==FIRST_VOLATILE_GPR);
    if (VM.VerifyAssertions) VM.assert(LAST_AIX_VOLATILE_GPR<=LAST_VOLATILE_GPR);


    // create one VM_Assembler object for each argument
    // This is needed for the following reason:
    //   -2 new arguments are added in front for native methods, so the normal arguments
    //    need to be shifted down in addition to being moved
    //   -to avoid overwriting each other, the arguments must be copied in reverse order
    //   -the analysis for mapping however must be done in forward order
    //   -the moving/mapping for each argument may involve a sequence of 1-3 instructions 
    //    which must be kept in the normal order
    // To solve this problem, the instructions for each argument is generated in its
    // own VM_Assembler in the forward pass, then in the reverse pass, each VM_Assembler
    // emist the instruction sequence and copies it into the main VM_Assembler
    VM_Assembler[] asmForArgs = new VM_Assembler[numArguments];

    for (int arg = 0; arg < numArguments; arg++) {
      
      boolean mustSaveFloatToSpill;
      asmForArgs[arg] = new VM_Assembler(0);
      VM_Assembler asmArg = asmForArgs[arg];

      // For 32-bit float arguments
      //
      if (types[arg].isFloatType()) {
	// Side effect of float arguments on the GPR's
	// (1a) reserve one GPR for each float if it is available
	if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR) {
	  nextAIXArgReg++;
	  mustSaveFloatToSpill = false;
	} else {
	  // (1b) if GPR has spilled, store the float argument in the callee spill area
	  // regardless of whether the FPR has spilled or not
	  mustSaveFloatToSpill = true;
	}

	// Check if the args need to be moved
	// (2a) leave those in FPR[1:13] as is unless the GPR has spilled
	if (nextVMArgFloatReg<=LAST_AIX_VOLATILE_FPR) {
	  if (mustSaveFloatToSpill) {
	    asmArg.emitSTFS(nextVMArgFloatReg, spillOffsetAIX, FP); 
	  }
	  spillOffsetAIX+=4;
	  nextAIXArgFloatReg++;
	  nextVMArgFloatReg++;  
	} else if (nextVMArgFloatReg<=LAST_VOLATILE_FPR) {
	  // (2b) run out of FPR in AIX, but still have 2 more FPR in VM,
	  // so FPR[14:15] goes to the callee spill area
	  asmArg.emitSTFS(nextVMArgFloatReg, spillOffsetAIX, FP);
	  nextVMArgFloatReg++;
	  spillOffsetAIX+=4;
	} else {
	  // (2c) run out of FPR in VM, now get the remaining args from the caller spill area 
	  // and move them into the callee spill area
	  asmArg.emitLFS(0, spillOffsetVM, FP);
	  asmArg.emitSTFS(0, spillOffsetAIX, FP);
	  spillOffsetVM+=4;
	  spillOffsetAIX+=4;
	}
      } else if (types[arg].isDoubleType()) {
	// For 64-bit float arguments 
	
	// Side effect of float arguments on the GPR's
	// (1a) reserve two GPR's for double
	if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR-1) {
	  nextAIXArgReg+=2;
	  mustSaveFloatToSpill = false;
	} else {
	  // if only one GPR is left, reserve it anyway although it won't be used
	  if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR)
	    nextAIXArgReg++;
	  mustSaveFloatToSpill = true;
	}

	// Check if the args need to be moved
	// (2a) leave those in FPR[1:13] as is unless the GPR has spilled
	if (nextVMArgFloatReg<=LAST_AIX_VOLATILE_FPR) {
	  if (mustSaveFloatToSpill) {
	    asmArg.emitSTFD(nextVMArgFloatReg, spillOffsetAIX, FP); 
	  }
	  spillOffsetAIX+=8;
	  nextAIXArgFloatReg++;
	  nextVMArgFloatReg++;  
	} else if (nextVMArgFloatReg<=LAST_VOLATILE_FPR) {
	  // (2b) run out of FPR in AIX, but still have 2 more FPR in VM,
	  // so FPR[14:15] goes to the callee spill area
	  asmArg.emitSTFD(nextVMArgFloatReg, spillOffsetAIX, FP);
	  nextVMArgFloatReg++;
	  spillOffsetAIX+=8;
	} else {
	  // (2c) run out of FPR in VM, now get the remaining args from the caller spill area 
	  // and move them into the callee spill area
	  asmArg.emitLFD(0, spillOffsetVM, FP);
	  asmArg.emitSTFD(0, spillOffsetAIX, FP);
	  spillOffsetVM+=8;
	  spillOffsetAIX+=8;
	}
      } else if (types[arg].isLongType()) {
	// For 64-bit int arguments
	//
	
	// (1a) fit in AIX register, move the pair
	if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR-1) {
	  asmArg.emitCAU(nextAIXArgReg+1, nextVMArgReg+1, 0);  // move lo-word first
	  asmArg.emitCAU(nextAIXArgReg, nextVMArgReg, 0);      // so it doesn't overwritten
	  nextAIXArgReg+=2;
	  nextVMArgReg+=2;
	  spillOffsetAIX+=8;
	} else if (nextAIXArgReg==LAST_AIX_VOLATILE_GPR &&
		   nextVMArgReg<=LAST_VOLATILE_GPR-1) {
	  // (1b) fit in VM register but straddle across AIX register/spill
	  spillOffsetAIX+=4;
	  asmArg.emitST(nextVMArgReg+1, spillOffsetAIX, FP);   // move lo-word first
	  spillOffsetAIX+=4;                                    // so it doesn't overwritten
	  asmArg.emitCAU(nextAIXArgReg, nextVMArgReg, 0);
	  nextAIXArgReg+=2;
	  nextVMArgReg+=2;	  
	} else if (nextAIXArgReg>LAST_AIX_VOLATILE_GPR &&
		   nextVMArgReg<=LAST_VOLATILE_GPR-1) {
	  // (1c) fit in VM register, spill in AIX without straddling register/spill
	  asmArg.emitST(nextVMArgReg++, spillOffsetAIX, FP);
	  spillOffsetAIX+=4;
	  asmArg.emitST(nextVMArgReg++, spillOffsetAIX, FP);
	  spillOffsetAIX+=4;
	} else if (nextVMArgReg==LAST_VOLATILE_GPR) {
	  // (1d) split across VM/spill, spill in AIX
	  asmArg.emitST(nextVMArgReg++, spillOffsetAIX, FP);
	  spillOffsetAIX+=4;
	  asmArg.emitL(0, spillOffsetVM, FP);
	  asmArg.emitST(0, spillOffsetAIX, FP);
	  spillOffsetAIX+=4;
	  spillOffsetVM+=4;
	} else {
	  // (1e) spill both in VM and AIX
	  asmArg.emitLFD(0, spillOffsetVM, FP);
	  asmArg.emitSTFD(0, spillOffsetAIX, FP);
	  spillOffsetAIX+=8;
	  spillOffsetVM+=8;	  
	}
      } else if (types[arg].isReferenceType() ) {	
	// For reference type, replace with handlers before passing to AIX
	//
	
	// (1a) fit in AIX register, move the register
	if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR) {
	  asmArg.emitSTU(nextVMArgReg++, 4, TI );          // append ref to end of JNIRefs array
	  asmArg.emitSF(nextAIXArgReg++, PROCESSOR_REGISTER, TI );  // pass offset in bytes of jref
	  spillOffsetAIX+=4;
	} else if (nextVMArgReg<=LAST_VOLATILE_GPR) {
	  // (1b) spill AIX register, but still fit in VM register
	  asmArg.emitSTU(nextVMArgReg++, 4, TI );    // append ref to end of JNIRefs array
	  asmArg.emitSF(0, PROCESSOR_REGISTER, TI );  // compute offset in bytes for jref
	  asmArg.emitST(0, spillOffsetAIX, FP);       // spill into AIX frame
	  spillOffsetAIX+=4;
	} else {
	  // (1c) spill VM register
	  asmArg.emitL(0, spillOffsetVM, FP);        // retrieve arg from VM spill area
	  asmArg.emitSTU(0, 4, TI );                  // append ref to end of JNIRefs array
	  asmArg.emitSF(0, PROCESSOR_REGISTER, TI );  // compute offset in bytes for jref
	  asmArg.emitST(0, spillOffsetAIX, FP);       // spill into AIX frame
	  spillOffsetVM+=4;
	  spillOffsetAIX+=4;
	}
      } else {
	// For all other types: int, short, char, byte, boolean

	// (1a) fit in AIX register, move the register
	if (nextAIXArgReg<=LAST_AIX_VOLATILE_GPR) {
	  asmArg.emitCAU(nextAIXArgReg++, nextVMArgReg++, 0);
	  spillOffsetAIX+=4;
	}

	// (1b) spill AIX register, but still fit in VM register
	else if (nextVMArgReg<=LAST_VOLATILE_GPR) {
	  asmArg.emitST(nextVMArgReg++, spillOffsetAIX, FP);
	  spillOffsetAIX+=4;
	} else {
	  // (1c) spill VM register
	  asmArg.emitL(0,spillOffsetVM, FP);        // retrieve arg from VM spill area
	  asmArg.emitST(0, spillOffsetAIX, FP);
	  spillOffsetVM+=4;
	  spillOffsetAIX+=4;
	}
      }
    }
    

    // Append the code sequences for parameter mapping 
    // to the current machine code in reverse order
    // so that the move does not overwrite the parameters
    for (int arg = numArguments-1; arg >= 0; arg--) {
      VM_MachineCode codeForArg = asmForArgs[arg].makeMachineCode();
      asm.appendInstructions(codeForArg.getInstructions());
    }

    // Now add the 2 new JNI parameters:  JNI environment and Class or "this" object
    
    // if static method, append ref for class, else append ref for "this"
    // and pass offset in JNIRefs array in r4 (as second arg to called native code)
    if ( method.isStatic() ) {
      klass.getClassForType();     // ensure the Java class object is created
      // JTOC saved above in JNIEnv is still valid, used by following emitLtoc
      asm.emitLtoc( 4, klass.getTibOffset() ); // r4 <- class TIB ptr from jtoc
      asm.emitL   ( 4, 0, 4 );                  // r4 <- first TIB entry == -> class object
      asm.emitL   ( 4, VM_Entrypoints.classForTypeField.getOffset(), 4 ); // r4 <- java Class for this VM_Class
      asm.emitSTU ( 4, 4, TI );                 // append class ptr to end of JNIRefs array
      asm.emitSF( 4, PROCESSOR_REGISTER, TI );  // pass offset in bytes
    } else {
      asm.emitSTU ( 3, 4, TI );                 // append this ptr to end of JNIRefs array
      asm.emitSF( 4, PROCESSOR_REGISTER, TI );  // pass offset in bytes
    }
    
    // store the new JNIRefs array TOP back into JNIEnv	
    asm.emitSF(TI, PROCESSOR_REGISTER, TI );     // compute offset for the current TOP
    asm.emitST(TI, VM_Entrypoints.JNIRefsTopField.getOffset(), S0);
  }


  // Emit code to interface with a call from native code that uses the AIX convention
  // for register and stack to a JNI Function that uses Japapeno conventions.
  //
  static void generateGlueCodeForJNIMethod(VM_Assembler asm, VM_Method mth) {
    int offset;

    asm.emitSTU(FP,-JNI_GLUE_FRAME_SIZE,FP);     // buy the glue frame

    // we may need to save CR in the previous frame also if CR will be used
    // CR is to be saved at FP+4 in the previous frame

    // Here we check if this is a JNI function that takes the vararg in the ... style
    // This includes CallStatic<type>Method, Call<type>Method, CallNonVirtual<type>Method
    // For these calls, the vararg starts at the 4th or 5th argument (GPR 6 or 7)
    // So, we save the GPR 6-10 and FPR 1-3 in a volatile register save area 
    // in the glue stack frame so that the JNI function can later repackage the arguments
    // based on the parameter types of target method to be invoked.
    // (For long argument lists, the additional arguments, have been saved in
    // the spill area of the AIX caller, and will be retrieved from there.)
    //
    // If we are compiling such a JNI Function, then emit the code to store
    // GPR 4-10 and FPR 1-6 into the volatile save area.

    String mthName = mth.getName().toString();
    if ((mthName.startsWith("Call") && mthName.endsWith("Method")) ||
	mthName.equals("NewObject")) {

	offset = STACKFRAME_HEADER_SIZE + 12;   // skip over slots for GPR 3-5
	for (int i = 6; i <= 10; i++ ) {
	    asm.emitST (i, offset, FP);
	    offset+=4;
	}
	// store FPRs 1-3 in first 3 slots of volatile FPR save area
	for (int i = 1; i <= 3; i++) {
	    asm.emitSTFD (i, offset, FP);
	    offset+=8;
	}
    }

    // Save AIX non-volatile GRPs and FPRs that will not be saved and restored
    // by RVM. These are GPR 13-16 & FPR 14-15.
    //
    offset = STACKFRAME_HEADER_SIZE + 80;   // skip 20 word volatile reg save area
    for (int i = 13; i <= 16; i++) {
      asm.emitST (i, offset, FP);
      offset += 4;
    }
    for (int i = 14; i <= 15; i++) {
      asm.emitSTFD (i, offset, FP);
      offset +=8;
    }
 
    // set the method ID for the glue frame
    // and save the return address in the previous frame
    //
    asm.emitLVAL(S0, INVISIBLE_METHOD_ID);
    asm.emitMFLR(0);
    asm.emitST  (S0, STACKFRAME_METHOD_ID_OFFSET, FP);
    asm.emitST  (0, JNI_GLUE_FRAME_SIZE + STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP);
    int CR_OFFSET = 4; // Save CR in caller's frame; see page 162 of PPC Compiler Writer's Guide
    asm.emitMFCR (S0);
    asm.emitST (S0, JNI_GLUE_FRAME_SIZE + CR_OFFSET, FP);

    // change the vpStatus of the current Processor to "in Java", if GC has started 
    // and we are "blocked_in_native" then loop doing sysYields until GC done and the
    // processor is unblocked.  With default jni, we can also be "blocked_in_native"
    // if the running thread has been detected as "stuck" and is being moved to a
    // native processor.
    //
    // on entry T0 = JNI Function Ptr (Handle) which should point into the array of function ptrs
    // the next word in that array contains the address of the current processors vpStatus word
    //
    // AIX non volatile gprs 13-16 have been saved & are available (also gprs 11-13 can be used).
    // S0=13, SP=14, TI=15, PROCESSOR_REGISTER=16 are available (&have labels) for changing state.
    // ...it would be nice to leave the passed arguments untouched, unless we are blocked
    // and have to call sysVirtualProcessorYield

    int label0    = asm.getMachineCodeIndex();	                    // inst index of the following instr
    asm.emitL     (TI, 4, T0);      // TI <- addr of processors vpStatus, from JNI function Ptr array
    asm.emitLWARX (S0, 0, TI);      // get status for processor
    asm.emitCMPI  (S0, VM_Processor.BLOCKED_IN_NATIVE);       // check if GC in progress, blocked in native mode
    VM_ForwardReference frBlocked = asm.emitForwardBC(EQ);

    asm.emitCAL   (S0,  VM_Processor.IN_JAVA, 0 );            // S0  <- new state value
    asm.emitSTWCXr(S0,  0, TI);                               // attempt to change state to native
    asm.emitBC    (NE, label0);                               // br if failure -retry lwarx by jumping to label0
    VM_ForwardReference frInJava = asm.emitForwardB();        // branch around code to call sysYield

    // branch to here if blocked in native, call sysVirtulaProcessorYield (AIX pthread yield)
    // must save AIX volatile gprs & fprs before the call and restore after
    //
    frBlocked.resolve(asm);
    offset = STACKFRAME_HEADER_SIZE;

    // save volatile GPRS 3-10
    for (int i = FIRST_AIX_VOLATILE_GPR; i <= LAST_AIX_VOLATILE_GPR; i++) {
      asm.emitST (i, offset, FP);
      offset+=4;
    }

    // save volatile FPRS 1-6
    for (int i = FIRST_AIX_VOLATILE_FPR; i <= 6; i++) {
      asm.emitSTFD (i, offset, FP);
      offset+=8;
    }

    // note JTOC is the RVM JTOC, set by native code when it branched thru the
    // JNI function pointer to this code
    asm.emitL     (SP, VM_Entrypoints.the_boot_recordField.getOffset(), JTOC); // get boot record address
    asm.emitCAL   (PROCESSOR_REGISTER, 0, JTOC);                    // save JTOC for later
    asm.emitL     (JTOC, VM_Entrypoints.sysTOCField.getOffset(), SP);          // load TOC for syscalls from bootrecord
    asm.emitL     (TI,   VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset(), SP);  // load addr of function
    asm.emitMTLR  (TI);
    asm.emitBLRL();                                                 // call sysVirtualProcessorYield in sys.C
    asm.emitCAL   (JTOC, 0,PROCESSOR_REGISTER);                     // restore RVM JTOC

    // restore the saved volatile GPRs 3-10 and FPRs 1-6
    offset = STACKFRAME_HEADER_SIZE;

    // restore volatile GPRS 3-10 
    for (int i = FIRST_AIX_VOLATILE_GPR; i <= LAST_AIX_VOLATILE_GPR; i++) {
      asm.emitL  (i, offset, FP);
      offset+=4;
    }

    // restore volatile FPRS 1-6  
    for (int i = FIRST_AIX_VOLATILE_FPR; i <= 6; i++) {
      asm.emitLFD (i, offset, FP);
      offset+=8;
    }

    asm.emitB (label0);  // br back to label0 to try lwarx again

    // NOW_IN_JAVA:
    // branch to here, after setting status to IN_JAVA
    //
    // GC is not in progress - so we can now reference moveable objects in the heap,
    // such as JNIEnvironment, VM_Thread, VM_Processor.
    //
    // T0 = the address of running threads entry in the jnifunctionpointers array
    // (as of 11/16/00 each thread gets 2 words in this array)
    // compute offset into this array, use this divieded by 2 as offset in threads array,
    // load VM_Thread, and from it load the jniEnv pointer.
    // ...use TI and PROCESSOR_REGISTER as temps for a while
    // ...JTOC is now the RVM JTOC
    //
    frInJava.resolve(asm);
    asm.emitL  (TI, VM_Entrypoints.JNIFunctionPointersField.getOffset(), JTOC);
    asm.emitSF (TI, TI, T0);                                         // TI <- byte offset into funcPtrs array
    asm.emitSRAI (TI, TI, 1);                                        // divide by 2
    asm.emitL  (PROCESSOR_REGISTER, VM_Entrypoints.threadsField.getOffset(), JTOC);
    asm.emitA  (TI, PROCESSOR_REGISTER, TI);
    asm.emitL  (TI, 0, TI);                                                  // TI <- address of VM_Thread 
    asm.emitL  (T0, VM_Entrypoints.jniEnvField.getOffset(), TI);                        // T0 <- JNIEnv ref (a REAL ref)

    // get pointer to top java frame from JNIEnv, compute offset from current
    // frame pointer (offset to avoid more interior pointers) and save offset
    // in this glue frame
    //
    asm.emitL  (TI, VM_Entrypoints.JNITopJavaFPField.getOffset(), T0);     // get addr of top java frame from JNIEnv
    asm.emitSF (TI, FP, TI);                                    // TI <- offset from current FP
    asm.emitST (TI, JNI_GLUE_FRAME_SIZE-4, FP);                 // store offset at end of glue frame

    // load TI & PR registers from JNIEnv before calling Java JNI Function
    asm.emitL  (TI, VM_Entrypoints.JNIEnvSavedTIField.getOffset(), T0);  
    asm.emitL  (PROCESSOR_REGISTER, VM_Entrypoints.JNIEnvSavedPRField.getOffset(), T0);  

    // check if now running on a native processor, if so we have to
    // transfer back to a blue/RVM processor (via call to becomeRVM)
    //
    asm.emitL     (S0, VM_Entrypoints.processorModeField.getOffset(), PROCESSOR_REGISTER); // get processorMode
    asm.emitCMPI  (S0, VM_Processor.RVM);                // are we still on blue processor
    VM_ForwardReference frBlue2 = asm.emitForwardBC(EQ); // skip transfer to blue if on blue

    // save volatile GPRs 3-10 and FPRs 1-6 before calling becomeRVMThread
    offset = STACKFRAME_HEADER_SIZE;

    // save volatile GPRS 3-10 
    for (int i = FIRST_AIX_VOLATILE_GPR; i <= LAST_AIX_VOLATILE_GPR; i++) {
      asm.emitST (i, offset, FP);
      offset+=4;
    }

    // save volatile FPRS 1-6
    for (int i = FIRST_AIX_VOLATILE_FPR; i <= 6; i++) {
      asm.emitSTFD (i, offset, FP);
      offset+=8;
    }

    // Branch to becomeRVMThread:  we will yield and get rescheduled to execute 
    // on a regular VM_Processor for Java code.  At this point PR, TI & JTOC
    // are valid for executing Java
    //
    asm.emitLtoc  (S0, VM_Entrypoints.becomeRVMThreadMethod.getOffset());  // always 2 instructions
    asm.emitMTLR  (S0);
    asm.emitBLRL  ();
     
    // At this point, we have resumed execution in a RVM VM_Processor
    // The PR register now points to this VM_Processor.  TI & JTOC are still valid.

    // restore the saved volatile GPRs 3-10 and FPRs 1-6
    offset = STACKFRAME_HEADER_SIZE;

    // restore volatile GPRS 3-10
    for (int i = FIRST_AIX_VOLATILE_GPR; i <= LAST_AIX_VOLATILE_GPR; i++) {
      asm.emitL  (i, offset, FP);
      offset+=4;
    }

    // restore volatile FPRS 1-6
    for (int i = FIRST_AIX_VOLATILE_FPR; i <= 6; i++) {
      asm.emitLFD (i, offset, FP);
      offset+=8;
    }

    // branch to here if making jni call on RVM/blue processor	
    frBlue2.resolve(asm);

    // BRANCH TO THE PROLOG FOR THE JNI FUNCTION
    VM_ForwardReference frNormalPrologue = asm.emitForwardBL();

    // relative branch and link past the following epilog, to the normal prolog of the method
    // the normal epilog of the method will return to the epilog here to pop the glue stack frame

    // RETURN TO HERE FROM EPILOG OF JNI FUNCTION
    // CAUTION:  START OF EPILOG OF GLUE CODE
    // The section of code from here to "END OF EPILOG OF GLUE CODE" is nestled between
    // the glue code prolog and the real JNI code.
    // T0 & T1 (R3 & R4) or F1 contain the return value from the function - DO NOT USE

    // assume: JTOC, TI, and PROCESSOR_REG are valid, and all RVM non-volatile 
    // GPRs and FPRs have been restored.  Our processor state should be ...IN_JAVA so 
    // it is OK to use saved refs to moveable objects (ie. PROCESSOR_REG)

    // establish T2 -> current threads JNIEnv structure, from activeThread field
    // of current processor      
    asm.emitL (T2, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // T2 <- activeThread of PR
    asm.emitL (T2, VM_Entrypoints.jniEnvField.getOffset(), T2);                         // T2 <- JNIEnvironment 

    // before returning to C, set pointer to top java frame in JNIEnv, using offset
    // saved in this glue frame during transition from C to Java.  GC will use this saved
    // frame pointer if it is necessary to do GC with a processors active thread
    // stuck (and blocked) in native C, ie. GC starts scanning the threads stack at that frame.
    //
    asm.emitL  (T3, JNI_GLUE_FRAME_SIZE-4, FP);                 // load offset from FP to top java frame
    asm.emitA  (T3, FP, T3);                                    // T3 <- address of top java frame
    asm.emitST (T3, VM_Entrypoints.JNITopJavaFPField.getOffset(), T2);     // store TopJavaFP back into JNIEnv

    // Check if nativeAffinity is set:  
    // -if yes, the caller is via CreateJavaVM or AttachCurrentThread
    //  from an external pthread and control must be returned to the C program there
    // -If no, return to C can be done on the current processor
    asm.emitL (S0, VM_Entrypoints.activeThreadField.getOffset(), PROCESSOR_REGISTER);   // T2 <- activeThread of PR
    asm.emitL (S0, VM_Entrypoints.nativeAffinityField.getOffset(), S0);                 // T2 <- nativeAffinity 
    asm.emitCMPI (S0, 0);
    VM_ForwardReference fr6 = asm.emitForwardBC(EQ);

    // check to see if this frame address is the sentinal since there may be no further Java frame below
    asm.emitCMPI (T3, VM_Constants.STACKFRAME_SENTINAL_FP);
    VM_ForwardReference fr5 = asm.emitForwardBC(NE);
    
    // save result GPRS 3-4  and FPRS 1
    offset = STACKFRAME_HEADER_SIZE;
    asm.emitST (T0, offset, FP);
    asm.emitST (T1, offset+4, FP);
    asm.emitST (T2, offset+8, FP);
    asm.emitST (T3, offset+12, FP);
    asm.emitSTFD (FIRST_AIX_VOLATILE_FPR, offset+16, FP);

    // Branch to becomeNativeThread:  we will yield and get rescheduled to execute 
    // on a VM_Processor for the external pthread code.
    //
    asm.emitLtoc  (S0, VM_Entrypoints.becomeNativeThreadMethod.getOffset());  // always 2 instructions
    asm.emitMTLR  (S0);
    asm.emitBLRL  ();

    // restore result GPRS 3-4  and FPRS 1
    offset = STACKFRAME_HEADER_SIZE;
    asm.emitL (T0, offset, FP);
    asm.emitL (T1, offset+4, FP);
    asm.emitL (T2, offset+8, FP);
    asm.emitL (T3, offset+12, FP);
    asm.emitLFD (FIRST_AIX_VOLATILE_FPR, offset+16, FP);

    // While in Java (JNI Function), on a RVM Processor, we allow the thread to be migrated
    // to a different Processor. Thus the current processor when returning from Java back
    // may be different from the processor when calling the Java JNI Function. We therefore
    // must update the saved processor registers, in the threads JNIEnvironment and in the
    // preceeding "Java to C" transition frame.
    // Note: must be done after the possible call to becomeNativeThread to pick up the right PR value
    //
    // update the saved PR of the frame if TopJavaFP is valid (branch here from check above)
    fr6.resolve(asm);
    fr5.resolve(asm);
    
    // check to see if this frame address is the sentinal since there may be no further Java frame below
    asm.emitCMPI (T3, VM_Constants.STACKFRAME_SENTINAL_FP);
    VM_ForwardReference fr4 = asm.emitForwardBC(EQ);
    asm.emitL  (S0, 0, T3);                   // get fp for caller of prev J to C transition frame
    asm.emitST (PROCESSOR_REGISTER, -JNI_PR_OFFSET, S0);  // store PR back into transition frame
    fr4.resolve(asm);

    asm.emitST (PROCESSOR_REGISTER, VM_Entrypoints.JNIEnvSavedPRField.getOffset(), T2);  // store PR back into JNIEnv

    // change the state of the VP to "in Native". With default jni transitions to
    // native C from Java ALWAYS start on a blue/RVM Processor (status = IN_JAVA)
    // and are never "BLOCKED_IN_JAVA"
    //
    asm.emitL     (T3, VM_Entrypoints.vpStatusAddressField.getOffset(), PROCESSOR_REGISTER); // T3 gets addr vpStatus word
    asm.emitCAL   (S0,  VM_Processor.IN_NATIVE, 0 );              // S0  <- new status value
    asm.emitST    (S0,  0, T3);                                   // change state to native

    asm.emitL     (S0, JNI_GLUE_FRAME_SIZE + CR_OFFSET, FP);
    asm.emitMTCRF (0xff, S0);

    // Restore those AIX nonvolatile registers saved in the prolog above
    // Here we only save & restore ONLY those registers not restored by RVM
    //
    offset = STACKFRAME_HEADER_SIZE + 80;   // skip 20 word volatile reg save area
    for (int i = 13; i <= 16; i++) {
      asm.emitL  (i, offset, FP);                     // 4 instructions
      offset += 4;
    }
    for (int i = 14; i <= 15; i++) {
      asm.emitLFD  (i, offset, FP);                   // 2 instructions
      offset +=8;
    }

    // pop frame
    asm.emitCAL(FP, JNI_GLUE_FRAME_SIZE, FP);

    // load return address & return to caller
    // T0 & T1 (or F1) should still contain the return value
    //
    asm.emitL(T2, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP);
    asm.emitMTLR(T2);
    asm.emitBLR (); // branch always, through link register

    //END OF EPILOG OF GLUE CODE
    frNormalPrologue.resolve(asm);
  } 
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 *   This class implements the JNI environment, it includes:
 * -The array of JNI function pointers accessible from C
 * -Implementation of all the JNI functions
 *
 * @author Ton Ngo 
 * @author Steve Smith
 */
import java.lang.reflect.*;
import com.ibm.JikesRVM.memoryManagers.VM_GCUtil;

public class VM_JNIEnvironment implements VM_JNIAIXConstants, VM_RegisterConstants
{
  private static boolean initialized = false;
  private static String[] names;

  /**
   * This is the JNI function table, the address of this array will be
   * passed to the native code
   */
  private static INSTRUCTION[][][] JNIFunctions;

  /**
   * This is a table of pointers to the shared JNI function table.  All entries 
   * point to the same function table.  Each thread uses the pointer at its thread id
   * offset to allow us to determine a threads id from the pointer it is using.
   * Needed when native calls Java (JNIFunctions) and passes its JNIEnv pointer.
   * Its offset into the JNIFunctionPts array is the same as the threads offset
   * in the Scheduler.threads array.
   */
    //  private static int[] JNIFunctionPointers;
    static int[] JNIFunctionPointers;        // made public so vpStatus could be set 11/16/00 SES
                                             // maybe need set & get functions ??

  /**
   * These are thread specific information, such as:
   *  -the list of references passed to native code, for GC purpose
   *  -saved RVM system registers
   */
  VM_Address   JNIEnvAddress;      // contain a pointer to the JNIFunctions array
  int          savedTIreg;         // for saving thread index register on entry to native, to be restored on JNI call from native
  VM_Processor savedPRreg;         // for saving processor register on entry to native, to be restored on JNI call from native
  boolean      alwaysHasNativeFrame;  // true if the bottom stack frame is native, such as thread for CreateJVM or AttachCurrentThread

  int[]       JNIRefs;          // references passed to native code
  int         JNIRefsTop;       // -> address of current top ref in JNIRefs array 
  int         JNIRefsMax;       // -> address of end (last entry) of JNIRefs array
  int         JNIRefsSavedFP;   // -> previous frame boundary in JNIRefs array
  public VM_Address  JNITopJavaFP;     // -> Top java frame when in C frames on top of the stack

  Throwable pendingException = null;

  // Saved context for thread attached to external pthread.  This context is
  // saved by the JNIService thread and points to the point in JNIStartUp thread
  // where it yields to the queue in the native VM_Processor.
  // When DetachCurrentThread is called, the JNIService thread restores this context 
  // to allow the thread to run VM_Thread.terminate on its original stack.
  VM_Registers savedContextForTermination;

  // temporarily use a fixed size array for JNI refs, later grow as needed
  static final int JNIREFS_ARRAY_LENGTH = 100;

  // allocate the first dimension of the function array in the boot image so that
  // we have an address pointing to it.  This is necessary for thread creation
  // since the VM_JNIEnvironment object will contain a field pointing to this array
  public static void init() {
    JNIFunctions = new int[FUNCTIONCOUNT][][];

    // 2 words for each thread
    JNIFunctionPointers = new int[VM_Scheduler.MAX_THREADS * 2];
  }

  /**
   *  Initialize the array of JNI functions
   *  To be called from VM_DynamicLibrary.java when a library is loaded,
   *  expecting native calls to be made
   *
   */
  public static void boot() {

    if (initialized)
      return;

    // fill an array of JNI names
    setNames();

    // fill in the TOC entries for each AIX linkage triplet
    for (int i=0; i<JNIFunctions.length; i++) {
      JNIFunctions[i] = new int[3][];
      JNIFunctions[i][TOC] = VM_Statics.getSlots();   // the JTOC value: address of TOC
    }

    // fill in the IP entries for each AIX linkage triplet
    try {
      VM_Class cls = VM_Class.forName("VM_JNIFunctions");
      VM_Method[] mths = cls.getDeclaredMethods();
      // VM.sysWrite("VM_JNIEnvironment:  scanning " + mths.length + " methods\n");
      for (int i=0; i<mths.length; i++) {
	String methodName = mths[i].getName().toString();
	int jniIndex = indexOf(methodName);
	if (jniIndex!=-1) {
	  JNIFunctions[jniIndex][IP] = mths[i].getCurrentInstructions();
	  // VM.sysWrite("   " + methodName + "=" + VM.intAsHexString(JNIFunctions[jniIndex][IP]));
	} 
	// else {
	//   VM.sysWrite("   " + methodName + " skipped\n");
	// }
      }

      VM_Address functionAddress = VM_Magic.objectAsAddress(JNIFunctions[NEWINTARRAY][IP]);
      // VM.sysWrite("   NewIntArray is at " + VM.intAsHexString(functionAddress) + "\n");
      functionAddress = VM_Magic.objectAsAddress(JNIFunctions[NEWINTARRAY][TOC]);
      // VM.sysWrite("   TOC is stored at " + VM.intAsHexString(functionAddress) + "\n");

    } catch (VM_ResolutionException e) {
      throw new InternalError("VM_JNIEnvironment fails to initialize, has the class been renamed\n");
    }

    initialized = true;

  }

  // Instance:  create a thread specific JNI environment.  threadSlot = creating threads
  // thread id == index of its entry in Scheduler.threads array
  //
  public VM_JNIEnvironment(int threadSlot) {
    // VM_Field functions = (VM_Field) VM.getMember("VM_JNIEnvironment", "JNIFunctions", "[[I");
    // int addr = VM_Magic.getTocPointer() + functions.getOffset();

    // as of 8/22 SES - let JNIEnvAddress be the address of the JNIFunctionPtr to be
    // used by the creating thread.  Passed as first arg (JNIEnv) to native C functions.

    // uses 2 words for each thread, the first is the function pointer
    // to be used when making native calls
    JNIFunctionPointers[threadSlot * 2] = VM_Magic.objectAsAddress(JNIFunctions).toInt();
    JNIFunctionPointers[(threadSlot * 2)+1] = 0;  // later contains addr of processor vpStatus word
    JNIEnvAddress = VM_Magic.objectAsAddress(JNIFunctionPointers).add(threadSlot*8);

    JNIRefs = new int[JNIREFS_ARRAY_LENGTH];
    JNIRefs[0] = 0;                                        // 0 entry for bottom of stack
    JNIRefsTop = 0;
    JNIRefsSavedFP = 0;
    JNIRefsMax = (JNIRefs.length - 1) * 4;   // byte offset to last entry

    // initially TOP and SavedFP -> entry 0 containing 0

    alwaysHasNativeFrame = false;

  }

  // push a reference onto thread local JNIRefs stack.  To be used by JNI
  // Functions when returning a reference back to JNI native C code
  // Taken:    Object to put on stack
  // Returned: offset of entry in JNIRefs stack
  // 
  public int pushJNIRef( Object ref ) {
    JNIRefsTop += 4;
    JNIRefs[ JNIRefsTop >> 2 ] = VM_Magic.objectAsAddress(ref).toInt();
    return JNIRefsTop;
  }

  // get a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  // Returned: reference at that offset
  public Object getJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP\n");
      return null;
    }
    return VM_Magic.addressAsObject( VM_Address.fromInt(JNIRefs[ offset>>2 ]) );
  }
	
  // remove a reference from the JNIRefs stack
  // Taken:    offset in JNIRefs stack
  public void deleteJNIRef( int offset ) {
    if (offset > JNIRefsTop) {
      VM.sysWrite("JNI ERROR: getJNIRef for illegal offset > TOP, ");
      VM.sysWrite(offset); 
      VM.sysWrite("(top is ");
      VM.sysWrite(JNIRefsTop);
      VM.sysWrite(")\n");
    }
    
    JNIRefs[ offset>>2 ] = 0;

    if (offset == JNIRefsTop) JNIRefsTop -= 4;
  }

  // record an exception as pending so that it will be delivered on the return
  // to the Java caller;  clear the exception by recording null
  // Taken:  an exception or error
  // Returned:  nothing
  //
  public void recordException(Throwable e) {
    // don't overwrite the first exception except to clear it
    if (pendingException==null || e==null)
      pendingException = e;
  }

  // return the pending exception
  // Taken:  nothing
  // Returned:  an exception or error
  //
  public Throwable getException() {
    return pendingException;
  }

  //
  // get the address of the JNIFunctions array, which should be in the JTOC
  // Taken:    nothing 
  // Returned: the address of the JNIFunctions array 
  // 
  public VM_Address getJNIenvAddress() {
    return JNIEnvAddress;
  }

  public INSTRUCTION[] getInstructions(int id) {    
    return JNIFunctions[id][IP];
  }

  //
  // get the JNI index for a function name
  // Taken:    a JNI function name
  // Returned: the index for this function, -1 if not found
  //
  private static int indexOf(String functionName) {
    for (int i=0; i<FUNCTIONCOUNT; i++) {
      if (names[i].equals(functionName))
	return i;
    }
    return -1;
  }





  private static String[] setNames() {
    names = new String[FUNCTIONCOUNT];
    names[0]                             = new String("undefined");
    names[RESERVED0]                     = new String("reserved0")                     ;	  
    names[RESERVED1]                     = new String("reserved1")                     ;	  
    names[RESERVED2]                     = new String("reserved2")                     ;	  
    names[RESERVED3]                     = new String("reserved3")                     ;	  
    names[GETVERSION]                    = new String("GetVersion")                    ;	  
    names[DEFINECLASS]                   = new String("DefineClass")                   ;	  
    names[FINDCLASS]                     = new String("FindClass")                     ;	  
    names[FROMREFLECTEDMETHOD]         	 = new String("FromReflectedMethod"); //  JDK1.2, #7      
    names[FROMREFLECTEDFIELD]          	 = new String("FromReflectedField");  //  JDK1.2, #8      
    names[TOREFLECTEDMETHOD]           	 = new String("ToReflectedMethod");   //  JDK1.2, #9      
    names[GETSUPERCLASS]                 = new String("GetSuperclass")                 ;	  
    names[ISASSIGNABLEFROM]              = new String("IsAssignableFrom")              ;	  
    names[TOREFLECTEDFIELD]            	 = new String("ToReflectedField");    //  JDK1.2, #12      
    names[THROW]                         = new String("Throw")                         ;	  
    names[THROWNEW]                      = new String("ThrowNew")                      ;	  
    names[EXCEPTIONOCCURRED]             = new String("ExceptionOccurred")             ;	  
    names[EXCEPTIONDESCRIBE]             = new String("ExceptionDescribe")             ;	  
    names[EXCEPTIONCLEAR]                = new String("ExceptionClear")                ;	  
    names[FATALERROR]                    = new String("FatalError")                    ;	  
    names[PUSHLOCALFRAME]              	 = new String("PushLocalFrame");      //  JDK1.2, #19      
    names[POPLOCALFRAME]               	 = new String("PopLocalFrame");       //  JDK1.2, #20      
    names[NEWGLOBALREF]                  = new String("NewGlobalRef")                  ;	  
    names[DELETEGLOBALREF]               = new String("DeleteGlobalRef")               ;	  
    names[DELETELOCALREF]                = new String("DeleteLocalRef")                ;	  
    names[ISSAMEOBJECT]                  = new String("IsSameObject")                  ;	  
    names[NEWLOCALREF]                 	 = new String("NewLocalRef");         //  JDK1.2, #25      
    names[ENSURELOCALCAPACITY]         	 = new String("EnsureLocalCapacity"); //  JDK1.2, #26   
    names[ALLOCOBJECT]                   = new String("AllocObject")                   ;	  
    names[NEWOBJECT]                     = new String("NewObject")                     ;	  
    names[NEWOBJECTV]                    = new String("NewObjectV")                    ;	  
    names[NEWOBJECTA]                    = new String("NewObjectA")                    ;	  
    names[GETOBJECTCLASS]                = new String("GetObjectClass")                ;	  
    names[ISINSTANCEOF]                  = new String("IsInstanceOf")                  ;	  
    names[GETMETHODID]                   = new String("GetMethodID")                   ;	  
    names[CALLOBJECTMETHOD]              = new String("CallObjectMethod")              ;	  
    names[CALLOBJECTMETHODV]             = new String("CallObjectMethodV")             ;	  
    names[CALLOBJECTMETHODA]             = new String("CallObjectMethodA")             ;	  
    names[CALLBOOLEANMETHOD]             = new String("CallBooleanMethod")             ;	  
    names[CALLBOOLEANMETHODV]            = new String("CallBooleanMethodV")            ;	  
    names[CALLBOOLEANMETHODA]            = new String("CallBooleanMethodA")            ;	  
    names[CALLBYTEMETHOD]                = new String("CallByteMethod")                ;	  
    names[CALLBYTEMETHODV]               = new String("CallByteMethodV")               ;	  
    names[CALLBYTEMETHODA]               = new String("CallByteMethodA")               ;	  
    names[CALLCHARMETHOD]                = new String("CallCharMethod")                ;	  
    names[CALLCHARMETHODV]               = new String("CallCharMethodV")               ;	  
    names[CALLCHARMETHODA]               = new String("CallCharMethodA")               ;	  
    names[CALLSHORTMETHOD]               = new String("CallShortMethod")               ;	  
    names[CALLSHORTMETHODV]              = new String("CallShortMethodV")              ;	  
    names[CALLSHORTMETHODA]              = new String("CallShortMethodA")              ;	  
    names[CALLINTMETHOD]                 = new String("CallIntMethod")                 ;	  
    names[CALLINTMETHODV]                = new String("CallIntMethodV")                ;	  
    names[CALLINTMETHODA]                = new String("CallIntMethodA")                ;	  
    names[CALLLONGMETHOD]                = new String("CallLongMethod")                ;	  
    names[CALLLONGMETHODV]               = new String("CallLongMethodV")               ;	  
    names[CALLLONGMETHODA]               = new String("CallLongMethodA")               ;	  
    names[CALLFLOATMETHOD]               = new String("CallFloatMethod")               ;	  
    names[CALLFLOATMETHODV]              = new String("CallFloatMethodV")              ;	  
    names[CALLFLOATMETHODA]              = new String("CallFloatMethodA")              ;	  
    names[CALLDOUBLEMETHOD]              = new String("CallDoubleMethod")              ;	  
    names[CALLDOUBLEMETHODV]             = new String("CallDoubleMethodV")             ;	  
    names[CALLDOUBLEMETHODA]             = new String("CallDoubleMethodA")             ;	  
    names[CALLVOIDMETHOD]                = new String("CallVoidMethod")                ;	  
    names[CALLVOIDMETHODV]               = new String("CallVoidMethodV")               ;	  
    names[CALLVOIDMETHODA]               = new String("CallVoidMethodA")               ;	  
    names[CALLNONVIRTUALOBJECTMETHOD]    = new String("CallNonvirtualObjectMethod")    ;	  
    names[CALLNONVIRTUALOBJECTMETHODV]   = new String("CallNonvirtualObjectMethodV")   ;	  
    names[CALLNONVIRTUALOBJECTMETHODA]   = new String("CallNonvirtualObjectMethodA")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHOD]   = new String("CallNonvirtualBooleanMethod")   ;	  
    names[CALLNONVIRTUALBOOLEANMETHODV]  = new String("CallNonvirtualBooleanMethodV")  ;	  
    names[CALLNONVIRTUALBOOLEANMETHODA]  = new String("CallNonvirtualBooleanMethodA")  ;	  
    names[CALLNONVIRTUALBYTEMETHOD]      = new String("CallNonvirtualByteMethod")      ;	  
    names[CALLNONVIRTUALBYTEMETHODV]     = new String("CallNonvirtualByteMethodV")     ;	  
    names[CALLNONVIRTUALBYTEMETHODA]     = new String("CallNonvirtualByteMethodA")     ;	  
    names[CALLNONVIRTUALCHARMETHOD]      = new String("CallNonvirtualCharMethod")      ;	  
    names[CALLNONVIRTUALCHARMETHODV]     = new String("CallNonvirtualCharMethodV")     ;	  
    names[CALLNONVIRTUALCHARMETHODA]     = new String("CallNonvirtualCharMethodA")     ;	  
    names[CALLNONVIRTUALSHORTMETHOD]     = new String("CallNonvirtualShortMethod")     ;	  
    names[CALLNONVIRTUALSHORTMETHODV]    = new String("CallNonvirtualShortMethodV")    ;	  
    names[CALLNONVIRTUALSHORTMETHODA]    = new String("CallNonvirtualShortMethodA")    ;	  
    names[CALLNONVIRTUALINTMETHOD]       = new String("CallNonvirtualIntMethod")       ;	  
    names[CALLNONVIRTUALINTMETHODV]      = new String("CallNonvirtualIntMethodV")      ;	  
    names[CALLNONVIRTUALINTMETHODA]      = new String("CallNonvirtualIntMethodA")      ;	  
    names[CALLNONVIRTUALLONGMETHOD]      = new String("CallNonvirtualLongMethod")      ;	  
    names[CALLNONVIRTUALLONGMETHODV]     = new String("CallNonvirtualLongMethodV")     ;	  
    names[CALLNONVIRTUALLONGMETHODA]     = new String("CallNonvirtualLongMethodA")     ;	  
    names[CALLNONVIRTUALFLOATMETHOD]     = new String("CallNonvirtualFloatMethod")     ;	  
    names[CALLNONVIRTUALFLOATMETHODV]    = new String("CallNonvirtualFloatMethodV")    ;	  
    names[CALLNONVIRTUALFLOATMETHODA]    = new String("CallNonvirtualFloatMethodA")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHOD]    = new String("CallNonvirtualDoubleMethod")    ;	  
    names[CALLNONVIRTUALDOUBLEMETHODV]   = new String("CallNonvirtualDoubleMethodV")   ;	  
    names[CALLNONVIRTUALDOUBLEMETHODA]   = new String("CallNonvirtualDoubleMethodA")   ;	  
    names[CALLNONVIRTUALVOIDMETHOD]      = new String("CallNonvirtualVoidMethod")      ;	  
    names[CALLNONVIRTUALVOIDMETHODV]     = new String("CallNonvirtualVoidMethodV")     ;	  
    names[CALLNONVIRTUALVOIDMETHODA]     = new String("CallNonvirtualVoidMethodA")     ;	  
    names[GETFIELDID]                    = new String("GetFieldID")                    ;	  
    names[GETOBJECTFIELD]                = new String("GetObjectField")                ;	  
    names[GETBOOLEANFIELD]               = new String("GetBooleanField")               ;	  
    names[GETBYTEFIELD]                  = new String("GetByteField")                  ;	  
    names[GETCHARFIELD]                  = new String("GetCharField")                  ;	  
    names[GETSHORTFIELD]                 = new String("GetShortField")                 ;	  
    names[GETINTFIELD]                   = new String("GetIntField")                   ;	  
    names[GETLONGFIELD]                  = new String("GetLongField")                  ;	  
    names[GETFLOATFIELD]                 = new String("GetFloatField")                 ;	  
    names[GETDOUBLEFIELD]                = new String("GetDoubleField")                ;	  
    names[SETOBJECTFIELD]                = new String("SetObjectField")                ;	  
    names[SETBOOLEANFIELD]               = new String("SetBooleanField")               ;	  
    names[SETBYTEFIELD]                  = new String("SetByteField")                  ;	  
    names[SETCHARFIELD]                  = new String("SetCharField")                  ;	  
    names[SETSHORTFIELD]                 = new String("SetShortField")                 ;	  
    names[SETINTFIELD]                   = new String("SetIntField")                   ;	  
    names[SETLONGFIELD]                  = new String("SetLongField")                  ;	  
    names[SETFLOATFIELD]                 = new String("SetFloatField")                 ;	  
    names[SETDOUBLEFIELD]                = new String("SetDoubleField")                ;	  
    names[GETSTATICMETHODID]             = new String("GetStaticMethodID")             ;	  
    names[CALLSTATICOBJECTMETHOD]        = new String("CallStaticObjectMethod")        ;	  
    names[CALLSTATICOBJECTMETHODV]       = new String("CallStaticObjectMethodV")       ;	  
    names[CALLSTATICOBJECTMETHODA]       = new String("CallStaticObjectMethodA")       ;	  
    names[CALLSTATICBOOLEANMETHOD]       = new String("CallStaticBooleanMethod")       ;	  
    names[CALLSTATICBOOLEANMETHODV]      = new String("CallStaticBooleanMethodV")      ;	  
    names[CALLSTATICBOOLEANMETHODA]      = new String("CallStaticBooleanMethodA")      ;	  
    names[CALLSTATICBYTEMETHOD]          = new String("CallStaticByteMethod")          ;	  
    names[CALLSTATICBYTEMETHODV]         = new String("CallStaticByteMethodV")         ;	  
    names[CALLSTATICBYTEMETHODA]         = new String("CallStaticByteMethodA")         ;	  
    names[CALLSTATICCHARMETHOD]          = new String("CallStaticCharMethod")          ;	  
    names[CALLSTATICCHARMETHODV]         = new String("CallStaticCharMethodV")         ;	  
    names[CALLSTATICCHARMETHODA]         = new String("CallStaticCharMethodA")         ;	  
    names[CALLSTATICSHORTMETHOD]         = new String("CallStaticShortMethod")         ;	  
    names[CALLSTATICSHORTMETHODV]        = new String("CallStaticShortMethodV")        ;	  
    names[CALLSTATICSHORTMETHODA]        = new String("CallStaticShortMethodA")        ;	  
    names[CALLSTATICINTMETHOD]           = new String("CallStaticIntMethod")           ;	  
    names[CALLSTATICINTMETHODV]          = new String("CallStaticIntMethodV")          ;	  
    names[CALLSTATICINTMETHODA]          = new String("CallStaticIntMethodA")          ;	  
    names[CALLSTATICLONGMETHOD]          = new String("CallStaticLongMethod")          ;	  
    names[CALLSTATICLONGMETHODV]         = new String("CallStaticLongMethodV")         ;	  
    names[CALLSTATICLONGMETHODA]         = new String("CallStaticLongMethodA")         ;	  
    names[CALLSTATICFLOATMETHOD]         = new String("CallStaticFloatMethod")         ;	  
    names[CALLSTATICFLOATMETHODV]        = new String("CallStaticFloatMethodV")        ;	  
    names[CALLSTATICFLOATMETHODA]        = new String("CallStaticFloatMethodA")        ;	  
    names[CALLSTATICDOUBLEMETHOD]        = new String("CallStaticDoubleMethod")        ;	  
    names[CALLSTATICDOUBLEMETHODV]       = new String("CallStaticDoubleMethodV")       ;	  
    names[CALLSTATICDOUBLEMETHODA]       = new String("CallStaticDoubleMethodA")       ;	  
    names[CALLSTATICVOIDMETHOD]          = new String("CallStaticVoidMethod")          ;	  
    names[CALLSTATICVOIDMETHODV]         = new String("CallStaticVoidMethodV")         ;	  
    names[CALLSTATICVOIDMETHODA]         = new String("CallStaticVoidMethodA")         ;	  
    names[GETSTATICFIELDID]              = new String("GetStaticFieldID")              ;	  
    names[GETSTATICOBJECTFIELD]          = new String("GetStaticObjectField")          ;	  
    names[GETSTATICBOOLEANFIELD]         = new String("GetStaticBooleanField")         ;	  
    names[GETSTATICBYTEFIELD]            = new String("GetStaticByteField")            ;	  
    names[GETSTATICCHARFIELD]            = new String("GetStaticCharField")            ;	  
    names[GETSTATICSHORTFIELD]           = new String("GetStaticShortField")           ;	  
    names[GETSTATICINTFIELD]             = new String("GetStaticIntField")             ;	  
    names[GETSTATICLONGFIELD]            = new String("GetStaticLongField")            ;	  
    names[GETSTATICFLOATFIELD]           = new String("GetStaticFloatField")           ;	  
    names[GETSTATICDOUBLEFIELD]          = new String("GetStaticDoubleField")          ;	  
    names[SETSTATICOBJECTFIELD]          = new String("SetStaticObjectField")          ;	  
    names[SETSTATICBOOLEANFIELD]         = new String("SetStaticBooleanField")         ;	  
    names[SETSTATICBYTEFIELD]            = new String("SetStaticByteField")            ;	  
    names[SETSTATICCHARFIELD]            = new String("SetStaticCharField")            ;	  
    names[SETSTATICSHORTFIELD]           = new String("SetStaticShortField")           ;	  
    names[SETSTATICINTFIELD]             = new String("SetStaticIntField")             ;	  
    names[SETSTATICLONGFIELD]            = new String("SetStaticLongField")            ;	  
    names[SETSTATICFLOATFIELD]           = new String("SetStaticFloatField")           ;	  
    names[SETSTATICDOUBLEFIELD]          = new String("SetStaticDoubleField")          ;	  
    names[NEWSTRING]                     = new String("NewString")                     ;	  
    names[GETSTRINGLENGTH]               = new String("GetStringLength")               ;	  
    names[GETSTRINGCHARS]                = new String("GetStringChars")                ;	  
    names[RELEASESTRINGCHARS]            = new String("ReleaseStringChars")            ;	  
    names[NEWSTRINGUTF]                  = new String("NewStringUTF")                  ;	  
    names[GETSTRINGUTFLENGTH]            = new String("GetStringUTFLength")            ;	  
    names[GETSTRINGUTFCHARS]             = new String("GetStringUTFChars")             ;	  
    names[RELEASESTRINGUTFCHARS]         = new String("ReleaseStringUTFChars")         ;	  
    names[GETARRAYLENGTH]                = new String("GetArrayLength")                ;	  
    names[NEWOBJECTARRAY]                = new String("NewObjectArray")                ;	  
    names[GETOBJECTARRAYELEMENT]         = new String("GetObjectArrayElement")         ;	  
    names[SETOBJECTARRAYELEMENT]         = new String("SetObjectArrayElement")         ;	  
    names[NEWBOOLEANARRAY]               = new String("NewBooleanArray")               ;	  
    names[NEWBYTEARRAY]                  = new String("NewByteArray")                  ;	  
    names[NEWCHARARRAY]                  = new String("NewCharArray")                  ;	  
    names[NEWSHORTARRAY]                 = new String("NewShortArray")                 ;	  
    names[NEWINTARRAY]                   = new String("NewIntArray")                   ;	  
    names[NEWLONGARRAY]                  = new String("NewLongArray")                  ;	  
    names[NEWFLOATARRAY]                 = new String("NewFloatArray")                 ;	  
    names[NEWDOUBLEARRAY]                = new String("NewDoubleArray")                ;	  
    names[GETBOOLEANARRAYELEMENTS]       = new String("GetBooleanArrayElements")       ;	  
    names[GETBYTEARRAYELEMENTS]          = new String("GetByteArrayElements")          ;	  
    names[GETCHARARRAYELEMENTS]          = new String("GetCharArrayElements")          ;	  
    names[GETSHORTARRAYELEMENTS]         = new String("GetShortArrayElements")         ;	  
    names[GETINTARRAYELEMENTS]           = new String("GetIntArrayElements")           ;	  
    names[GETLONGARRAYELEMENTS]          = new String("GetLongArrayElements")          ;	  
    names[GETFLOATARRAYELEMENTS]         = new String("GetFloatArrayElements")         ;	  
    names[GETDOUBLEARRAYELEMENTS]        = new String("GetDoubleArrayElements")        ;	  
    names[RELEASEBOOLEANARRAYELEMENTS]   = new String("ReleaseBooleanArrayElements")   ;	  
    names[RELEASEBYTEARRAYELEMENTS]      = new String("ReleaseByteArrayElements")      ;	  
    names[RELEASECHARARRAYELEMENTS]      = new String("ReleaseCharArrayElements")      ;	  
    names[RELEASESHORTARRAYELEMENTS]     = new String("ReleaseShortArrayElements")     ;	  
    names[RELEASEINTARRAYELEMENTS]       = new String("ReleaseIntArrayElements")       ;	  
    names[RELEASELONGARRAYELEMENTS]      = new String("ReleaseLongArrayElements")      ;	  
    names[RELEASEFLOATARRAYELEMENTS]     = new String("ReleaseFloatArrayElements")     ;	  
    names[RELEASEDOUBLEARRAYELEMENTS]    = new String("ReleaseDoubleArrayElements")    ;	  
    names[GETBOOLEANARRAYREGION]         = new String("GetBooleanArrayRegion")         ;	  
    names[GETBYTEARRAYREGION]            = new String("GetByteArrayRegion")            ;	  
    names[GETCHARARRAYREGION]            = new String("GetCharArrayRegion")            ;	  
    names[GETSHORTARRAYREGION]           = new String("GetShortArrayRegion")           ;	  
    names[GETINTARRAYREGION]             = new String("GetIntArrayRegion")             ;	  
    names[GETLONGARRAYREGION]            = new String("GetLongArrayRegion")            ;	  
    names[GETFLOATARRAYREGION]           = new String("GetFloatArrayRegion")           ;	  
    names[GETDOUBLEARRAYREGION]          = new String("GetDoubleArrayRegion")          ;	  
    names[SETBOOLEANARRAYREGION]         = new String("SetBooleanArrayRegion")         ;	  
    names[SETBYTEARRAYREGION]            = new String("SetByteArrayRegion")            ;	  
    names[SETCHARARRAYREGION]            = new String("SetCharArrayRegion")            ;	  
    names[SETSHORTARRAYREGION]           = new String("SetShortArrayRegion")           ;	  
    names[SETINTARRAYREGION]             = new String("SetIntArrayRegion")             ;	  
    names[SETLONGARRAYREGION]            = new String("SetLongArrayRegion")            ;	  
    names[SETFLOATARRAYREGION]           = new String("SetFloatArrayRegion")           ;	  
    names[SETDOUBLEARRAYREGION]          = new String("SetDoubleArrayRegion")          ;	  
    names[REGISTERNATIVES]               = new String("RegisterNatives")               ;	  
    names[UNREGISTERNATIVES]             = new String("UnregisterNatives")             ;	  
    names[MONITORENTER]                  = new String("MonitorEnter")                  ;	  
    names[MONITOREXIT]                   = new String("MonitorExit")                   ;	  
    names[GETJAVAVM]                     = new String("GetJavaVM")                     ;	  
    names[GETSTRINGREGION]             	 = new String("GetStringRegion");           // JDK 1.2, #220
    names[GETSTRINGUTFREGION]         	 = new String("GetStringUTFRegion");        // JDK 1.2, #221
    names[GETPRIMITIVEARRAYCRITICAL]   	 = new String("GetPrimitiveArrayCritical"); // JDK 1.2, #222
    names[RELEASEPRIMITIVEARRAYCRITICAL] = new String("ReleasePrimitiveArrayCritical"); // JDK 1.2, #223
    names[GETSTRINGCRITICAL]           	 = new String("GetStringCritical");         // JDK 1.2, # 224
    names[RELEASESTRINGCRITICAL]       	 = new String("ReleaseStringCritical");     // JDK 1.2, #225
    names[NEWWEAKGLOBALREF]            	 = new String("NewWeakGlobalRef");    	    // JDK 1.2, #226
    names[DELETEWEAKGLOBALREF]         	 = new String("DeleteWeakGlobalRef"); 	    // JDK 1.2, #227
    names[EXCEPTIONCHECK]              	 = new String("ExceptionCheck");      	    // JDK 1.2, #228

    return names;

  }


  /*****************************************************************************
   * Utility function called from VM_JNIFunction
   * (cannot be placed in VM_JNIFunction because methods there are specially compiled
   * to be called from native)
   *****************************************************************************/


  /**
   * Get a VM_Field of an object given the index for this field
   * @param obj an Object
   * @param fieldIndex an index into the VM_Field array that describes the fields of this object
   * @return the VM_Field pointed to by the index, or null if the index or the object is invalid
   *
   */
  public static VM_Field getFieldAtIndex (Object obj, int fieldIndex) {   
    // VM.sysWrite("GetObjectField: field at index " + fieldIndex + "\n");

    VM_Type objType = VM_Magic.getObjectType(obj);
    if (objType.isClassType()) {
      VM_Field[] fields = objType.asClass().getInstanceFields();
      if (fieldIndex>=fields.length) {
	return null;                                      // invalid field index
      } else {
	return fields[fieldIndex];
      } 
    } else {
      // object is not a class type, probably an array or an invalid reference
      return null;
    }
  }



  /**
   * Common code shared by the JNI functions NewObjectA, NewObjectV, NewObject
   * (object creation)
   * @param methodID the method ID for a constructor
   * @return a new object created by the specified constructor
   */
  public static Object invokeInitializer(Class cls, int methodID, VM_Address argAddress, 
					 boolean isJvalue, boolean isDotDotStyle) 
    throws Exception {

    // get the parameter list as Java class
    VM_Method mth = VM_MethodDictionary.getValue(methodID);
    VM_Type[] argTypes = mth.getParameterTypes();
    Class[]   argClasses = new Class[argTypes.length];
    for (int i=0; i<argClasses.length; i++) {
      argClasses[i] = argTypes[i].getClassForType();
    }

    Constructor constMethod = cls.getConstructor(argClasses);
    if (constMethod==null)
      throw new Exception("Constructor not found");


    // Package the parameters for the constructor
    VM_Address varargAddress;
    if (isDotDotStyle) 
      // flag is false because this JNI function has 3 args before the var args
      varargAddress = pushVarArgToSpillArea(methodID, false);    
    else
      varargAddress = argAddress;

    Object argObjs[];
    if (isJvalue)
      argObjs = packageParameterFromJValue(mth, argAddress);
    else
      argObjs = packageParameterFromVarArg(mth, varargAddress);

    // construct the new object
    Object newobj = constMethod.newInstance(argObjs);
    
    return newobj;

  }


  /**
   * Common code shared by the JNI functions CallStatic<type>Method
   * (static method invocation)
   * @param methodID the method ID
   * @param expectReturnType the return type of the method to be invoked
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(int methodID, VM_Type expectReturnType)
    throws Exception {
    
    VM_Address varargAddress = pushVarArgToSpillArea(methodID, false);    
    return packageAndInvoke(null, methodID, varargAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>Method
   * (virtual method invocation)
   * @param obj the object instance 
   * @param methodID the method ID
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args  true if the calling JNI Function takes 4 args before the vararg
   *                   false if the calling JNI Function takes 3 args before the vararg
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithDotDotVarArg(Object obj, int methodID, 
					      VM_Type expectReturnType, boolean skip4Args)
    throws Exception {

      VM_Address varargAddress = pushVarArgToSpillArea(methodID, skip4Args);    
      return packageAndInvoke(obj, methodID, varargAddress, expectReturnType, skip4Args, true);

  }


  /**
   * This method supports var args passed from C
   *
   * In the AIX C convention, the caller keeps the first 8 words in registers and 
   * the rest in the spill area in the caller frame.  The callee will push the values
   * in registers out to the spill area of the caller frame and use the beginning 
   * address of this spill area as the var arg address
   *
   * For the JNI functions that takes var args, their prolog code will save the
   * var arg in the glue frame because the values in the register may be lost by 
   * subsequent calls.
   *
   * This method copies the var arg values that were saved earlier in glue frame into
   * the spill area of the original caller, thereby doing the work that the callee
   * normally performs in the AIX C convention.
   *
   * NOTE: This method contains internal stack pointer.
   * For now we assume that the stack will not be relocatable while native code is running
   * because native code can hold an address into the stack, so this code is OK,
   * but this is an issue to be resolved later
   *
   * NOTE:  this method assumes that it is immediately above the 
   * invokeWithDotDotVarArg frame, the JNI frame, the glue frame and 
   * the C caller frame in the respective order.  
   * Therefore, this method will not work if called from anywhere else
   *
   *
   *
   *   |  fp  | <- VM_JNIEnvironment.pushVarArgToSpillArea
   *   | mid  |
   *   | xxx  |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- VM_JNIEnvironment.invokeWithDotDotVarArg frame
   *   | mid  |
   *   | xxx  |
   *   |      |
   *   |      |
   *   |      |
   *   |------|   
   *   |  fp  | <- JNI method frame
   *   | mid  |
   *   | xxx  |
   *   |      |
   *   |      |
   *   |      |
   *   |------|
   *   |  fp  | <- glue frame
   *   | mid  |
   *   + xxx  +
   *   | r3   |   volatile save area
   *   | r4   |
   *   | r5   |
   *   | r6   |   vararg GPR[6-10]save area   <- VARARG_AREA_OFFSET
   *   | r7   |
   *   | r8   |
   *   | r9   |
   *   | r10  |
   *   | fpr1 |   vararg FPR[1-3] save area (also used as volatile FPR[1-6] save area)
   *   | fpr2 |
   *   | fpr3 |
   *   | fpr4 |
   *   | fpr5 |
   *   + fpr6 +
   *   | r13  |   nonvolatile GPR[13-31] save area
   *   | ...  |
   *   + r31  +
   *   | fpr14|   nonvolatile FPR[14-31] save area
   *   | ...  |
   *   | fpr31|
   *   |topjav|   offset to preceding Java to C glue frame
   *   |------|  
   *   | fp   | <- Native C caller frame
   *   | cr   |
   *   | lr   |
   *   | resv |
   *   | resv |
   *   + toc  +
   *   |   0  |    spill area initially not filled
   *   |   1  |    to be filled by this method
   *   |   2  |
   *   |   3  |
   *   |   4  |
   *   |   5  |
   *   |   6  |
   *   |   7  |
   *   |   8  |    spill area already filled by caller
   *   |   9  |
   *   |      |
   *   |      |
   *   |      |
   *
   * @param methodID an index into VM_MethodDictionary
   * @param skip4Args if true, the calling JNI function has 4 args before the vararg
   *                  if false, the calling JNI function has 3 args before the vararg
   * @return the starting address of the vararg in the caller stack frame
   */
  private static VM_Address pushVarArgToSpillArea(int methodID, boolean skip4Args) {

    int glueFrameSize = JNI_GLUE_FRAME_SIZE;

    // get the FP for this stack frame and traverse 2 frames to get to the glue frame
    VM_Address fp = VM_Address.fromInt(VM_Magic.getMemoryWord(VM_Magic.getFramePointer().add(VM_Constants.STACKFRAME_FRAME_POINTER_OFFSET)));
    fp = VM_Address.fromInt(VM_Magic.getMemoryWord(fp.add(VM_Constants.STACKFRAME_FRAME_POINTER_OFFSET)));
    VM_Address gluefp = VM_Address.fromInt(VM_Magic.getMemoryWord(fp.add(VM_Constants.STACKFRAME_FRAME_POINTER_OFFSET)));

    // compute the offset into the area where the vararg GPR[6-10] and FPR[1-3] are saved
    // skipping the args which are not part of the arguments for the target method
    // For Call<type>Method functions and NewObject, skip 3 args
    // For CallNonvirtual<type>Method functions, skip 4 args
    int varargGPROffset = VARARG_AREA_OFFSET + (skip4Args ? 4 : 0);
    int varargFPROffset = varargGPROffset + 5*4 ;

    // compute the offset into the spill area of the native caller frame, 
    // skipping the args which are not part of the arguments for the target method
    // For Call<type>Method functions, skip 3 args
    // For CallNonvirtual<type>Method functions, skip 4 args
    int spillAreaLimit  = glueFrameSize + AIX_FRAME_HEADER_SIZE + 8*4;
    int spillAreaOffset = glueFrameSize + AIX_FRAME_HEADER_SIZE + 
                          (skip4Args ? 4*4 : 3*4);

    // address to return pointing to the var arg list
    VM_Address varargAddress = gluefp.add(spillAreaOffset);

    // VM.sysWrite("pushVarArgToSpillArea:  var arg at " + 
    // 		   VM.intAsHexString(varargAddress) + "\n");
 
    VM_Method targetMethod = VM_MethodDictionary.getValue(methodID);
    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;

    for (int i=0; i<argCount && spillAreaOffset<spillAreaLimit ; i++) {
      int hiword, loword;

      if (argTypes[i].isFloatType() || argTypes[i].isDoubleType()) {
	// move 2 words from the vararg FPR save area into the spill area of the caller
	hiword = VM_Magic.getMemoryWord(gluefp.add(varargFPROffset));
	varargFPROffset+=4;
	loword = VM_Magic.getMemoryWord(gluefp.add(varargFPROffset));
	varargFPROffset+=4;
	VM_Magic.setMemoryWord(gluefp.add(spillAreaOffset), hiword);
	spillAreaOffset+=4;
	VM_Magic.setMemoryWord(gluefp.add(spillAreaOffset), loword);
	spillAreaOffset+=4;
      } 

      else if (argTypes[i].isLongType()) {
	// move 2 words from the vararg GPR save area into the spill area of the caller
	hiword = VM_Magic.getMemoryWord(gluefp.add(varargGPROffset));
	varargGPROffset+=4;
	VM_Magic.setMemoryWord(gluefp.add(spillAreaOffset), hiword);
	spillAreaOffset+=4;
	// this covers the case when the long value straddles the spill boundary
	if (spillAreaOffset<spillAreaLimit) {
	  loword = VM_Magic.getMemoryWord(gluefp.add(varargGPROffset));
	  varargGPROffset+=4;
	  VM_Magic.setMemoryWord(gluefp.add(spillAreaOffset), loword);
	  spillAreaOffset+=4;
	}
      }

      else {
	hiword = VM_Magic.getMemoryWord(gluefp.add(varargGPROffset));
	varargGPROffset+=4;
	VM_Magic.setMemoryWord(gluefp.add(spillAreaOffset), hiword);
	spillAreaOffset+=4;
      }

    }

    // At this point, all the vararg values should be in the spill area in the caller frame
    // return the address of the beginning of the vararg to use in invoking the target method
    return varargAddress;

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodV
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {

    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, true);

  }

  /**
   * Common code shared by the JNI functions Call<type>MethodV
   * @param obj the object instance 
   * @param methodID the method ID
   * @param argAddress a raw address for the variable argument list
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithVarArg(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {

    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, true);

  }

  /**
   * Common code shared by the JNI functions CallStatic<type>MethodA
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(int methodID, VM_Address argAddress, VM_Type expectReturnType) 
    throws Exception {
    return packageAndInvoke(null, methodID, argAddress, expectReturnType, false, false);
  }

  /**
   * Common code shared by the JNI functions Call<type>MethodA
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args received from the JNI function, passed on to VM_Reflection.invoke()
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object invokeWithJValue(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args) 
    throws Exception {
    return packageAndInvoke(obj, methodID, argAddress, expectReturnType, skip4Args, false);
  }


  /**
   * Common code shared by invokeWithJValue, invokeWithVarArg and invokeWithDotDotVarArg
   * @param obj the object instance 
   * @param methodID an index into the VM_MethodDictionary
   * @param argAddress a raw address for the argument array
   * @param expectReturnType the return type for checking purpose
   * @param skip4Args This flag is received from the JNI function and passed directly to 
   *                     VM_Reflection.invoke().  
   *                     It is true if the actual method is to be invoked, which could be
   *                     from the superclass.
   *                     It is false if the method from the real class of the object 
   *                     is to be invoked, which may not be the actual method specified by methodID
   * @param isVarArg  This flag describes whether the array of parameters is in var arg format or
   *                  jvalue format
   * @return an object that may be the return object or a wrapper for the primitive return value 
   */
  public static Object packageAndInvoke(Object obj, int methodID, VM_Address argAddress, 
					VM_Type expectReturnType, boolean skip4Args, 
					boolean isVarArg) 
    throws Exception {
  
    VM_Method targetMethod;
    int returnValue;

    // VM.sysWrite("JNI CallXXXMethod:  method ID " + methodID + " with args at " + 
    // 		   VM.intAsHexString(argAddress) + "\n");
    
    targetMethod = VM_MethodDictionary.getValue(methodID);
    VM_Type returnType = targetMethod.getReturnType();

    // VM.sysWrite("JNI CallXXXMethod:  " + targetMethod.getDeclaringClass().toString() +
    //		"." + targetMethod.getName().toString() + "\n");

    if (expectReturnType==null) {   // for reference return type 
      if (!returnType.isReferenceType())
	throw new Exception("Wrong return type for method: expect reference type instead of " + returnType);      
    } 
    else {    // for primitive return type
      if (returnType!=expectReturnType) 
	throw new Exception("Wrong return type for method: expect " + expectReturnType + 
			    " instead of " + returnType);
    }  

    // Repackage the arguments into an array of objects based on the signature of this method
    Object[] argObjectArray;
    if (isVarArg)
      argObjectArray = packageParameterFromVarArg(targetMethod, argAddress);
    else
      argObjectArray = packageParameterFromJValue(targetMethod, argAddress);

    // now invoke the method
    Object returnObj = VM_Reflection.invoke(targetMethod, obj, argObjectArray, skip4Args);
    
    return returnObj;
  }


  /**
   * Repackage the arguments passed as a variable argument list into an array of Object,
   * used by the JNI functions CallStatic<type>MethodV
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromVarArg(VM_Method targetMethod, VM_Address argAddress) {
    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromVarArg: packaging " + argCount + " arguments\n");

    VM_Address addr = argAddress;
    for (int i=0; i<argCount; i++) {
      int loword, hiword;
      hiword = VM_Magic.getMemoryWord(addr);

      // VM.sysWrite("JNI packageParameterFromVarArg:  arg " + i + " = " + hiword + 
      // " or " + VM.intAsHexString(hiword) + "\n");

      addr = addr.add(4);

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	// NOTE:  in VarArg convention, C compiler will expand a float to a double that occupy 2 words
	// so we have to extract it as a double and convert it back to a float
	loword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);                       
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapFloat((float) (Double.longBitsToDouble(doubleBits)));
	
      } else if (argTypes[i].isDoubleType()) {
	loword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	loword = VM_Magic.getMemoryWord(addr);
	addr = addr.add(4);
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte	
	argObjectArray[i] = VM_Reflection.wrapBoolean(hiword);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) hiword);

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) hiword);

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) hiword);

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(hiword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(hiword);

      } else {
	return null;
      }

    }

    return argObjectArray;
    

  }

  /**
   * Repackage the arguments passed as an array of jvalue into an array of Object,
   * used by the JNI functions CallStatic<type>MethodA
   * @param mth the target VM_Method
   * @param argAddress an address into the C space for the array of jvalue unions;  
   *                   each element is 2-word and holds the argument of the appropriate type
   * @return an Object array holding the arguments wrapped at Objects
   */
  static Object[] packageParameterFromJValue(VM_Method targetMethod, VM_Address argAddress) {
    VM_Type[] argTypes = targetMethod.getParameterTypes();
    int argCount = argTypes.length;
    Object[] argObjectArray = new Object[argCount];

    // get the VM_JNIEnvironment for this thread in case we need to dereference any object arg
    VM_JNIEnvironment env = VM_Thread.getCurrentThread().getJNIEnv();

    // VM.sysWrite("JNI packageParameterFromJValue: packaging " + argCount + " arguments\n");

    for (int i=0; i<argCount; i++) {
	
      VM_Address addr = argAddress.add(8*i);
      int hiword = VM_Magic.getMemoryWord(addr);
      int loword = VM_Magic.getMemoryWord(addr.add(4));

      // VM.sysWrite("JNI packageParameterFromJValue:  arg " + i + " = " + hiword + 
      //	  " or " + VM.intAsHexString(hiword) + "\n");

      // convert and wrap the argument according to the expected type

      if (argTypes[i].isFloatType()) {
	argObjectArray[i] = VM_Reflection.wrapFloat(Float.intBitsToFloat(hiword));

      } else if (argTypes[i].isDoubleType()) {
	long doubleBits = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapDouble(Double.longBitsToDouble(doubleBits));

      } else if (argTypes[i].isLongType()) { 
	long longValue = (((long) hiword) << 32) | (loword & 0xFFFFFFFFL);
	argObjectArray[i] = VM_Reflection.wrapLong(longValue);

      } else if (argTypes[i].isBooleanType()) {
	// the 0/1 bit is stored in the high byte	
	argObjectArray[i] = VM_Reflection.wrapBoolean((hiword & 0xFF000000) >>> 24);

      } else if (argTypes[i].isByteType()) {
	// the target byte is stored in the high byte
	argObjectArray[i] = VM_Reflection.wrapByte((byte) ((hiword & 0xFF000000) >>> 24));

      } else if (argTypes[i].isCharType()) {
	// char is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapChar((char) ((hiword & 0xFFFF0000) >>> 16));

      } else if (argTypes[i].isShortType()) {
	// short is stored in the high 2 bytes
	argObjectArray[i] = VM_Reflection.wrapShort((short) ((hiword & 0xFFFF0000) >>> 16));

      } else if (argTypes[i].isReferenceType()) {
	// for object, the arg is a JREF index, dereference to get the real object
	argObjectArray[i] =  env.getJNIRef(hiword);   

      } else if (argTypes[i].isIntType()) {
	argObjectArray[i] = VM_Reflection.wrapInt(hiword);

      } else {
	return null;
      }

    }

    return argObjectArray;
    
  }


  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java byte[] with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java byte[]
   */
  static byte[] createByteArrayFromC(VM_Address stringAddress) {
    int word;
    int length = 0;
    VM_Address addr = stringAddress;

    // scan the memory for the null termination of the string
    while (true) {
      word = VM_Magic.getMemoryWord(addr);
      int byte0 = ((word >> 24) & 0xFF);
      int byte1 = ((word >> 16) & 0xFF);
      int byte2 = ((word >> 8) & 0xFF);
      int byte3 = (word & 0xFF);
      if (byte0==0)
	break;
      length++;
      if (byte1==0) 
	break;
      length++;
      if (byte2==0)
	break;
      length++;
      if (byte3==0)
	break;
      length++;
      addr = addr.add(4);
    }

   byte[] contents = new byte[length];
   VM_Memory.memcopy(VM_Magic.objectAsAddress(contents), stringAddress, length);
   
   return contents;
  }


  /**
   * Given an address in C that points to a null-terminated string,
   * create a new Java String with a copy of the string
   * @param stringAddress an address in C space for a string
   * @return a new Java String
   */
  static String createStringFromC(VM_Address stringAddress) {

    byte[] contents = createByteArrayFromC( stringAddress );
    return new String(contents);

  }

  public void dumpJniRefsStack () {
    int jniRefOffset = JNIRefsTop;
    VM.sysWrite("\n* * dump of JNIEnvironment JniRefs Stack * *\n");
    VM.sysWrite("* JNIRefs = ");
    VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs));
    VM.sysWrite(" * JNIRefsTop = ");
    VM.sysWrite(JNIRefsTop,false);
    VM.sysWrite(" * JNIRefsSavedFP = ");
    VM.sysWrite(JNIRefsSavedFP);
    VM.sysWrite(".\n*\n");
    while ( jniRefOffset >= 0 ) {
      VM.sysWrite(jniRefOffset,false);
      VM.sysWrite(" ");
      VM.sysWrite(VM_Magic.objectAsAddress(JNIRefs).add(jniRefOffset));
      VM.sysWrite(" ");
      VM_GCUtil.dumpRef(VM_Address.fromInt(JNIRefs[jniRefOffset >> 2]));
      jniRefOffset -= 4;
    }
    VM.sysWrite("\n* * end of dump * *\n");
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Iterator for stack frames inserted at the transition from Java to
 * JNI Native C.  It will report JREFs associated with the executing
 * C frames which are in the "JREFs stack" attached to the executing
 * Threads JNIEnvironment.  It will update register location addresses
 * for the non-votatile registers to point to the register save area
 * in the transition frame.
 *
 * If GC happens, the saved non-volatile regs may get modified (ex. a ref
 * to a live object that gets moved), and a restore flag in the frame is
 * set to cause the returning Native code to restore those registers from
 * this save area.  If GC does not occur, the Native C code has restored
 * these regs, and the transition return code does not do the restore.
 *
 * @author Steve Smith
 */
import com.ibm.JikesRVM.memoryManagers.VM_GCMapIterator;

public final class VM_JNIGCMapIterator extends VM_GCMapIterator 
  implements VM_BaselineConstants,
	     VM_Uninterruptible {

  // non-volitile regs are saved at the end of the transition frame,
  // after the saved JTOC and SP, and preceeded by a GC flag.
  //
  // JNI Java to Native C transition frame...
  //
  //	 <-- | saved FP       |  <- this.framePtr
  //	 |   |    ...         |
  //	 |   |    ...         |
  //	 |   | GC flag        |
  //	 |   | saved affinity |
  //	 |   | proc reg       |
  //	 |   | non vol 17     |
  //	 |   |    ...         |
  //	 |   | non vol 31     |
  //	 |   | saved SP       |
  //	 |   | saved JTOC     |
  //	 --> |                |  <- callers FP
  //
  // The following constant is the offset from the callers FP to
  // the GC flag at the beginning of this area.  
  //

   // additional instance fields added by this subclass of VM_GCMapIterator
  private int[]  jniRefs;
  private int jniNextRef;
  private int jniFramePtr;
  private VM_Address jniSavedProcessorRegAddr;
  private VM_Address jniSavedReturnAddr;
  
  public VM_JNIGCMapIterator(int[] registerLocations) {
     this.registerLocations = registerLocations;
   }

  // Override newStackWalk() in parent class VM_GCMapIterator to
  // initialize iterator for scan of JNI JREFs stack of refs
  // Taken:    thread
  // Returned: nothing
  //
  public void newStackWalk(VM_Thread thread) {
    super.newStackWalk(thread);   // sets this.thread
    VM_JNIEnvironment env = this.thread.getJNIEnv();
    // the "primordial" thread, created by JDK in the bootimage, does not have
    // a JniEnv object, all threads created by the VM will.
    if (env != null) {
      this.jniRefs = env.JNIRefs;
      this.jniNextRef = env.JNIRefsTop;
      this.jniFramePtr = env.JNIRefsSavedFP;
      this.jniSavedProcessorRegAddr = VM_Address.zero(); // necessary so getNextRefAddr() can be used to report jniRefs in a "frame", without calling setup.  
    }
  }

  public void setupIterator(VM_CompiledMethod compiledMethod, 
		     int instructionOffset, 
		     VM_Address framePtr) { 
    this.framePtr = framePtr;
    // processore reg (R16) was saved in reg save area at offset -72 
    // from callers frameptr, and after GC will be used to set 
    // processor reg upon return to java.  it must be reported
    // so it will be relocated, if necessary
    //
    VM_Address callers_fp = VM_Address.fromInt(VM_Magic.getMemoryWord(this.framePtr));
    jniSavedProcessorRegAddr = callers_fp.sub(JNI_PR_OFFSET);
    jniSavedReturnAddr       = callers_fp.sub(JNI_PROLOG_RETURN_ADDRESS_OFFSET);

    // set the GC flag in the Java to C frame to indicate GC occurred
    // this forces saved non volatile regs to be restored from save area
    // where those containing refs have been relocated if necessary
    //
    VM_Magic.setMemoryWord(callers_fp.sub(JNI_GC_FLAG_OFFSET), 1);
  }
  
  // return (address of) next ref in the current "frame" on the
  // threads JNIEnvironment stack of refs	  
  // When at the end of the current frame, update register locations to point
  // to the non-volatile registers saved in the JNI transition frame.
  //
  public VM_Address getNextReferenceAddress() {
    int nextFP;
    VM_Address ref_address;

    if ( jniNextRef > jniFramePtr ) {
      ref_address = VM_Magic.objectAsAddress(jniRefs).add(jniNextRef);
      jniNextRef = jniNextRef - 4;
      return ref_address;
    }

    // report location of saved processor reg in the Java to C frame
    if ( !jniSavedProcessorRegAddr.isZero() ) {
      ref_address = jniSavedProcessorRegAddr;
      jniSavedProcessorRegAddr = VM_Address.zero();
      return ref_address;
    }

    // jniNextRef -> savedFramePtr for another "frame" of refs for another
    // sequence of Native C frames lower in the stack, or to 0 if this is the
    // last jni frame in the JNIRefs stack.  If more frames, initialize for a
    // later scan of those refs.
    //
    if ( jniFramePtr > 0) {
      jniFramePtr = jniRefs[jniFramePtr >> 2];
      jniNextRef = jniNextRef - 4 ;
    }
    
    // set register locations for non-volatiles to point to registers saved in
    // the JNI transition frame at a fixed negative offset from the callers FP.
    int registerLocation = VM_Magic.getMemoryWord(this.framePtr) - JNI_RVM_NONVOLATILE_OFFSET;

    for (int i = LAST_NONVOLATILE_GPR; i >= FIRST_NONVOLATILE_GPR - 1; --i) {
      registerLocations[i] = registerLocation;
      registerLocation -= 4;
    }

    return VM_Address.zero();  // no more refs to report
  }

  public VM_Address getNextReturnAddressAddress() {
    VM_Address  ref_address;
    if ( !jniSavedReturnAddr.isZero() ) {
      ref_address = jniSavedReturnAddr;
      jniSavedReturnAddr = VM_Address.zero();
      return ref_address;
    }

    return VM_Address.zero();
  }

  public void reset() {}
  
  public void cleanupPointers() {}
  
  public int getType() {
    return VM_CompiledMethod.JNI;
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine specific helper functions for dynamic linking.
 *
 * @author Bowen Alpern 
 * @author Derek Lieber
 * @date 17 Sep 1999  
 */
class VM_DynamicLinkerHelper implements VM_Constants, VM_Uninterruptible {

  /**
   * Reach up two stack frames into a frame that is compiled
   * with the DynamicBridge register protocol and grap 
   * the receiver object of the invoke (ie the first param).
   * NOTE: assumes that caller has disabled GC.
   */
  static Object getReceiverObject() throws VM_PragmaNoInline {
    // reach into register save area and fetch "this" parameter
    VM_Address callingFrame = VM_Magic.getCallerFramePointer(VM_Magic.getFramePointer());
    callingFrame = VM_Magic.getCallerFramePointer(callingFrame);
    callingFrame = VM_Magic.getCallerFramePointer(callingFrame);
    VM_Address location = callingFrame.sub((LAST_NONVOLATILE_FPR - FIRST_VOLATILE_FPR + 1) * 8 + 
					   (LAST_NONVOLATILE_GPR - FIRST_VOLATILE_GPR + 1) * 4); 
    
    return VM_Magic.addressAsObject(VM_Magic.getMemoryAddress(location));
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generates a custom IMT-conflict resolution stub.
 * We create a binary search tree.
 * 
 * @author Bowen Alpern
 * @author Dave Grove
 */
class VM_InterfaceMethodConflictResolver implements VM_BaselineConstants,
						    VM_AssemblerConstants {

  // Create a conflict resolution stub for the set of interface method signatures l.
  // 
  static INSTRUCTION[] createStub(int[] sigIds, VM_Method[] targets) {
    // (1) Create an assembler.
    int numEntries = sigIds.length;
    VM_Assembler asm = new VM_Assembler(numEntries); // pretend each entry is a bytecode

    // (2) signatures must be in ascending order (to build binary search tree).
    if (VM.VerifyAssertions) {
      for (int i=1; i<sigIds.length; i++) {
	VM.assert(sigIds[i-1] < sigIds[i]);
      }
    }

    // (3) Assign synthetic bytecode numbers to each switch such that we'll generate them
    // in ascending order.  This lets us use the general forward branching mechanisms
    // of the VM_Assembler.
    int[] bcIndices = new int[numEntries];
    assignBytecodeIndices(0, bcIndices, 0, numEntries -1);
    
    // (4) Generate the stub.
    insertStubPrologue(asm);
    insertStubCase(asm, sigIds, targets, bcIndices, 0, numEntries-1);
    
    INSTRUCTION[] stub = asm.makeMachineCode().getInstructions();

    // (5) synchronize icache with generated machine code that was written through dcache
    if (VM.runningVM)    
      VM_Memory.sync(VM_Magic.objectAsAddress(stub), stub.length << LG_INSTRUCTION_WIDTH); 

    return stub;
  }

  // Assign ascending bytecode indices to each case (in the order they will be generated)
  private static int assignBytecodeIndices(int bcIndex, int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    bcIndices[middle] = bcIndex++;
    if (low == middle && middle == high) {
      return bcIndex;
    } else {
      // Recurse.
      if (low < middle) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	bcIndex = assignBytecodeIndices(bcIndex, bcIndices, middle+1, high);
      }
      return bcIndex;
    }
  }

  // Make a stub prologue: get TIB into S0
  // factor out to reduce code space in each call.
  //
  private static void insertStubPrologue (VM_Assembler asm) {
    VM_ObjectModel.baselineEmitLoadTIB(asm, S0, T0);
  }

  // Generate a subtree covering from low to high inclusive.
  private static void insertStubCase(VM_Assembler asm,  
				     int[] sigIds, 
				     VM_Method[] targets,
				     int[] bcIndices, int low, int high) {
    int middle = (high + low)/2;
    asm.resolveForwardReferences(bcIndices[middle]);
    if (low == middle && middle == high) {
      // a leaf case; can simply invoke the method directly.
      VM_Method target = targets[middle];
      if (target.isStatic()) {
	// an error case.
	asm.emitLtoc(S0, target.getOffset());
      } else {
	asm.emitL   (S0, target.getOffset(), S0);
      }
      asm.emitMTCTR(S0);
      asm.emitBCTR ();
    } else {
      asm.emitCMPI (SP, sigIds[middle]);
      if (low < middle) {
	asm.emitShortBC(LT, 0, bcIndices[(low+middle-1)/2]);
      }
      if (middle < high) {
	asm.emitShortBC(GT, 0, bcIndices[(middle+1+high)/2]);
      }
      // invoke the method for middle.
      VM_Method target = targets[middle];
      if (target.isStatic()) {
	// an error case.
	asm.emitLtoc(S0, target.getOffset());
      } else {
	asm.emitL   (S0, target.getOffset(), S0);
      }
      asm.emitMTCTR(S0);
      asm.emitBCTR ();
      // Recurse.
      if (low < middle) {
	insertStubCase(asm, sigIds, targets, bcIndices, low, middle-1);
      } 
      if (middle < high) {
	insertStubCase(asm, sigIds, targets, bcIndices, middle+1, high);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Generate a "trampoline" that jumps to the shared lazy compilation stub.
 * We do this to enable the optimizing compiler to use ptr equality of
 * target instructions to imply logical (source) equality of target methods.
 * This is used to perform guarded inlining using the "method test."
 * Without per-method lazy compilation trampolines, ptr equality of target
 * instructions does not imply source equality, since both targets may in fact
 * be the globally shared lazy compilation stub.
 * 
 * @author Dave Grove
 */
class VM_LazyCompilationTrampolineGenerator implements VM_BaselineConstants {

  /** 
   * Generate a new lazy compilation trampoline. 
   */
  static INSTRUCTION[] getTrampoline () {
    VM_Assembler asm = new VM_Assembler(0);
    asm.emitLtoc (S0, VM_Entrypoints.lazyMethodInvokerMethod.getOffset());
    asm.emitMTCTR(S0);
    asm.emitBCTR ();
    return asm.makeMachineCode().getInstructions();
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Machine dependent portion of Reflective method invoker.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 * @date 15 Jul 1998 
 */
public class VM_MachineReflection implements VM_Constants {
  //-----------//
  // interface //
  //-----------//
   

   //----------------//
   // implementation //
   //----------------//
   
   // Determine number/type of registers/spills required to call specified method.
   // See also: VM_Compiler.loadParameters()
   //
  static int 
    countParameters(VM_Method method) {
    int GPRs   = 0;
    int FPRs   = 0;
    int Spills = 0;
    int gp = FIRST_VOLATILE_GPR;
    int fp = FIRST_VOLATILE_FPR;
    if (!method.isStatic()) {
      if (gp > LAST_VOLATILE_GPR) Spills++;
      else {GPRs++; gp++;}
    }
    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
	if (gp > LAST_VOLATILE_GPR) Spills+=2;
	else {
	  {GPRs++; gp++;}
	  if (gp > LAST_VOLATILE_GPR) Spills++;
	  else {GPRs++; gp++;}
	}
      } else if (t.isFloatType()) {
	if (fp > LAST_VOLATILE_FPR) Spills++;
	else {FPRs++; fp++;}
      } else if (t.isDoubleType()) {
	if (fp > LAST_VOLATILE_FPR) Spills+=2;
	else {FPRs++; fp++;}
      } else { // t is object, int, short, char, byte, or boolean
	if (gp > LAST_VOLATILE_GPR) Spills++;
	else {GPRs++; gp++;}
      }
    }

    // spills[] carries burden of aligning stack frame
    int frameSize  = (Spills << 2)           // spill area
      + STACKFRAME_HEADER_SIZE; // header
    frameSize = (frameSize + STACKFRAME_ALIGNMENT_MASK) & ~STACKFRAME_ALIGNMENT_MASK;
    Spills = (frameSize-STACKFRAME_HEADER_SIZE) >> 2;        

    // hack to return triple
    return (Spills<<(REFLECTION_FPRS_BITS+REFLECTION_GPRS_BITS)) |
      (FPRs<<REFLECTION_GPRS_BITS) | GPRs;
  }

  // Collect parameters into arrays of registers/spills, as required to call specified method.
  //
  static void 
    packageParameters(VM_Method method, Object thisArg, Object[] otherArgs,
		      int[] GPRs, double[] FPRs, int[] Spills) {
    int GPR   = GPRs.length;
    int FPR   = FPRs.length;
    int Spill = Spills.length;
    int gp = FIRST_VOLATILE_GPR;
    int fp = FIRST_VOLATILE_FPR;
    if (!method.isStatic()) {
      if (gp > LAST_VOLATILE_GPR)
	Spills[--Spill] = VM_Reflection.unwrapObject(thisArg);
      else {
	gp++;
	GPRs[--GPR] = VM_Reflection.unwrapObject(thisArg);
      }
    }
    VM_Type [] types = method.getParameterTypes();
    for (int i=0; i<types.length; i++) {
      VM_Type t = types[i];
      if (t.isLongType()) {
	long l = VM_Reflection.unwrapLong(otherArgs[i]);
	if (gp > LAST_VOLATILE_GPR) {
	  Spills[--Spill] = (int)(l>>>32);
	  Spills[--Spill] = (int)l;
	} else {
	  gp++;
	  GPRs[--GPR] = (int)(l>>>32);
	  if (gp > LAST_VOLATILE_GPR) Spills[--Spill] = (int)(l);
	  else {
	    gp++;
	    GPRs[--GPR] = (int)(l);
	  }
	}
      } else if (t.isFloatType()) {
	if (fp > LAST_VOLATILE_FPR) {
	  float f = VM_Reflection.unwrapFloat(otherArgs[i]);
	  Spills[--Spill] = Float.floatToIntBits(f);
	} else {
	  fp++;
	  FPRs[--FPR] = VM_Reflection.unwrapFloat(otherArgs[i]);
	}
      } else if (t.isDoubleType()) {
	if (fp > LAST_VOLATILE_FPR) {
	  double d = VM_Reflection.unwrapDouble(otherArgs[i]);
	  long l = Double.doubleToLongBits(d);
	  Spills[--Spill] = (int)(l>>>32);
	  Spills[--Spill] = (int)l;
	} else {
	  fp++;
	  FPRs[--FPR] = VM_Reflection.unwrapDouble(otherArgs[i]);
	}
      } else if (t.isBooleanType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = VM_Reflection.unwrapBooleanAsInt(otherArgs[i]);
	}
      } else if (t.isByteType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = (int) VM_Reflection.unwrapByte(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = (int) VM_Reflection.unwrapByte(otherArgs[i]);
	}
      } else if (t.isCharType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = (int) VM_Reflection.unwrapChar(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = (int) VM_Reflection.unwrapChar(otherArgs[i]);
	}
      } else if (t.isShortType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = (int) VM_Reflection.unwrapShort(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = (int) VM_Reflection.unwrapShort(otherArgs[i]);
	}
      } else if (t.isIntType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = VM_Reflection.unwrapInt(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = VM_Reflection.unwrapInt(otherArgs[i]);
	}
      } else if (!t.isPrimitiveType()) {
	if (gp > LAST_VOLATILE_GPR)
	  Spills[--Spill] = VM_Reflection.unwrapObject(otherArgs[i]);
	else {
	  gp++;
	  GPRs[--GPR] = VM_Reflection.unwrapObject(otherArgs[i]);
	}
      } else  {
	if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * A place to put hand written machine code typically invoked by VM_Magic 
 * methods.
 *
 * Hand coding of small inline instruction sequences is typically handled by 
 * each compiler's implementation of VM_Magic methods.  A few VM_Magic methods
 * are so complex that their implementations require many instructions.  
 * But our compilers do not inline arbitrary amounts of machine code. 
 * We therefore write such code blocks here, out of line.
 *
 * These code blocks can be shared by all compilers. They can be branched to
 * via a jtoc offset (obtained from VM_Entrypoints.XXXInstructionsMethod).
 *
 * 17 Mar 1999 Derek Lieber
 *
 * 15 Jun 2001 Dave Grove and Bowen Alpern (Derek believed that compilers 
 * could inline these methods if they wanted.  We do not believe this would 
 * be very easy since they return thru the LR.)
 *
 * @author Derek Lieber
 */
class VM_OutOfLineMachineCode implements VM_BaselineConstants, VM_AssemblerConstants {
  //-----------//
  // interface //
  //-----------//
   
  static void
    init() {
    reflectiveMethodInvokerInstructions       = generateReflectiveMethodInvokerInstructions();
    saveThreadStateInstructions               = generateSaveThreadStateInstructions();
    threadSwitchInstructions                  = generateThreadSwitchInstructions();
    restoreHardwareExceptionStateInstructions = generateRestoreHardwareExceptionStateInstructions();
    getTimeInstructions                       = generateGetTimeInstructions();
    invokeNativeFunctionInstructions          = generateInvokeNativeFunctionInstructions();
  }

  //----------------//
  // implementation //
  //----------------//

  private static INSTRUCTION[] reflectiveMethodInvokerInstructions;
  private static INSTRUCTION[] saveThreadStateInstructions;
  private static INSTRUCTION[] threadSwitchInstructions;
  private static INSTRUCTION[] restoreHardwareExceptionStateInstructions;
  private static INSTRUCTION[] getTimeInstructions;
  private static INSTRUCTION[] invokeNativeFunctionInstructions;
   
  // Machine code for reflective method invocation.
  // See also: "VM_MagicCompiler.generateMethodInvocation".
  //
  // Registers taken at runtime:
  //   T0 == address of method entrypoint to be called
  //   T1 == address of gpr registers to be loaded
  //   T2 == address of fpr registers to be loaded
  //   T3 == address of spill area in calling frame
  //
  // Registers returned at runtime:
  //   standard return value conventions used
  //
  // Side effects at runtime:
  //   artificial stackframe created and destroyed
  //   R0, volatile, and scratch registers destroyed
  //
  private static INSTRUCTION[]
    generateReflectiveMethodInvokerInstructions() {
    VM_Assembler asm = new VM_Assembler(0);
      
    //
    // free registers: 0, S0
    //
    asm.emitMFLR(0);                                         // save...
    asm.emitST  (0, STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP); // ...return address
      
    asm.emitMTCTR(T0);                          // CTR := start of method code
      
    //
    // free registers: 0, S0, T0
    //
      
    // create new frame
    //
    asm.emitCAL  (S0,  0, FP);                  // S0 := old frame pointer
    asm.emitL    (T0, VM_ObjectModel.getArrayLengthOffset(), T3); // T0 := number of spill words
    asm.emitCAL  (T3, -4, T3);                  // T3 -= 4 (predecrement, ie. T3 + 4 is &spill[0] )
    int spillLoopLabel = asm.getMachineCodeIndex();
    asm.emitAIr  (T0, T0, -1);                  // T0 -= 1 (and set CR)
    VM_ForwardReference fr1 = asm.emitForwardBC(LT); // if T0 < 0 then break
    asm.emitLU   (0,   4, T3);                  // R0 := *(T3 += 4)
    asm.emitSTU  (0,  -4, FP);                  // put one word of spill area
    asm.emitB    (spillLoopLabel); // goto spillLoop:
    fr1.resolve(asm);
      
    asm.emitSTU  (S0, -STACKFRAME_HEADER_SIZE, FP);     // allocate frame header and save old fp
    asm.emitLVAL (T0, INVISIBLE_METHOD_ID);
    asm.emitST   (T0, STACKFRAME_METHOD_ID_OFFSET, FP); // set method id

    //
    // free registers: 0, S0, T0, T3
    //
      
    // load up fprs
    //
    VM_ForwardReference setupFPRLoader = asm.emitForwardBL();

    FPRLoader:
    for (int i = LAST_VOLATILE_FPR; i >= FIRST_VOLATILE_FPR; --i)
      asm.emitLFDU(i, +8, T2);                 // FPRi := fprs[i]
         
    //
    // free registers: 0, S0, T0, T2, T3
    //
      
    // load up gprs
    //
    VM_ForwardReference setupGPRLoader = asm.emitForwardBL();

    GPRLoader:
    for (int i = LAST_VOLATILE_GPR; i >= FIRST_VOLATILE_GPR; --i)
      asm.emitLU  (i, +4, S0);                 // GPRi := gprs[i]
      
    //
    // free registers: 0, S0
    //
      
    // invoke method
    //
    asm.emitBCTRL();                            // branch and link to method code

    // emit method epilog
    //
    asm.emitL    (FP,  0, FP);                                    // restore caller's frame
    asm.emitL    (S0,  STACKFRAME_NEXT_INSTRUCTION_OFFSET, FP);   // pick up return address
    asm.emitMTLR (S0);                                            //
    asm.emitBLR();                                                // return to caller

    setupFPRLoader.resolve(asm);
    asm.emitMFLR (T3);                          // T3 := address of first fpr load instruction
    asm.emitL    (T0, VM_ObjectModel.getArrayLengthOffset(), T2); // T0 := number of fprs to be loaded
    asm.emitCAL  (T3, VOLATILE_FPRS<<2,    T3); // T3 := address of first instruction following fpr loads
    asm.emitSLI  (T0, T0,                   2); // T0 := number of bytes of fpr load instructions
    asm.emitSF   (T3, T0,                  T3); // T3 := address of instruction for highest numbered fpr to be loaded
    asm.emitMTLR (T3);                          // LR := """
    asm.emitCAL  (T2, -8, T2);                  // predecrement fpr index (to prepare for update instruction)
    asm.emitBLR  ();                            // branch to fpr loading instructions

    setupGPRLoader.resolve(asm);
    asm.emitMFLR (T3);                          // T3 := address of first gpr load instruction
    asm.emitL    (T0, VM_ObjectModel.getArrayLengthOffset(), T1); // T0 := number of gprs to be loaded
    asm.emitCAL  (T3, VOLATILE_GPRS<<2,    T3); // T3 := address of first instruction following gpr loads
    asm.emitSLI  (T0, T0,                   2); // T0 := number of bytes of gpr load instructions
    asm.emitSF   (T3, T0,                  T3); // T3 := address of instruction for highest numbered gpr to be loaded
    asm.emitMTLR (T3);                          // LR := """
    asm.emitCAL  (S0, -4, T1);                  // predecrement gpr index (to prepare for update instruction)
    asm.emitBLR  ();                            // branch to gpr loading instructions
     
    return asm.makeMachineCode().getInstructions();
  }

  // Machine code to implement "VM_Magic.saveThreadState()".
  //
  // Registers taken at runtime:
  //   T0 == address of VM_Registers object
  //
  // Registers returned at runtime:
  //   none
  //
  // Side effects at runtime:
  //   T1 destroyed
  //
  private static INSTRUCTION[] generateSaveThreadStateInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int fprsOffset = VM_Entrypoints.registersFPRsField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();

    asm.emitLIL(T1, -1);           // T1 = -1
    asm.emitST (T1, ipOffset, T0); // registers.ip = -1

    // save non-volatile fprs
    //
    asm.emitL(T1, fprsOffset, T0); // T1 := registers.fprs[]
    for (int i = FIRST_NONVOLATILE_FPR; i <= LAST_NONVOLATILE_FPR; ++i)
      asm.emitSTFD(i, i*8, T1);

    // save non-volatile gprs
    //
    asm.emitL(T1, gprsOffset, T0); // T1 := registers.gprs[]
    for (int i = FIRST_NONVOLATILE_GPR; i <= LAST_NONVOLATILE_GPR; ++i)
      asm.emitST(i, i*4, T1);

    // save fp, sp, and ti
    //
    asm.emitST(FP, FP*4, T1);
    asm.emitST(TI, TI*4, T1);
    asm.emitST(SP, SP*4, T1);
      
    // return to caller
    //
    asm.emitBLR();
     
    return asm.makeMachineCode().getInstructions();
  }
      
  /**
   * Machine code to implement "VM_Magic.threadSwitch()".
   * 
   *  Parameters taken at runtime:
   *    T0 == address of VM_Thread object for the current thread
   *    T1 == address of VM_Registers object for the new thread
   * 
   *  Registers returned at runtime:
   *    none
   * 
   *  Side effects at runtime:
   *    sets current Thread's beingDispatched field to false
   *    saves current Thread's nonvolatile hardware state in its VM_Registers object
   *    restores new thread's VM_Registers nonvolatile hardware state.
   *    execution resumes at address specificed by restored thread's VM_Registers ip field
   */
  private static INSTRUCTION[] generateThreadSwitchInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int fprsOffset = VM_Entrypoints.registersFPRsField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    int regsOffset = VM_Entrypoints.threadContextRegistersField.getOffset();

    // (1) Save nonvolatile hardware state of current thread.
    asm.emitMFLR (T3);                         // T3 gets return address
    asm.emitL    (T2, regsOffset, T0);         // T2 = T0.contextRegisters
    asm.emitST   (T3, ipOffset, T2);           // T0.contextRegisters.ip = return address

    // save non-volatile fprs
    asm.emitL(T3, fprsOffset, T2); // T3 := T0.contextRegisters.fprs[]
    for (int i = FIRST_NONVOLATILE_FPR; i <= LAST_NONVOLATILE_FPR; ++i)
      asm.emitSTFD(i, i*8, T3);

    // save non-volatile gprs
    asm.emitL(T3, gprsOffset, T2); // T3 := registers.gprs[]
    for (int i = FIRST_NONVOLATILE_GPR; i <= LAST_NONVOLATILE_GPR; ++i)
      asm.emitST(i, i*4, T3);

    // save other 'nonvol' gprs: fp and ti
    asm.emitST(FP, FP*4, T3);
    asm.emitST(TI, TI*4, T3);

    // (2) Set currentThread.beingDispatched to false
    asm.emitLIL(0, 0);                                        // R0 := 0
    asm.emitST (0, VM_Entrypoints.beingDispatchedField.getOffset(), T0); // T0.beingDispatched := R0

    // (3) Restore nonvolatile hardware state of new thread.

    // restore non-volatile fprs
    asm.emitL(T0, fprsOffset, T1); // T0 := T1.fprs[]
    for (int i = FIRST_NONVOLATILE_FPR; i <= LAST_NONVOLATILE_FPR; ++i)
      asm.emitLFD(i, i*8, T0);

    // restore non-volatile gprs
    asm.emitL(T0, gprsOffset, T1); // T0 := T1.gprs[]
    for (int i = FIRST_NONVOLATILE_GPR; i <= LAST_NONVOLATILE_GPR; ++i)
      asm.emitL(i, i*4, T0);

    // restore fp, and ti
    asm.emitL(FP, FP*4, T0);
    asm.emitL(TI, TI*4, T0);

    // resume execution at saved ip (T1.ipOffset)
    asm.emitL(T0, ipOffset, T1);
    asm.emitMTLR(T0);
    asm.emitBLR();

    return asm.makeMachineCode().getInstructions();
  }
  
      
  // Machine code to implement "VM_Magic.restoreHardwareExceptionState()".
  //
  // Registers taken at runtime:
  //   T0 == address of VM_Registers object
  //
  // Registers returned at runtime:
  //   none
  //
  // Side effects at runtime:
  //   all registers are restored except condition registers, count register,
  //   JTOC_POINTER, and PROCESSOR_REGISTER with execution resuming at "registers.ip"
  //
  private static INSTRUCTION[] generateRestoreHardwareExceptionStateInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int   ipOffset = VM_Entrypoints.registersIPField.getOffset();
    int fprsOffset = VM_Entrypoints.registersFPRsField.getOffset();
    int gprsOffset = VM_Entrypoints.registersGPRsField.getOffset();
    int lrOffset   = VM_Entrypoints.registersLRField.getOffset();

    // restore LR
    //
    asm.emitL(REGISTER_ZERO, lrOffset, T0);
    asm.emitMTLR(REGISTER_ZERO);

    // restore IP (hold it in CT register for a moment)
    //
    asm.emitL(REGISTER_ZERO, ipOffset, T0);
    asm.emitMTCTR(REGISTER_ZERO);
      
    // restore fprs
    //
    asm.emitL(T1, fprsOffset, T0); // T1 := registers.fprs[]
    for (int i = 0; i < NUM_FPRS; ++i)
      asm.emitLFD(i, i*8, T1);

    // restore gprs
    //
    asm.emitL(T1, gprsOffset, T0); // T1 := registers.gprs[]

    for (int i = FIRST_NONVOLATILE_GPR; i <= LAST_NONVOLATILE_GPR; ++i)
      asm.emitL(i, i*4, T1);

    for (int i = FIRST_SCRATCH_GPR; i <= LAST_SCRATCH_GPR; ++i)
      asm.emitL(i, i*4, T1);

    for (int i = FIRST_VOLATILE_GPR; i <= LAST_VOLATILE_GPR; ++i)
      if (i != T1)
	asm.emitL(i, i*4, T1);

    // restore specials
    //
    asm.emitL(REGISTER_ZERO, REGISTER_ZERO*4, T1);
    asm.emitL(FP, FP*4, T1);
    asm.emitL(TI, TI*4, T1);
      
    // restore last gpr
    //
    asm.emitL(T1, T1*4, T1);

    // resume execution at IP
    //
    asm.emitBCTR();

    return asm.makeMachineCode().getInstructions();
  }

  // Machine code to implement "VM_Magic.getTime()".
  //
  // Registers taken at runtime:
  //   T0 == address of VM_Processor object
  //
  // Registers returned at runtime:
  //   F0 == return value
  //
  // Side effects at runtime:
  //   T0..T3 and F0..F3 are destroyed
  //   scratch fields used in VM_Processor object
  //
  private static INSTRUCTION[] generateGetTimeInstructions() {
    VM_Assembler asm = new VM_Assembler(0);

    int scratchSecondsOffset     = VM_Entrypoints.scratchSecondsField.getOffset();
    int scratchNanosecondsOffset = VM_Entrypoints.scratchNanosecondsField.getOffset();

    asm.emitOR    (T3, T0, T0);                             // t3 := address of VM_Processor object
    asm.emitLFDtoc(F2, VM_Entrypoints.billionthField.getOffset(), T1); // f2 := 1e-9
    asm.emitLFDtoc(F3, VM_Entrypoints.IEEEmagicField.getOffset(), T1); // f3 := IEEEmagic

    asm.emitSTFD  (F3, scratchNanosecondsOffset, T3);       // scratch_nanos   := IEEEmagic
    asm.emitSTFD  (F3, scratchSecondsOffset,     T3);       // scratch_seconds := IEEEmagic

    int loopLabel = asm.getMachineCodeIndex();
    if (VM.BuildForLinux) {
      asm.emitMFTBU (T0);                                     // t0 := real time clock, upper
      asm.emitMFTB  (T1);                                     // t1 := real time clock, lower
      asm.emitMFTBU (T2);                                     // t2 := real time clock, upper
    } else {
      asm.emitMFSPR (T0, 4 );                                 // t0 := real time clock, upper
      asm.emitMFSPR (T1, 5 );                                 // t1 := real time clock, lower
      asm.emitMFSPR (T2, 4 );                                 // t2 := real time clock, upper
    }
    asm.emitCMP   (T0, T2);                                 // t0 == t2?
    asm.emitST    (T1, scratchNanosecondsOffset + 4, T3);   // scratch_nanos_lo   := nanos
    asm.emitST    (T0, scratchSecondsOffset     + 4, T3);   // scratch_seconds_lo := seconds
    asm.emitBC    (NE, loopLabel);                          // seconds have rolled over, try again

    asm.emitLFD   (F0, scratchNanosecondsOffset, T3);       // f0 := IEEEmagic + nanos
    asm.emitLFD   (F1, scratchSecondsOffset,     T3);       // f1 := IEEEmagic + seconds

    asm.emitFS    (F0, F0, F3);                             // f0 := f0 - IEEEmagic == (double)nanos
    asm.emitFS    (F1, F1, F3);                             // f1 := f1 - IEEEmagic == (double)seconds

    asm.emitFMA   (F0, F2, F0, F1);                         // f0 := f2 * f0 + f1

    // return to caller
    //
    asm.emitBLR();

    return asm.makeMachineCode().getInstructions();
  }

  // on entry:
  //   JTOC = TOC for native call
  //   TI - IP address of native function to branch to
  //   S0 -> threads JNIEnvironment, which contains saved PR & TI regs
  //   Parameter regs R4-R10, FP1-FP6 loaded for call to native
  //   (R3 will be set here before branching to the native function)
  // 
  //   GPR3 (T0), SP, PR regs are available for scratch regs on entry
  //
  private static INSTRUCTION[] generateInvokeNativeFunctionInstructions() {

    VM_Assembler asm = new VM_Assembler(0);
    int lockoutLockOffset = VM_Entrypoints.lockoutProcessorField.getOffset();
    //
    // store the return address to the Java to C glue prolog, which is now in LR
    // into transition frame. If GC occurs, the JNIGCMapIterator will cause
    // this ip address to be relocated if the generated glue code is moved.
    //
    asm.emitL     (SP, 0, FP);
    asm.emitMFLR  (T0);
    asm.emitST    (T0, -JNI_PROLOG_RETURN_ADDRESS_OFFSET, SP);  // save return address in stack frame
    //
    // Load required JNI function ptr into first parameter reg (GPR3/T0)
    // This pointer is in the JNIEnvAddress field of JNIEnvironment
    //
    asm.emitL (T0, VM_Entrypoints.JNIEnvAddressField.getOffset(), S0);
    //
    // change the vpstatus of the VP to "in Native"
    //
    asm.emitL     (PROCESSOR_REGISTER, VM_Entrypoints.JNIEnvSavedPRField.getOffset(), S0); 
    asm.emitL     (SP, VM_Entrypoints.vpStatusAddressField.getOffset(), PROCESSOR_REGISTER); // SP gets addr vpStatus word
    asm.emitCAL   (S0,  VM_Processor.IN_NATIVE, 0 );              // S0  <- new status value
    asm.emitST    (S0,  0, SP);                                   // change state to native

    // set word following JNI function ptr to addr of current processors vpStatus word
    asm.emitST    (SP,  4, T0);

    //
    asm.emitMTLR  (TI);                                // move native code address to link reg
    //
    // goto the native code
    //
    asm.emitBLRL  ();                                       // call native method
    //
    // save the return value in R3-R4 in the glue frame spill area since they may be overwritten
    // in the call to becomeRVMThreadOffset
    asm.emitST    (T0, AIX_FRAME_HEADER_SIZE, FP);
    asm.emitST    (T1, AIX_FRAME_HEADER_SIZE+4, FP);
    //
    // try to return to Java state, by testing state word of process
    //
    int label1    = asm.getMachineCodeIndex();                            // inst index of the following load
    asm.emitL     (PROCESSOR_REGISTER, 0, FP);                            // get previous frame
    asm.emitL     (JTOC, -4, PROCESSOR_REGISTER);                         // load JTOC reg
    asm.emitL     (PROCESSOR_REGISTER, - JNI_PR_OFFSET, PROCESSOR_REGISTER); //load processor register  
    asm.emitL     (T3, VM_Entrypoints.vpStatusAddressField.getOffset(), PROCESSOR_REGISTER); // T3 gets addr of vpStatus word
    asm.emitLWARX (S0, 0, T3);                                            // get status for processor
    asm.emitCMPI  (S0, VM_Processor.BLOCKED_IN_NATIVE);                   // are we blocked in native code?
    VM_ForwardReference fr = asm.emitForwardBC(NE);
    //
    // if blocked in native, call C routine to do pthread_yield
    //
    asm.emitL     (T2, VM_Entrypoints.the_boot_recordField.getOffset(), JTOC);       // T2 gets boot record address
    asm.emitL     (JTOC, VM_Entrypoints.sysTOCField.getOffset(), T2);                // load TOC for syscalls from bootrecord
    asm.emitL     (T1,   VM_Entrypoints.sysVirtualProcessorYieldIPField.getOffset(), T2);  // load addr of function
    asm.emitMTLR  (T1);
    asm.emitBLRL  ();                                          // call sysVirtualProcessorYield in sys.C
    asm.emitB     (label1); // retest the blocked in native
    //
    //  br to here -not blocked in native
    //
    fr.resolve(asm);
    asm.emitCAL   (S0,  VM_Processor.IN_JAVA, 0 );           // S0  <- new state value
    asm.emitSTWCXr(S0,  0, T3);                              // attempt to change state to java
    asm.emitBC    (NE, label1);                              // br if failure -retry lwarx
    //
    // return to caller
    //
    asm.emitL     (T3, 0 , FP);                                // get previous frame
    asm.emitL     (S0, -JNI_PROLOG_RETURN_ADDRESS_OFFSET, T3); // get return address from stack frame
    asm.emitMTLR  (S0);
    asm.emitBLR   ();
    //
    return asm.makeMachineCode().getInstructions();

  }  // generateInvokeNativeFunctionInstructions
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$
/**
 * This class provides a layer of abstraction that the rest of the VM must
 * use in order to access the current <code>VM_Processor</code> object.
 *
 * @see VM_Processor
 *
 * @author Stephen Fink
 */
public final class VM_ProcessorLocalState implements VM_Uninterruptible {
  
  /**
   * The C bootstrap program has placed a pointer to the initial
   * VM_Processor in the processor register.  This is OK, so do nothing.
   */
  static void boot() {
    // do nothing - everything is already set up.
  }

  /**
   * Return the current VM_Processor object
   */
  public static VM_Processor getCurrentProcessor() throws VM_PragmaInline {
    return VM_Magic.getProcessorRegister();
  }

  /**
   * Set the current VM_Processor object
   */
  public static void setCurrentProcessor(VM_Processor p) throws VM_PragmaInline {
    VM_Magic.setProcessorRegister(p);
  }
}
/*
 * (C) Copyright IBM Corp. 2002
 */
//$Id$

/**
 * @author Julian Dolby
 * @date May 20, 2002
 */

import java.security.ProtectionDomain;
//-#if RVM_WITH_GNU_CLASSPATH
//-#else
 import com.ibm.oti.vm.AppClassLoader;
//-#endif
import java.io.ByteArrayOutputStream;
import java.io.InputStream;
import java.io.IOException;
import java.io.File;
import java.net.URL;

//-#if RVM_WITH_GNU_CLASSPATH
class VM_ApplicationClassLoader extends ClassLoader {
//-#else
class VM_ApplicationClassLoader extends AppClassLoader {
//-#endif

    VM_ApplicationClassLoader(ClassLoader parent) {
	super( parent );
    }

   // Set "java.class.path" property, which is used by this class
   //
   public static void setPathProperty() {
       String   classpath = null;
       String[] repositories = VM_ClassLoader.getApplicationRepositories();

       if (repositories != null)
	   for (int i = 0, n = repositories.length; i < n; ++i) {
	       String name = repositories[i];
	       if (name.startsWith("."))
		   name =
		       System.getProperty("user.dir") +
		       File.separator +
		       name.substring(1);
 
	       if (classpath == null)
		   classpath = name;
	       else
		   classpath += File.pathSeparator + name;
	   }

       if (classpath == null)
	   classpath = System.getProperty("user.dir");
       
       System.setProperty("java.class.path", classpath);
   }
    

    private String findRepository(String resourceName) {
	for(int i = 0; i < parsedPath.length; i++) 
	    if (resourceName.startsWith(parsedPath[i]))
		return parsedPath[i];
	    else if (resourceName.startsWith( toURLString(parsedPath[i]) ))
		return parsedPath[i];

	VM.sysWrite("Cannot find repository for " + resourceName + "\n");
	throw new Error();
    }

    protected Class findClass (String className) throws ClassNotFoundException {
      VM_Atom classDescriptor = VM_Atom.findOrCreateAsciiAtom(className.replace('.','/')).descriptorFromClassName();
      VM_Class cls = (VM_Class) VM_ClassLoader.findOrCreateType(classDescriptor, this);
	
      try {
        URL x = findResource(classDescriptor.classFileNameFromDescriptor());
        InputStream is = x.openConnection().getInputStream();
        //-#if RVM_WITH_GNU_CLASSPATH
        VM_ClassLoader.defineClassInternal(className, is, this, null /*(ProtectionDomain)getFilePD(findRepository(x.getFile())));*/ );
        //-#else
        VM_ClassLoader.defineClassInternal(className, is, this, (ProtectionDomain)getFilePD(findRepository(x.getFile())));
        //-#endif

      } catch (Throwable e) {
	throw new ClassNotFoundException(className);
      }
	
      return cls.getClassForType();
    }

}


    
		
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Description of a java "array" type. <p>
 * 
 * This description is not read from a ".class" file, but rather
 * is manufactured by the vm as execution proceeds. 
 * 
 * @see VM_Class
 * @see VM_Primitive
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
import com.ibm.JikesRVM.memoryManagers.VM_Collector;
import com.ibm.JikesRVM.memoryManagers.VM_WriteBarrier;

public final class VM_Array extends VM_Type
  implements VM_Constants, VM_ClassLoaderConstants  {

  //-----------//
  // interface //
  //-----------//
   
  //--------------------------------------------------------------------------------------------------//
  //                                       Section 1.                                                 //
  //                              The following are always available.
  //--------------------------------------------------------------------------------------------------//
   
  /**
   * Name - something like "[I" or "[Ljava.lang.String;"
   */
  public final String getName() { 
    return descriptor.toString().replace('/','.');
  }

  /** 
   * Stack space requirement. 
   */
  public final int getStackWords() throws VM_PragmaUninterruptible {
    return 1;
  }
      
  /** 
   * Element type.
   */
  public final VM_Type getElementType() throws VM_PragmaUninterruptible { 
    return elementType;
  }

  // Convenience method.
  final VM_Type getInnermostElementType() throws VM_PragmaUninterruptible {
    return innermostElementType;
  }
      
  /**
   * Size, in bytes, of an array element, log base 2.
   * @return log base 2 of array element size
   */
  final int getLogElementSize() throws VM_PragmaUninterruptible {
    switch (getDescriptor().parseForArrayElementTypeCode()) 
      {
      case VM_Atom.ClassTypeCode:   return 2;
      case VM_Atom.ArrayTypeCode:   return 2;
      case VM_Atom.BooleanTypeCode: return 0;
      case VM_Atom.ByteTypeCode:    return 0;
      case VM_Atom.ShortTypeCode:   return 1;
      case VM_Atom.IntTypeCode:     return 2;
      case VM_Atom.LongTypeCode:    return 3;
      case VM_Atom.FloatTypeCode:   return 2;
      case VM_Atom.DoubleTypeCode:  return 3;
      case VM_Atom.CharTypeCode:    return 1;
      }
    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
    return -1;
  }

  /**
   * Total size, in bytes, of an instance of this array type (including object header).
   * @return size
   */
  public final int getInstanceSize(int numelts) throws VM_PragmaUninterruptible {
    return VM_ObjectModel.computeArrayHeaderSize(this) + (numelts << getLogElementSize());
  }

   //--------------------------------------------------------------------------------------------------//
   //                                       Section 2.                                                 //
   //         The following are available after "resolve()" has been called.                           //
   //--------------------------------------------------------------------------------------------------//

  /**
   * Does this class override java.lang.Object.finalize()?
   */
  public final boolean hasFinalizer() throws VM_PragmaUninterruptible {
    return false;
  }

  /**
   * Static fields of this array type.
   */
  public final VM_Field[] getStaticFields() {
    return VM_Type.JavaLangObjectType.getStaticFields();
  }
 
  /**
   * Non-static fields of this array type.
   */
  public final VM_Field[] getInstanceFields() {
    return VM_Type.JavaLangObjectType.getInstanceFields();
  }

  /**
   * Statically dispatched methods of this array type.
   */
  public final VM_Method[] getStaticMethods() {
    return VM_Type.JavaLangObjectType.getStaticMethods();
  }
 
  /**
   * Virtually dispatched methods of this array type.
   */
  public final VM_Method[] getVirtualMethods() {
    return VM_Type.JavaLangObjectType.getVirtualMethods();
  }

  /**
   * Runtime type information for this array type.
   */
  public final Object[] getTypeInformationBlock() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return typeInformationBlock;
  }

  public final ClassLoader getClassLoader() {
      return elementType.getClassLoader();
  }

   //--------------------------------------------------------------------------------------------------//
   //                                       Section 3.                                                 //
   //         The following are available after "instantiate()" has been called.                       //
   //--------------------------------------------------------------------------------------------------//

   //--------------------------------------------------------------------------------------------------//
   //                                       Section 4.                                                 //
   //                                     Static methods.
   //--------------------------------------------------------------------------------------------------//

  /**
   * Load, resolve, instantiate, and initialize specified array.
   * @param arrayName - something like "[I" or "[Ljava.lang.String;"
   * @return array description
   */
  public static VM_Array forName(String arrayName) throws VM_ResolutionException {
    VM_Atom arrayDescriptor = VM_Atom.findOrCreateAsciiAtom(arrayName.replace('.','/'));
    
    ClassLoader cl = VM_SystemClassLoader.getVMClassLoader();
    VM_Array ary =
	VM_ClassLoader.findOrCreateType(arrayDescriptor, cl).asArray();

    ary.load();
    ary.resolve();
    ary.instantiate();
    ary.initialize();

    return ary;
  }

  /**
   * Get description of specified primitive array.
   * @param atype array type number (see "newarray" bytecode description in Java VM Specification)
   * @return array description
   */
  public static VM_Array getPrimitiveArrayType(int atype) {
    switch (atype)
      {
      case  4: 
	return arrayOfBooleanType;
         
      case  5: 
	return arrayOfCharType;
         
      case  6: 
	return arrayOfFloatType;
         
      case  7: 
	return arrayOfDoubleType;
         
      case  8: 
	return arrayOfByteType;
         
      case  9: 
	return arrayOfShortType;
         
      case 10: 
	return arrayOfIntType;
         
      case 11: 
	return arrayOfLongType;
      }

    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
    return null;
  }

  // NOTE: arraycopy for byte[] and boolean[] are identical
  public static void arraycopy(byte[] src, int srcPos, byte[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos >= (dstPos+4))) {
	VM_Memory.arraycopy8Bit(src, srcPos, dst, dstPos, len);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for byte[] and boolean[] are identical
  public static void arraycopy(boolean[] src, int srcPos, boolean[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos >= (dstPos+4))) {
	VM_Memory.arraycopy8Bit(src, srcPos, dst, dstPos, len);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for short[] and char[] are identical
  public static void arraycopy(short[] src, int srcPos, short[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos >= (dstPos+2))) {
	VM_Memory.arraycopy(src, srcPos, dst, dstPos, len);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for short[] and char[] are identical
  public static void arraycopy(char[] src, int srcPos, char[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos >= (dstPos+2))) {
	VM_Memory.arraycopy(src, srcPos, dst, dstPos, len);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }  

   
  // NOTE: arraycopy for int[] and float[] are identical
  public static void arraycopy(int[] src, int srcPos, int[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos > dstPos)) {
	VM_Memory.aligned32Copy(VM_Magic.objectAsAddress(dst).add(dstPos<<2),
				VM_Magic.objectAsAddress(src).add(srcPos<<2),
				len<<2);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for int[] and float[] are identical
  public static void arraycopy(float[] src, int srcPos, float[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos > dstPos)) {
	VM_Memory.aligned32Copy(VM_Magic.objectAsAddress(dst).add(dstPos<<2),
				VM_Magic.objectAsAddress(src).add(srcPos<<2),
				len<<2);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for long[] and double[] are identical
  public static void arraycopy(long[] src, int srcPos, long[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos > dstPos)) {
	VM_Memory.aligned32Copy(VM_Magic.objectAsAddress(dst).add(dstPos<<3),
				VM_Magic.objectAsAddress(src).add(srcPos<<3),
				len<<3);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  // NOTE: arraycopy for long[] and double[] are identical
  public static void arraycopy(double[] src, int srcPos, double[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      // handle as two cases, for efficiency and in case subarrays overlap
      if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos > dstPos)) {
	VM_Memory.aligned32Copy(VM_Magic.objectAsAddress(dst).add(dstPos<<3),
				VM_Magic.objectAsAddress(src).add(srcPos<<3),
				len<<3);
      } else if (srcPos < dstPos) {
	srcPos += len;
	dstPos += len;
	while (len-- != 0)
	  dst[--dstPos] = src[--srcPos];
      } else {
	while (len-- != 0)
	  dst[dstPos++] = src[srcPos++];
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }
   
  public static void arraycopy(Object[] src, int srcPos, Object[] dst, int dstPos, int len) {
    // Don't do any of the assignments if the offsets and lengths
    // are in error
    if (srcPos >= 0 && dstPos >= 0 && len >= 0 && 
	(srcPos+len) <= src.length && (dstPos+len) <= dst.length) {
      int dstStart,dstEnd;
      try {
	VM_Type lhs =VM_Magic.getObjectType(dst).asArray().getElementType();
	VM_Type rhs =VM_Magic.getObjectType(src).asArray().getElementType();
	if ((lhs==rhs) || 
	    (lhs == VM_Type.JavaLangObjectType) || 
	    VM_Runtime.isAssignableWith(lhs, rhs)) {
	  
	  if (len == 0) return;

	  if (VM_Collector.NEEDS_WRITE_BARRIER) {
	    dstStart = dstPos;           
	    dstEnd = dstPos + len - 1;
	    VM.disableGC();     // prevent GC until writebarrier updated
	  }

	  // handle as two cases, for efficiency and in case subarrays overlap
	  if ((! VM.BuildForRealtimeGC) && (src != dst || srcPos > dstPos)) {
	    VM_Memory.aligned32Copy(VM_Magic.objectAsAddress(dst).add(dstPos<<2),
				    VM_Magic.objectAsAddress(src).add(srcPos<<2),
				    len<<2);
	    if (VM.BuildForConcurrentGC) { // dfb: must increment for copied pointers
		VM_Address start = VM_Magic.objectAsAddress(dst).add(dstPos<<2);
		VM_Address end = start.add(len<<2);
		VM_Processor p = VM_Processor.getCurrentProcessor();
		int diff = end.diff(start);
		for (int i = 0; i < diff; i += 4) {
		//-#if RVM_WITH_CONCURRENT_GC // because VM_RCBuffers only available with concurrent memory managers
		VM_RCBuffers.addIncrement(VM_Magic.getMemoryAddress(start.add(i)), p);
		//-#endif
	      }
	    }
	  } else if (srcPos < dstPos) {
	    srcPos = (srcPos + len) << 2;
	    dstPos = (dstPos + len) << 2;
	    while (len-- != 0) {
	      srcPos -= 4;
	      dstPos -= 4;
	      if (! VM.BuildForRealtimeGC)
		  VM_Magic.setObjectAtOffset(dst, dstPos, VM_Magic.getObjectAtOffset(src, srcPos));
	      else
		  dst[dstPos>>2] = src[srcPos>>2];

	      if (VM.BuildForConcurrentGC) {
		//-#if RVM_WITH_CONCURRENT_GC // because VM_RCBuffers only available with concurrent memory managers
		VM_RCBuffers.addIncrement(VM_Magic.getMemoryAddress(VM_Magic.objectAsAddress(dst).add(dstPos)),
					  VM_Processor.getCurrentProcessor());
		//-#endif
	      }
	    }
	  } else {
	    while (len-- != 0)
	      dst[dstPos++] = src[srcPos++];
	  }
	  if (VM_Collector.NEEDS_WRITE_BARRIER) {
	    // generate write buffer entries for modified target array entries
	    VM_WriteBarrier.arrayCopyWriteBarrier(dst, dstStart, dstEnd);
	    VM.enableGC();
	  }
	} else { 
	  // not sure if copy might cause ArrayStoreException, must handle with
	  // element by element assignments, in the right order.
	  // handle as two cases, in case subarrays overlap
	  if (src != dst || srcPos > dstPos) {
	    while (len-- != 0)
	      dst[dstPos++] = src[srcPos++];
	  } else {
	    VM_Array ary = VM_Magic.getObjectType(src).asArray();
	    Object temp[] = 
	      (Object[])VM_Runtime.quickNewArray(len, ary.getInstanceSize(len), 
						 ary.getTypeInformationBlock());
	    int cnt = len;
	    int tempPos = 0;
	    while (cnt-- != 0)
	      temp[tempPos++] = src[srcPos++];
	    tempPos = 0;
	    while (len-- != 0)
	      dst[dstPos++] = temp[tempPos++];
	  }
	}
      } catch (VM_ResolutionException e) {
	failWithNoClassDefFoundError(e);
      }
    } else {
      failWithIndexOutOfBoundsException();
    }
  }

  private static void failWithIndexOutOfBoundsException() throws VM_PragmaNoInline {
    throw new ArrayIndexOutOfBoundsException();
  }

  private static void failWithNoClassDefFoundError(VM_ResolutionException e) throws VM_PragmaNoInline {
    throw new NoClassDefFoundError(e.getException().toString());
  }
   
  private static Object[] javaLangObjectTIB;
  public static VM_Array arrayOfBooleanType;
  public static VM_Array arrayOfByteType;
  public static VM_Array arrayOfShortType;
  public static VM_Array arrayOfIntType;
  public static VM_Array arrayOfLongType;
  public static VM_Array arrayOfFloatType;
  public static VM_Array arrayOfDoubleType;
  public static VM_Array arrayOfCharType;

  private VM_Type  elementType;
  private VM_Type  innermostElementType;
  private Object[] typeInformationBlock;
   
  // To guarantee uniqueness, only the VM_ClassLoader class may construct VM_Array instances.
  // All VM_Array creation should be performed by calling "VM_ClassLoader.findOrCreate" methods.
  //
  private VM_Array() { }

  VM_Array(VM_Atom descriptor, int dictionaryId, ClassLoader classloader) {
    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Array: create " + descriptor + " with " + classloader + "\n");
    this.descriptor     = descriptor;
    this.dimension      = descriptor.parseForArrayDimensionality();
    this.depth          = 1;
    this.dictionaryId   = dictionaryId;
    this.tibSlot        = VM_Statics.allocateSlot(VM_Statics.TIB);
    this.elementType    = VM_ClassLoader.findOrCreateType(descriptor.parseForArrayElementDescriptor(), classloader);
    if (this.elementType.isArrayType()) {
      this.innermostElementType = this.elementType.asArray().getInnermostElementType();
    } else {
      this.innermostElementType = this.elementType;
    }

    if (VM.BuildForConcurrentGC)
      this.acyclic = elementType.isAcyclicReference(); // Array is acyclic if its references are acyclic

    // install partial type information block (type-slot but no method-slots) for use in type checking.
    // later, during instantiate(), we'll replace it with full type information block (including method-slots).
    //
    Object[] tib = VM_RuntimeStructures.newTIB(1);
    tib[0] = this;
    VM_Statics.setSlotContents(tibSlot, tib);
  }

  // Ensure that the elementType is loaded
  // JVM spec says anewarray forces loading of base class   
  // 
  // TODO: this should throw VM_ResolutionException
  public final synchronized void load() {
    if (isLoaded())
      return;

    if (!elementType.isLoaded()) {
      // JVM spec says anewarray forces instantiation of base class
      try {
        elementType.load(); 
      }	catch (VM_ResolutionException e) {
        System.err.println("VM_Array.load: cannot load element type::: " + elementType); // TODO: we should throw e
      }
    }
    state = CLASS_LOADED;
    if (VM.verboseClassLoading) VM.sysWrite("[Loaded "+this.descriptor+"]\n");
    if (VM.verboseClassLoading) VM.sysWrite("[Loaded superclasses of "+this.descriptor+"]\n");
  }

  // Ensure that the elementType is resolved
  // JVM spec says anewarray forces resolution of base class   
  //
  // TODO: this should throw VM_ResolutionException
  public final synchronized void resolve() {
    if (isResolved())
      return;
    if (VM.VerifyAssertions) VM.assert(state == CLASS_LOADED);

    if (elementType.isLoaded() && !elementType.isResolved()) {
      try {
	elementType.resolve(); 
      }	catch (VM_ResolutionException e) {
	System.err.println("VM_Array.resolve: cannot resolve element type::: " + elementType); // TODO: we should throw e
      }
    }
    
    // Using the type information block for java.lang.Object as a template,
    // build a type information block for this new array type by copying the
    // virtual method fields and substuting an appropriate type field.
    //
    if (javaLangObjectTIB == null) {
      VM_Class cls = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("Ljava/lang/Object;"), VM_SystemClassLoader.getVMClassLoader()).asClass();
      javaLangObjectTIB = cls.getTypeInformationBlock();
    }
       
    typeInformationBlock = VM_RuntimeStructures.newTIB(javaLangObjectTIB.length);
    VM_Statics.setSlotContents(tibSlot, typeInformationBlock);
    typeInformationBlock[0] = this;
    if (VM.BuildForFastDynamicTypeCheck) {
      typeInformationBlock[TIB_SUPERCLASS_IDS_INDEX] = VM_DynamicTypeCheck.buildSuperclassIds(this);
      typeInformationBlock[TIB_DOES_IMPLEMENT_INDEX] = VM_DynamicTypeCheck.buildDoesImplement(this);
      if (!elementType.isPrimitiveType() && elementType.isResolved()) {
	typeInformationBlock[TIB_ARRAY_ELEMENT_TIB_INDEX] = elementType.getTypeInformationBlock();
      }
    }
 
    state = CLASS_RESOLVED;
  }

  // Build type information block and install it in jtoc.
  //
  public final synchronized void instantiate() {
    if (isInstantiated())
      return;
    if (VM.VerifyAssertions) VM.assert(state == CLASS_RESOLVED);
  
    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Array: instantiate " + descriptor + "\n");
    
    // Initialize TIB slots for virtual methods (copy from superclass == Object)
    for (int i = TIB_FIRST_VIRTUAL_METHOD_INDEX, n = javaLangObjectTIB.length; i < n; ++i)
      typeInformationBlock[i] = javaLangObjectTIB[i];
    
    state = CLASS_INITIALIZED; // arrays have no "initialize" phase
  }

  // No-op (arrays have no <clinit> method).
  //
  public final void initialize() { }


  static void init() {
    arrayOfBooleanType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[Z"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfCharType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[C"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfFloatType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[F"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfDoubleType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[D"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfByteType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[B"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfShortType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[S"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfIntType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[I"), VM_SystemClassLoader.getVMClassLoader()).asArray();
    arrayOfLongType = VM_ClassLoader.findOrCreateType(VM_Atom.findOrCreateAsciiAtom("[J"), VM_SystemClassLoader.getVMClassLoader()).asArray();
  }

}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/** 
 * A utf8-encoded byte string.
 *
 * <p> VM_Atom's of a given value are stored only once in the vm,
 * so they may be compared for equality using the "==" operator.
 *
 * <p> Atoms are used to represent names, descriptors, and string literals
 * appearing in a class's constant pool.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
public final class VM_Atom implements VM_Constants, VM_ClassLoaderConstants {

   /**
    * Find or create an atom.
    * @param str atom value, as string literal whose characters are unicode
    * @return atom
    */
  public static VM_Atom findOrCreateUnicodeAtom(String str) {
    byte[] utf8 = VM_UTF8Convert.toUTF8(str);
    return VM_AtomDictionary.getValue(findOrCreateAtomId(utf8));
  }

  /**
   * Find or create an atom.
   * @param str atom value, as string literal whose characters are from 
   * ascii subset of unicode (not including null)
   * @return atom
   */ 
  public static VM_Atom findOrCreateAsciiAtom(String str) {
    int    len   = str.length();
    byte[] ascii = new byte[len];
    str.getBytes(0, len, ascii, 0);
    return VM_AtomDictionary.getValue(findOrCreateAtomId(ascii));
  }
   
  /**
   * Find or create an atom.
   * @param utf8 atom value, as utf8 encoded bytes
   * @return id, for use by VM_AtomDictionary.getValue()
   */
  static int findOrCreateAtomId(byte[] utf8) {
    VM_Atom atom = new VM_Atom(utf8);
    return VM_AtomDictionary.findOrCreateId(atom, atom);
  }

  //-------------//
  // conversions //
  //-------------//
   
  /**
   * Return printable representation of "this" atom.
   * Does not correctly handle UTF8 translation.
   */ 
  public final String toString() {
    return new String(val, 0);
  }

  /**
   * Return printable representation of "this" atom.
   */ 
  final String toUnicodeString() throws java.io.UTFDataFormatException { 
    return VM_UTF8Convert.fromUTF8(val);
  }

  /**
   * Return array descriptor corresponding to "this" array-element descriptor.
   * this: array-element descriptor - something like "I" or "Ljava/lang/Object;"
   * @return array descriptor - something like "[I" or "[Ljava/lang/Object;"
   */  
  final VM_Atom arrayDescriptorFromElementDescriptor() {
    byte sig[] = new byte[1 + val.length];
    sig[0] = (byte)'[';
    for (int i = 0, n = val.length; i < n; ++i)
      sig[i + 1] = val[i];
    return findOrCreateAtom(sig);
  }

  /**
   * Return class descriptor corresponding to "this" class name.
   * this: class name       - something like "java/lang/Object"
   * @return class descriptor - something like "Ljava/lang/Object;"
   */ 
  public final VM_Atom descriptorFromClassName() {
    byte sig[] = new byte[1 + val.length + 1];
    sig[0] = (byte)'L';
    for (int i = 0, n = val.length; i < n; ++i)
      sig[i + 1] = val[i];
    sig[sig.length - 1] = (byte)';';
    return findOrCreateAtom(sig);
  }

  /**
   * Return class name corresponding to "this" class descriptor.
   * this: class descriptor - something like "Ljava/lang/String;"
   * @return class name       - something like "java.lang.String"
   */ 
  final String classNameFromDescriptor() {
    if (VM.VerifyAssertions) VM.assert(val[0] == 'L'); // !!TODO: should we also handle "array" type descriptors?
    // return new String(val,    1, val.length - 2).replace('/','.');  // preferred (unicode)
    return new String(val, 0, 1, val.length - 2).replace('/','.');  // deprecated (ascii)
  }
   
  /**
   * Return name of class file corresponding to "this" class descriptor.
   * this: class descriptor - something like "Ljava/lang/String;"
   * @return class file name  - something like "java/lang/String.class"
   */ 
  final String classFileNameFromDescriptor() {
    if (VM.VerifyAssertions && val[0] != 'L') VM.assert(false, toString());
    // return new String(val,    1, val.length - 2) + ".class"; // preferred (unicode)
    return new String(val, 0, 1, val.length - 2) + ".class"; // deprecated (ascii)
  }

  //----------------//
  // classification //
  //----------------//
   
  /**
   * Is "this" atom a reserved member name?
   * Note: Sun has reserved all member names starting with '<' for future use.
   *       At present, only <init> and <clinit> are used.
   */ 
  final boolean isReservedMemberName() {
    return val[0] == '<';
  }

  /**
   * Is "this" atom a class descriptor?
   */ 
  final boolean isClassDescriptor() {
    return val[0] == 'L';
  }
      
  /**
   * Is "this" atom an array descriptor?
   */ 
  final boolean isArrayDescriptor() {
    return val[0] == '[';
  }
      
  /**
   * Is "this" atom a method descriptor?
   */ 
  final boolean isMethodDescriptor() {
    return val[0] == '(';
  }
      
  //--------------------//
  // descriptor parsing //
  //--------------------//
   
  /**
   * Parse "this" method descriptor to obtain description of method's 
   * return type.
   * this: method descriptor - something like "(III)V"
   * @return type description
   */
  final VM_Type parseForReturnType(ClassLoader classloader) {
    if (VM.VerifyAssertions) VM.assert(val[0] == '(');

    int i = 0;
    while (val[i++] != ')');
    switch (val[i])
      {
      case VoidTypeCode:    return VM_Type.VoidType;
      case BooleanTypeCode: return VM_Type.BooleanType;
      case ByteTypeCode:    return VM_Type.ByteType;
      case ShortTypeCode:   return VM_Type.ShortType;
      case IntTypeCode:     return VM_Type.IntType;
      case LongTypeCode:    return VM_Type.LongType;
      case FloatTypeCode:   return VM_Type.FloatType;
      case DoubleTypeCode:  return VM_Type.DoubleType;
      case CharTypeCode:    return VM_Type.CharType;
      case ClassTypeCode:   // fall through
      case ArrayTypeCode:   return VM_ClassLoader.findOrCreateType(findOrCreateAtom(val, i, val.length - i), classloader);
      default:              if (VM.VerifyAssertions) VM.assert(NOT_REACHED); return null;
      }
  }
      
  /**
   * Parse "this" method descriptor to obtain descriptions of method's 
   * parameters.
   * this: method descriptor     - something like "(III)V"
   * @return parameter descriptions
   */ 
  final VM_Type[] parseForParameterTypes(ClassLoader classloader) {
    if (VM.VerifyAssertions) VM.assert(val[0] == '(');

    VM_TypeVector sigs = new VM_TypeVector();
    for (int i = 1;;)
      switch (val[i++])
	{
	case VoidTypeCode:    sigs.addElement(VM_Type.VoidType);     continue;
	case BooleanTypeCode: sigs.addElement(VM_Type.BooleanType);  continue;
	case ByteTypeCode:    sigs.addElement(VM_Type.ByteType);     continue;
	case ShortTypeCode:   sigs.addElement(VM_Type.ShortType);    continue;
	case IntTypeCode:     sigs.addElement(VM_Type.IntType);      continue;
	case LongTypeCode:    sigs.addElement(VM_Type.LongType);     continue;
	case FloatTypeCode:   sigs.addElement(VM_Type.FloatType);    continue;
	case DoubleTypeCode:  sigs.addElement(VM_Type.DoubleType);   continue;
	case CharTypeCode:    sigs.addElement(VM_Type.CharType);     continue;
	case ClassTypeCode: {
	  int off = i - 1;
	  while (val[i++] != ';');
	  sigs.addElement(VM_ClassLoader.findOrCreateType(findOrCreateAtom(val, off, i - off), classloader));
	  continue;
	}
	case ArrayTypeCode: {
	  int off = i - 1;
	  while (val[i] == ArrayTypeCode) ++i;
	  if (val[i++] == ClassTypeCode) while (val[i++] != ';');
	  sigs.addElement(VM_ClassLoader.findOrCreateType(findOrCreateAtom(val, off, i - off), classloader));
	  continue;
	}
	case (byte)')': // end of parameter list
	  return sigs.finish();
            
	default: if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
	}
  }

  /**
   * Parse "this" field, parameter, or return descriptor to obtain its 
   * type code.
   * this: descriptor - something like "Ljava/lang/String;" or "[I" or "I"
   * @return type code  - something like ObjectTypeCode, ArrayTypeCode, or 
   * IntTypeCode
   * 
   * The type code will be one of the following constants:
   * 
   * <pre>
   *               constant         value
   *           ----------------     -----
   *            ClassTypeCode        'L'
   *            ArrayTypeCode        '['
   *            VoidTypeCode         'V'
   *            BooleanTypeCode      'Z'
   *            ByteTypeCode         'B'
   *            ShortTypeCode        'S'
   *            IntTypeCode          'I'
   *            LongTypeCode         'J'
   *            FloatTypeCode        'F'
   *            DoubleTypeCode       'D'
   *            CharTypeCode         'C'
   * </pre>
   */
  final byte parseForTypeCode() {
    return val[0];
  }

  /**
   * Parse "this" array descriptor to obtain number of dimensions in 
   * corresponding array type.
   * this: descriptor     - something like "[Ljava/lang/String;" or "[[I"
   * @return dimensionality - something like "1" or "2"
   */ 
  final int parseForArrayDimensionality() {
    if (VM.VerifyAssertions) VM.assert(val[0] == '[');
    for (int i = 0; ; ++i)
      if (val[i] != '[')
	return i;
  }

  /**
   * Parse "this" array descriptor to obtain type code for its element type.
   * this: descriptor - something like "[Ljava/lang/String;" or "[I"
   * @return type code  - something like VM.ObjectTypeCode or VM.IntTypeCode
   * The type code will be one of the constants appearing in the table above.
   */ 
  final byte parseForArrayElementTypeCode() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(val[0] == '[');
    return val[1];
  }

  /**
   * Parse "this" array descriptor to obtain descriptor for array's element 
   * type.
   * this: array descriptor         - something like "[I"
   * @return array element descriptor - something like "I"
   */
  final VM_Atom parseForArrayElementDescriptor() {
    if (VM.VerifyAssertions) VM.assert(val[0] == '[');
    return findOrCreateAtom(val, 1, val.length - 1);
  }

  //-----------//
  // debugging //
  //-----------//
   
  final void sysWrite() throws VM_PragmaUninterruptible {
    for (int i = 0, n = val.length; i < n; ++i)
      VM.sysWrite((char)val[i]);
  }

  final int length() throws VM_PragmaUninterruptible {
    return val.length;
  }

  /**
   * Access internal representation.
   * (Note: this is intended for the debugger only)
   */ 
  final byte[] getBytes() {
    return val;
  }

  //----------------//
  // implementation //
  //----------------//

  private byte val[];  
  private int  hash;  
   
  /**
   * To guarantee uniqueness, only the VM_Atom class may construct 
   * VM_Atom instances.
   * All VM_Atom creation should be performed by calling 
   * "VM_Atom.findOrCreate" methods.
   */ 
  private VM_Atom() {}
   
  /**
   * Create atom from given utf8 sequence.
   */ 
  private VM_Atom(byte utf8[]) {
    int hash = 99989;
    for (int i = utf8.length; --i >= 0; )
      hash = 99991 * hash + utf8[i];
          
    this.val  = utf8;
    this.hash = hash;
  }

  private static VM_Atom findOrCreateAtom(byte utf8[]) {
    return VM_AtomDictionary.getValue(findOrCreateAtomId(utf8));
  }

  private static VM_Atom findOrCreateAtom(byte utf8[], int off, int len) {
    byte val[] = new byte[len];
    for (int i = 0; i < len; ++i)
      val[i] = utf8[off++];
    return VM_AtomDictionary.getValue(findOrCreateAtomId(val));
  }

  public final int hashCode() {
    return hash;
  }
   
  /**
   * Hash VM_Dictionary keys.
   */ 
  static int dictionaryHash(VM_Atom atom) {
    return atom.hash;
  }

  /**
   * Compare VM_Dictionary keys.
   * @return 0 iff "leftKey" is null
   *           1 iff "leftKey" is to be considered a duplicate of "rightKey"
   *          -1 otherwise
   */
  static int dictionaryCompare(VM_Atom left, VM_Atom right) {
    if (left == null)
      return 0;
         
    if (left.val.length != right.val.length)
      return -1;

    byte[] leftVal  = left.val;
    byte[] rightVal = right.val;
    for (int i = leftVal.length; --i >= 0; )
      if (leftVal[i] != rightVal[i])
	return -1;

    return 1;
  }  
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Information about java byte codes that appear in the "code" attribute 
 * of a .class file.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_BytecodeConstants  {
  // The following mnemonics are defined in Chapter 10 of The Java Virtual Machine Specification.
  //
  public static final int JBC_nop = 0;
  public static final int JBC_aconst_null = 1;
  public static final int JBC_iconst_m1 = 2;
  public static final int JBC_iconst_0 = 3;
  public static final int JBC_iconst_1 = 4;
  public static final int JBC_iconst_2 = 5;
  public static final int JBC_iconst_3 = 6;
  public static final int JBC_iconst_4 = 7;
  public static final int JBC_iconst_5 = 8;
  public static final int JBC_lconst_0 = 9;
  public static final int JBC_lconst_1 = 10;
  public static final int JBC_fconst_0 = 11;
  public static final int JBC_fconst_1 = 12;
  public static final int JBC_fconst_2 = 13;
  public static final int JBC_dconst_0 = 14;
  public static final int JBC_dconst_1 = 15;
  public static final int JBC_bipush = 16;
  public static final int JBC_sipush = 17;
  public static final int JBC_ldc = 18;
  public static final int JBC_ldc_w = 19;
  public static final int JBC_ldc2_w = 20;
  public static final int JBC_iload = 21;
  public static final int JBC_lload = 22;
  public static final int JBC_fload = 23;
  public static final int JBC_dload = 24;
  public static final int JBC_aload = 25;
  public static final int JBC_iload_0 = 26;
  public static final int JBC_iload_1 = 27;
  public static final int JBC_iload_2 = 28;
  public static final int JBC_iload_3 = 29;
  public static final int JBC_lload_0 = 30;
  public static final int JBC_lload_1 = 31;
  public static final int JBC_lload_2 = 32;
  public static final int JBC_lload_3 = 33;
  public static final int JBC_fload_0 = 34;
  public static final int JBC_fload_1 = 35;
  public static final int JBC_fload_2 = 36;
  public static final int JBC_fload_3 = 37;
  public static final int JBC_dload_0 = 38;
  public static final int JBC_dload_1 = 39;
  public static final int JBC_dload_2 = 40;
  public static final int JBC_dload_3 = 41;
  public static final int JBC_aload_0 = 42;
  public static final int JBC_aload_1 = 43;
  public static final int JBC_aload_2 = 44;
  public static final int JBC_aload_3 = 45;
  public static final int JBC_iaload = 46;
  public static final int JBC_laload = 47;
  public static final int JBC_faload = 48;
  public static final int JBC_daload = 49;
  public static final int JBC_aaload = 50;
  public static final int JBC_baload = 51;
  public static final int JBC_caload = 52;
  public static final int JBC_saload = 53;
  public static final int JBC_istore = 54;
  public static final int JBC_lstore = 55;
  public static final int JBC_fstore = 56;
  public static final int JBC_dstore = 57;
  public static final int JBC_astore = 58;
  public static final int JBC_istore_0 = 59;
  public static final int JBC_istore_1 = 60;
  public static final int JBC_istore_2 = 61;
  public static final int JBC_istore_3 = 62;
  public static final int JBC_lstore_0 = 63;
  public static final int JBC_lstore_1 = 64;
  public static final int JBC_lstore_2 = 65;
  public static final int JBC_lstore_3 = 66;
  public static final int JBC_fstore_0 = 67;
  public static final int JBC_fstore_1 = 68;
  public static final int JBC_fstore_2 = 69;
  public static final int JBC_fstore_3 = 70;
  public static final int JBC_dstore_0 = 71;
  public static final int JBC_dstore_1 = 72;
  public static final int JBC_dstore_2 = 73;
  public static final int JBC_dstore_3 = 74;
  public static final int JBC_astore_0 = 75;
  public static final int JBC_astore_1 = 76;
  public static final int JBC_astore_2 = 77;
  public static final int JBC_astore_3 = 78;
  public static final int JBC_iastore = 79;
  public static final int JBC_lastore = 80;
  public static final int JBC_fastore = 81;
  public static final int JBC_dastore = 82;
  public static final int JBC_aastore = 83;
  public static final int JBC_bastore = 84;
  public static final int JBC_castore = 85;
  public static final int JBC_sastore = 86;
  public static final int JBC_pop = 87;
  public static final int JBC_pop2 = 88;
  public static final int JBC_dup = 89;
  public static final int JBC_dup_x1 = 90;
  public static final int JBC_dup_x2 = 91;
  public static final int JBC_dup2 = 92;
  public static final int JBC_dup2_x1 = 93;
  public static final int JBC_dup2_x2 = 94;
  public static final int JBC_swap = 95;
  public static final int JBC_iadd = 96;
  public static final int JBC_ladd = 97;
  public static final int JBC_fadd = 98;
  public static final int JBC_dadd = 99;
  public static final int JBC_isub = 100;
  public static final int JBC_lsub = 101;
  public static final int JBC_fsub = 102;
  public static final int JBC_dsub = 103;
  public static final int JBC_imul = 104;
  public static final int JBC_lmul = 105;
  public static final int JBC_fmul = 106;
  public static final int JBC_dmul = 107;
  public static final int JBC_idiv = 108;
  public static final int JBC_ldiv = 109;
  public static final int JBC_fdiv = 110;
  public static final int JBC_ddiv = 111;
  public static final int JBC_irem = 112;
  public static final int JBC_lrem = 113;
  public static final int JBC_frem = 114;
  public static final int JBC_drem = 115;
  public static final int JBC_ineg = 116;
  public static final int JBC_lneg = 117;
  public static final int JBC_fneg = 118;
  public static final int JBC_dneg = 119;
  public static final int JBC_ishl = 120;
  public static final int JBC_lshl = 121;
  public static final int JBC_ishr = 122;
  public static final int JBC_lshr = 123;
  public static final int JBC_iushr = 124;
  public static final int JBC_lushr = 125;
  public static final int JBC_iand = 126;
  public static final int JBC_land = 127;
  public static final int JBC_ior = 128;
  public static final int JBC_lor = 129;
  public static final int JBC_ixor = 130;
  public static final int JBC_lxor = 131;
  public static final int JBC_iinc = 132;
  public static final int JBC_i2l = 133;
  public static final int JBC_i2f = 134;
  public static final int JBC_i2d = 135;
  public static final int JBC_l2i = 136;
  public static final int JBC_l2f = 137;
  public static final int JBC_l2d = 138;
  public static final int JBC_f2i = 139;
  public static final int JBC_f2l = 140;
  public static final int JBC_f2d = 141;
  public static final int JBC_d2i = 142;
  public static final int JBC_d2l = 143;
  public static final int JBC_d2f = 144;
  public static final int JBC_int2byte = 145;
  public static final int JBC_int2char = 146;
  public static final int JBC_int2short = 147;
  public static final int JBC_lcmp = 148;
  public static final int JBC_fcmpl = 149;
  public static final int JBC_fcmpg = 150;
  public static final int JBC_dcmpl = 151;
  public static final int JBC_dcmpg = 152;
  public static final int JBC_ifeq = 153;
  public static final int JBC_ifne = 154;
  public static final int JBC_iflt = 155;
  public static final int JBC_ifge = 156;
  public static final int JBC_ifgt = 157;
  public static final int JBC_ifle = 158;
  public static final int JBC_if_icmpeq = 159;
  public static final int JBC_if_icmpne = 160;
  public static final int JBC_if_icmplt = 161;
  public static final int JBC_if_icmpge = 162;
  public static final int JBC_if_icmpgt = 163;
  public static final int JBC_if_icmple = 164;
  public static final int JBC_if_acmpeq = 165;
  public static final int JBC_if_acmpne = 166;
  public static final int JBC_goto = 167;
  public static final int JBC_jsr = 168;
  public static final int JBC_ret = 169;
  public static final int JBC_tableswitch = 170;
  public static final int JBC_lookupswitch = 171;
  public static final int JBC_ireturn = 172;
  public static final int JBC_lreturn = 173;
  public static final int JBC_freturn = 174;
  public static final int JBC_dreturn = 175;
  public static final int JBC_areturn = 176;
  public static final int JBC_return = 177;
  public static final int JBC_getstatic = 178;
  public static final int JBC_putstatic = 179;
  public static final int JBC_getfield = 180;
  public static final int JBC_putfield = 181;
  public static final int JBC_invokevirtual = 182;
  public static final int JBC_invokespecial = 183;
  public static final int JBC_invokestatic = 184;
  public static final int JBC_invokeinterface = 185;
  public static final int JBC_xxxunusedxxx = 186;
  public static final int JBC_new = 187;
  public static final int JBC_newarray = 188;
  public static final int JBC_anewarray = 189;
  public static final int JBC_arraylength = 190;
  public static final int JBC_athrow = 191;
  public static final int JBC_checkcast = 192;
  public static final int JBC_instanceof = 193;
  public static final int JBC_monitorenter = 194;
  public static final int JBC_monitorexit = 195;
  public static final int JBC_wide = 196;
  public static final int JBC_multianewarray = 197;
  public static final int JBC_ifnull = 198;
  public static final int JBC_ifnonnull = 199;
  public static final int JBC_goto_w = 200;
  public static final int JBC_jsr_w = 201;

   // Length of each instruction introduced by the above bytecodes.
   // -1 indicates a variable length instruction.
   // -2 indicates an unused instruction.
   //
  public static final byte JBC_length[] =
  {
    1, // nop
    1, // aconst_null
    1, // iconst_m1
    1, // iconst_0
    1, // iconst_1
    1, // iconst_2
    1, // iconst_3
    1, // iconst_4
    1, // iconst_5
    1, // lconst_0
    1, // lconst_1
    1, // fconst_0
    1, // fconst_1
    1, // fconst_2
    1, // dconst_0
    1, // dconst_1
    2, // bipush
    3, // sipush
    2, // ldc
    3, // ldc_w
    3, // ldc2_w
    2, // iload
    2, // lload
    2, // fload
    2, // dload
    2, // aload
    1, // iload_0
    1, // iload_1
    1, // iload_2
    1, // iload_3
    1, // lload_0
    1, // lload_1
    1, // lload_2
    1, // lload_3
    1, // fload_0
    1, // fload_1
    1, // fload_2
    1, // fload_3
    1, // dload_0
    1, // dload_1
    1, // dload_2
    1, // dload_3
    1, // aload_0
    1, // aload_1
    1, // aload_2
    1, // aload_3
    1, // iaload
    1, // laload
    1, // faload
    1, // daload
    1, // aaload
    1, // baload
    1, // caload
    1, // saload
    2, // istore
    2, // lstore
    2, // fstore
    2, // dstore
    2, // astore
    1, // istore_0
    1, // istore_1
    1, // istore_2
    1, // istore_3
    1, // lstore_0
    1, // lstore_1
    1, // lstore_2
    1, // lstore_3
    1, // fstore_0
    1, // fstore_1
    1, // fstore_2
    1, // fstore_3
    1, // dstore_0
    1, // dstore_1
    1, // dstore_2
    1, // dstore_3
    1, // astore_0
    1, // astore_1
    1, // astore_2
    1, // astore_3
    1, // iastore
    1, // lastore
    1, // fastore
    1, // dastore
    1, // aastore
    1, // bastore
    1, // castore
    1, // sastore
    1, // pop
    1, // pop2
    1, // dup
    1, // dup_x1
    1, // dup_x2
    1, // dup2
    1, // dup2_x1
    1, // dup2_x2
    1, // swap
    1, // iadd
    1, // ladd
    1, // fadd
    1, // dadd
    1, // isub
    1, // lsub
    1, // fsub
    1, // dsub
    1, // imul
    1, // lmul
    1, // fmul
    1, // dmul
    1, // idiv
    1, // ldiv
    1, // fdiv
    1, // ddiv
    1, // irem
    1, // lrem
    1, // frem
    1, // drem
    1, // ineg
    1, // lneg
    1, // fneg
    1, // dneg
    1, // ishl
    1, // lshl
    1, // ishr
    1, // lshr
    1, // iushr
    1, // lushr
    1, // iand
    1, // land
    1, // ior
    1, // lor
    1, // ixor
    1, // lxor
    3, // iinc
    1, // i2l
    1, // i2f
    1, // i2d
    1, // l2i
    1, // l2f
    1, // l2d
    1, // f2i
    1, // f2l
    1, // f2d
    1, // d2i
    1, // d2l
    1, // d2f
    1, // int2byte
    1, // int2char
    1, // int2short
    1, // lcmp
    1, // fcmpl
    1, // fcmpg
    1, // dcmpl
    1, // dcmpg
    3, // ifeq
    3, // ifne
    3, // iflt
    3, // ifge
    3, // ifgt
    3, // ifle
    3, // if_icmpeq
    3, // if_icmpne
    3, // if_icmplt
    3, // if_icmpge
    3, // if_icmpgt
    3, // if_icmple
    3, // if_acmpeq
    3, // if_acmpne
    3, // goto
    3, // jsr
    2, // ret
    -1, // tableswitch
    -1, // lookupswitch
    1, // ireturn
    1, // lreturn
    1, // freturn
    1, // dreturn
    1, // areturn
    1, // return
    3, // getstatic
    3, // putstatic
    3, // getfield
    3, // putfield
    3, // invokevirtual
    3, // invokenonvirtual
    3, // invokestatic
    5, // invokeinterface
    -2, // xxxunusedxxx
    3, // new
    2, // newarray
    3, // anewarray
    1, // arraylength
    1, // athrow
    3, // checkcast
    3, // instanceof
    1, // monitorenter
    1, // monitorexit
    -1, // wide
    4, // multianewarray
    3, // ifnull
    3, // ifnonnull
    5, // goto_w
    5, // jsr_w
  };
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.IOException;
import java.io.FileNotFoundException;
// Following for beginning of classloader support
import java.lang.ClassLoader;
import java.io.InputStream;
import java.io.DataInputStream;

/**
 *  Description of a java "class" type.
 * 
 * <p> This description is read from a ".class" file as classes/field/methods
 * referenced by the running program need to be bound in to the running image.
 * 
 * @see VM_Array
 * @see VM_Primitive
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
public final class VM_Class extends VM_Type
  implements VM_Constants, VM_ClassLoaderConstants {

  //------------------------------------------------------------------//
  //                              Section 0.                          //
  //                   The following are always available.            //
  //------------------------------------------------------------------//

  /**
   * Name - something like "java.lang.String".
   */ 
  public final String getName() {
    return descriptor.classNameFromDescriptor();
  }

  /**
   * Stack space requirement.
   */ 
  public final int getStackWords() throws VM_PragmaUninterruptible {
    return 1;
  }

  //--------------------------------------------------------------------//
  //                           Section 1.                               //
  //  The following are available after the class has been "loaded".    //
  //--------------------------------------------------------------------//


  //
  // Attributes.
  //

  /**
   * An "interface" description rather than a "class" description?
   */ 
  public final boolean isInterface() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_INTERFACE) != 0; 
  } 

  /**
   * Usable from other packages?
   */ 
  final boolean isPublic() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_PUBLIC) != 0; 
  }

  /**
   * Non-subclassable?
   */ 
  final boolean isFinal() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_FINAL) != 0; 
  }

  /**
   * Non-instantiable?
   */ 
  final boolean isAbstract() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_ABSTRACT) != 0; 
  }

  /**
   * Use new-style "invokespecial" semantics for method calls in this class?
   */ 
  final boolean isSpecial() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_SPECIAL) != 0; 
  }

  public int getModifiers() {
    return modifiers;
  }

  /**
   * Name of source file from which class was compiled - 
   * something like "c:\java\src\java\lang\Object.java".
   * (null --> "unknown - wasn't recorded by compiler").
   */
  final VM_Atom getSourceName() { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return sourceName;
  }

  /**
   * Superclass of this class (null means "no superclass", 
   * ie. class is "java/lang/Object").
   */
  public final VM_Class getSuperClass() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return superClass;
  }

  /**
   * Currently loaded classes that "extend" this class.
   */ 
  final VM_Class[] getSubClasses() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return subClasses;
  }

  /**
   * Interfaces implemented directly by this class 
   * (ie. not including superclasses).
   */
  public final VM_Class[] getDeclaredInterfaces() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return declaredInterfaces;
  }

  /**
   * Fields defined directly by this class (ie. not including superclasses).
   */ 
  public final VM_Field[] getDeclaredFields() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return declaredFields;
  }

  /**
   * Methods defined directly by this class (ie. not including superclasses).
   */
  public final VM_Method[] getDeclaredMethods() throws VM_PragmaUninterruptible { 
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return declaredMethods;
  }

  /**
   * Static initializer method for this class (null -> no static initializer
   *  or initializer already been run).
   */ 
  final VM_Method getClassInitializerMethod() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return classInitializerMethod;
  }

  /** 
   * Find description of a field of this class.
   * @param fieldName field name - something like "foo"
   * @param fieldDescriptor field descriptor - something like "I"
   * @return description (null --> not found)
   */ 
  final VM_Field findDeclaredField(VM_Atom fieldName, VM_Atom fieldDescriptor) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    for (int i = 0, n = declaredFields.length; i < n; ++i) {
      VM_Field field = declaredFields[i];
      if (field.getName() == fieldName && 
	  field.getDescriptor() == fieldDescriptor)
	return field;
    }
    return null;
  }

  /** 
   * Find description of a method of this class.
   * @param methodName method name - something like "foo"
   * @param methodDescriptor method descriptor - something like "()I"
   * @return description (null --> not found)
   */ 
  final VM_Method findDeclaredMethod(VM_Atom methodName, 
                                     VM_Atom methodDescriptor) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    for (int i = 0, n = declaredMethods.length; i < n; ++i) {
      VM_Method method = declaredMethods[i];
      if (method.getName() == methodName && 
	  method.getDescriptor() == methodDescriptor)
	return method;
    }
    return null;
  }

  /**
   * Find description of "public static void main(String[])" 
   * method of this class.
   * @return description (null --> not found)
   */ 
  final VM_Method findMainMethod() {
    VM_Atom   mainName       = VM_Atom.findOrCreateAsciiAtom(("main"));
    VM_Atom   mainDescriptor = VM_Atom.findOrCreateAsciiAtom
      (("([Ljava/lang/String;)V"));
    VM_Method mainMethod     = this.findDeclaredMethod(mainName, 
                                                       mainDescriptor);

    if (mainMethod == null   ||
	!mainMethod.isPublic() ||
	!mainMethod.isStatic() ) { 
      // no such method
      return null;
    }
    return mainMethod;
  }

  //
  // Constant pool accessors.
  //
  // The constant pool holds literals and external references used by 
  // the bytecodes of this class's methods.
  // Items are fetched by specifying their "constant pool index".
  //

  /**
   * Get offset of a literal constant, in bytes.
   * Offset is with respect to virtual machine's "table of contents" (jtoc).
   */ 
  final int getLiteralOffset(int constantPoolIndex) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    // jtoc slot number --> jtoc offset
    return constantPool[constantPoolIndex] << 2; 
  }

  /**
   * Get description of a literal constant.
   */ 
  final byte getLiteralDescription(int constantPoolIndex) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    // jtoc slot number --> description
    return VM_Statics.getSlotDescription(constantPool[constantPoolIndex]); 
  }

  /**
   * Get contents of a "typeRef" constant pool entry.
   * @return id of type that was referenced, for use 
   * by "VM_TypeDictionary.getValue()"
   */
  final int getTypeRefId(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return constantPool[constantPoolIndex];
  }

  /**
   * Get contents of a "typeRef" constant pool entry.
   * @return type that was referenced
   */
  final VM_Type getTypeRef(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return VM_TypeDictionary.getValue(getTypeRefId(constantPoolIndex));
  }

  /**
   * Get contents of a "fieldRef" constant pool entry.
   * @return id of field that was referenced, for use by 
   * "VM_FieldDictionary.getValue()"
   */
  final int getFieldRefId(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return constantPool[constantPoolIndex];
  }

  /**
   * Get contents of a "fieldRef" constant pool entry.
   * @return field that was referenced
   */
  final VM_Field getFieldRef(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return VM_FieldDictionary.getValue(constantPool[constantPoolIndex]);
  }

  /**
   * Get contents of a "methodRef" constant pool entry.
   * @return id of method that was referenced, for use by 
   * "VM_MethodDictionary.getValue()"
   */ 
  final int getMethodRefId(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return constantPool[constantPoolIndex];
  }

  /**
   * Get contents of a "methodRef" constant pool entry.
   * @return method that was referenced
   */
  final VM_Method getMethodRef(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return VM_MethodDictionary.getValue(constantPool[constantPoolIndex]);
  }

  /**
   * Get contents of a "utf" constant pool entry.
   */
  final VM_Atom getUtf(int constantPoolIndex) throws VM_PragmaUninterruptible {
    return VM_AtomDictionary.getValue(constantPool[constantPoolIndex]);
  }

  /**
   * Does this object implement the VM_SynchronizedObject interface?
   * @see VM_SynchronizedObject
   */ 
  final boolean isSynchronizedObject() throws VM_PragmaUninterruptible {
    VM_Class[] interfaces = getDeclaredInterfaces();
    for (int i = 0, n = interfaces.length; i < n; ++i)
      if (interfaces[i].isSynchronizedObjectType()) return true;
    return false;
  }

  /**
   * Should the methods of this class be compiled with special 
   * register save/restore logic?
   * @see VM_DynamicBridge
   */
  final boolean isDynamicBridge () throws VM_PragmaUninterruptible {
    VM_Class[] interfaces = getDeclaredInterfaces();
    for (int i = 0, n = interfaces.length; i < n; ++i)
      if (interfaces[i].isDynamicBridgeType()) return true;
    return false;
  }

  /**
   * The methods of this class are only called from native code, 
   * they are compiled with
   * a special prolog to interface with the native stack frame.
   */
  public final boolean isBridgeFromNative() throws VM_PragmaUninterruptible {
    // The only class that returns true is the VM_JNIFunctions
    // which must have been loaded by the first call to System.loadLibrary
    // If this class is not loaded yet, we can assume that it
    // is not the VM_JNIFunctions
    if (!isLoaded())
      return false;
    VM_Class[] interfaces = getDeclaredInterfaces();
    for (int i = 0, n = interfaces.length; i < n; ++i)
      if (interfaces[i].isNativeBridgeType()) return true;
    return false;
  }

  /**
   * Should the methods of this class save incoming registers ?
   * @see VM_SaveVolatile
   */
  final boolean isSaveVolatile() throws VM_PragmaUninterruptible {
    VM_Class[] interfaces = getDeclaredInterfaces();
    for (int i = 0, n = interfaces.length; i < n; ++i)
      if (interfaces[i].isSaveVolatileType()) return true;
    return false; 
  }

  //--------------------------------------------------------------------//
  //                         Section 2.                                 //
  // The following are available after the class has been "resolved".   //
  //--------------------------------------------------------------------//

  /**
   * Which class loader loaded this class?  (null => system class loader):CRA 
   * final ClassLoader 
   */
  public ClassLoader getClassLoader () { 
    return classloader; 
  } 

  /**
   * Does this class override java.lang.Object.finalize()?
   */
  public final boolean hasFinalizer() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return (finalizeMethod != null);
  }

  /**
   * Get finalize method that overrides java.lang.Object.finalize(), 
   * if one exists
   */
  public final VM_Method getFinalizer() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return finalizeMethod;
  }

  /**
   * Static fields of this class.
   * Values in these fields are shared by all class instances.
   */
  public final VM_Field[] getStaticFields() {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return staticFields;
  }

  /**
   * Non-static fields of this class (composed with supertypes, if any).
   * Values in these fields are distinct for each class instance.
   */
  public final VM_Field[] getInstanceFields() {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return instanceFields;
  }

  /**
   * Statically dispatched methods of this class.
   */
  public final VM_Method[] getStaticMethods() {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return staticMethods;
  }

  /**
   * Virtually dispatched methods of this class 
   * (composed with supertypes, if any).
   */
  public final VM_Method[] getVirtualMethods() {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return virtualMethods;
  }

  /**
   * Total size, in bytes, of an instance of this class 
   * (including object header).
   */
  public final int getInstanceSize() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return instanceSize;
  }

  final int getInstanceSizeInternal() throws VM_PragmaUninterruptible {
    return instanceSize;
  }

  /**
   * Add a field to the object; only meant to be called from VM_ObjectModel et al.
   * must be called when lock on class object is already held (ie from resolve).
   */
  final void increaseInstanceSize(int numBytes) throws VM_PragmaUninterruptible {
    instanceSize += numBytes;
  }

  /**
   * Offsets of reference-containing instance fields of this class type.
   * Offsets are with respect to object pointer -- see VM_Field.getOffset().
   */
  public final int[] getReferenceOffsets() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return referenceOffsets;
  }

  /**
   * Find specified virtual method description.
   * @param memberName   method name - something like "foo"
   * @param memberDescriptor method descriptor - something like "I" or "()I"
   * @return method description (null --> not found)
   */
  final VM_Method findVirtualMethod(VM_Atom memberName, 
                                    VM_Atom memberDescriptor) {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    VM_Method methods[] = getVirtualMethods();
    for (int i = 0, n = methods.length; i < n; ++i) {
      VM_Method method = methods[i];
      if (method.getName() == memberName && 
	  method.getDescriptor() == memberDescriptor)
	return method;
    }
    return null;
  }

  /**
   * Find specified static method description.
   * @param memberName method name - something like "foo"
   * @param memberDescriptor method descriptor - something like "I" or "()I"
   * @return method description (null --> not found)
   */
  final VM_Method findStaticMethod(VM_Atom memberName, 
                                   VM_Atom memberDescriptor) {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    VM_Method methods[] = getStaticMethods();
    for (int i = 0, n = methods.length; i < n; ++i) {
      VM_Method method = methods[i];
      if (method.getName() == memberName && 
	  method.getDescriptor() == memberDescriptor)
	return method;
    }
    return null;
  }

  /**
   * Find specified initializer method description.
   * @param    init method descriptor - something like "(I)V"
   * @return method description (null --> not found)
   */
  final VM_Method findInitializerMethod(VM_Atom memberDescriptor) {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    VM_Method methods[] = getStaticMethods();
    for (int i = 0, n = methods.length; i < n; ++i) {
      VM_Method method = methods[i];
      if (method.isObjectInitializer() && 
          method.getDescriptor() == memberDescriptor)
        return method;
    }
    return null;
  }

  /**
   * Runtime type information for this class type.
   */
  public final Object[] getTypeInformationBlock() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    return typeInformationBlock;
  }

  //--------------------------------------------------------------------//
  //                          Section 3.                                //
  // The following are available after the class has been "instantiated"//
  //--------------------------------------------------------------------//

  //--------------------------------------------------------------------//
  //                          Section 4.                                //
  //                     Miscellaneous queries.                         //
  //---------------------------------------------------------------------//


  /**
   * Support for user-written class loaders:
   * It's required to find the classloader of the class
   * whose method requires another class to be loaded;
   * the initiating loader of the required class is the
   * defining loader of the requiring class.
   *
   * @author Julian Dolby
   * 
   * @param frames specifies the number of frames back from the 
   * caller to the method whose class's loader is required
   */
  public static ClassLoader getClassLoaderFromStackFrame(int skip) {
      skip++; // account for stack frame of this function
      VM_StackBrowser browser = new VM_StackBrowser();
      VM.disableGC();
      browser.init();
      while (skip-- > 0) browser.up();
      VM.enableGC();
      return browser.getClassLoader();
  }

  /**
   * Load, resolve, instantiate, and initialize specified class.
   * @param className class name - something like "java.lang.String"
   * @return class description
   */
  public static VM_Class forName(String className) 
    throws VM_ResolutionException {
    VM_Atom classDescriptor = VM_Atom.findOrCreateAsciiAtom
      (className.replace('.','/')).descriptorFromClassName();

    ClassLoader cl = VM_SystemClassLoader.getVMClassLoader();
    VM_Class cls = 
	VM_ClassLoader.findOrCreateType(classDescriptor, cl).asClass();

    cls.load();
    cls.resolve();
    cls.instantiate();
    cls.initialize();

    return cls;
  }

  /**
   * Find specified method using "invokespecial" lookup semantics.
   * <p> There are three kinds of "special" method invocation:
   * <ul>
   *   <li> an instance initializer method, eg. <init>
   *   <li> a non-static but private and/or final method in the current class
   *   <li> a non-static method for which the non-overridden (superclass) 
   *   version is desired
   * </ul>
   *
   * @param sought method sought
   * @return method found (null --> not found)
   */ 
  static VM_Method findSpecialMethod(VM_Method sought) {
    if (sought.isObjectInitializer())
      return sought;   // <init>

    VM_Class cls = sought.getDeclaringClass();
    if (!cls.isSpecial())
      return sought;   // old-style invokespecial semantics

    for (; cls != null; cls = cls.getSuperClass()) {
      VM_Method found = cls.findDeclaredMethod(sought.getName(), 
                                               sought.getDescriptor());
      if (found != null)
	return found; // new-style invokespecial semantics
    }
    return null;        // not found
  }


  // getter and setter for constant pool
  final int[] getConstantPool () {
    return constantPool;
  }

  final void setConstantPool(int[] pool) {
    constantPool = pool;
  }

  //----------------//
  // implementation //
  //----------------//

  //
  // The following are always valid.
  //

  // add field to identify the class Loader for this class
  // 06/19/00 CRA:
  //
  ClassLoader  classloader; 

  //
  // The following are valid only when "state >= CLASS_LOADED".
  //
  private int[]        constantPool;
  private int          modifiers;
  private VM_Class     superClass;
  private VM_Class[]   subClasses;
  private VM_Class[]   declaredInterfaces;
  private VM_Field[]   declaredFields;
  private VM_Method[]  declaredMethods;
  private VM_Atom      sourceName;
  private VM_Method    classInitializerMethod;

  // Cannonical empty arrays
  private static final VM_Class[] emptyVMClass = new VM_Class[0];
  private static final VM_Field[] emptyVMField = new VM_Field[0];
  private static final VM_Method[] emptyVMMethod = new VM_Method[0];

  //
  // The following are valid only when "state >= CLASS_RESOLVED".
  //

  // --- Field size and offset information --- //
  //
  /**
   * fields shared by all instances of class
   */
  private VM_Field[]   staticFields;           
  /**
   * fields distinct for each instance of class
   */
  private VM_Field[]   instanceFields;         
  /**
   * total size of per-instance data, in bytes
   */
  private int          instanceSize;  
  /**
   * offsets of reference-containing instance fields
   */
  private int[]        referenceOffsets;       
  //
  // --- Method-dispatching information ---    //
  //
  /**
   * static methods of class
   */
  private VM_Method[]  staticMethods;          
  /**
   * virtual methods of class
   */
  private VM_Method[]  virtualMethods;         

  /**
   * method that overrides java.lang.Object.finalize()
   * null => class does not have a finalizer
   */
  private VM_Method    finalizeMethod;         

  //
  // The following are valid only when "state >= IS_INSTANTIATED".
  //

  /**
   * type and virtual method dispatch table for class
   */
  private Object[]     typeInformationBlock;   


  /**
   * To guarantee uniqueness, only the VM_ClassLoader class may 
   * construct VM_Class instances.
   * All VM_Class creation should be performed by calling 
   * "VM_ClassLoader.findOrCreate" methods.
   */ 
  private VM_Class() { }

  VM_Class(VM_Atom descriptor, int dictionaryId, ClassLoader classloader) {
    this.descriptor   = descriptor;
    this.dictionaryId = dictionaryId;
    this.tibSlot      = VM_Statics.allocateSlot(VM_Statics.TIB);
    this.subClasses   = emptyVMClass;
    this.classloader  = classloader;

    // install partial type information block 
    // (type-slot but no method-slots) for use in type checking.
    // later, during instantiate(), we'll replace it with full type 
    // information block (including method-slots).
    //
    if (VM.VerifyAssertions) VM.assert(TIB_TYPE_INDEX == 0);
    Object[] tib = VM_RuntimeStructures.newTIB(1);
    tib[TIB_TYPE_INDEX] = this;
    VM_Statics.setSlotContents(tibSlot, tib);
  }

  // for close world testing
  public static boolean classLoadingDisabled = false;

  /**
   * Read this class's description from its .class file.
   */ 
  public final synchronized void load() throws VM_ResolutionException {
    if (isLoaded()) return;

    if (classLoadingDisabled) {
      throw new RuntimeException("ClassLoading Disabled : "
				 +this.getDescriptor());
    }

    if (VM.TraceClassLoading && VM.runningVM) 
      VM.sysWrite("VM_Class: (begin) load " + descriptor + "\n");

    VM_Thread myThread;
    
    try {
      classloader.loadClass( getName().toString() );
    } catch (ClassNotFoundException e) { 
      // no .class file
      throw new VM_ResolutionException(descriptor, 
				       new NoClassDefFoundError
					 (descriptor.classNameFromDescriptor()));
    } catch (ClassFormatError e) { 
      // not really a .class file
      throw new VM_ResolutionException(descriptor, e);
    }

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (end)   load " + descriptor + "\n");
  }

  /**
   * Read this class's description from specified data stream.
   */ 
  final synchronized void load(DataInputStream input) throws ClassFormatError, IOException {
    if (isLoaded()) return;

    if (classLoadingDisabled) {
      throw new RuntimeException("ClassLoading Disabled : "
				 +this.getDescriptor());
    }

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (begin) load file " 
                                          + descriptor + "\n");
    if (VM.VerifyAssertions) VM.assert(state == CLASS_VACANT);

    if (input.readInt() != 0xCAFEBABE)
      throw new ClassFormatError("bad magic number");

    if (input.readUnsignedShort() != 3 || input.readUnsignedShort() != 45)
      throw new ClassFormatError("bad version number");

    //
    // pass 1: read constant pool
    //
    int  tmpPool[] = new int[input.readUnsignedShort()];
    byte tmpTags[] = new byte[tmpPool.length];

    // note: slot 0 is unused
    for (int i = 1; i < tmpPool.length; ++i) {
      switch (tmpTags[i] = input.readByte())
	{
	case TAG_UTF: 
	  {
	    byte utf[] = new byte[input.readUnsignedShort()];
	    input.readFully(utf);
	    tmpPool[i] = VM_Atom.findOrCreateAtomId(utf);
	    break;  
	  }

	case TAG_UNUSED:
	  if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
	  break;

	case TAG_INT:
	  tmpPool[i] = VM_Statics.findOrCreateIntLiteral(input.readInt());
	  break;

	case TAG_FLOAT:
	  tmpPool[i] = VM_Statics.findOrCreateFloatLiteral(input.readInt());
	  break;

	case TAG_LONG:
	  tmpPool[i++] = VM_Statics.findOrCreateLongLiteral(input.readLong());
	  break;

	case TAG_DOUBLE:
	  tmpPool[i++] = VM_Statics.findOrCreateDoubleLiteral(input.readLong());
	  break;

	case TAG_TYPEREF:
	  tmpPool[i] = input.readUnsignedShort();
	  break;

	case TAG_STRING:
	  tmpPool[i] = input.readUnsignedShort();
	  break;

	case TAG_FIELDREF:
	case TAG_METHODREF:
	case TAG_INTERFACE_METHODREF: 
	  {
	    int classDescriptorIndex         = input.readUnsignedShort();
	    int memberNameAndDescriptorIndex = input.readUnsignedShort();
	    tmpPool[i] = (classDescriptorIndex << 16) | 
              memberNameAndDescriptorIndex;
	    break; 
	  }

	case TAG_MEMBERNAME_AND_DESCRIPTOR:
	  {
	    int memberNameIndex = input.readUnsignedShort();
	    int descriptorIndex = input.readUnsignedShort();
	    tmpPool[i] = (memberNameIndex << 16) | descriptorIndex;
	    break;  
	  }
	
	default:
	  throw new ClassFormatError("bad constant pool");
	}
    }

    //
    // pass 2: post-process type, method, field, and string constant 
    // pool entries
    //         (we must do this in a second pass because of forward references)
    //

    try {
      constantPool = new int[tmpPool.length];
      for (int i = 1, n = tmpPool.length; i < n; ++i) {
	switch (tmpTags[i])
	  {
	  case TAG_UTF: // in: atom dictionary id
	    constantPool[i] = tmpPool[i];
	    break; // out: atom dictionary id

	  case TAG_UNUSED:
	    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
	    break;

	  case TAG_INT: 
	  case TAG_FLOAT: // in: jtoc slot number
	    constantPool[i] = tmpPool[i];
	    break; // out: jtoc slot number
	    
	  case TAG_LONG:
	  case TAG_DOUBLE: // in: jtoc slot number
	    constantPool[i] = tmpPool[i];
	    ++i;
	    break; // out: jtoc slot number

	  case TAG_TYPEREF: { // in: utf index
	    VM_Atom typeName = VM_AtomDictionary.getValue(tmpPool[tmpPool[i]]);
	    if (typeName.isArrayDescriptor())
	      constantPool[i] = VM_ClassLoader.findOrCreateTypeId(typeName, classloader);
	    else
	      constantPool[i] = VM_ClassLoader.findOrCreateTypeId
		(typeName.descriptorFromClassName(), classloader);
	    break; } // out: type dictionary id

	  case TAG_STRING: 
	    { // in: utf index
	      int utfIndex = tmpPool[i];
	      constantPool[i] = VM_Statics.findOrCreateStringLiteral
		(VM_AtomDictionary.getValue(tmpPool[utfIndex]));
	      break; 
	    } // out: jtoc slot number

	  case TAG_FIELDREF:
	  case TAG_METHODREF:
	  case TAG_INTERFACE_METHODREF: 
	    { // in: classname+membername+memberdescriptor indices
	      int bits                         = tmpPool[i];
	      int classNameIndex               = (bits >> 16) & 0xffff;
	      int memberNameAndDescriptorIndex = bits & 0xffff;
	      int memberNameAndDescriptorBits  = tmpPool[memberNameAndDescriptorIndex];
	      int memberNameIndex              = (memberNameAndDescriptorBits >> 16) & 0xffff;
	      int memberDescriptorIndex        = (memberNameAndDescriptorBits       ) & 0xffff;

	      VM_Atom className        = VM_AtomDictionary.getValue
		(tmpPool[tmpPool[classNameIndex]]);
	      VM_Atom classDescriptor  = className.descriptorFromClassName();
	      VM_Atom memberName       = VM_AtomDictionary.getValue
		(tmpPool[memberNameIndex]);
	      VM_Atom memberDescriptor = VM_AtomDictionary.getValue
		(tmpPool[memberDescriptorIndex]);

	      constantPool[i] = (tmpTags[i] == TAG_FIELDREF)
		? VM_ClassLoader.findOrCreateFieldId(classDescriptor, memberName, memberDescriptor, classloader)
		: VM_ClassLoader.findOrCreateMethodId(classDescriptor, memberName, memberDescriptor, classloader);
	      break; } // out: field or method dictionary id
	    
	  case TAG_MEMBERNAME_AND_DESCRIPTOR: // in: member+descriptor indices
	    constantPool[i] = -1;
	    break; // out: nothing 
	    // (this constant pool entry is no longer needed)

	  default:
	    if (VM.VerifyAssertions) VM.assert(NOT_REACHED);
	  }
      }
    } catch (java.io.UTFDataFormatException x) {
      throw new ClassFormatError(x.toString());
    }
    
    modifiers         = input.readUnsignedShort();
    VM_Type myType    = getTypeRef(input.readUnsignedShort()); 
    if (myType != this) { 
      // eg. file contains a different class than would be 
      // expected from its .class file name
      throw new ClassFormatError("expected class \"" + this 
                                 + "\" but found \"" + myType + "\"");
    }
    VM_Type superType = getTypeRef(input.readUnsignedShort()); // possibly null
    if (superType != null) {
      superClass = superType.asClass();
      superClass.addSubClass(this);
    }

    int numInterfaces = input.readUnsignedShort();
    if (numInterfaces == 0) {
      declaredInterfaces = emptyVMClass;
    } else {
      declaredInterfaces = new VM_Class[numInterfaces];
      for (int i = 0, n = declaredInterfaces.length; i < n; ++i)
	declaredInterfaces[i] = getTypeRef(input.readUnsignedShort()).asClass();
    }

    int numFields = input.readUnsignedShort();
    if (numFields == 0) {
      declaredFields = emptyVMField;
    } else {
      declaredFields = new VM_Field[numFields];
      for (int i = 0, n = declaredFields.length; i < n; ++i) {
	int      modifiers       = input.readUnsignedShort();
	VM_Atom  fieldName       = VM_AtomDictionary.getValue(constantPool[input.readUnsignedShort()]);
	VM_Atom  fieldDescriptor = VM_AtomDictionary.getValue(constantPool[input.readUnsignedShort()]);
	VM_Field field           = VM_ClassLoader.findOrCreateField(getDescriptor(), fieldName, fieldDescriptor, classloader);
	
	field.load(input, modifiers);
	declaredFields[i] = field;
      }
    }

    int numMethods = input.readUnsignedShort();
    if (numMethods == 0) {
      declaredMethods = emptyVMMethod;
    } else {
      declaredMethods = new VM_Method[numMethods];
      for (int i = 0, n = declaredMethods.length; i < n; ++i) {
	int       modifiers        = input.readUnsignedShort();
	VM_Atom   methodName       = VM_AtomDictionary.getValue(constantPool[input.readUnsignedShort()]);
	VM_Atom   methodDescriptor = VM_AtomDictionary.getValue(constantPool[input.readUnsignedShort()]);
	VM_Method method           = VM_ClassLoader.findOrCreateMethod(getDescriptor(), methodName, methodDescriptor, classloader);
	
	method.load(input, modifiers);
	declaredMethods[i] = method;
	if (method.isClassInitializer())
	  classInitializerMethod = method;
      }
    }

    for (int i = 0, n = input.readUnsignedShort(); i < n; ++i) {
      VM_Atom attName   = getUtf(input.readUnsignedShort());
      int     attLength = input.readInt();

      // Class attributes
      if (attName == VM_ClassLoader.sourceFileAttributeName && attLength == 2) {
	sourceName = getUtf(input.readUnsignedShort());
	continue;
      }

      if (attName == VM_ClassLoader.innerClassesAttributeName) {
	input.skipBytes(attLength);
	continue;
      }

      if (attName == VM_ClassLoader.deprecatedAttributeName) {
	input.skipBytes(attLength);
	continue;
      }

      input.skipBytes(attLength);
    }

    state = CLASS_LOADED;

    VM_Callbacks.notifyClassLoaded(this);

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (end)   load file " + 
                                          descriptor + "\n");
  }

  /**
   * Generate size and offset information for members of this class and
   * allocate space in jtoc for static fields, static methods, and virtual 
   * method table. 
   * Side effects: superclasses and superinterfaces are resolved.
   */ 
  public final synchronized void resolve() throws VM_ResolutionException {
    if (isResolved())
      return;

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (begin) resolve " 
                                          + descriptor + "\n");
    if (VM.VerifyAssertions) VM.assert(state == CLASS_LOADED);

    // load/resolve superclass
    //
    if (VM.verboseClassLoading) VM.sysWrite("[Loading superclasses of "+
                                            descriptor.classNameFromDescriptor()
                                            +"]\n");
    if (superClass != null) {
      superClass.load();
      superClass.resolve();
      depth = 1 + superClass.depth;
      thinLockOffset = superClass.thinLockOffset;
      instanceSize = superClass.instanceSize;
    } else {
      instanceSize = VM_ObjectModel.computeScalarHeaderSize(this);
    }
    for (int i=0; i<declaredInterfaces.length; i++) {
      declaredInterfaces[i].load();
      declaredInterfaces[i].resolve();
    }

    if (isSynchronizedObject() || this == VM_Type.JavaLangClassType)
      VM_ObjectModel.allocateThinLock(this);

    if (VM.verboseClassLoading) VM.sysWrite("[Preparing "+
                                            descriptor.classNameFromDescriptor()
                                            +"]\n");

    // build field and method lists for this class
    //
    {
      VM_FieldVector  staticFields   = new VM_FieldVector();
      VM_FieldVector  instanceFields = new VM_FieldVector();
      VM_MethodVector staticMethods  = new VM_MethodVector();
      VM_MethodVector virtualMethods = new VM_MethodVector();

      // start with fields and methods of superclass
      //
      if (superClass != null) {
	VM_Field fields[] = superClass.getInstanceFields();
	for (int i = 0, n = fields.length; i < n; ++i)
	  instanceFields.addElement(fields[i]);

	VM_Method methods[] = superClass.getVirtualMethods();
	for (int i = 0, n = methods.length; i < n; ++i)
	  virtualMethods.addElement(methods[i]);
      }

      // append fields defined by this class
      //
      VM_Field fields[] = getDeclaredFields();
      for (int i = 0, n = fields.length; i < n; ++i) {
	VM_Field field = fields[i];
	if (field.isStatic())
	  staticFields.addElement(field);
	else
	  instanceFields.addElement(field);
      }

      // append/overlay methods defined by this class
      //
      VM_Method methods[] = getDeclaredMethods();
      for (int i = 0, n = methods.length; i < n; ++i) {
	VM_Method method = methods[i];

	if (method.isObjectInitializer() || method.isStatic())  {
	  VM_Callbacks.notifyMethodOverride(method, null);
	  staticMethods.addElement(method);
	  if (VM.VerifyUnint) {
	    if (!method.isInterruptible() && method.isSynchronized()) {
	      if (VM.ParanoidVerifyUnint || !VM_PragmaLogicallyUninterruptible.declaredBy(method)) {
		VM.sysWriteln("WARNING: "+method+" cannot be both uninterruptible and synchronized");
	      }
	    }
	  }
	  continue;
	}

	// Now deal with virtual methods
	if (method.isSynchronized()) {
	  VM_ObjectModel.allocateThinLock(this);
	  if (VM.VerifyUnint) {
	    if (!method.isInterruptible()) {
	      if (VM.ParanoidVerifyUnint || !VM_PragmaLogicallyUninterruptible.declaredBy(method)) {
		VM.sysWriteln("WARNING: "+method+" cannot be both uninterruptible and synchronized");
	      }
	    }
	  }
	}

	// method could override something in superclass - check for it
	//
	int superclassMethodIndex = -1;
	for (int j = 0, m = virtualMethods.size(); j < m; ++j) {
	  VM_Method alreadyDefinedMethod = virtualMethods.elementAt(j);
	  if (alreadyDefinedMethod.getName() == method.getName() &&
	      alreadyDefinedMethod.getDescriptor() == method.getDescriptor()) {
	    // method already defined in superclass
	    superclassMethodIndex = j;
	    break;
	  }
	}

	if (superclassMethodIndex == -1) {
	  VM_Callbacks.notifyMethodOverride(method, null);
	  virtualMethods.addElement(method);                          // append
	} else {
	  VM_Method superc = (VM_Method)virtualMethods.elementAt(superclassMethodIndex);
	  if (VM.VerifyUnint) {
	    if (!superc.isInterruptible() && method.isInterruptible()) {
	      VM.sysWriteln("WARNING: interruptible "+method+" overrides uninterruptible "+superc);
	    }
	  }
	  VM_Callbacks.notifyMethodOverride(method, superc);
	  virtualMethods.setElementAt(method, superclassMethodIndex); // override
	}
      }

      this.staticFields   = staticFields.finish();
      this.instanceFields = instanceFields.finish();
      this.staticMethods  = staticMethods.finish();
      this.virtualMethods = virtualMethods.finish();
    }

    // allocate space for class fields
    //
    for (int i = 0, n = staticFields.length; i < n; ++i) {
      VM_Field field     = staticFields[i];
      VM_Type  fieldType = field.getType();
      byte     slotType;
      if (fieldType.isReferenceType())
	slotType = VM_Statics.REFERENCE_FIELD;
      ///	(SJF:: Note: we DO need to have a JTOC entry even 
      //               for final static
      ///	       private fields. (for serialization) )
      else if (fieldType.getStackWords() == 2)
	slotType = VM_Statics.WIDE_NUMERIC_FIELD;
      else
	slotType = VM_Statics.NUMERIC_FIELD;
      field.offset = (VM_Statics.allocateSlot(slotType) << 2);

      // (SJF): Serialization nastily accesses even final private static
      //	   fields via pseudo-reflection! So, we must shove the
      //	   values of final static fields into the JTOC.  Now
      //	   seems to be a good time.
      if (field.isFinal()) {
	setFinalStaticJTOCEntry(field,field.offset);
      }
    }

    // lay out instance fields
    //
    VM_ObjectModel.layoutInstanceFields(this);

    // count reference fields and update dynamic linking data structures
    int referenceFieldCount = 0;
    for (int i = 0, n = instanceFields.length; i < n; ++i) {
      VM_Field field     = instanceFields[i];
      if (field.getType().isReferenceType())
	referenceFieldCount += 1;
      // Should be ok to do here instead of in initialize, because
      // "new" will ensure that the class is instantiated before it 
      // creates an instance.
      VM_TableBasedDynamicLinker.setFieldOffset(field, field.offset);
    }

    // record offsets of those instance fields that contain references
    //
    referenceOffsets = new int[referenceFieldCount];
    for (int i = 0, j = 0, n = instanceFields.length; i < n; ++i) {
      VM_Field field = instanceFields[i];
      if (field.getType().isReferenceType())
	referenceOffsets[j++] = field.offset;
    }

    // Allocate space for static method pointers
    //
    for (int i = 0, n = staticMethods.length; i < n; ++i) {
      VM_Method method = staticMethods[i];
      if (method.isClassInitializer()) {
	method.offset = 0xdeadbeef; // should never be used.
      } else {
	method.offset = VM_Statics.allocateSlot(VM_Statics.METHOD) << 2;
      }
    }

    // create "type information block" and initialize its first four words
    //
    typeInformationBlock = VM_RuntimeStructures.newTIB(TIB_FIRST_VIRTUAL_METHOD_INDEX + virtualMethods.length);
    VM_Statics.setSlotContents(tibSlot, typeInformationBlock);
    typeInformationBlock[0] = this;
    if (VM.BuildForFastDynamicTypeCheck) {
      typeInformationBlock[TIB_SUPERCLASS_IDS_INDEX] = VM_DynamicTypeCheck.buildSuperclassIds(this);
      typeInformationBlock[TIB_DOES_IMPLEMENT_INDEX] = VM_DynamicTypeCheck.buildDoesImplement(this);
      // element type for arrays (empty for classes)
    }

    // lay out virtual method section of type information block 
    // (to be filled in by instantiate)
    for (int i = 0, n = virtualMethods.length; i < n; ++i) {
      VM_Method method = virtualMethods[i];
      method.offset = (TIB_FIRST_VIRTUAL_METHOD_INDEX + i) << 2;
      // Should be OK to do here instead of in initialize because 
      // "new" will ensure that the class is instantiated before 
      // it creates an instance
      VM_TableBasedDynamicLinker.setMethodOffset(method, method.offset);
    }

    // RCGC: Determine if class is inherently acyclic
    if (VM.BuildForConcurrentGC) {	
      acyclic = false;	// must initially be false for recursive types
      boolean foundCyclic = false;
      for (int i = 0; i < instanceFields.length; i++) {
        if (!instanceFields[i].getType().isAcyclicReference()) {
          foundCyclic = true; 
          break;
        }
      }
      if (!foundCyclic)
        acyclic = true;
    }

    state = CLASS_RESOLVED; // can't move this beyond "finalize" code block

    VM_Callbacks.notifyClassResolved(this);

    // check for a "finalize" method that overrides the one in java.lang.Object
    //
    VM_Method finalize = findVirtualMethod(VM_ClassLoader.StandardObjectFinalizerMethodName, 
                                           VM_ClassLoader.StandardObjectFinalizerMethodDescriptor);
    if (finalize.getDeclaringClass().getSuperClass() != null)
      finalizeMethod = finalize;
    else
      finalizeMethod = null;

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (end)   resolve " + descriptor + "\n");
  }


  // RCGC: A reference to class is acyclic if the class is acyclic and final
  //    (otherwise the reference could be to a subsequently loaded cyclic subclass).
  //
  protected final boolean isAcyclicReference() throws VM_PragmaUninterruptible {
    return acyclic && isFinal();
  }

  /** 
   * Insert the value of a final static field into the JTOC 
   */
  private void setFinalStaticJTOCEntry(VM_Field field, int fieldOffset) {
    if (!field.isFinal()) return;
    // value Index: index into the classes constant pool.
    int valueIndex = field.getConstantValueIndex();

    // index for field value in JTOC
    int fieldIndex = fieldOffset >> 2;

    // if there's no value in the constant pool, bail out
    if (valueIndex <= 0) return;

    int literalOffset= field.getDeclaringClass().getLiteralOffset(valueIndex);

    // if field is object, should use reference form of setSlotContents.
    // But getSlotContentsAsObject() uses Magic to recast as Object, and
    // Magic is not allowed when BootImageWriter is executing under JDK,
    // so we only do the "proper" thing when the vm is running.  This is OK
    // for now, because the bootImage is not collected (all object get BIG
    // reference counts
    //
    if (VM.runningVM && VM_Statics.isReference(fieldIndex)) {
      Object obj = VM_Statics.getSlotContentsAsObject(literalOffset>>2);
      VM_Statics.setSlotContents(fieldIndex,obj);
    } else if (field.getSize() == 4) {
      // copy one word from constant pool to JTOC
      int value = VM_Statics.getSlotContentsAsInt(literalOffset>>2);
      VM_Statics.setSlotContents(fieldIndex,value);
    } else {
      // copy two words from constant pool to JTOC
      long value = VM_Statics.getSlotContentsAsLong(literalOffset>>2);
      VM_Statics.setSlotContents(fieldIndex,value);
    }
  }

  /**
   * Copy the values of all static final fields into 
   * the JTOC.  Note: This method should only be run AFTER
   * the class initializer has run.
   */
  void setAllFinalStaticJTOCEntries() {
    if (VM.VerifyAssertions) VM.assert (isInitialized());
    VM_Field[] fields = getStaticFields();
    for (int i=0; i<fields.length; i++) {
      VM_Field f = fields[i];
      if (f.isFinal()) {
        setFinalStaticJTOCEntry(f,f.getOffset());
      }
    }
  }

  /**
   * Compile this class's methods, build type information block, populate jtoc.
   * Side effects: superclasses are instantiated.
   */
  public final synchronized void instantiate() {
    if (isInstantiated())
      return;

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (begin) instantiate " 
                                          + descriptor + "\n");
    if (VM.VerifyAssertions) VM.assert(state == CLASS_RESOLVED);

    // instantiate superclass
    //
    if (superClass != null)
      superClass.instantiate();
    if (VM.runningVM) {
      // can't instantiate if building bootimage, since this can cause
      // class initializer to be lost (when interface is not included in bootimage).
      // since we don't need to instantiate/initialize for the purposes of 
      // dynamic type checking and interface invocation, defer it until runtime
      // and the class actually refers to a static field of the interface.
      for (int i=0; i<declaredInterfaces.length; i++) {
	declaredInterfaces[i].instantiate();
      }
    }

    // Initialize slots in the TIB for virtual methods
    for (int slot = TIB_FIRST_VIRTUAL_METHOD_INDEX, i = 0, 
	   n = virtualMethods.length; i < n; ++i, ++slot) {
      VM_Method method = virtualMethods[i];
      if (method.isPrivate() && method.getDeclaringClass() != this) {
	typeInformationBlock[slot] = null; // an inherited private method....will never be invoked via this TIB
      } else {
	typeInformationBlock[slot] = method.getCurrentInstructions();
      }
    }

    // compile static methods and put their addresses into jtoc
    for (int i = 0, n = staticMethods.length; i < n; ++i) {
      // don't bother compiling <clinit> here;
      // compile it right before we invoke it in initialize.
      // This also avoids putting clinit's in the bootimage.
      VM_Method method = staticMethods[i];
      if (!method.isClassInitializer()) {
	VM_Statics.setSlotContents(method.getOffset() >> 2, method.getCurrentInstructions());
      }
    }

    VM_InterfaceInvocation.initializeDispatchStructures(this);

    if (VM.writingBootImage) { 
      // host jvm will initialize this class as side effect of building
      // boot image, so we must set state as if initialize() had been called
      //
      for (int i = 0, n = staticFields.length; i < n; ++i) {
	VM_Field field = staticFields[i];
	VM_TableBasedDynamicLinker.setFieldOffset(field, field.getOffset());
      }
      for (int i = 0, n = staticMethods.length; i < n; ++i) {
	VM_Method method = staticMethods[i];
	VM_TableBasedDynamicLinker.setMethodOffset(method, method.getOffset());
      }
      state = CLASS_INITIALIZED; 
    } else {
      state = CLASS_INSTANTIATED;
    }

    VM_Callbacks.notifyClassInstantiated(this);
    if (VM.writingBootImage)
      VM_Callbacks.notifyClassInitialized(this);

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (end)   instantiate " 
							  + descriptor + "\n");
  }

  /**
   * Execute this class's static initializer, <clinit>.
   * Side effects: superclasses are initialized, static fields receive 
   * initial values.
   */ 
  public final synchronized void initialize() {
    if (isInitialized())
      return;

    if (state == CLASS_INITIALIZING) {
      // recursion: <clinit> called something that called <clinit>
      // java language specification says results are undefined: user gets
      // static field values of either zero (default initial value) 
      // or whatever values happen to be initialized so far.
      // TODO: Thus by the logic of the above statement, we can set
      // the offsetTables _for_the_current_thread_ to non-zero values.
      // Unfortunately, we have global offsetTables and thus are backed into
      // a corner and have to make the non-zero values visible to all
      // threads. Then again, it isn't obvious that doing it on a 
      // per-thread basis would work either, since that could get us
      // into a deadlock if the program has cyclic clinits.
      for (int i = 0, n = staticFields.length; i < n; ++i) {
	VM_Field field = staticFields[i];
	VM_TableBasedDynamicLinker.setFieldOffset(field, field.getOffset());
      }
      for (int i = 0, n = staticMethods.length; i < n; ++i) {
	VM_Method method = staticMethods[i];
	VM_TableBasedDynamicLinker.setMethodOffset(method, method.getOffset());
      }
      return;
    }

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (begin) initialize " + 
                                          descriptor + "\n");
    if (VM.VerifyAssertions) VM.assert(state == CLASS_INSTANTIATED);
    state = CLASS_INITIALIZING;
    if (VM.verboseClassLoading) VM.sysWrite("[Initializing "+
                                            descriptor.classNameFromDescriptor()
                                            +"]\n");

    // run super <clinit>
    //
    if (superClass != null)
      superClass.initialize();
    if (VM.runningVM) {
      for (int i=0; i<declaredInterfaces.length; i++) {
	declaredInterfaces[i].initialize();
      }
    }

    // run <clinit>
    //
    if (classInitializerMethod != null) {
      VM_CompiledMethod cm = classInitializerMethod.getCurrentCompiledMethod();
      while (cm == null) {
	classInitializerMethod.compile();
	cm = classInitializerMethod.getCurrentCompiledMethod();
      }

      if (VM.verboseClassLoading) VM.sysWrite("[Running static initializer for "
					      +descriptor.
					      classNameFromDescriptor()+"]\n");

      VM_Magic.invokeClassInitializer(cm.getInstructions());

      // <clinit> is no longer needed: reclaim space by removing references to it
      classInitializerMethod.invalidateCompiledMethod(cm);
      classInitializerMethod               = null;
    }

    // now that <clinit> has run, it's safe to fill in offset tables
    //
    for (int i = 0, n = staticFields.length; i < n; ++i) {
      VM_Field field = staticFields[i];
      VM_TableBasedDynamicLinker.setFieldOffset(field, field.getOffset());
    }
    for (int i = 0, n = staticMethods.length; i < n; ++i) {
      VM_Method method = staticMethods[i];
      VM_TableBasedDynamicLinker.setMethodOffset(method, method.getOffset());
    }

    // report that a class is about to be marked initialized to 
    // the opt compiler so it can invalidate speculative CHA optimizations
    // before an instance of this class could actually be created.
    //-#if RVM_WITH_OPT_COMPILER
    if (OptCLDepManager != null) OptCLDepManager.classInitialized(this);
    //-#endif

    state = CLASS_INITIALIZED;

    VM_Callbacks.notifyClassInitialized(this);

    if (VM.TraceClassLoading && VM.runningVM) VM.sysWrite("VM_Class: (end)   initialize " 
                                          + descriptor + "\n");
  }

  //-#if RVM_WITH_PREMATURE_CLASS_RESOLUTION
  // Initialize a preresolved class immediately prior to its first use.
  //
  static final void initializeClassIfNecessary (int classId) {
    VM_Class c = (VM_Class) VM_TypeDictionary.getValue(classId);
    if (c.isInitialized()) return;
    c.instantiate();
    c.initialize();
  }
  //-#endif


  /**
   * Add to list of classes that derive from this one.
   */
  private void addSubClass(VM_Class sub) {
    int        n    = subClasses.length;
    VM_Class[] tmp  = new VM_Class[n + 1];

    for (int i = 0; i < n; ++i)
      tmp[i] = subClasses[i];
    tmp[n] = sub;

    subClasses = tmp;
  }

  //------------------------------------------------------------//
  // Support for speculative optimizations that may need to 
  // invalidate compiled code when new classes are loaded.
  //------------------------------------------------------------//
  //-#if RVM_WITH_OPT_COMPILER
  static OPT_ClassLoadingDependencyManager OptCLDepManager;
  //-#endif

  /**
   * Given a method declared by this class, update all
   * dispatching tables to refer to the current compiled
   * code for the method.
   */
  public void updateMethod(VM_Method m) {
    if (VM.VerifyAssertions) VM.assert(isResolved());
    if (VM.VerifyAssertions) VM.assert(m.getDeclaringClass() == this);
    if (m.isClassInitializer()) return; // we never put this method in the jtoc anyways!

    if (m.isStatic() || m.isObjectInitializer()) {
      updateJTOCEntry(m);
    } else {
      updateVirtualMethod(m);
    }
  }

  /**
   * Update the JTOC slot for the given static method to point to
   * the current compiled code for the given method.
   * NOTE: This method is intentionally not synchronized to avoid deadlocks.
   *       We instead rely on the fact that we are always updating the JTOC with
   *       the most recent instructions for the method.
   */
  public void updateJTOCEntry(VM_Method m) {
    if (VM.VerifyAssertions) VM.assert(m.getDeclaringClass() == this);
    if (VM.VerifyAssertions) VM.assert(isResolved());
    if (VM.VerifyAssertions) VM.assert(m.isStatic() || m.isObjectInitializer());
    int slot = m.getOffset() >>> 2;
    VM_Statics.setSlotContents(slot, m.getCurrentInstructions());
  }


  /**
   * Update this class's TIB entry for the given method to point to
   * the current compiled code for the given method.
   * NOTE: This method is intentionally not synchronized to avoid deadlocks.
   *       We instead rely on the fact that we are always updating the JTOC with
   *       the most recent instructions for the method.
   */
  public void updateTIBEntry(VM_Method m) {
    if (VM.VerifyAssertions) {
      VM_Method vm = findVirtualMethod(m.getName(), m.getDescriptor());
      VM.assert(vm == m);
    }
    int offset = m.getOffset() >>> 2;
    typeInformationBlock[offset] = m.getCurrentInstructions();
    VM_InterfaceInvocation.updateTIBEntry(this, m);
  }


  /**
   * Update the TIB entry's for all classes that inherit the given method 
   * to point to the current compiled code for the given method.
   * NOTE: This method is intentionally not synchronized to avoid deadlocks.
   *       We instead rely on the fact that we are always updating the JTOC with
   *       the most recent instructions for the method.
   */
  public void updateVirtualMethod(VM_Method m) {
    VM_Method dm = findDeclaredMethod(m.getName(), m.getDescriptor());
    if (dm != null && dm != m) return;  // this method got overridden
    updateTIBEntry(m);
    if (m.isPrivate()) return; // can't override
    VM_Class[] subClasses = getSubClasses(); 
    for (int i = 0; i < subClasses.length; i++) {
      VM_Class sc = subClasses[i];
      if (sc.isResolved()) {
	sc.updateVirtualMethod(m);
      }
    }
  }

  //------------------------------------------------------------//
  // Additional fields and methods for Interfaces               //
  //------------------------------------------------------------//

  private static final VM_Synchronizer interfaceCountLock = new VM_Synchronizer();
  private static int          interfaceCount     = 0;
  private static VM_Class[]   interfaces         = new VM_Class[100];
  private int                 interfaceId        = -1; 
  VM_Method[]                 noIMTConflictMap; // used by VM_InterfaceInvocation to support resetTIB

  /**
   * VM_Classes used as Interfaces get assigned an interface id.
   *   If the class is not an interface, attempting to use this
   *   id will cause an IncompatibleClassChangeError to be thrown
   */ 
  int getInterfaceId () {
    if (interfaceId == -1) {
      assignInterfaceId();
    }
    return interfaceId;
  }

  int getDoesImplementIndex() {
    return getInterfaceId() >>> 5;
  }

  int getDoesImplementBitMask() {
    return 1 << (getInterfaceId() & 31);
  }

  static VM_Class getInterface(int id) {
    return interfaces[id];
  }

  private synchronized void assignInterfaceId() {
    if (interfaceId == -1) {
      synchronized(interfaceCountLock) {
	interfaceId = interfaceCount++;
	if (interfaceId == interfaces.length) {
	  VM_Class[] tmp = new VM_Class[interfaces.length*2];
	  System.arraycopy(interfaces, 0, tmp, 0, interfaces.length);
	  interfaces = tmp;
	}
	interfaces[interfaceId] = this;
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.util.StringTokenizer;
import java.io.*;
import java.util.zip.ZipFile;
import java.util.zip.ZipEntry;
import java.util.Hashtable;
import java.security.ProtectionDomain;

/**
 * Manufacture type descriptions as needed by the running virtual machine. <p>
 * 
 * @author Bowen Alpern
 * @author Derek Lieber
 */
public class VM_ClassLoader
implements VM_Constants, VM_ClassLoaderConstants {

  /**
   * Set list of places to be searched for vm classes and resources.
   * @param classPath path specification in standard "classpath" format
   */
  public static void setVmRepositories(String classPath) {
    VM_SystemClassLoader cl = VM_SystemClassLoader.getVMClassLoader();
    cl.classPath = classPath;
    cl.parsePath();
  }

  /**
   * Set list of places to be searched for application classes and resources.
   * @param classPath path specification in standard "classpath" format
   */
  public static void setApplicationRepositories(String classPath) {
    VM_StringVector vec = new VM_StringVector();

    if (applicationRepositories != null)
      for(int i  = 0; i < applicationRepositories.length; i++)
        vec.addElement( applicationRepositories[i]);

    for (StringTokenizer st = new StringTokenizer(classPath, System.getProperty("path.separator"), false); st.hasMoreTokens(); )
      vec.addElement(st.nextToken());

    applicationRepositories = vec.finish();
    //-#if RVM_WITH_GNU_CLASSPATH
    VM_SystemClassLoader.getVMClassLoader().classPath = classPath;
    //-#endif
  }

  /**
   * Get list of places currently being searched for application 
   * classes and resources.
   * @return names of directories, .zip files, and .jar files
   */ 
  public static String[] getApplicationRepositories() {
    return applicationRepositories;
  }

  /**
   * Find a type description, or create one if this is a type we haven't 
   * seen before.
   * @param descriptor descriptor for desired type - 
   * something like "Ljava/lang/String;" or "[I" or "I"
   * @return type description
   */ 
  public static VM_Type findOrCreateType(VM_Atom descriptor, ClassLoader cl) {
    switch ( descriptor.parseForTypeCode() ) {
      case ClassTypeCode: 
      case ArrayTypeCode: 
        return VM_TypeDictionary.getValue(findOrCreateTypeId(descriptor, cl));
      case BooleanTypeCode:
        return VM_Type.BooleanType;
      case ByteTypeCode:
        return VM_Type.ByteType;
      case ShortTypeCode:
        return VM_Type.ShortType;
      case IntTypeCode:
        return VM_Type.IntType;
      case LongTypeCode:
        return VM_Type.LongType;
      case FloatTypeCode:
        return VM_Type.FloatType;
      case DoubleTypeCode:
        return VM_Type.DoubleType;
      case CharTypeCode:
        return VM_Type.CharType;

      case VoidTypeCode:
        return VM_Type.VoidType;

      default:
        VM.assert(NOT_REACHED);
        return null;
    }
  }

  /**
   * Find a type dictionary id, or create one if this is a type we haven't 
   * seen before.
   * @param descriptor descriptor for desired type - 
   * something like "Ljava/lang/String;" or "[I" or "I"
   * @return type dictionary id
   */ 
  static int findOrCreateTypeId(VM_Atom descriptor, ClassLoader classloader) {
    int     typeId = VM_TypeDictionary.findOrCreateId(descriptor, null);
    VM_Type type   = VM_TypeDictionary.getValue(typeId);

    if (type != null)
      return typeId;

    if (descriptor.isArrayDescriptor()) { // new array type
      VM_Array ary = new VM_Array(descriptor, typeId, classloader);
      VM_TypeDictionary.setValue(typeId, ary);
      return typeId;
    } else { 
      // new class type
      VM_Class cls = new VM_Class(descriptor, typeId, classloader);
      VM_TypeDictionary.setValue(typeId, cls);
      return typeId;
    }
  }

  /**
   * Find a primitive type description, 
   * or create one if this is a type we haven't seen before.
   * @param name name for desired type       - something like "void"
   * @param descriptor descriptor for desired type - something like "V"
   * @return type description
   */
  static VM_Type findOrCreatePrimitiveType(VM_Atom name, VM_Atom descriptor) {
    int     typeId = VM_TypeDictionary.findOrCreateId(descriptor, null);
    VM_Type type   = VM_TypeDictionary.getValue(typeId);
    if (type == null)
      VM_TypeDictionary.setValue(typeId, type = new VM_Primitive(name, descriptor, typeId));
    return type;
  }

  /**
   * Find a field description, or create one if this is a field we 
   * haven't seen before.
   * @param classDescriptor class descriptor - 
   * something like "Ljava/lang/String;"
   * @param fieldName field name - something like "value"
   * @param fieldDescriptor field descriptor - something like "[I"
   * @return field description
   */ 
  static VM_Field findOrCreateField(VM_Atom classDescriptor, 
                                    VM_Atom fieldName, 
                                    VM_Atom fieldDescriptor,
                                    ClassLoader classloader) {
    return VM_FieldDictionary.getValue(findOrCreateFieldId(classDescriptor, 
                                                           fieldName, 
                                                           fieldDescriptor,
                                                           classloader));
  }

  /**
   * Find a field dictionary id, or create one if this is a field we 
   * haven't seen before.
   * @param classDescriptor class descriptor - 
   * something like "Ljava/lang/String;"
   * @param fieldName field name - something like "value"
   * @param fieldDescriptor field descriptor - something like "[I"
   * @return field dictionary id
   */ 
  static int findOrCreateFieldId(VM_Atom classDescriptor, 
                                 VM_Atom fieldName, 
                                 VM_Atom fieldDescriptor,
                                 ClassLoader classloader) {
    VM_Triplet fieldKey = new VM_Triplet(classDescriptor, fieldName, 
                                         fieldDescriptor);
    int        fieldId  = VM_FieldDictionary.findOrCreateId(fieldKey, null);

    if (VM_FieldDictionary.getValue(fieldId) == null) {
      VM_Class cls = VM_ClassLoader.findOrCreateType(classDescriptor, classloader).asClass();
      VM_FieldDictionary.setValue(fieldId, new VM_Field(cls, 
                                                        fieldName, 
                                                        fieldDescriptor, 
                                                        fieldId));
    }

    // keep size of co-indexed array in pace with dictionary
    //
    VM_TableBasedDynamicLinker.ensureFieldCapacity(fieldId);

    return fieldId;
  }

  /**
   * Find an interface signature id, or create one if this is an 
   * interface signature we haven't seen before.
   * @param interfaceMethodName interface method name - something like "getNext"
   * @param interfaceMethodDescriptor interface method descriptor - 
   * something like "(I)I"
   * @return interface signature id
   */ 
  static int findOrCreateInterfaceMethodSignatureId(VM_Atom interfaceMethodName, 
                                                    VM_Atom interfaceMethodDescriptor) {
    VM_InterfaceMethodSignature key = new VM_InterfaceMethodSignature(interfaceMethodName, interfaceMethodDescriptor);
    int id = VM_InterfaceMethodSignatureDictionary.findOrCreateId(key, UNRESOLVED_INTERFACE_METHOD_OFFSET);
    return id;
  }

  /**
   * Find a method description, or create one if this is a method we 
   * haven't seen before.
   * @param classDescriptor class descriptor - something like 
   * "Ljava/lang/String;"
   * @param methodName method name - something like "charAt"
   * @param methodDescriptor  method descriptor - something like "(I)C"
   * @return method description
   */
  static VM_Method findOrCreateMethod(VM_Atom classDescriptor, 
                                      VM_Atom methodName, 
                                      VM_Atom methodDescriptor,
                                      ClassLoader classloader) {
    return VM_MethodDictionary.getValue(findOrCreateMethodId(classDescriptor, methodName, methodDescriptor, classloader));
  }

  /**
   * Find a method dictionary id, or create one if this is a method we 
   * haven't seen before.
   * @param classDescriptor class descriptor - something like 
   * "Ljava/lang/String;"
   * @param methodName method name - something like "charAt"
   * @param methodDescriptor  method descriptor - something like "(I)C"
   * @return method dictionary id
   */
  static int findOrCreateMethodId(VM_Atom classDescriptor, 
                                  VM_Atom methodName, 
                                  VM_Atom methodDescriptor,
                                  ClassLoader classloader) {
    VM_Triplet methodKey = new VM_Triplet(classDescriptor, methodName, 
                                          methodDescriptor);
    int        methodId  = VM_MethodDictionary.findOrCreateId(methodKey, null);
    if (VM_MethodDictionary.getValue(methodId) == null) {
      VM_Class cls = VM_ClassLoader.findOrCreateType(classDescriptor, classloader).asClass();
      VM_MethodDictionary.setValue(methodId, 
                                   new VM_Method(cls, methodName, 
                                                 methodDescriptor,
                                                 methodId,
                                                 classloader));
    }
    // keep size of co-indexed array in pace with dictionary
    //
    VM_TableBasedDynamicLinker.ensureMethodCapacity(methodId);
    return methodId;
  }

  public static void loadLibrary(String libname) {
    currentDynamicLibraryId++;
    if (currentDynamicLibraryId>=(dynamicLibraries.length-1))
      dynamicLibraries = growArray(dynamicLibraries, currentDynamicLibraryId << 1); // grow array by 2x
    if (VM.VerifyAssertions) VM.assert(dynamicLibraries[currentDynamicLibraryId] == null);

    // prepend "lib" if there is no path in the name
    // attach the suffix .a to the library name for AIX, .so for Linux
    //-#if RVM_FOR_LINUX  
    String suf = ".so";
    //-#else
    String suf = ".a";
    //-#endif
    if (libname.indexOf('/')==-1)
      dynamicLibraries[currentDynamicLibraryId] = new VM_DynamicLibrary("lib" + libname + suf);
    else
      dynamicLibraries[currentDynamicLibraryId] = new VM_DynamicLibrary(libname + suf);
  }

  public static void load(String libname) {
    currentDynamicLibraryId++;
    if (currentDynamicLibraryId>=(dynamicLibraries.length-1))
      dynamicLibraries = growArray(dynamicLibraries, currentDynamicLibraryId << 1); // grow array by 2x
    if (VM.VerifyAssertions) VM.assert(dynamicLibraries[currentDynamicLibraryId] == null);

    dynamicLibraries[currentDynamicLibraryId] = new VM_DynamicLibrary(libname);
  }


  static VM_DynamicLibrary[] getDynamicLibraries() {
    return dynamicLibraries;
  }

  //----------------//
  // implementation //
  //----------------//

  // Places from which to load .class files.
  //
  private static String[] applicationRepositories;

  // Names of special methods.
  //
  static VM_Atom StandardClassInitializerMethodName;        // "<clinit>"
  static VM_Atom StandardClassInitializerMethodDescriptor;  // "()V"

  static VM_Atom StandardObjectInitializerMethodName;       // "<init>"
  static VM_Atom StandardObjectInitializerMethodDescriptor; // "()V"

  static VM_Atom StandardObjectFinalizerMethodName;         // "finalize"
  static VM_Atom StandardObjectFinalizerMethodDescriptor;   // "()V"

  // Names of .class file attributes.
  //
  static VM_Atom codeAttributeName;                   // "Code"
  static VM_Atom constantValueAttributeName;          // "ConstantValue"
  static VM_Atom lineNumberTableAttributeName;        // "LineNumberTable"
  static VM_Atom exceptionsAttributeName;             // "Exceptions"
  static VM_Atom sourceFileAttributeName;             // "SourceFile"
  static VM_Atom localVariableTableAttributeName;     // "LocalVariableTable"
  static VM_Atom deprecatedAttributeName;             // "Deprecated"
  static VM_Atom innerClassesAttributeName;           // "InnerClasses"
  static VM_Atom syntheticAttributeName;              // "Synthetic"
  static VM_Atom arrayNullCheckAttributeName;         // "ArrayNullCheckAttribute"

  /**
   * Dynamic libraries for native code
   * Note: this is static for now, but it needs to be a list per class loader
   */
  private static VM_DynamicLibrary[] dynamicLibraries;

  /**
   * Index of most recently allocated slot in dynamicLibraries.
   */
  private static int currentDynamicLibraryId;

  /**
   * Initialize for bootimage.
   */
  static void init(String vmClassPath) {
    // specify place where vm classes and resources live
    //
    setVmRepositories(vmClassPath);
    applicationRepositories = null;

    // create special method- and attribute- names
    //
    StandardClassInitializerMethodName        = VM_Atom.findOrCreateAsciiAtom("<clinit>");
    StandardClassInitializerMethodDescriptor  = VM_Atom.findOrCreateAsciiAtom("()V");

    StandardObjectInitializerMethodName       = VM_Atom.findOrCreateAsciiAtom("<init>");
    StandardObjectInitializerMethodDescriptor = VM_Atom.findOrCreateAsciiAtom("()V");

    StandardObjectFinalizerMethodName         = VM_Atom.findOrCreateAsciiAtom("finalize");
    StandardObjectFinalizerMethodDescriptor   = VM_Atom.findOrCreateAsciiAtom("()V");

    codeAttributeName                   = VM_Atom.findOrCreateAsciiAtom("Code");
    constantValueAttributeName          = VM_Atom.findOrCreateAsciiAtom("ConstantValue");
    lineNumberTableAttributeName        = VM_Atom.findOrCreateAsciiAtom("LineNumberTable");
    exceptionsAttributeName             = VM_Atom.findOrCreateAsciiAtom("Exceptions");
    sourceFileAttributeName             = VM_Atom.findOrCreateAsciiAtom("SourceFile");
    localVariableTableAttributeName     = VM_Atom.findOrCreateAsciiAtom("LocalVariableTable");
    deprecatedAttributeName             = VM_Atom.findOrCreateAsciiAtom("Deprecated");
    innerClassesAttributeName           = VM_Atom.findOrCreateAsciiAtom("InnerClasses");
    syntheticAttributeName              = VM_Atom.findOrCreateAsciiAtom("Synthetic");
    arrayNullCheckAttributeName		= VM_Atom.findOrCreateAsciiAtom("ArrayNullCheckAttribute");

    dynamicLibraries = new VM_DynamicLibrary[0];

    VM_Type.init();

    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    com.ibm.oti.vm.AbstractClassLoader.setBootstrapClassLoader( VM_SystemClassLoader.getVMClassLoader() );
    //-#endif
  }

  /**
   * Initialize for execution.
   * @param vmClasses name of directory containing vm .class and .zip/.jar 
   * files (null -> use values specified by setVmRepositories() when 
   * bootimage was created)
   * @return nothing
   */
  static void boot(String vmClasses) {
    setVmRepositories( vmClasses );
    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    com.ibm.oti.vm.AbstractClassLoader.resCache.cache.clear();
    //-#endif
  }

  /**
   * Expand an array.
   */ 
  private static int[] growArray(int[] array, int newLength) {
    // assertion: no special array initialization needed (default 0 is ok)
    if (VM.VerifyAssertions) VM.assert(NEEDS_DYNAMIC_LINK == 0); 
    int[] newarray = VM_RuntimeStructures.newContiguousIntArray(newLength);
    for (int i = 0, n = array.length; i < n; ++i)
      newarray[i] = array[i];

    VM_Magic.sync();
    return newarray;
  }

  // Expand an array.
  //

  /**
   * Create id for use by C signal handler as placeholder to mark stackframe
   * introduced when a hardware trap is encountered. This method is completely
   * artifical: it has no code, class description, etc. 
   * Its only purpose is to mark the place
   * on the stack where a trap was encountered, 
   * for identification when walking the stack
   * during gc.
   */ 
  static int createHardwareTrapCompiledMethodId() {
    VM_Method method = VM_ClassLoader.findOrCreateMethod(VM_Atom.findOrCreateAsciiAtom("L<hardware>;"),
                                                         VM_Atom.findOrCreateAsciiAtom("<trap>"),
                                                         VM_Atom.findOrCreateAsciiAtom("()V"),
                                                         VM_SystemClassLoader.getVMClassLoader());
    VM_CompiledMethod compiledMethod   = VM_CompiledMethods.createCompiledMethod(method, VM_CompiledMethod.TRAP);
    INSTRUCTION[]     instructions     = VM_RuntimeStructures.newInstructions(0);
    compiledMethod.compileComplete(instructions);
    return compiledMethod.getId();
  }


  /**
   * Expand an array.
   */ 
  private static VM_DynamicLibrary[] growArray(VM_DynamicLibrary[] array, 
                                               int newLength) {
    VM_DynamicLibrary[] newarray = VM_RuntimeStructures.newContiguousDynamicLibraryArray(newLength);
    for (int i = 0, n = array.length; i < n; ++i)
      newarray[i] = array[i];

    VM_Magic.sync();
    return newarray;
  }


  public static final void resolveClassInternal(Class clazz) {
    VM_Type cls = clazz.getVMType();
    try {
      cls.resolve();
    } catch (VM_ResolutionException e) { 
      VM.sysWrite("ERROR: DROPPING EXCEPTION: "+e+" ON THE FLOOR\n");
    }
    cls.instantiate();
    cls.initialize();
  }

  public static final Class defineClassInternal(String className, 
                                                byte[] classRep, 
                                                int offset, 
                                                int length, 
                                                ClassLoader classloader, 
                                                ProtectionDomain pd) throws ClassFormatError {
    Class c = defineClassInternal(className, new ByteArrayInputStream(classRep, offset, length), classloader);
    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    c.pd = pd;
    //-#endif
    return c;
  }

  public static final Class defineClassInternal(String className, 
                                                byte[] classRep, 
                                                int offset, 
                                                int length, 
                                                ClassLoader classloader) throws ClassFormatError {
    return defineClassInternal(className, new ByteArrayInputStream(classRep, offset, length), classloader);
  }

  public static final Class defineClassInternal(String className, 
                                                InputStream is, 
                                                ClassLoader classloader, 
                                                ProtectionDomain pd) throws ClassFormatError {
    Class c = defineClassInternal(className, is, classloader);
    //-#if RVM_WITH_GNU_CLASSPATH
    //-#else
    c.pd = pd;
    //-#endif
    return c;
  }


  public static final Class defineClassInternal(String className, 
                                                InputStream is, 
                                                ClassLoader classloader) throws ClassFormatError {
    if (className == null) {
      VM.sysFail("ClassLoader.defineClass class name == null not implemented"); //!!TODO
      return null;
    }

    VM_Atom classDescriptor = VM_Atom.findOrCreateAsciiAtom(className.replace('.','/')).descriptorFromClassName();
    VM_Class cls = VM_ClassLoader.findOrCreateType(classDescriptor, classloader).asClass();

    if (!cls.isLoaded()) {
      if (VM.TraceClassLoading  && VM.runningVM)
        VM.sysWrite("loading " + cls + " with " + classloader);

      cls.classloader = classloader;
      try {
        cls.load(new DataInputStream(is));
      } catch (IOException e) {
        throw new ClassFormatError(e.getMessage());
      }
    }

    return cls.getClassForType();
  }

  /**
   * In some bizarre circumstances a key can be created for a member that
   * does not exist.
   *
   * <pre>
   * e.g. |               |                      |                    |
   *      |imports p;     |package p;            |package p;          |
   *      |class C {      |class A {             |class B extends A { |
   *      |               |                      |                    |
   *      | void bar () { | public void foo () { |                    |
   *      |   ... B.foo() |   ...                |                    |
   *      | }             | }                    |                    |
   *      |               |                      |                    |
   *      |}              |}                     |}                   |
   * </pre>
   *
   * <p> Here, a key to the method dictionary is created for the triple
   * <p.B, "foo", "()V">, even though no such method exists.
   *
   * <p> This honors package protection levels.  There was a bug in the
   * older versions of both javac and jikes that resulted in complete
   * resolution of method calls and field references.  We can no longer
   * rely on this bug.
   *
   * <p> In such cases, an empty VM_Member object gets created.  This object
   * has bogus information stored in it, and will have the loaded state
   * inconsistend with the loaded state of its declaring class.  When such
   * an object is detected, the following methods repair the appropriate
   * dictionaries and return the correct member.
   */ 
  static VM_Member repairMember(VM_Member m) {
    if (m instanceof VM_Method) {
      return repairMethod((VM_Method) m);
    } else { // m instanceof VM_Field
      return repairField((VM_Field) m);
    }
  }

  static VM_Method repairMethod(VM_Method m) {
    VM_Method newm = VM_MethodDictionary.getValue(m.getDictionaryId());
    if (newm != m) return newm;  // already done!
    VM_Class c = m.getDeclaringClass();
    VM_Atom name = m.getName();
    VM_Atom desc = m.getDescriptor();
    while ((c = c.getSuperClass()) != null) {
      VM_Method [] dm = c.getDeclaredMethods();
      for (int i=0; i<dm.length; i++ ) {
        VM_Method n = dm[i];
        if (name == n.getName() && desc == n.getDescriptor() && n.isLoaded()) {
          VM_MethodDictionary.setValue(m.getDictionaryId(), n);
          return n;
        }
      }
    }
    throw new NoSuchMethodError(m.getDeclaringClass()+": "+name+" " +desc+" no such method found");
  }

  static VM_Field repairField(VM_Field f) {
    VM_Field newf = VM_FieldDictionary.getValue(f.getDictionaryId());
    if (newf != f) return newf;  // already done!
    VM_Class c = f.getDeclaringClass();
    VM_Atom name = f.getName();
    VM_Atom desc = f.getDescriptor();
    while ((c = c.getSuperClass()) != null) {
      VM_Field [] df = c.getDeclaredFields();
      for (int i=0; i<df.length; i++ ) {
        VM_Field n = df[i];
        if (name == n.getName() && desc == n.getDescriptor() && n.isLoaded()) {
          VM_FieldDictionary.setValue(f.getDictionaryId(), n);
          return n;
        }
      }
    }
    throw new NoSuchFieldError(f.getDeclaringClass()+": "+name+" "+desc+" no such field found");
  }

  /**
   * like repairMethod, except a) we crawl the interface hierarchy
   * and b) some of the VM_Classes we look at may not be loaded yet,
   * so we have to know whether or not we are allowed to perform classloading
   * to resolve the ghost reference. 
   * we'll return null if it might be a ghost reference and we couldn't 
   * resolve it.
   */
  static VM_Method repairInterfaceMethod(VM_Method m, boolean canLoad) 
    throws VM_ResolutionException {
      VM_Method newm = VM_MethodDictionary.getValue(m.getDictionaryId());
      if (newm != m) return newm;  // already done!
      newm = repairInterfaceMethodHelper(m, canLoad, m.getDeclaringClass(), 
                                         m.getName(), m.getDescriptor());
      if (canLoad && newm == null) {
        throw new VM_ResolutionException(m.getDeclaringClass().getDescriptor(), 
                                         new IncompatibleClassChangeError());
      }
      return newm;
    }

  private static VM_Method repairInterfaceMethodHelper(VM_Method m, 
                                                       boolean canLoad,
                                                       VM_Class I, 
                                                       VM_Atom name, 
                                                       VM_Atom desc) 
    throws VM_ResolutionException {
      if (!I.isLoaded()) {
        if (canLoad) {
          VM_Runtime.initializeClassForDynamicLink(I);
          if (!I.isInterface()) 
            throw new VM_ResolutionException(I.getDescriptor(), 
                                             new IncompatibleClassChangeError());
        } else {
          return null;
        }
      }

      VM_Method [] dm = I.getDeclaredMethods();
      for (int i=0; i<dm.length; i++ ) {
        VM_Method n = dm[i];
        if (name == n.getName() && desc == n.getDescriptor() && n.isLoaded()) {
          VM_MethodDictionary.setValue(m.getDictionaryId(), n);
          return n;
        }
      }

      VM_Class [] superInterfaces = I.getDeclaredInterfaces();
      for (int i=0; i<superInterfaces.length; i++) {
        VM_Class superInterface = superInterfaces[i];
        VM_Method n = repairInterfaceMethodHelper(m, canLoad, 
                                                  superInterface, name, desc);
        if (n != null) return n;
      }

      return null;
    }

  /**
   *  Is dynamic linking code required to access one member when 
   * referenced from another?
   *
   * @param referent the member being referenced
   * @param referrer the declaring class of the method containing the reference
   */
  static public boolean needsDynamicLink(VM_Member referent, VM_Class referrer)
  {
    VM_Class referentClass = referent.getDeclaringClass();

    if (referentClass.isInitialized()) {
      // No dynamic linking code is required to access this field or call this method
      // because its size and offset are known and its class's static initializer
      // has already run, thereby compiling this method or initializing this field.
      //
      return false;
    }

    if (referent instanceof VM_Field && referentClass.isResolved() && 
        referentClass.getClassInitializerMethod() == null) {
      // No dynamic linking code is required to access this field
      // because its size and offset is known and its class has no static
      // initializer, therefore its value need not be specially initialized
      // (its default value of zero or null is sufficient).
      //
      return false;
    }

    if (VM.writingBootImage && referentClass.isInBootImage()) {
      // Loads, stores, and calls within boot image are compiled without dynamic
      // linking code because all boot image classes are explicitly loaded/resolved/compiled
      // and have had their static initializers run by the boot image writer.
      //
      if (!referentClass.isResolved()) VM.sysWrite("unresolved: \"" + referent + "\" referenced from \"" + referrer + "\"\n");
      if (VM.VerifyAssertions) VM.assert(referentClass.isResolved());
      return false;
    }

    if (referentClass == referrer) {
      // Intra-class references don't need to be compiled with dynamic linking
      // because they execute *after* class has been loaded/resolved/compiled.
      //
      return false;
    }

    // This member needs size and offset to be computed, or its class's static
    // initializer needs to be run when the member is first "touched", so
    // dynamic linking code is required to access the member.
    //
    return true;
  }


}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * @author Bowen Alpern
 * @author Derek Lieber
 */
interface VM_ClassLoaderConstants {
  // Attribute modifiers for class-, method-, and field- descriptions.
  //
  //                                                    applicability
  //                      name         value         class  field  method
  //               ---------------   ----------      -----  -----  ------
  static final int ACC_PUBLIC       = 0x00000001;  //   X      X      X
  static final int ACC_PRIVATE      = 0x00000002;  //   -      X      X
  static final int ACC_PROTECTED    = 0x00000004;  //   -      X      X
  static final int ACC_STATIC       = 0x00000008;  //   -      X      X
  static final int ACC_FINAL        = 0x00000010;  //   X      X      X
  static final int ACC_SYNCHRONIZED = 0x00000020;  //   -      -      X  <- same value as ACC_SPECIAL
  static final int ACC_SPECIAL      = 0x00000020;  //   X      -      -  <- same value as ACC_SYNCHRONIZED
  static final int ACC_VOLATILE     = 0x00000040;  //   -      X      -
  static final int ACC_TRANSIENT    = 0x00000080;  //   -      X      -
  static final int ACC_NATIVE       = 0x00000100;  //   -      -      X
  static final int ACC_INTERFACE    = 0x00000200;  //   X      -      -
  static final int ACC_ABSTRACT     = 0x00000400;  //   X      -      X

  static final int ACC_LOADED       = 0x80000000;  //   -      X      X  <- used to indicate loaded field/method

   // Possible states of a class description.
   //
  static final int CLASS_VACANT       = 0; // nothing present yet
  static final int CLASS_LOADED       = 1; // .class file contents read successfully
  static final int CLASS_RESOLVED     = 2; // fields & methods layed out, tib & statics allocated
  static final int CLASS_INSTANTIATED = 3; // methods compiled, tib created, jtoc populated
  static final int CLASS_INITIALIZING = 4; // <clinit> is running
  static final int CLASS_INITIALIZED  = 5; // statics initialized

   // Constant pool entry tags.
   //
  static final byte TAG_UTF                        =  1;
  static final byte TAG_UNUSED                     =  2;
  static final byte TAG_INT                        =  3;
  static final byte TAG_FLOAT                      =  4;
  static final byte TAG_LONG                       =  5;
  static final byte TAG_DOUBLE                     =  6;
  static final byte TAG_TYPEREF                    =  7;
  static final byte TAG_STRING                     =  8;
  static final byte TAG_FIELDREF                   =  9;
  static final byte TAG_METHODREF                  = 10;
  static final byte TAG_INTERFACE_METHODREF        = 11;
  static final byte TAG_MEMBERNAME_AND_DESCRIPTOR  = 12;

   // Type codes for class, array, and primitive types.
   //
  static final byte ClassTypeCode   = (byte)'L';
  static final byte ArrayTypeCode   = (byte)'[';
  static final byte VoidTypeCode    = (byte)'V';
  static final byte BooleanTypeCode = (byte)'Z';
  static final byte ByteTypeCode    = (byte)'B';
  static final byte ShortTypeCode   = (byte)'S';
  static final byte IntTypeCode     = (byte)'I';
  static final byte LongTypeCode    = (byte)'J';
  static final byte FloatTypeCode   = (byte)'F';
  static final byte DoubleTypeCode  = (byte)'D';
  static final byte CharTypeCode    = (byte)'C';
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

/**
 * Data structures and code for fast dynamic type checking.
 * <p>
 * As a convention, we convert all dynamic type checking 
 * operations into the following question: LHS :?= RHS
 * (i.e. can an instance of the RHS class be stored in a
 * variable of the LHS class or interface.)  This question
 * arises for four bytecodes: instanceof, checkcast, aastore
 * and invokeinterface and entry into catch blocks. 
 * This gives us a uniform terminology, but in some cases 
 * (instanceof) can be somewhat counter-intuitive since despite 
 * the fact that the Java source code is written as 
 * <code>x instanceof C</code>, for the purposes of dynamic type checking
 * <code>x</code> is the RHS and <code>C</code> is the LHS!
 * <p>
 * The idea of the optimizations presented below is to treat
 * each context in which these queries arises as a special
 * case to be optimised in isolation.  Consider the following
 * taxonomy of dynamic type checking conexts:
 * <p>
 * (1) Is the LHS unknown at compile time?  True only for aastore?
 *    If so, the following test will be fast in most instances:
 *    is the runtime type of the LHS array the same as compile-time 
 *    type of the variable that contains it?  If so, the Java-to-bytecode
 *    compiler (and the verifier) guarantees that the test passes.  
 *    Unfortunately, this test can only be used in two of three cases:
 *    when the LHS variable is a field or a parameter.  When the LHS is 
 *    in a local variable the Java-to-bytecode compiler has thrown away
 *    the necessary type information.
 * <p>
 * (2) Otherwise, is the LHS an array?
 *    If so, there are three sub-cases
 *    (2a) LHS is [^k primitive:
 *        If so, the dimensionality of the RHS must be k
 *        and the baseclass of the RHS must be the same primitive
 *    (2b) LHS is [^k class:
 *        If so, the dimensionality of the RHS must be k
 *        and the baseclass of the RHS must be assignable with class (see #3)
 *    (2c) LHS is [^k Ljava.lang.Object:
 *        If so, either the dimensionality of the RHS is greater than k
 *        or, this dimensionality is k and the baseclass is NOT primitive
 * <p>
 * (3) Otherwise, is the LHS unresolved?
 *    If so, fall back to calling instanceOfUnresolved at runtime.
 * <p>
 * (4) Otherwise, is the LHS an interface?  
 *    If so, query the doesImplement array of the RHS's TIB at the entry 
 *    for the interface ID. If a class does not directly implement any
 *    interfaces then it inherits the doesImplement array from its superclass.
 * <p>
 * (5) Otherwise, is the depth of the LHS greater than 
 * MIN_SUPERCLASS_IDS_SIZE? If so, if LHS depth is greater that 
 * RHS's superclassIds.length, the test fails.  Else, see #6.
 * <p>
 * (6) Otherwise.  If the LHS depth component of the RHS's superclassIds
 *    array is the LHS class ID, the test succeeds.  Else, it fails.
 *
 * @see OPT_DynamicTypeCheckExpansion
 * @see VM_Type
 * @see VM_Class
 * @see VM_Array
 * 
 * @author Bowen Alpern
 * @author Dave Grove
 */
class VM_DynamicTypeCheck implements VM_TIBLayoutConstants {

  /**
   * Minimum length of the superclassIds array in TIB.
   * Note: this array is padded to save a index out of
   * bounds test for classes with shallow class depth.
   */
  static final int MIN_SUPERCLASS_IDS_SIZE = 6; // a short[], so multiple of 2.

  /**
   * Minimum length of the doesImplements array in TIB.
   * Note: this array is padded to save a index out of
   * bounds test for the first 32 * MIN_DOES_IMPLEMENT_SIZE interfaces loaded.
   */
  static final int MIN_DOES_IMPLEMENT_SIZE = 5; // an int[]

  /**
   * Create the superclass Id vector for a VM_Type.
   *
   * @param t a VM_Type to create a superclass Id vector for
   * @return the superclass Id vector
   */
  static short[] buildSuperclassIds(VM_Type t) {
    int depth   = t.getTypeDepth();
    int size    = MIN_SUPERCLASS_IDS_SIZE <= depth ? depth+1 : MIN_SUPERCLASS_IDS_SIZE;
    short[] tsi = new short[size];
    VM_Type p;                          
    if (depth == 0) {        // t is Object (or eventually some interfaces TODO!!)
      int id = t.getDictionaryId();
      if (VM.VerifyAssertions) VM.assert(id <= 0xFFFF); // when this fails, make superclassIds int[] 
      tsi[0] = (short) id;
      return tsi;
    } else if (depth == 1) { // t is array or top level class
      if (VM.VerifyAssertions) VM.assert(t.isArrayType() || t.asClass().getSuperClass() == VM_Type.JavaLangObjectType);
      p = VM_Type.JavaLangObjectType; //  TODO!! handle interfaces better
    } else if (1 < depth) {  // t is a non Object, non top level class
      p = t.asClass().getSuperClass();
    } else {                 // t is a primitive
      VM.assert(VM.NOT_REACHED);
      p = null;
    }
    short[] psi = p.getSuperclassIds();
    for (int i=0; i<depth; i++) {
      tsi[i] = psi[i];
    }
    int id = t.getDictionaryId();
    if (VM.VerifyAssertions) VM.assert(id <= 0xFFFF); // when this fails, make superclassIds int[] 
    tsi[depth] = (short) id;
    return tsi;
  }

  private static int[] arrayDoesImplement;
  /**
   * Create the doesImplement vector for a VM_Array.
   * All arrays implement exactly java.io.Serializable and java.lang.Cloneable.
   * 
   * @param t a VM_Array to create a doesImplement vector for
   * @return the doesImplement vector
   */
  static int[] buildDoesImplement(VM_Array t) {
    if (arrayDoesImplement == null) {
      int cloneIdx = VM_Type.JavaLangCloneableType.getDoesImplementIndex();
      int serialIdx = VM_Type.JavaIoSerializableType.getDoesImplementIndex();
      int size = Math.max(cloneIdx, serialIdx);
      size = Math.max(MIN_DOES_IMPLEMENT_SIZE, size+1);
      int [] tmp = new int[size];
      tmp[cloneIdx] = VM_Type.JavaLangCloneableType.getDoesImplementBitMask();
      tmp[serialIdx] |= VM_Type.JavaIoSerializableType.getDoesImplementBitMask();
      arrayDoesImplement = tmp;
    }
    return arrayDoesImplement;
  }
  
  /**
   * Create the doesImplement vector for a VM_Class.
   * 
   * @param t a VM_Class to create a doesImplement vector for
   * @return the doesImplement vector
   */
  static int[] buildDoesImplement(VM_Class t) {
    if (t.isJavaLangObjectType()) {
      // object implements no interfaces.
      return new int[MIN_DOES_IMPLEMENT_SIZE];
    }

    VM_Class [] superInterfaces = t.getDeclaredInterfaces();

    if (!t.isInterface() && superInterfaces.length == 0) {
      // I add nothing new; share with parent.
      return t.getSuperClass().getDoesImplement();
    }

    // I need one of my own; first figure out how big it needs to be.
    int size = t.isInterface() ? t.getDoesImplementIndex() : 0;
    for (int i=0; i<superInterfaces.length; i++) {
      VM_Class superInterface = superInterfaces[i];
      size = Math.max(size, superInterface.getDoesImplement().length);
    }
    if (t.getSuperClass() != null) {
      size = Math.max(size, t.getSuperClass().getDoesImplement().length);
    }
    size = Math.max(MIN_DOES_IMPLEMENT_SIZE, size+1);

    // then create and populate it
    int[] mine = new int[size];
    if (t.isInterface()) {
      mine[t.getDoesImplementIndex()] = t.getDoesImplementBitMask();
    }
    if (t.getSuperClass() != null) {
      int[] parent = t.getSuperClass().getDoesImplement();
      for (int j=0; j<parent.length; j++) {
	mine[j] |= parent[j];
      }
    }
    for (int i=0; i<superInterfaces.length; i++) {
      int[] parent = superInterfaces[i].getDoesImplement();
      for (int j=0; j<parent.length; j++) {
	mine[j] |= parent[j];
      }
    }
    
    return mine;
  }


  /**
   * Handle the case when LHSclass is unresolved at compile time.
   *     If necessary load LHSclass and then answer is rhsTIB the TIB 
   *     of an instanceof LHSclass?
   * 
   * @param LHSclass a class or interface that may not be fully loaded
   * @param rhsTIB the TIB of an object that might be an instance of LHSclass
   * @return <code>true</code> if the object is an instance of LHSClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfUnresolved(VM_Class LHSclass, Object[] rhsTIB)  
    throws VM_ResolutionException {
    if (!LHSclass.isInitialized()) {
      VM_Runtime.initializeClassForDynamicLink(LHSclass);
    }
    return instanceOfResolved(LHSclass, rhsTIB);
  }   


  /**
   * LHSclass is a fully loaded class or interface.  
   *   Is rhsTIB the TIB of an instanceof LHSclass?
   * 
   * @param LHSclass a fully loaded class or interface class
   * @param rhsTIB the TIB of an object that might be an instance of LHSclass
   * @return <code>true</code> if the object is an instance of LHSClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfResolved(VM_Class LHSclass, Object[] rhsTIB) 
    throws VM_ResolutionException {
    if (LHSclass.isInterface()) {
      return instanceOfInterface(LHSclass, rhsTIB);
    } else {
      return instanceOfClass(LHSclass, rhsTIB);
    }
  }    


  /**
   * LHSclass is a fully loaded class.
   *  Is rhsTIB the TIB of a subclass of LHSclass?
   * 
   * @param LHSclass a (fully loaded) class
   * @param rhsTIB the TIB of an object that might be an instance of LHSclass
   * @return <code>true</code> if the object is an instance of LHSClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfClass(VM_Class LHSclass, Object[] rhsTIB) {
    short[] superclassIds = VM_Magic.objectAsShortArray(rhsTIB[TIB_SUPERCLASS_IDS_INDEX]);
    int LHSDepth = LHSclass.getTypeDepth();
    if (LHSDepth >= superclassIds.length) return false;
    int LHSId = LHSclass.getDictionaryId();
    return superclassIds[LHSDepth] == LHSId;
  }    


  /** 
   * LHSclass is a fully loaded interface.
   *   Is rhsTIB the TIB of a class that implements LHSclass?
   * 
   * @param LHSclass a class (that is a fully loaded interface)
   * @param rhsTIB the TIB of an object that might be an instance of LHSclass
   * @return <code>true</code> if the object is an instance of LHSClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfInterface(VM_Class LHSclass, Object[] rhsTIB) throws VM_ResolutionException {
    int[] doesImplement = VM_Magic.objectAsIntArray(rhsTIB[TIB_DOES_IMPLEMENT_INDEX]);
    int idx = LHSclass.getDoesImplementIndex();
    int mask = LHSclass.getDoesImplementBitMask();
    return idx < doesImplement.length && ((doesImplement[idx] & mask) != 0);
  }


  /**
   * LHSArray is an unresolved [^LHSDimension of LHSInnermostClass
   *   Is rhsType an instance of LHSArray?
   * 
   * @param LHSInnermostElementclass the innermost element type
   * @param LHSDimension the dimensionality of the array
   * @param RHStype the TIB of an object that might be an instanceof 
   *        [^LHSDimension of LHSInnermostClass
   * @return <code>true</code> if RHStype is an instanceof
   *        [^LHSDimension of LHSInnermostClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfUnresolvedArray(VM_Class LHSInnermostElementClass, 
					   int LHSDimension,  VM_Type RHSType) 
    throws VM_ResolutionException {
    if (!LHSInnermostElementClass.isInitialized()) {
      VM_Runtime.initializeClassForDynamicLink(LHSInnermostElementClass);
    }
    return instanceOfArray(LHSInnermostElementClass, LHSDimension, RHSType);
  }
    

  /**
   * LHSArray is [^LHSDimension of java.lang.Object
   *   Is rhsType an instance of LHSArray?
   * 
   * @param LHSDimension the dimensionality of the array
   * @param RHSType a type that might be an instanceof 
   *        [^LHSDimension of java.lang.Object
   * @return <code>true</code> if RHStype is an instanceof
   *       [^LHSDimension of LHSInnermostClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfObjectArray(int LHSDimension, VM_Type RHSType) {
    int RHSDimension = RHSType.getDimensionality();
    if (RHSDimension < LHSDimension) return false;
    if (RHSDimension > LHSDimension) return true;
    return RHSType.asArray().getInnermostElementType().isClassType(); // !primitive 
  }


  /**
   * LHSArray is [^LHSDimension of LHSInnermostClass, LHSInnermostClass 
   * is not java.lang.Object.
   *   Is rhsType an instance of LHSArray?
   * 
   * @param LHSInnermostElementclass the innermost element type
   * @param LHSDimension the dimensionality of the array
   * @param RHStype a type that might be an instanceof 
   *        [^LHSDimension of LHSInnermostClass
   * @return <code>true</code> if RHStype is an instanceof
   *        [^LHSDimension of LHSInnermostClass
   *         or <code>false</code> if it is not
   */
  static boolean instanceOfArray(VM_Class LHSInnermostElementClass, 
				 int LHSDimension,  VM_Type RHSType) 
    throws VM_ResolutionException {
    int RHSDimension = RHSType.getDimensionality();
    if (RHSDimension != LHSDimension) return false;
    VM_Type RHSInnermostElementType = RHSType.asArray().
                                      getInnermostElementType();
    if (RHSInnermostElementType.isPrimitiveType()) return false;
    return instanceOfResolved(LHSInnermostElementClass, 
			      RHSInnermostElementType.
                                getTypeInformationBlock());
  }
  
  /**
   * RHSType is resolved.
   *   Can we store an object of type RHSType in a variable of type LHSType?
   * 
   * @param LHSType the left-hand-side type
   * @param RHSType the right-hand-size type
   * @return <code>true</code> if we can store an object of 
   *         RHSType into a variable of type LSType
   *         or <code>false</code> if we cannot.
   */
  static boolean instanceOf(VM_Type LHSType, VM_Type RHSType) 
    throws VM_ResolutionException {
    if (LHSType == RHSType) return true;
    if (!LHSType.isResolved()) {
      LHSType.load();
      LHSType.resolve();
    }
    int LHSDimension = LHSType.getDimensionality();
    int RHSDimension = RHSType.getDimensionality();
    if (LHSDimension < 0 || RHSDimension < 0) return false;
    if (LHSDimension == 0) return instanceOfResolved(LHSType.asClass(), 
                                                     RHSType.getTypeInformationBlock());
    VM_Type LHSInnermostElementType = LHSType.asArray().getInnermostElementType();
    if (LHSInnermostElementType == VM_Type.JavaLangObjectType){
      return instanceOfObjectArray(LHSDimension, RHSType);
    } else if (!LHSInnermostElementType.isPrimitiveType()) {
      return instanceOfArray(LHSInnermostElementType.asClass(), 
                             LHSDimension, RHSType);
    } else {
      return false;
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.DataInputStream;
import java.io.IOException;

/** 
 * A java method's try/catch/finally information.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
class VM_ExceptionHandlerMap {
  //-----------//
  // Interface //
  //-----------//

  final int[] getStartPC()   { return startPCs;   }
  final int[] getEndPC()     { return endPCs;     }
  final int[] getHandlerPC() { return handlerPCs; }
  final VM_Type getExceptionType(int i) { return exceptionTypes[i]; }
   
  //----------------//
  // Implementation //
  //----------------//

  /**
   * bytecode offset at which i-th try block begins
   * 0-indexed from start of method's bytecodes[]
   */
  int[] startPCs;

  /**
   * bytecode offset at which i-th try block ends (exclusive)
   * 0-indexed from start of method's bytecodes[]
   */
  int[] endPCs;

  /**
   * bytecode offset at which exception handler for i-th try block begins
   * 0-indexed from start of method's bytecodes[]
   */
  int[] handlerPCs; 

  /**
   * exception type for which i-th handler is to be invoked
   * - something like "java/lang/IOException".
   * NOTE: When constructing the VM_ExceptionHandlerMap we replace
   * 'null' entries (means a finally block that catches everything)
   * with VM_Type.JavaLangThrowableType so we don't have to do anything
   * special anywhere else in the VM.
   */
  VM_Type[] exceptionTypes; 

  VM_ExceptionHandlerMap(DataInputStream input, 
			 VM_Class declaringClass, 
			 int n) throws IOException {
    startPCs       = new int[n];
    endPCs         = new int[n];
    handlerPCs     = new int[n];
    exceptionTypes = new VM_Type[n];
    for (int i = 0; i < n; ++i) {
      startPCs[i]       = input.readUnsignedShort();
      endPCs[i]         = input.readUnsignedShort();
      handlerPCs[i]     = input.readUnsignedShort();
      VM_Type et = declaringClass.getTypeRef(input.readUnsignedShort()); // possibly null
      if (et == null) {
	// A finally block...set to java.lang.Throwable to avoid
	// needing to think about this case anywhere else in the VM.
	exceptionTypes[i] = VM_Type.JavaLangThrowableType;
      } else {
	exceptionTypes[i] = et;
      }
    }
  }
}
/*
 * (C) Copyright IBM Corp. 2001
 */
//$Id$

import java.io.DataInputStream;
import java.io.IOException;
import com.ibm.JikesRVM.memoryManagers.VM_Collector;
import com.ibm.JikesRVM.memoryManagers.VM_WriteBarrier;

/**
 * A field of a java class.
 *
 * @author Bowen Alpern
 * @author Derek Lieber
 */
public final class VM_Field extends VM_Member implements VM_ClassLoaderConstants {
  //-----------//
  // Interface //
  //-----------//

  //------------------------------------------------------------------//
  //                       Section 0.                                 //
  //       The following are always available.                        //
  //------------------------------------------------------------------//

  /**
   * Get type of this field's value.
   */ 
  public final VM_Type getType() throws VM_PragmaUninterruptible {
    return type;
  }

  /**
   * Get size of this field's value, in bytes.
   */ 
  final int getSize() throws VM_PragmaUninterruptible {
    return type.getStackWords() << 2;
  }

  //------------------------------------------------------------------//
  //                       Section 1.                                 //
  //  The following are available after the declaring class has been  //
  //  "loaded".                                                       //
  //------------------------------------------------------------------//

  //
  // Attributes.
  //

  /**
   * Shared among all instances of this class?
   */ 
  public final boolean isStatic() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(declaringClass.isLoaded());
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_STATIC) != 0;
  }

  /**
   * May only be assigned once?
   */ 
  final boolean isFinal() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(declaringClass.isLoaded());
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_FINAL) != 0;
  }

  /**
   * Value not to be cached in a register?
   */ 
  final boolean isVolatile() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(declaringClass.isLoaded());
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_VOLATILE) != 0;
  }

  /**
   * Value not to be written/read by persistent object manager?
   */ 
  final boolean isTransient() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(declaringClass.isLoaded());
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return (modifiers & ACC_TRANSIENT) != 0;
  }

  /**
   * Get index of constant pool entry containing this 
   * "static final constant" field's value.
   * @return constant pool index (0 --> field is not a "static final constant")
   */ 
  final int getConstantValueIndex() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return constantValueIndex;
  }

  //--------------------------------------------------------------------//
  //                         Section 2.                                 //
  // The following are available after the declaring class has been     //
  // "resolved".                                                        //
  //----------------------------------------------- --------------------//

  /**
   * The actual field that this object represents
   */
  public final VM_Field resolve() {
    if (VM.VerifyAssertions) VM.assert(declaringClass.isResolved());
    if (!isLoaded()) return VM_ClassLoader.repairField(this);
    return this;
  }
 
  /**
   * Get offset of this field's value, in bytes.
   * 
   * <p> For static field, offset is with respect to
   * virtual machine's "table of contents" (jtoc).
   * 
   * <p> For non-static field, offset is with respect to
   * object pointer.
   */ 
  public final int getOffset() throws VM_PragmaUninterruptible {
    if (VM.VerifyAssertions) VM.assert(getDeclaringClass().isResolved());
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    return offset;
  }


  //-------------------------------------------------------------------//
  // The following are available for an object instance that owns      //
  // this field.                                                       //
  // Obtain the value of an object described by this VM_Field          //
  //-------------------------------------------------------------------//


  /**
    * If this field is an object, return that object
    * If this field is a primitive, get the value and wrap it in an object
    *
    */
  public final Object getObject(Object obj) {
    if (type.isReferenceType()) {
      return getObjectValue(obj);
    } else {
      // TODO: wrap the primitive value as object
      if (type.isCharType())     return new Character(getCharValue(obj));
      if (type.isDoubleType())   return new Double(getDoubleValue(obj));
      if (type.isFloatType())    return new Float(getFloatValue(obj));
      if (type.isLongType())     return new Long(getLongValue(obj));
      if (type.isIntType())      return new Integer(getIntValue(obj));
      if (type.isShortType())    return new Short(getShortValue(obj));
      if (type.isByteType())     return new Byte(getByteValue(obj));
      if (type.isBooleanType())  return new Boolean(getBooleanValue(obj));
      return null;
    }
  }

  /**
    * Read one object ref from heap using RVM object model, GC safe.
    * @param obj the object whose field is to be read, 
    * or null if the field is static.
    * @return the reference described by this VM_Field from the given object.
    */
  public final Object getObjectValue(Object obj) throws IllegalArgumentException {
    if (VM.VerifyAssertions) VM.assert(isLoaded());
    if (!type.isReferenceType()) throw new IllegalArgumentException("field type mismatch");
    if (isStatic()) {
      return VM_Statics.getSlotContentsAsObject(offset>>>2);
    } else {
      if (obj == null)
	throw new NullPointerException();
      if (!getDeclaringClass().getClassForType().isInstance(obj))
	throw new IllegalArgumentException();
      return VM_Magic.getObjectAtOffset(obj, offset);
    }
  }

  public final boolean getBooleanValue(Object obj) throws IllegalArgumentException {
    if (!type.isBooleanType()) throw new IllegalArgumentException("field type mismatch");
    int bits = get32Bits(obj);
    if (bits == 0)
      return false;
    else
      return true;
  }

  public final byte getByteValue(Object obj) throws IllegalArgumentException {
    if (!type.isByteType()) throw new IllegalArgumentException("field type mismatch");
    int bits = get32Bits(obj);
    return (byte)bits;
  }

  public final char getCharValue(Object obj) throws IllegalArgumentException {
    if (!type.isCharType()) throw new IllegalArgumentException("field type mismatch");
    return (char)get32Bits(obj);
  }

  public final short getShortValue(Object obj) throws IllegalArgumentException {
    if (!type.isShortType()) throw new IllegalArgumentException("field type mismatch");
    return (short)get32Bits(obj);
  }

  public final int getIntValue(Object obj) throws IllegalArgumentException {
    if (!type.isIntType()) throw new IllegalArgumentException("field type mismatch");
    return get32Bits(obj);
  }

  public final long getLongValue(Object obj) throws IllegalArgumentException {
    if (!type.isLongType()) throw new IllegalArgumentException("field type mismatch");
    return get64Bits(obj);
  }

  public final float getFloatValue(Object obj) throws IllegalArgumentException {
    if (!type.isFloatType()) throw new IllegalArgumentException("field type mismatch");
    return Float.intBitsToFloat(get32Bits(obj));
  }

  public final double getDoubleValue(Object obj) throws IllegalArgumentException {
    if (!type.isDoubleType()) throw new IllegalArgumentException("field type mismatch");
    return Double.longBitsToDouble(get64Bits(obj));
  }

  private int get32Bits(Object obj) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());

    // then a static object, get from jtoc
    if (isStatic()) {
      return VM_Statics.getSlotContentsAsInt(offset >>> 2);  // divide by 4 to get words from bytes
    } else {
      if (VM.VerifyAssertions)
	VM.assert(getSize()==4); // assume instance fields are 1 or 2 words
      if (obj==null)
	throw new NullPointerException();
      return VM_Magic.getIntAtOffset(obj, offset);
    }
  }

  private long get64Bits(Object obj) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());

    if (isStatic()) {
      long result = VM_Statics.getSlotContentsAsLong(offset >>> 2);
      return result;
    } else {
      if (VM.VerifyAssertions)
	VM.assert(getSize()==8); // assume instance fields are 1 or 2 words
      if (obj==null)
	throw new NullPointerException();
      long result = VM_Magic.getLongAtOffset(obj, offset);
      return result;
    }
  }


  //--------------------------------------------------------------------------------------------------//
  //               The assign a value to this field of an object.                                     //
  //--------------------------------------------------------------------------------------------------//

  public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException
  {
    VM.assert(false, "FINISH ME\n");
    // !!TODO: get and modify form Field
  }

  /**
   * assign one object ref from heap using RVM object model, GC safe.
   * @param obj the object whose field is to be modified, or null if the field is static.
   * @param ref the object reference to be assigned.
   * @return void
   */
  public final void setObjectValue(Object obj, Object ref) throws IllegalArgumentException {
    if (VM.VerifyAssertions) VM.assert(isLoaded());

    if ( ref != null ) {
      VM_Type actualType = VM_Magic.getObjectType(ref);
      boolean ok = false;
      try {
	ok = ((type == actualType) ||
	      (type == VM_Type.JavaLangObjectType) ||
	      VM_Runtime.isAssignableWith(type, actualType));
      } catch (VM_ResolutionException e) {}
      if (!ok) throw new IllegalArgumentException();
    }

    if (isStatic()) {
      if (VM_Collector.NEEDS_WRITE_BARRIER) {
	VM_WriteBarrier.resolvedPutStaticWriteBarrier(offset, ref);
      }
      VM_Statics.setSlotContents(offset>>>2, ref);
    } else {
      if (obj == null)
	throw new NullPointerException();
      if (!getDeclaringClass().getClassForType().isInstance(obj))
	throw new IllegalArgumentException();
      if (VM_Collector.NEEDS_WRITE_BARRIER) {
	VM_WriteBarrier.resolvedPutfieldWriteBarrier(obj, offset, ref);
      }
      VM_Magic.setObjectAtOffset(obj, offset, ref);
    }
  }


  public final void setBooleanValue(Object obj, boolean b) throws IllegalArgumentException {
    if (type.isBooleanType()) 
      if (b==true)
	put32(obj, 1 );
      else
	put32(obj, 0 );
    else
      throw new IllegalArgumentException("field type mismatch");
  }


  public final void setByteValue(Object obj, byte b) throws IllegalArgumentException {
    if (type.isLongType())
      setLongValue(obj, (long) b);
    else if (type.isIntType()) 
      setIntValue(obj, (int) b);
    else if (type.isShortType())
      setShortValue(obj, (short) b);
    else if (type.isCharType()) 
      setCharValue(obj, (char) b);
    else if (type.isByteType()) 
      put32(obj, (int)b);
    else if (type.isDoubleType())
      setDoubleValue(obj, (double)b);
    else if (type.isFloatType()) 
      setFloatValue(obj, (float)b);
    else
      throw new IllegalArgumentException("field type mismatch");
  }


  public final void setCharValue(Object obj, char c) throws IllegalArgumentException {
    if (type.isLongType())
      setLongValue(obj, (long) c);
    else if (type.isIntType()) 
      setIntValue(obj, (int) c);
    else if (type.isShortType())
      setShortValue(obj, (short) c);
    else if (type.isCharType()) 
      put32(obj, (int)c);
    else if (type.isDoubleType())
      setDoubleValue(obj, (double)c);
    else if (type.isFloatType()) 
      setFloatValue(obj, (float)c);
    else
      throw new IllegalArgumentException("field type mismatch");
  }

  public final void setShortValue(Object obj, short i) throws IllegalArgumentException {
    if (type.isLongType())
      setLongValue(obj, (long) i);
    else if (type.isIntType()) 
      setIntValue(obj, (int) i);
    else if (type.isShortType())
      put32(obj, (int)i);
    else if (type.isDoubleType())
      setDoubleValue(obj, (double)i);
    else if (type.isFloatType()) 
      setFloatValue(obj, (float)i);
    else
      throw new IllegalArgumentException("field type mismatch");
  }

  public final void setIntValue(Object obj, int i) throws IllegalArgumentException {
    if (type.isLongType())
      setLongValue(obj, (long) i);
    else if (type.isIntType()) 
      put32(obj, i);
    else if (type.isDoubleType())
      setDoubleValue(obj, (double)i);
    else if (type.isFloatType()) 
      setFloatValue(obj, (float)i);
    else
      throw new IllegalArgumentException("field type mismatch");
  }

  public final void setFloatValue(Object obj, float f) throws IllegalArgumentException {
    if (type.isDoubleType())
      setDoubleValue(obj, (double)f);
    else if (type.isFloatType()) 
      put32(obj, Float.floatToIntBits(f));
    else
      throw new IllegalArgumentException("field type mismatch");
  }

  public final void setLongValue(Object obj, long l) throws IllegalArgumentException {
    if (type.isLongType()) 
      put64(obj, l);
    else if (type.isDoubleType())
      setDoubleValue(obj, (double)l);
    else if (type.isFloatType()) 
      setFloatValue(obj, (float)l);
    else
      throw new IllegalArgumentException("field type mismatch");
  }

  public final void setDoubleValue(Object obj, double d) throws IllegalArgumentException {
    if (!type.isDoubleType()) throw new IllegalArgumentException("field type mismatch");
    put64(obj, Double.doubleToLongBits(d));
  }

  private void put32(Object obj, int value) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());

    if (isStatic()) {
      VM_Statics.setSlotContents( offset >>> 2, value );
    } else {
      if (obj == null) throw new NullPointerException();
      VM_Magic.setIntAtOffset(obj, offset, value);
    }
  }

  private void put64(Object obj, long value) {
    if (VM.VerifyAssertions) VM.assert(isLoaded());

    if (isStatic()) {
      VM_Statics.setSlotContents( offset >>> 2, value );
    } else {
      if (obj == null) throw new NullPointerException();
      VM_Magic.setLongAtOffset(obj, offset, value);
    }
  }


  //----------------//
  // Implementation //
  //----------------//

  private VM_Type type;               // field's type
  private int     constantValueIndex; // constant pool index of field's value (0 --> not a "static final constant")
  public  int     offset;             // field's jtoc/obj offset, in bytes

  // To guarantee uniqueness, only the VM_ClassLoader class may construct VM_Field instances.
  // All VM_Field creation should be performed by calling "VM_ClassLoader.findOrCreate" methods.
  //
  private VM_Field() {
    if (VM.VerifyAssertions) VM.assert(VM.NOT_REACHED);
  }

  VM_Field(VM_Class declaringClass, VM_Atom name, 
	   VM_Atom descriptor, int dictionaryId) {
    super(declaringClass, name, descriptor, dictionaryId);
    type = VM_ClassLoader.findOrCreateType(getDescriptor(), declaringClass.classloader);
    offset = VM_Member.UNINITIALIZED_OFFSET;
  }

  final void load(DataInputStream input, int modifiers) throws IOException {
    this.modifiers = modifiers;
    readAttributes(input);
    this.modifiers |= ACC_LOADED;
  }

  private void readAttributes(DataInputStream input) throws IOException {
    for (int i = 0, n = input.readUnsignedShort(); i < n; ++i) {
      VM_Atom attName   = declaringClass.getUtf(input.readUnsignedShort());
      int     attLength = input.readInt();

      // Field attributes
      if (attName == VM_ClassLoader.constantValueAttributeName) {
	constantValueIndex = input.readUnsignedShort();
	continue;
      }

      if (attName == VM_ClassLoader.deprecatedAttributeName) { // boring
	input.skipBytes(attLength);
	continue;
      }

      if (attName == VM_ClassLoader.syntheticAttributeName) { // boring
	input.skipBytes(attLength);
	continue;
      }
      
      input.skipBytes(attLength);
    }
  }
}

